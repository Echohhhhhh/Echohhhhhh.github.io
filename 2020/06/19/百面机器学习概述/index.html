<!DOCTYPE html>




<html class="theme-next pisces" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png?v=5.1.4">






  <meta name="keywords" content="machine learning,">










<meta name="description" content="总结《百面机器学习》有关知识点。">
<meta name="keywords" content="machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="百面机器学习概述一">
<meta property="og:url" content="http://yoursite.com/2020/06/19/百面机器学习概述/index.html">
<meta property="og:site_name" content="Echo&#39;s blog">
<meta property="og:description" content="总结《百面机器学习》有关知识点。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/归一化.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/PR曲线和ROC曲线.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/百面机器学习概述/linear.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/百面机器学习概述/linear1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/BP1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/BP2.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/BP3.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L1_1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L1_2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L1_3.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/logistic.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid2.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/逻辑回归.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/softmax.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/softmax1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/softmax.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/多分类1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/多分类2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/多分类.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/前向1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/前向2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/smote.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM3.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM4.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM5.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM4.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM6.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM7.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM8.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/决策树.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/knn1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/knn2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/kmeans1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/kmeans2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/kmeans3.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/贝叶斯1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/贝叶斯2.png">
<meta property="og:updated_time" content="2020-07-16T16:24:06.806Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="百面机器学习概述一">
<meta name="twitter:description" content="总结《百面机器学习》有关知识点。">
<meta name="twitter:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/归一化.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/19/百面机器学习概述/">





  <title>百面机器学习概述一 | Echo's blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b759ac2a7fa45129e3ef060bf68259f0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Echo's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">远方到底有多远</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/19/百面机器学习概述/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Echo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Echo's blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">百面机器学习概述一</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-19T23:12:34+08:00">
                2020-06-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  24.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  93
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>总结《百面机器学习》有关知识点。</p>
<a id="more"></a>
<!-- TOC -->
<ul>
<li><a href="#1-特征工程">1. 特征工程</a><ul>
<li><a href="#11-特征归一化">1.1. 特征归一化</a></li>
<li><a href="#12-类别型特征">1.2. 类别型特征</a></li>
<li><a href="#13-文本表示模型">1.3. 文本表示模型</a></li>
<li><a href="#14-word2vec">1.4. Word2Vec</a></li>
<li><a href="#15-图像不足时处理方法">1.5. 图像不足时处理方法</a></li>
</ul>
</li>
<li><a href="#2-模型评估">2. 模型评估</a><ul>
<li><a href="#21-常见评估指标">2.1. 常见评估指标</a></li>
<li><a href="#22-p-r曲线和roc曲线">2.2. P-R曲线和ROC曲线</a></li>
<li><a href="#23-余弦距离">2.3. 余弦距离</a></li>
<li><a href="#24-ab测试">2.4. A/B测试</a></li>
<li><a href="#25-模型评估方法">2.5. 模型评估方法</a></li>
<li><a href="#26-过拟合和欠拟合">2.6. 过拟合和欠拟合</a></li>
<li><a href="#27-正则化">2.7. 正则化</a></li>
<li><a href="#28-梯度消失和梯度爆炸">2.8. 梯度消失和梯度爆炸</a></li>
</ul>
</li>
<li><a href="#3-经典算法">3. 经典算法</a><ul>
<li><a href="#31-回归模型">3.1. 回归模型</a><ul>
<li><a href="#311-单变量线性回归">3.1.1. 单变量线性回归</a></li>
<li><a href="#312-多变量线性回归">3.1.2. 多变量线性回归</a></li>
<li><a href="#313-带有激活函数的反向传播">3.1.3. 带有激活函数的反向传播</a></li>
<li><a href="#314-l1正则和l2正则">3.1.4. L1正则和L2正则</a></li>
<li><a href="#315-面试问题">3.1.5. 面试问题</a></li>
</ul>
</li>
<li><a href="#32-逻辑回归">3.2. 逻辑回归</a><ul>
<li><a href="#321-逻辑回归推导">3.2.1. 逻辑回归推导</a></li>
<li><a href="#322-逻辑回归常见问题">3.2.2. 逻辑回归常见问题</a></li>
</ul>
</li>
<li><a href="#33-softmax回归">3.3. Softmax回归</a></li>
<li><a href="#34-回归和分类总结">3.4. 回归和分类总结</a></li>
<li><a href="#35-数据不平衡问题">3.5. 数据不平衡问题</a><ul>
<li><a href="#351-smote算法">3.5.1. SMOTE算法</a></li>
</ul>
</li>
<li><a href="#36-svm">3.6. SVM</a><ul>
<li><a href="#361-线性可分svm">3.6.1. 线性可分SVM</a></li>
<li><a href="#362-近似线性svm">3.6.2. 近似线性SVM</a></li>
<li><a href="#363-核函数">3.6.3. 核函数</a></li>
<li><a href="#364-面试问题">3.6.4. 面试问题</a></li>
</ul>
</li>
<li><a href="#37-决策树">3.7. 决策树</a><ul>
<li><a href="#371-基础树id3c45cart">3.7.1. 基础树(ID3/C4.5/CART)</a><ul>
<li><a href="#3711-id3">3.7.1.1. ID3</a></li>
<li><a href="#3712-c45">3.7.1.2. C4.5</a></li>
<li><a href="#3713-cart分类回归树">3.7.1.3. CART(分类回归树)</a></li>
<li><a href="#3714-总结">3.7.1.4. 总结</a></li>
</ul>
</li>
<li><a href="#372-树的剪枝">3.7.2. 树的剪枝</a></li>
</ul>
</li>
<li><a href="#38-k近邻knn">3.8. K近邻(KNN)</a><ul>
<li><a href="#381-knn原理">3.8.1. KNN原理</a></li>
<li><a href="#382-kd树">3.8.2. KD树</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-聚类">4. 聚类</a><ul>
<li><a href="#41-k-means聚类">4.1. K-Means聚类</a></li>
<li><a href="#42-k-means的优化">4.2. K-Means的优化</a><ul>
<li><a href="#421-k-means">4.2.1. K-Means++</a></li>
<li><a href="#422-isodata">4.2.2. ISODATA</a></li>
</ul>
</li>
<li><a href="#43-聚类评价指标">4.3. 聚类评价指标</a></li>
</ul>
</li>
<li><a href="#5-贝叶斯分类">5. 贝叶斯分类</a><ul>
<li><a href="#51-朴素贝叶斯分类器">5.1. 朴素贝叶斯分类器</a></li>
<li><a href="#52-半朴素贝叶斯分类器">5.2. 半朴素贝叶斯分类器</a></li>
</ul>
</li>
<li><a href="#6-生成模型和判别模型">6. 生成模型和判别模型</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-特征工程">1. 特征工程</span></h1><h2><span id="11-特征归一化">1.1. 特征归一化</span></h2><ul>
<li>目的：消除特征之间量纲的影响，防止学习到的结果向数值大的特征倾斜。</li>
<li><p>Max-Min归一化：将数据归一化到[0,1]之间，等比缩放。</p>
<script type="math/tex; mode=display">\frac{X_{i}-X_{\min }}{X_{\max }-X_{\min }}</script></li>
<li><p>Z-Score归一化（零均值归一化）：将数据归一化到均值为0，标准差为1的分布上。</p>
<script type="math/tex; mode=display">x^{\prime}=\frac{x-\bar{x}}{\sigma}</script></li>
<li><p><strong>数据标准化的目的</strong></p>
<ol>
<li>避免量纲大的特征对模型起决定性作用，归一化有可能提高精度<br>比如房子的面积和房子个数，一个范围可能是[100,10000]，另一个可能是[1,5]，在进行距离有关的计算时，导致计算得到的距离依赖于房子的面积，房子面积越相近，计算得到的距离越小，但这并不代表房间个数就不重要。为了消除特征间数据量的差异，对每一维进行归一化，对每个特征同等看待。</li>
<li>在梯度下降时，不同特征的参数下降变得更稳定，加速模型参数找到最优解<br>因为在梯度下降中需要损失函数对w求偏导，这个偏导中肯定有x，因此w的梯度下降速度就会受到x的影响。因为在更新的时候不同的w使用的是一个学习率，所以x可能会导致不同的w的下降速度相差很大，下降过程不稳定，通过归一化可以对不同特征的参数下降速度有所控制，使得下降过程相对稳定。体现的图像上就是<br><img src="/2020/06/19/百面机器学习概述/归一化.jpg" alt=""></li>
</ol>
</li>
</ul>
<ul>
<li>什么时候需要归一化<br>需要使用梯度下降和计算距离的模型需要归一化。因为例如：线性回归、逻辑回归、SVM、深度神经网络，KNN,K-Means，<br>概率模型和树形模型不需要归一化，因为它们不关心变量的值，只关心变量的分布和变量之间的条件概率。例如决策树，随机森林。</li>
</ul>
<h2><span id="12-类别型特征">1.2. 类别型特征</span></h2><p>将类别特征—&gt;数值特征，有以下方法</p>
<ul>
<li>序号编码：有大小关系的，比如低中高，可以编码成1,2,3</li>
<li>one-hot编码：没有大小关系，问题：高维稀疏，需要将其嵌入成低维稠密向量</li>
<li>二进制编码：先编码再转换成二进制</li>
</ul>
<h2><span id="13-文本表示模型">1.3. 文本表示模型</span></h2><ul>
<li>词袋模型：将文章按照词分割，使用每个词的权重来表示文章</li>
<li>TF-IDF：用来计算一个词的权重，TF=一个词w在文章d中出现的频率，IDF：特属词的特征，逆文档频率</li>
<li>N-gram模型：将连续的n个词作为一个特征放在向量中。</li>
<li>词嵌入模型：将词映射成低维稠密向量(50~300维度)，Word2Vec是最常用的词嵌入模型。</li>
</ul>
<h2><span id="14-word2vec">1.4. Word2Vec</span></h2><p>Word2Vec有2种网络结构：CBOW，skip-gram</p>
<ul>
<li>CBOW：多预测一</li>
<li>skip-gram：一预测多</li>
</ul>
<h2><span id="15-图像不足时处理方法">1.5. 图像不足时处理方法</span></h2><p>图像不足易出现过拟合问题。解决图像不足有2种方法：</p>
<ul>
<li>基于模型：采用一些措施降低过拟合。例如简化模型，L1L2正则化、Dropout，集成学习。</li>
<li>基于数据：通过一些方法增加数据。平移、旋转、裁剪、修改图片亮度，锐度、添加噪声等方法。</li>
</ul>
<h1><span id="2-模型评估">2. 模型评估</span></h1><h2><span id="21-常见评估指标">2.1. 常见评估指标</span></h2><ul>
<li>准确率(Accuracy)</li>
<li>精确率(Precision)</li>
<li>召回率(Recall)</li>
</ul>
<p>其中这些指标都有一定的局限性。对于一个排序模型，怎么评估排序模型的好坏：</p>
<ul>
<li>设置不同的N，计算P@N,R@N</li>
<li>设置更综合的评价指标，PR曲线，ROC曲线，F1值</li>
</ul>
<p>RMSE：回归问题的评价指标。如果在一个问题上，RMSE非常高，但是观察预测值和真实值，发现90%的预测值都很接近真实值，为什么RMSE还是这么高。分析由于剩下的10%存在非常大的异常值，即使90%预测很准，但是这10%导致最终的RMSE差别很大。解决方案：</p>
<ul>
<li>如果10%是噪声数据，提前去除噪声</li>
<li>如果10%不是噪声数据，需要对异常数据进行建模</li>
<li>换一个对异常值不敏感的指标，例如MAPE</li>
</ul>
<h2><span id="22-p-r曲线和roc曲线">2.2. P-R曲线和ROC曲线</span></h2><ul>
<li><p>P-R曲线<br>P-R曲线横坐标是召回率，纵坐标是准确率。<br>为什么要有P-R曲线？因为精确率precision和召回率recall指标都有一定的局限性，所以使用P-R曲线可以综合地评估一个模型的效果。<br>在P-R曲线中，通过改变正负样本间的阈值来改变precision和recall。当判定为正样本的阈值很大时，说明选出的正样本都是很有把握的，precision较大，recall较小。当判定为正样本的阈值很小时，即尽可能不漏掉正样本，导致precision降低，recall变大。</p>
</li>
<li><p>ROC曲线<br>横坐标是假阳率FPR，纵坐标是真阳率TPR。</p>
</li>
</ul>
<script type="math/tex; mode=display">假阳率=\frac{负例被判为正例}{真正的负例}\quad
真阳率=\frac{正例被判为正例}{真正的正例}</script><p>可以看出真阳率也就是召回率。<br>同P-R曲线类似，ROC曲线也是通过不断改变正负样本的阈值生成的。</p>
<ul>
<li>P-R曲线和ROC曲线有什么不同</li>
</ul>
<p>当测试集中的负样本数量增加10倍时，P-R曲线发生了明显的变化，ROC曲线几乎不变。ROC曲线能够尽量降低测试集带来的干扰，适用于正负样本不均衡的数据集中。ROC适用的场景更多，被广泛应用在排序，推荐，广告等领域。</p>
<p><a href="https://blog.csdn.net/songyunli1111/article/details/82285266" target="_blank" rel="noopener">为什么ROC曲线不受样本不均衡问题的影响</a></p>
<p>AUC是概率值，表示正样本排在负样本前面的概率。AUC越大，说明中正样越有可能排在负样本前面，常用在推荐，排序领域。</p>
<p>ROC有真阳率和假阳率，所以ROC同时关注正负样本。而PR曲线更关注正样本。</p>
<ul>
<li>如果你更关注正例，使用P-R曲线，例如检测癌症，电信诈骗等。如果你同时关注正例和负例，则使用ROC曲线。</li>
<li>如果你不想让训练集的正负比例过度的影响模型效果，用ROC</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/PR曲线和ROC曲线.png" alt=""></p>
<h2><span id="23-余弦距离">2.3. 余弦距离</span></h2><p>余弦相似度：</p>
<script type="math/tex; mode=display">\cos (A, B)=\frac{A \cdot B}{\|A\|_{2}\|B\|_{2}}</script><p>余弦距离体现方向上的相对差异，欧式距离体现数值上的绝对差异。</p>
<h2><span id="24-ab测试">2.4. A/B测试</span></h2><p>划分实验组和对照组<br>实验组和对照组：选取样本时要求独立性和无偏性。</p>
<h2><span id="25-模型评估方法">2.5. 模型评估方法</span></h2><p>通常将数据划分为训练集和测试集，但在<strong>样本划分</strong>和<strong>模型验证</strong>的过程中，存在着不同的方法。</p>
<ol>
<li>Holdout验证<br>随机将数据划分为训练集和测试集，在测试集上进行模型验证。<br>缺点：模型的效果取决于样本划分，具有随机性</li>
<li>k折交叉验证<br>将样本划分为k个大小相等的子集，一个子集作为测试集，k-1个子集作为训练集。将k次评估结果平均作为该模型最终的结果。<br>优点：（1）解决当数据量较小时，模型评估不准确的问题（2）消除数据对评估结果的随机性影响（3）可以用来选择超参数<br>缺点：耗时，需要训练多个模型求评估结果的平均值</li>
<li>自助法<br>如果数据比较小，holdout和k折交叉验证都需要划分数据集，导致训练集变小。自助发采用随机抽样，在全部样本中进行n次有放回的抽样，得到大小为n的训练集。这n个样本中有的样本是重复的，有的样本没有被抽到，这些没有抽到的样本作为测试集。</li>
</ol>
<h2><span id="26-过拟合和欠拟合">2.6. 过拟合和欠拟合</span></h2><ul>
<li>过拟合<br>模型在训练数据上loss很小，在测试集上loss大</li>
<li>欠拟合<br>模型在训练集和测试集上loss都很大，效果都不好</li>
<li><p>降低过拟合的方法</p>
<ol>
<li>增加数据集，减少噪声的影响。如果没有这么多数据，可以生成一些数据。例如图像可以通过平移，旋转等生成数据</li>
<li>正则化，在loss中添加L1或L2正则，防止参数过大</li>
<li>集成学习，将多个模型集成在一起，降低单一模型的过拟合风险</li>
<li>降低模型复杂度，例如减少网络参数，神经元个数等。</li>
<li>早停，当模型在验证机上的loss连续N次没有提升时，停止训练</li>
<li>Dropout，在训练过程中按照给定的概率随机删除隐藏层的一些神经单元。由于模型训练的随机性，减轻了不同特征之间的协同效应。<strong>Dropout只能在训练时使用，在测试集上不能使用</strong></li>
</ol>
</li>
<li><p>降低欠拟合的方法</p>
<ol>
<li>添加新的特征，当特征少时容易出现欠拟合，模型对训练数据的拟合程度不好。</li>
<li>增加模型复杂度</li>
<li>减少正则化系数</li>
</ol>
</li>
</ul>
<h2><span id="27-正则化">2.7. 正则化</span></h2><p>正则化为了避免过拟合，即在损失函数后加上一个正则项（惩罚项），模型越复杂，正则化值就越大。</p>
<script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><p>第一项是损失值，第二项是正则化项，$\lambda$调整两者的权重。下面拿回归问题举例，损失值为平方损失。</p>
<ul>
<li>L1正则<br>惩罚项为权重绝对值的和</li>
</ul>
<script type="math/tex; mode=display">L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\lambda\|w\|_{1}</script><ul>
<li>L2正则</li>
</ul>
<p>惩罚项为权重的平方和</p>
<script type="math/tex; mode=display">L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\frac{\lambda}{2}\|w\|^{2}</script><h2><span id="28-梯度消失和梯度爆炸">2.8. 梯度消失和梯度爆炸</span></h2><p>梯度消失和梯度爆炸经常出现在较深的网络中。反向传播用到链式法则，导数连乘。如果激活函数求导后与权重相乘的积大于1，层数增多时，求出的梯度将以指数增加，发生梯度爆炸。如果小于1，梯度将以指数衰减，发生梯度消失。<br>sigmoid的导数最大为0.25，易发生梯度消失问题</p>
<p>如何解决梯度消失和梯度爆炸：</p>
<ol>
<li>使用ReLU代替sigmoid解决梯度消失问题。Relu的正数部分导数为1，不存在梯度消失和梯度爆炸的问题</li>
<li>梯度裁剪解决梯度爆炸，设置一个梯度裁剪阈值，如果梯度大于这个阈值使，将其强制限制在这个范围内。</li>
<li>残差网络，解决梯度消失的问题</li>
<li>BN批归一化层，BN主要是对x进行归一化到均值为0，方差为1。在L对w反向求到时，结果中肯定有x的存在，所以x的大小也会影响梯度消失和梯度爆炸，BN通过将输出进行归一化来消除x带的放大缩小的影响。</li>
</ol>
<h1><span id="3-经典算法">3. 经典算法</span></h1><h2><span id="31-回归模型">3.1. 回归模型</span></h2><p>回归模型分为线性回归和非线性回归。</p>
<p>线性回归又分为单变量线性回归和多变量线性回归。</p>
<h3><span id="311-单变量线性回归">3.1.1. 单变量线性回归</span></h3><p>使用房子面积和房价举例。训练集中x表示房子面积，y表示房价。使用$(x,y)$表示一个训练样，其中$(x^{(i)},y^{(i)})$表示第$i$个训练样本。。</p>
<ul>
<li>$h(x)=w_0+w_1x$，通过训练集学习$w_0,w_1$，使得$x$可以得到对应的$y$。因为这里没有激活函数，$x和y$是线性关系，所以成为单变量线性回归模型，因为输入的特征$x$只有1个特征（房子面积）</li>
<li>损失函数：$Loss = \frac{1}{2m}\sum_{i=1}^{i=m}(h(x^{i})-y^{i})^2$<br>求得是平均损失，有m个样本，所以前面损失除以m，这里的1/2是为了求导简化添加的。</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/百面机器学习/../百面机器学习概述/linear.jpg" alt=""></p>
<h3><span id="312-多变量线性回归">3.1.2. 多变量线性回归</span></h3><p>输入的x有多个特征，上面单变量回归问题中，x只表示房子面积，在多变量线性回归中，x1=房子面积，x2=卧室个数，x3=房子楼层，x4=房子年龄</p>
<ul>
<li>$h(x)=w_0+w_1x_1+w_2x_2+w_3x_3+w_4x_4$<br>令$x=[x_1,x_2,x_3,x_4]$为列向量，$w=[w_1,w_2,w_3,w_4]$为列向量，则$h(x)=w^Tx$</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/百面机器学习/../百面机器学习概述/linear1.jpg" alt=""></p>
<h3><span id="313-带有激活函数的反向传播">3.1.3. 带有激活函数的反向传播</span></h3><p>上面介绍的单变量和多变量回归都是线性的，没有激活函数，这里介绍带有激活函数的回归问题。</p>
<p>首先定义损失函数</p>
<script type="math/tex; mode=display">L=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i),y^i)^2</script><script type="math/tex; mode=display">h(x)=f(xW+b)</script><p>这里的$f$表示激活函数，可以是Sigmoid，Tanh，ReLU</p>
<p><img src="/2020/06/19/百面机器学习概述/BP1.jpg" alt=""></p>
<p><img src="/2020/06/19/百面机器学习概述/BP2.jpg" alt=""></p>
<p><strong>不同激活函数的导数</strong></p>
<p><img src="/2020/06/19/百面机器学习概述/BP3.jpg" alt=""></p>
<h3><span id="314-l1正则和l2正则">3.1.4. L1正则和L2正则</span></h3><p>正则就是给参数添加限制，缩小参数的解空间，降低结构风险。<br>L1正则和L2正则都是为了避免过拟合设计的。使用L1正则化的叫做Lasso回归，使用L2正则化的叫做岭回归。<br>这两种回归为了<strong>解决线性回归出现过拟合</strong>的问题。通过<strong>在损失函数中引入正则化项</strong>来解决</p>
<ul>
<li><p>Lasso回归损失函数<br> 添加L1正则项</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}|w_j|</script></li>
<li><p>岭回归损失函数<br> 添加L2正则项</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}w_j^2</script></li>
</ul>
<p>我们从梯度下降的角度分析下为什么加入正则化可以避免过拟合。下面以L2正则化为例。L2正则化的梯度下降公式为</p>
<script type="math/tex; mode=display">w_j = w_j - \alpha * \frac{1}{m}\sum_{i=1}^{i=m}(h(x^i)-y^i)x^i_j-2\lambda w_j</script><p>当惩罚项越大，即$\lambda$越大时，更新后得到的$w_j$也就越小，模型复杂度变小，模型更简单。</p>
<p>下面以线性回归为例，模型只包含2个参数。下面等高线图表示$w_1,w_2$和损失的关系。当$w_1,w_2$取值为图像中最里面紫色圆圈上的点时，损失最小。</p>
<p><img src="/2020/06/19/百面机器学习概述/L1_1.png" alt=""></p>
<p>当加上L1正则化后，目标函数图像为</p>
<p><img src="/2020/06/19/百面机器学习概述/L1_2.png" alt=""></p>
<p>原先损失函数的第一项就是等高线，L1正则化损失函数的第二项就是菱形上的点。图中的菱形函数为$\sum_{j=1}^{j=2}w_j=F$。我们现在求解的目标是不仅要让第一项小，即$w_1,w_2$的值靠近紫色的圆圈，还要让第二项小，即$|w_1|+|w_2|=F$小，即$F$比较小。如下图所示，如果让菱形靠近紫色的圆圈，虽然损失函数第一项小了，但是第二项会变大。因此我们要取到一个恰好的值，让第一项的值+第二项的值最小。<br>我们发现对于同一个等高线来说，即损失函数的第一项相同，当菱形和等高线相切（只有一个交点）时，菱形的边长最小，即$w_1+w_2$最小，也就是相加得到的损失最小。</p>
<p>由上面的说明可以看出，L1正则化后的解一定是某个菱形和某个等高线的切点。经过观察发现，对于图中的每条等高线，与这个等高线相切的菱形的切点一般出现在坐标轴上(x轴或y轴)，例如上面的解为$(0,y)$，这也是说L1正则化更容易得到稀疏解(解向量中0比较多)的原因。</p>
<p><img src="/2020/06/19/百面机器学习概述/L1_3.png" alt=""></p>
<p>当加入L2正则化后，目标函数图像为</p>
<p><img src="/2020/06/19/百面机器学习概述/L2.png" alt=""></p>
<p>菱形变成了圆，同样求等高线和圆的切点作为最终的解。与L1正则化相比，切点不容易出现在坐标轴上，然后仍然比较靠近坐标轴。因此L2正则化得到的解比较小（靠近0），但是比较平滑（不等于0）。所以通过添加L2正则化向，模型不会得到特别大的权重，偏向于学习比较小的权重。因此L2正则化又叫做权重衰减。</p>
<p><strong>总结</strong></p>
<p>L1正则化使权重稀疏，L2正则化使权重平滑。</p>
<ul>
<li>L1正则化趋向于使用更少的特征，没用的特征权重为0，</li>
<li>L2正则化趋向于使用更多的特征，但这些特征的权重都接近0。这样可以避免模型严重依赖与其中的少数特征，而是倾向于使用所有的特征。</li>
<li>如果不是进行特征选择，最常用L2正则化。</li>
</ul>
<p>问题：</p>
<ol>
<li><strong>什么时候用L1正则化，什么时候用L2正则化？</strong><br>如果想让参数变得稀疏，即很多参数都为0，意味着模型使用的特征没有那么多，就用L1正则。<br>如果想用所有的特征，同时特征的参数又不这么大，用L2正则。<br><strong>2. L1正则中实现参数稀疏有什么好处？</strong><br>参数稀疏，说明有些参数为0，这样就可以实现特征的选择，参数为0的那些特征不会参与到模型计算中。一般而言，大部分特征对模型是没有贡献的，有些无用的特征虽然可以减少训练集的误差，但是在测试集上，反而会产生干扰。通过引入稀疏参数，可以将无用特征的权重设置为0.<br><strong>3. L2正则中为什么参数越小表示模型越简单</strong><br>越是复杂的模型，越想要对训练集中所有的点都要拟合，包括异常点，这就会造成在较小的区间中产生较大的波动，这个较大的波动也会反映在这个区间的导数比较大。而只有较大的参数才会有较大的导数，因此参数越小，模型越简单。</li>
</ol>
<h3><span id="315-面试问题">3.1.5. 面试问题</span></h3><p><strong>1. 简单介绍以下线性回归</strong></p>
<ul>
<li>线性：输入x和输出y的关系是线性的，即图像是直线</li>
<li>非线性：输入x和输出y的关系不是一次函数，图像不是直线</li>
<li>线性回归就是利用已有的样本，通过监督学习，学习由x到y的映射，然后利用学到的映射函数对未知的x进行预测。由于预测的值是连续值，所以是回归问题。</li>
</ul>
<p><strong>2. 线性回归的损失函数</strong></p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2</script><ol>
<li><p>简述岭回归和Lasso回归<br>这两种回归为了<strong>解决线性回归出现过拟合</strong>的问题。通过<strong>在损失函数中引入正则化项</strong>来解决</p>
<ul>
<li>Lasso回归损失函数<br>添加L1正则项<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}|w_j|</script></li>
<li>岭回归损失函数<br>添加L2正则项<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}w_j^2</script></li>
</ul>
</li>
</ol>
<p><strong>4. 线性回归的假设</strong><br>   线性回归假设因变量y符合正态分布   </p>
<h2><span id="32-逻辑回归">3.2. 逻辑回归</span></h2><p>逻辑回归是分类模型，虽然名字中有回归二字，但却是分类模型，用来二分类任务。<br>二分类的y有正负样本，即$y\in{(0,1)}$，$y$只有2个取值，一般将我们想要找的样本作为正样本。例如垃圾邮件分类中，想要找垃圾邮件，所以将垃圾邮件划分为正样本，非垃圾邮件为负样本。肿瘤良性判断中将恶性肿瘤设置为正样本，良性肿瘤设置为负样本。</p>
<ul>
<li><p>如何用连续的数组预测离散的y</p>
<p>线性回归输出的是连续值，而分类问题的标签y是离散值，属于{0,1}。怎么用回归模型预测离散的标签呢？<br>一个直观的办法是设定一个阈值，比如0，如果预测的值&gt;0，则属于正样本，否则属于负样本。<br>另一种方法是不去直接预测标签，而是预测样本属于正样本的概率。概率是连续值，并且在[0,1]之间。但是回归问题的输出值并不是在[0,1]之间，为了限制回归问题的值域，使用sigmoid函数，又成为logistic函数。因为sigmoid函数可以将输入$x\in[-\infty,\infty]$映射到$[0,1]$之间，输出的$h(x)$正好可以作为样本属于正例的概率。这种方法成为<strong>逻辑回归模型=logistic函数+回归模型</strong></p>
</li>
</ul>
<h3><span id="321-逻辑回归推导">3.2.1. 逻辑回归推导</span></h3><p>sigmoid函数：</p>
<script type="math/tex; mode=display">g(x)=\frac{1}{1+e^{-x}}</script><p><img src="/2020/06/19/百面机器学习概述/sigmoid.png" alt=""></p>
<p>输出值在<code>0~1</code>之间。原先的回归问题的假设函数简化为$h(x)=w^Tx$，为了让$h(x)$的输出在0~1之间，在外面套上sigmoid函数。</p>
<script type="math/tex; mode=display">h(x)=\frac{1}{1+e^{-w^{T}x}}</script><p>求得的$h(x)$表示样本$x$被判为正样本的概率，例如$h(x)=0.7$表示邮件是垃圾邮件的概率是0.7。因为真实标签值$y$只能为0和1，所以需要将$h(x)$和$y$对应起来。<br>如果$h(x)&gt;=0.5$，也就是$w^Tx&gt;=0$，则预测$y=1$<br>如果$h(x)&lt;0.5$，也就是$w^Tx&lt;0$，则预测$y=0$</p>
<ul>
<li>逻辑回归损失函数</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/logistic.png" alt=""></p>
<ol>
<li><p>求单个样本预测正确的概率<br>$h(x)$表示样本为正例$(y=1)$的概率$P(y=1|x,w)=h(x)$<br>则样本为负例的概率为$P(y=0|x,w)=1-h(x)$</p>
</li>
<li><p>将上述公式整合</p>
<script type="math/tex; mode=display">P(y \mid {x})=\left\{\begin{array}{r}
h(x), y=1 \\
1-h(x), y=0
\end{array}\right.</script><p>将上述2种情况整合成一个公式为</p>
<script type="math/tex; mode=display">P(y^i|x^i)=h(x^i)^{y_i}*(1-h(x^i))^{1-y^i}</script><p>对于样本$(x^i,y^i)$，如果$y^i=1$，则概率为$h(x^i)$，如果$y^i=0$，则概率为$1-h(x^i)$</p>
</li>
<li><p>求$m$个样本的似然函数</p>
<blockquote>
<p>极大似然估计：利用已知的样本结果，反推最有可能（最大概率）导致这些样本结果出现的模型参数，即在模型已知的情况下，求参数。</p>
</blockquote>
<p>如果有$m$个样本，分别为$(x^1,y^1),(x^2,y^2),…,(x^m,y^m)$，这m个样本假设相互独立，组合概率为每个样本概率的乘积，即最大似然估计为：</p>
<script type="math/tex; mode=display">P_总=P(y^1|x^1)P(y^2|x^2)...P(y^m|x^m)
=\prod_{i=1}^{i=m}h(x^i)^{y_i}*(1-h(x^i))^{1-y^i}</script><p>其中<script type="math/tex">h(x)=\frac{1}{1+e^{-w^Tx}}</script></p>
</li>
<li><p>求对数似然<br>模型需要做的就是该概率最大，即连乘的乘积最大，但是连乘很复杂，通过<strong>对两边取对数将连乘变成累加的形式</strong></p>
</li>
</ol>
<script type="math/tex; mode=display">\begin{array}{l}
log(P_总)=log(\prod_{i=1}^{i=m}h(x^i)^{y_i}*(1-h(x^i))^{1-y^i})\\
   =\sum_{i=1}^{i=m}log(h(x^i)^{y_i}*(1-h(x^i))^{1-y^i})\\=\sum_{i=1}^{i=m}(y^ilog(h(x^i))+(1-y^i)log(1-h(x^i)))
\end{array}</script><ol>
<li><p>求逻辑回归损失函数<br>上面的最大化$P_总$其实是我们的目标函数，但是如果在最大化目标函数时，对参数$w$进行求导时，非常复杂，所以就先取对数，将连乘换成累加。然后为了迎合一般都是最小化损失函数，所以加上一个符号。<br>模型最好的效果是让$log(P_总)$越大越好，但是损失函数却是越小越好，所以我们将其取负数作为损失函数，应为损失函数求的是平均误差，所以需要除以样本个数$m$,即逻辑回归的损失函数为交叉熵损失函数</p>
<script type="math/tex; mode=display">Loss(w)=-\frac{1}{m}\sum_{i=1}^{i=m}(y^ilog(h(x^i))+(1-y^i)log(1-h(x^i)))</script><ul>
<li><p>为什么可以用似然函数<br>因为逻辑回归的目标是让预测为正的概率最大，且预测为负的概率最大，即每个样本都要保证得到最大的概率，将所有样本预测后的概率相乘就最大，即得到似然函数。</p>
</li>
<li><p>为什么损失函数要取对数</p>
<ul>
<li>线性回归模型的平方损失函数对sigmoid函数求导无法保证是凸函数，在优化求$w$的过程中，求得的解有可能是局部最优，而不是全局最优</li>
<li>取对数后，方便后续的求导</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>梯度下降</p>
<p>首先先看sigmoid的导数</p>
<script type="math/tex; mode=display">g(x)=\frac{1}{1+e^{-x}}</script><p><img src="/2020/06/19/百面机器学习概述/sigmoid.jpg" alt=""></p>
<p><img src="/2020/06/19/百面机器学习概述/sigmoid1.jpg" alt=""></p>
<p>得到导数之后，更新参数$w_j$<br><img src="/2020/06/19/百面机器学习概述/sigmoid2.jpg" alt=""></p>
<ol>
<li>梯度下降公式中的$m$如果是样本总数，则每次更新参数时需要考虑所有的样本，称为批量梯度下降(BGD)。这种方法容易求得全局最优解，但是由于样本个数太多，训练过程非常慢。</li>
<li>如果$m=1$，即每次更新参数时只考虑一个样本，称为随机梯度下降(SGD)。这种方法训练速度快，但是准确率下降，并不是全局最优。</li>
<li>综上所述，当m为所有样本的一小部分时，比如m=32，即每次更新参数时只考虑一小部分样本，称为小批量梯度下降(MBGD)。它克服了上述两种方法的缺点又兼顾它们的优点，在实际中最常使用。</li>
</ol>
</li>
</ul>
<h3><span id="322-逻辑回归常见问题">3.2.2. 逻辑回归常见问题</span></h3><p><strong>1. 用一句话概括逻辑回归</strong><br>   逻辑回归假设数据服从伯努利分布，通过极大化似然函数，使用梯度下降求解参数，达到二分类的目的。</p>
<p><strong>2. 逻辑回归的目的</strong><br>   进行二分类<br>   逻辑回归作为回归，输出值是连续的，怎么应用在分类上呢？这里的y确实是一个连续的值，但是它的输出值在[0,1]之间，可以选定一个阈值来进行划分，如果输出值大于0.5，则判定为正样本，否则判定为负样本。</p>
<p><strong>3. 逻辑回归的基本假设</strong></p>
<ul>
<li><p>逻辑回归假设数据服从伯努利分布。伯努利分布就是抛硬币，正面的概率为$p$，反面的概率为$1-p$<br>在逻辑回归中，样本被判定为正例的概率为$h(x)$，则被判为负例的概率为$1-h(x)$</p>
</li>
<li><p>第二个假设是样本为正的概率是</p>
<script type="math/tex; mode=display">p=\frac{1}{1+e^{-w^Tx}}</script></li>
</ul>
<p><strong>4. 逻辑回归的损失函数</strong><br>   逻辑回归的损失函数是它的极大对数似然函数的相反数</p>
<script type="math/tex; mode=display">Loss(w)=-\frac{1}{m}\sum_{i=1}^{i=m}(y^ilog(h(x^i))+(1-y^i)log(1-h(x^i)))</script><p><strong>5. 随机梯度下降、批量梯度下降、小批量梯度下降的优缺点</strong></p>
<ul>
<li>梯度下降公式中的$m$如果是样本总数，则每次更新参数时需要考虑所有的样本，称为批量梯度下降(BGD)。这种方法容易求得全局最优解，但是由于样本个数太多，训练过程非常慢。</li>
<li>如果$m=1$，即每次更新参数时只考虑一个样本，称为随机梯度下降(SGD)。这种方法训练速度快，但是准确率下降，并不是全局最优。</li>
<li>综上所述，当m为所有样本的一小部分时，比如m=32，即每次更新参数时只考虑一小部分样本，称为小批量梯度下降(MBGD)。它克服了上述两种方法的缺点又兼顾它们的优点，在实际中最常使用。</li>
</ul>
<p><strong>6. 逻辑回归的优缺点</strong><br>   优点：</p>
<ul>
<li>形式简单，模型可解释性好。模型的权重$w$表示不同特征对最终结果的影响，如果$w_j$大，说明第$j$个特征的权重比较高，对最终的结果影响较大</li>
<li>模型效果不错。在工程上可以接受，如果特征工作做的好，效果不会太差。</li>
<li>训练速度快。在分类时，计算两仅仅和特征的数目有关</li>
<li>资源占用小，尤其是内存。因为只需要存储各个维度的特征值</li>
<li><p>方便输出结果。逻辑回归可以很方便的得到最后的分类结果，因为输出的结果表示每个样本属于正例的概率，可以很容易的对概率进行划分阈值。<br>缺点</p>
</li>
<li><p>准确率不是很高，因为形式简单</p>
</li>
<li>很难处理不平衡的数据。距离：如果正负样本比例1:1000，则将所有的样本都预测为负样本，模型的损失值就很小，但是这样的模型对于正样本的召回率并不高</li>
<li>处理非线性数据较麻烦。逻辑回归一般只处理线性可分的数据，一般用于二分类</li>
<li>无法筛选特征，需要提前做特征工程。</li>
</ul>
<p><strong>7. 逻辑回归的输出是真实概率吗</strong><br>  如果数据满足以上的2个假设，则输出的数据表示样本属于正例的概率。但是这2个假设并不是那么容易满足，所以很多情况下，逻辑回归的输出值无法作为真实的概率，只能看做置信度。</p>
<p><strong>8. 使用逻辑回归怎么进行多分类</strong><br>  可以将多分类问题转换成二分类问题</p>
<p><strong>9. 逻辑回归和线性回归的区别</strong></p>
<ol>
<li>线性回归阈值$[-\infty,\infty]$,逻辑回归的阈值$[0,1]$</li>
<li>拟合函数不同，线性回归$h(x)=w^Tx$，逻辑回归$h(x)=\frac{1}{1+e^{-w^Tx}}$</li>
<li>损失函数形式不同。线性回归是最小二乘法（平方损失），逻辑回归是极大似然估计</li>
</ol>
<p><strong>10. 逻辑回归是分类问题，为什么叫回归？逻辑回归也叫做“对数几率回归”，这里的“对数几率”如何理解？</strong><br>  逻辑回归的前面部分和线性回归一样，都是$W^Tx+b$，只是最后输出层加了sigmoid函数，将输出转换到[0,1]之间，所以逻辑回归的可以写成</p>
<p>  $y=\frac{1}{1+e^{-z}}$<br>  $z=W^Tx+b$</p>
<p>  上式可以转换成</p>
<script type="math/tex; mode=display">ln\frac{y}{1-y}=W^Tx+b</script><p>  $y$表示样本属于正例的概率，$1-y$表示样本属于负例的概率，而两者的比$\frac{y}{1-y}$称为“几率”，取对数$ln\frac{y}{1-y}$叫做“对数几率”。<br>  由此可以式子$y=\frac{1}{1+e^{-W^Tx+b}}$是在用线性回归模型的预测结果$W^Tx+b$去逼近真实的对数几率，因此也叫做“对数几率回归”</p>
<p><strong>11.  分类为什么不用平方损失MSE</strong></p>
<ol>
<li><p>因为在逻辑回归中，输出层使用的sigmoid激活函数，如果使用MSE作为损失函数，权重和损失函数的的曲线是非凸的，有很多局部最优点。</p>
<p><img src="/2020/06/19/百面机器学习概述/逻辑回归.jpg" alt=""> </p>
</li>
<li><p>在分类问题中，例如样本的真实标签是第3类，我们预测出来的值不需要知道对于每类的预测概率，比如预测出属于第三类的概率为0.6，那另外2类的概率为多少我们并不在意，我们只需要让第三类的预测值大于另外2类就可以了。如果用MSE损失就太严格了</p>
</li>
</ol>
<h2><span id="33-softmax回归">3.3. Softmax回归</span></h2><p>逻辑回归用来解决二分类问题，softmax回归用来解决多分类问题。</p>
<p>假设对一个2*2的图像进行分类，判断图像是狗、猫、鸡的哪一种。即输入x有4个特征$x_1,x_2,x_3,x_4$，真实标签$y=1,2,3$。</p>
<p>softmax回归和线性回归一样，也是一个单层全连接层。输入有4个，输出有3个，所以参数有12个。</p>
<p><img src="/2020/06/19/百面机器学习概述/softmax.png" alt=""></p>
<p>像上面的线性回归，输出的y值是连续的，我们可以把输出的y值看做样本属于某一类值置信度。例如$o_1=o_3=10,o_2=1000$，则图片被预测为猫。但是由于输出的值不稳定，并且连续的预测值和真实的离散值误差难以衡量。<br>为了解决这个问题，这里用到softmax运算。softmax将原先输出的连续值$o$转换为概率。</p>
<p><img src="/2020/06/19/百面机器学习概述/softmax1.png" alt=""></p>
<p>最终softmax回归模型函数为</p>
<p>$o^i=W^Tx+b$<br>$y^i=softmax(o^i)$</p>
<p>逻辑回归和softmax回归的不同</p>
<ol>
<li>逻辑回归最后的输出层只有1个神经元，输出的结果表示样本属于正例的概率。softmax回归最后的输出层有n个神经元(n为类别数)，输出样本属于每个类别的概率。即sigmoid输出的是一个数，softmax输出的是一个向量。</li>
<li>逻辑回归最后输出用sigmoid将连续值转换为属于正例的概率。softmax回归最后输出用softmax将连续值转换为属于各类的概率。都是将连续值转换为概率。</li>
</ol>
<ul>
<li>softmax回归是逻辑回归在多分类的拓展</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/softmax.jpg" alt=""></p>
<ul>
<li><p>逻辑回归怎么用来解决多分类问题<br>将多分类问题拆分成多个二分类问题。具体怎么拆分有3种方法：</p>
<ol>
<li>一对一。假设多分类中一共有k类，则将任意2类进行组合，一共形成k(k-1)/2个组合。将一个样本输入到这k(k-1)/2个分类器中，得到k(k-1)/2个分类结果，然后取个数最多的类别作为这个样本最终预测类别</li>
</ol>
<p><img src="/2020/06/19/百面机器学习概述/多分类1.png" alt=""> </p>
<ol>
<li>一对多。假设多分类中共有k类，一共形成k个分类器，第一个分类器将k1作为正例，其余类作为负例。第二个分类器将k2作为正例，其余类作为负例，以此类推。一个样本输入到这k个分类器中，输出k个分类结果，如果只有一个正例，则该样本属于这个类，如果输出多个正例，则看置信度。</li>
</ol>
<p><img src="/2020/06/19/百面机器学习概述/多分类2.png" alt=""></p>
<ol>
<li>多对多。每次将多个类作为正例，多个类作为负例。</li>
</ol>
</li>
</ul>
<p>  <img src="/2020/06/19/百面机器学习概述/多分类.png" alt=""></p>
<ul>
<li><p>如果一个样本有多个类别标签，怎么办？</p>
<p>这就不是一个多分类问题了，而是多标签学习。假设一共有k类，训练k个分类器。第$i$个分类器判断每个样本是否归为第$i$类。训练分类器时，需要将标签重新整理为属于第$i$类和不属于第$i$类。</p>
</li>
</ul>
<h2><span id="34-回归和分类总结">3.4. 回归和分类总结</span></h2><p><img src="/2020/06/19/百面机器学习概述/前向1.png" alt=""></p>
<p>回归问题和分类问题在前半部分都是一样的，输入$X$乘以权重$W$，然后再经过激活函数$f$，堆叠多层，得到输出$out$</p>
<p>回归问题和分类问题只是最后的输出层不一样。回归问题经过前面的堆叠得到最后的输出$y$</p>
<p>分类问题（多分类）还需要将$y$经过一个softmax层，将输出值归一化到[0,1]之间，且和为1。</p>
<p><img src="/2020/06/19/百面机器学习概述/前向2.png" alt=""></p>
<h2><span id="35-数据不平衡问题">3.5. 数据不平衡问题</span></h2><p>在分类问题中可能会遇到类别不平衡问题。例如1000个样本中，2个正例，998个负例。这样模型在预测值偏向于将样本预测为负例，但是在实际中却没有价值，因为它检测不到正例。</p>
<p>解决正负样本不均衡问题，有以下3个方法</p>
<ol>
<li><p>欠采样<br>对负样本进行负采样，使得正负样本均衡。欠采样需要丢弃一些负样本，但也不能随意丢弃，否则会丢失一些重要信息。代表算法为EasyEnsemble利用集成学习，将负样本划分为若干个集合进行训练，这样每个模型的数据都进行了欠采样。</p>
<p>欠采样方法有：<br>(1) Easy Ensemble算法：每次从样本多的那个类别中抽出一个子集E，子集E的样本个数等于少数类别的样本数，然后让E和少数样本训练得到一个分类器。多次抽取形成不同的子集E，就会训练得到不同的分类器，给定一个测试样本，测试样本的分类结果是多个分类器结果的融合。</p>
</li>
<li><p>过采样<br>生成一些正样本，使得正负样本均衡。但是过采样不能将原先的正样本复制n份，这样会造成严重的过拟合。过采样代表算法为SMOTE通过对训练集中的正样本进行插值生成一些正样本。<br>SMOTE算法对少数类别的样本点x，选择同类别的k个邻居，然后这K个邻居中随机选择1个邻居y，然后在x和y的连线上随机选择1个点作为新生成的样本。</p>
<p><img src="/2020/06/19/百面机器学习概述/smote.png" alt=""></p>
</li>
</ol>
<ol>
<li><p>调整阈值<br>在逻辑回归中，我们通常将$h(x)&gt;0.5$判定为正例，否则为负例。这说明正例和负例出现的可能性相同。但是如果正负样本不平衡，正样本出现非常小，可能阈值就需要变小。假设正样本:负样本=2:10，则将阈值变成0.2，例$h(x)&gt;0.2$我们就判定为正例，否则为负例。</p>
</li>
<li><p>转换问题角度<br>当样本数目极其不平衡时，可以将问题转化为单类学习，异常检测等问题。</p>
</li>
</ol>
<h3><span id="351-smote算法">3.5.1. SMOTE算法</span></h3><p>SMOTE是一种合成少数类过采样技术。SMOTE算法的基本思想是对少数样本进行分析和模拟，并将人工模拟的新样本添加到数据集中，该算法的模型过程采用KNN技术，步骤如下：</p>
<ol>
<li>对每个少数样本x，计算x到其他少数样本的欧式距离，为x找到K个最近邻居</li>
<li><p>从K个近邻中随机挑选1个样本，在x和这1个样本之间的连线上，线性差值生成新样本。</p>
<script type="math/tex; mode=display">x_{\text {new}}=x+\operatorname{rand}(0,1) \times(\tilde{x}-x)</script></li>
<li><p>重复步骤2进行N次，则少数样本数变成原来的N倍</p>
</li>
</ol>
<h2><span id="36-svm">3.6. SVM</span></h2><p>SVM用来做二分类，主要分为三部分</p>
<ol>
<li>数据线性可分：最优划分超平面/硬间隔SVM/线性可分SVM</li>
<li>数据近似线性可分：软间隔SVM/近似线性可分SVM</li>
<li>数据线性不可分：核方法</li>
</ol>
<h3><span id="361-线性可分svm">3.6.1. 线性可分SVM</span></h3><p>SVM就是解决二分类问题，分类学习最基本的想法就是找到一个划分超平面，将不同类别的数据集分开。目的就是找到一个最优的分割线将两类分开。</p>
<p>分类问题有线性可分和线性不可分。线性可分就是在二维空间中能找到一条直线将2类样本划分开。线性不可分就是不能找到一条直线。对于线性不可分的样本通常是通过高斯核函数将样本映射到高维空间，然后转换为高维空间线性可分的问题。<br>下图中的直线都可以将2类样本分开，但是哪个才是好的分割线呢？分类器的价值不在于它多么擅长分割训练样本，而是对于哪些未知的样本它的分类效果怎么样。下图中中间较粗的那条线，在正确划分训练样本的前提下，尽可能地同时远离两个聚类。而其他的线都是有些“倾斜”，一头远离一类，另一头靠近另一类。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM1.png" alt=""></p>
<p>假设划分超平面的公式为</p>
<script type="math/tex; mode=display">w^Tx+b=0</script><p>这就是我们初中学的直线公式$ax+b=0$</p>
<p>理想中超平面进行划分时，如果结果大于0就划分为正例，否则划分为负例。下面就是逻辑回归的划分方式：</p>
<script type="math/tex; mode=display">\left\{\begin{array}{ll}
\omega^{T} x_{i}+b>0, & y_{i}=+1 \\
\omega^{T} x_{i}+b<0, & y_{i}=-1
\end{array}\right.</script><p>而SVM是逻辑回归的强化，SVM设置更严格的划分条件，变成：</p>
<script type="math/tex; mode=display">\left\{\begin{array}{l}
w^{T} x+b \geq 1 \quad y=1 \\
w^{T} x+b \leq-1 \quad y=-1
\end{array}\right.</script><p>上面我们简单的介绍下SVM和逻辑回归的不同，下面我们将详细介绍SVM</p>
<p>假设SVM中的分割超平面为<script type="math/tex">w^Tx+b=0</script></p>
<p>$w=(w_1,w_2,…w_d)$是有个列向量，拆分开就是$w_1x_1+w_2x_2+…+w_dx_d+b=0$</p>
<p>超平面有以下几个性质</p>
<p><strong>性质1</strong>：等比例缩放$w,b$，超平面不变，仍然是同一条直线或超平面<br><strong>性质2</strong>：点$x=(x_1,x_2,…,x_d)$到超平面的距离为</p>
<script type="math/tex; mode=display">\frac{|w_1x_1+w_2x_2+...+w_dx_d+b|}{\sqrt{w_1^2+w_2^2+...+w_d^2}}
=\frac{|w^Tx+b|}{||w||}</script><p>下面介绍2个概念：<strong>函数间隔、几何间隔</strong></p>
<blockquote>
<p>二维平面中点$(x,y)$到$AX+By+c=0$的距离为$\frac{|Ax+By+c|}{\sqrt{A^2+B^2}}$</p>
</blockquote>
<p>  <strong>超平面</strong>：$w^Tx+b=0$<br>  <strong>函数间隔</strong>：$\hat{\gamma}=y(w^Tx+b)=yf(x)=|f(x)|$<br>  函数间隔就是标签$y$乘上$f(x)$的值，函数间隔永远为正。但是由于SVM中$y$的取值只能为1和-1，所以函数间隔的值就是$|f(x)|$<br>  几何间隔表示点到超平面的距离，在SVM中点到超平面的距离为$\frac{|w^Tx+b|}{||w||}=\frac{|f(x)|}{||w||}=\frac{\hat{\gamma}}{||w||}$<br>  <strong>几何间隔</strong>：$\gamma=\frac{|w^Tx+b|}{||w||}=\frac{y(w^Tx+b)}{||w||}=\frac{\hat{\gamma}}{||w||}$</p>
<p>  函数间隔$y(w^Tx+b)$可以表示分类预测的准确性和置信度，值越大越好，说明模型将样本分正确的概率越大，但函数间隔并不表示点到超平面的距离，因为假如将$w,b$成比例变成$2w,2b$，超平面的位置没有变，但是$f(x)$却变成原来的2倍，即函数间隔变成原来的2倍。在实际中，定义点到直线的距离时，用的是几何间隔。$w,b$成倍数增加时，几何间隔不变。函数间隔是几何间隔没有除以$||w||$的表示，几何间隔是函数间隔归一化的结果。函数间隔是我们自己定义的，而几何间隔是客观存在的，无论$w,b$扩大几倍，对几何间隔没有影响。</p>
<p><strong>SVM的目标</strong></p>
<p>SVM的目标是找到一个最优的划分超平面。那怎么定义最优呢？在SVM中评价指标就是几何间隔。在下图中，我们直观的感觉图a的分类效果比b和c好，因为在图a中，与分割线最近的样本点相比b和c来说，这个样本点距离分割线最远。SVM正是遵循这一思想，在众多分割超平面中，找样本点（超平面附近的点）与分割超平面距离最大的那个超平面，即最小几何间隔中，最大的那个。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM3.png" alt=""></p>
<p>SVM的最佳划分超平面需要满足2个条件：（1）能够将所有的正负样本划分正确，即函数间隔大于0，（2）离超平面最近的样本点与超平面的几何距离最大</p>
<p>第一个条件公式为$\hat{\gamma}=y(w^Tx+b)&gt;0$<br>第二个条件为首先找到最近的样本点，即最小的函数间距，然后再最大化函数间距。</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min\tilde{\gamma}^{i}=\min \frac{y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)}{\| w \mid}=\frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m \\
\max _{w, b} \frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m
\end{array}</script><p>结合上面的2个条件，求解最优划分超平面的公式为</p>
<script type="math/tex; mode=display">\begin{aligned}
\max _{w, b} \frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m \\
\text { subject to } y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)>0
\end{aligned}</script><p>但是上面这个目标函数太复杂了，需要对其进行简化。这里就需要用到超平面的性质1：对$w,b$进行缩放，超平面不变。如果在求解的过程中，继续保持这个性质的话，就算上面的公式可以求得最优的划分超平面，但是$w,b$的取值却有无数个，即解不唯一。但是我们要求一个固定的超平面，对应的$w,b$是唯一的，就需要添加限制条件。限制条件可以有多种选择：</p>
<p>选择1：限制$||w||=1$，即超平面的法向量模长为1，这样就消除了等比缩放的影响。但是添加了这个限制对上面公式的简化没有什么帮助<br>选择2：$miny^i(w^Tx^i+b)=1$，限制最小的函数间隔等于1，这也是SVM所采取的方式。如果$\hat{\gamma}=1$，假设将$w,b$放大为$kw,kb$，理论上来说缩放后的函数间隔$\hat{\gamma}_{i+1}=k\hat{\gamma}$，但是我们限制了函数间隔只能为1，即$\hat{\gamma}_{i+1}=\hat{\gamma}=1$，那$k$也只能为1，即不存在等比缩放的问题。</p>
<blockquote>
<p>注意：SVM中将$miny^i(w^Tx^i+b)=1$，其实将值限制为2,3,4…，限制为多少都没关系，都可以消除等比缩放带来的影响，但是不可以不限制。</p>
</blockquote>
<p>有了这个限制条件，原先的目标就需要变了</p>
<p><strong>原先的目标函数</strong>：</p>
<script type="math/tex; mode=display">\begin{aligned}
\max _{w, b} \frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m \\
\text { subject to } y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)>0
\end{aligned}</script><p>增加了$min y^i(w^Tx^i+b)=1$的限制，也就是说$y^i(w^Tx^i+b)&gt;=1$，第一个限制条件变成$\max _{w, b} \frac{1}{|w|}$，最大化$\frac{1}{||w||}$其实就是最小化$||w||$，也就是$min \frac{1}{2}||w||^2$，</p>
<p><strong>最终SVM的目标函数</strong></p>
<script type="math/tex; mode=display">\begin{aligned}
\min _{w, b} \frac{1}{2}||w||^2 , i=1,2, \ldots, m \\
\text { subject to } y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)>=1
\end{aligned}</script><p>于是我们得到了3个平面：最优划分超平面$w^Tx+b=0$，与划分超平面间隔平行的2个超平面$w^Tx+b=1$和$w^Tx+b=-1$</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM2.png" alt=""></p>
<p>这是一个凸二次规划的问题，对于一个优化问题，通常可以从2个角度考虑：主问题和对偶问题。常常利用拉格朗日对偶性将主问题转换为对偶问题，通过求解对偶问题的解来得到原始问题的解，这是因为对偶问题的复杂度往往低于原始问题。</p>
<blockquote>
<p>拉格朗日乘子法知识点<br>拉格朗日乘子法将原问题转换为对偶问题进行求解，主问题有等式约束和不等式约束<br>主问题：</p>
<script type="math/tex; mode=display">\begin{array}{c}
\min _{x} f(x)\\
s . t . h_{i}(x)=0(i=1, \ldots, m)\\g_{j}(x) \leq 0(j=1, \ldots, n)
\end{array}</script><p>拉格朗日函数为</p>
<script type="math/tex; mode=display">L(x, \lambda, \mu)=f(x)+\sum_{i=1}^{m} \lambda_{i} h_{i}(x)+\sum_{j=1}^{n} \mu_{j} g_{j}(x)</script><p>由不等式约束引入的KKT条件</p>
<script type="math/tex; mode=display">\left\{\begin{array}{l}
g_{j}(x) \leq 0 \\
\mu_{j} \geq 0 \\
\mu_{j} g_{j}(x)=0
\end{array}\right.</script><p>其中$\lambda=\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)^{T} \text { 和 } \mu=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{n}\right)^{T}$是拉格朗日乘子</p>
</blockquote>
<p>根据上面的知识，我们得到<strong>SVM的拉格朗日函数</strong></p>
<p>[<br>L(\omega, b, \alpha)=\frac{1}{2}|\omega|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\omega^{T} x_{i}+b\right)\right)<br>]</p>
<script type="math/tex; mode=display">L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{m} \alpha_{i} y_{i}\left(\omega^{T} x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}</script><p>其中 $\alpha=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right)$,拉格朗日乘子 $\alpha_{i} \geq 0$</p>
<p>一个优化问题可以从两个角度考虑：主问题和对偶问题。在约束最优化问题中，常常利用拉格朗日对偶性将原问题转化为对偶问题，通过对偶问题来得到原始问题的解。<br>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：</p>
<script type="math/tex; mode=display">\max _{\alpha} \min _{w, b} L(w, b, \alpha)</script><p>所以为了求得对偶问题的解，需要先求$L(w,b,\alpha)$对$w,b$的极小，再求对$\alpha$的极大。</p>
<p>（1）求$\min _{w, b} L(w, b, \alpha)$<br>将拉格朗日函数$L(w,b,\alpha)$分别对$w,b$求偏导，并令其为0</p>
<script type="math/tex; mode=display">\begin{array}{l}
\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{m} \alpha_{i} y_{i}=0
\end{array}</script><p>得到：</p>
<script type="math/tex; mode=display">\begin{array}{l}
w=\sum_{j=1}^{m} \alpha_{i} y_{i} x_{i} \\
\sum_{i=1}^{m} \alpha_{i} y_{i}=0
\end{array}</script><p>将求得$w$带入到拉格朗日函数中</p>
<script type="math/tex; mode=display">\begin{aligned}
L(w, b, \alpha) &=\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}-\sum_{i=1}^{m} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{m} \alpha_{j} y_{j} x_{j}^T\right) x_{i}+b\right)+\sum_{i=1}^{m} \alpha_{i} \\
&=\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}-\sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}-\sum_{i=1}^{m} \alpha_{i} y_{i}b+\sum_{i=1}^{m} \alpha_{i} \\
&=-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{m} \alpha_{i}
\end{aligned}</script><p>即</p>
<script type="math/tex; mode=display">\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}+\sum_{i=1}^{m} \alpha_{i}</script><p>（2）求$\min _{w, b} L(w, b, \alpha)$关于$\alpha$的极大，即是对偶问题</p>
<script type="math/tex; mode=display">\begin{array}{ll}
\max _{\alpha} & -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}+\sum_{i=1}^{m} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, m
\end{array}</script><p>将上面的求极大变成下面的求极小问题</p>
<script type="math/tex; mode=display">\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}-\sum_{i=1}^{m} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, m
\end{array}</script><p>最终求得的$w,b$只和支持向量有关。因此SVM的一个重要性质是：SVM训练完之后，大部分的训练样本不需要保留，最终模型只和支持向量有关。</p>
<h3><span id="362-近似线性svm">3.6.2. 近似线性SVM</span></h3><p>在实际应用中，完全线性可分是很少的，例如下图中，没有一条直线可以将2类完全分开。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM4.png" alt=""></p>
<p>于是就有了软间隔，与线性可分的硬间隔相比，条件没有那么苛刻，我们允许个别样本点出现在隔离带里面，例如下</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM5.png" alt=""></p>
<p>硬间隔要求所有样本点都满足</p>
<script type="math/tex; mode=display">y^i(w^Tx^i+b)>=1</script><p>软间隔引入松弛变量，允许部分样本点满足</p>
<script type="math/tex; mode=display">y^i(w^Tx^i+b)+\xi_{i}>=1</script><p>当$\xi_{i}=0$时，样本分类正确<br>当0&lt;$\xi_{i}<1$时，样本分类正确 当$\xi_{i}="">=1$时，样本分类正确</1$时，样本分类正确></p>
<p>增加松弛变量后，SVM的目标变成：</p>
<script type="math/tex; mode=display">\begin{aligned}
&\min _{w} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \xi_{i}\\
&\text {s.t.} \quad y_{i}\left(w^{T} x_{i}+b\right)\geq 1-\xi_{i}, \quad \xi_{i} \geq 0, \quad i=1,2, \ldots, n
\end{aligned}</script><p>松弛变量通过学习得到，且要惩罚大的松弛变量。<br>$C$是一个大于0的数，表示错误样本的惩罚程度，当$C$无穷大时，表示对错误样本的惩罚无穷大，不允许分错样本，这样$\xi_i$就无穷小，就是线性可分SVM。当$C$为有限值时，才会允许部分样本分错。</p>
<p>注意：在间隔内的那部分样本点是不是支持向量？是</p>
<p>上述原问题的拉格朗日函数是</p>
<script type="math/tex; mode=display">L(w, b, \xi, \alpha, \mu)=\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \xi_{i}-\sum_{i=1}^{m} \alpha_{i}\left(y_{i}\left(w^T x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}</script><script type="math/tex; mode=display">\alpha_{i} \geqslant 0, \mu_{i} \geqslant 0</script><p>对偶问题是拉格朗日的极大极小问题，所以对偶问题为</p>
<script type="math/tex; mode=display">\max _{\alpha,\mu} \min _{w, b,\xi} L(w, b, \xi, \alpha, \mu)</script><p>（1）极小化$\min _{w, b,\xi} L(w, b, \xi, \alpha, \mu)$，对$w, b,\xi$求导使其为0</p>
<script type="math/tex; mode=display">\begin{array}{l}
\nabla_{w} L(w, b, \xi, \alpha, \mu)=w-\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L(w, b, \xi, \alpha, \mu)=-\sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\
\nabla_{\xi_{i}} L(w, b, \xi, \alpha, \mu)=C-\alpha_{i}-\mu_{i}=0
\end{array}</script><p>得到</p>
<script type="math/tex; mode=display">\begin{array}{c}
w=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i} \\
\sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\
C-\alpha_{i}-\mu_{i}=0
\end{array}</script><p>将上述公式带入到拉格朗日函数中，得到</p>
<script type="math/tex; mode=display">\min _{w, b, \xi} L(w, b, \xi, \alpha, \mu)=-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T x_{j}+\sum_{i=1}^{m} \alpha_{i}</script><p>（2）对$\min _{w, b, \xi} L(w, b, \xi, \alpha, \mu)$求$\alpha$的极大，得到对偶问题</p>
<script type="math/tex; mode=display">\max _{\alpha}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T  x_{j}+\sum_{i=1}^{m} \alpha_{i}</script><script type="math/tex; mode=display">\begin{aligned}
&\text { s.t. } \quad \sum_{i=1}^{m} \alpha_{i} y_{i}=0\\
&\begin{array}{l}
C-\alpha_{i}-\mu_{i}=0 \\
\alpha_{i} \geqslant 0 \\
\mu_{i} \geqslant 0, \quad i=1,2, \cdots, m
\end{array}
\end{aligned}</script><p>将上述极大变成极小为：</p>
<script type="math/tex; mode=display">\min _{\alpha}\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}^T  x_{j}-\sum_{i=1}^{m} \alpha_{i}</script><p>SVM中的2个重要参数：惩罚系数$C$和$\gamma$<br>C和gamma都起到了正则化的作用<br>惩罚系数C表示对误差的惩罚程度，C越大，说明越不能容忍误差，容易出现过拟合；C越小，容易欠拟合<br>gamma是选择RBF函数（高斯核函数）作为核函数后，该函数自带的一个参数，隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少,容易过拟合；gamma越小，支持向量越多，容易欠拟合。支持向量的个数影响训练和预测的速度。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM4.jpg" alt=""></p>
<h3><span id="363-核函数">3.6.3. 核函数</span></h3><p>上面讨论的硬间隔和软间隔都是样本完全线性可分或者近似线性可分的，但是也会遇到样本不是线性可分的，例如下图：</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM6.png" alt=""></p>
<p>解决方案：将样本使用核函数，向高维空间转化，使得在高维空间中线性可分。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM7.png" alt=""></p>
<p>核函数的好坏对SVM至关重要。若核函数选择不合适，将样本映射到一个不合适的特征空间，很有可能分类效果不佳。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM8.png" alt=""></p>
<h3><span id="364-面试问题">3.6.4. 面试问题</span></h3><ul>
<li><p><strong>介绍函数间隔和几何间隔</strong></p>
<blockquote>
<p>二维平面中点$(x,y)$到$AX+By+c=0$的距离为$\frac{|Ax+By+c|}{\sqrt{A^2+B^2}}$</p>
</blockquote>
<p><strong>超平面</strong>：$w^Tx+b=0$<br><strong>函数间隔</strong>：$\hat{\gamma}=y(w^Tx+b)=yf(x)=|f(x)|$<br>函数间隔就是标签$y$乘上$f(x)$的值，函数间隔永远为正。但是由于SVM中$y$的取值只能为1和-1，所以函数间隔的值就是$|f(x)|$<br>几何间隔表示点到超平面的距离，在SVM中点到超平面的距离为$\frac{|w^Tx+b|}{||w||}=\frac{|f(x)|}{||w||}=\frac{\hat{\gamma}}{||w||}$<br><strong>几何间隔</strong>：$\gamma=\frac{|w^Tx+b|}{||w||}=\frac{y(w^Tx+b)}{||w||}=\frac{\hat{\gamma}}{||w||}$</p>
<p>函数间隔$y(w^Tx+b)$可以表示分类预测的准确性和置信度，值越大越好，说明模型将样本分正确的概率越大，但函数间隔并不表示点到超平面的距离，因为假如将$w,b$成比例变成$2w,2b$，超平面的位置没有变，但是$f(x)$却变成原来的2倍，即函数间隔变成原来的2倍。在实际中，定义点到直线的距离时，用的是几何间隔。$w,b$成倍数增加时，几何间隔不变。函数间隔是几何间隔没有除以$||w||$的表示，几何间隔是函数间隔归一化的结果。函数间隔是我们自己定义的，而几何间隔是客观存在的，无论$w,b$扩大几倍，对几何间隔没有影响。</p>
</li>
<li><p><strong>SVM中$w^Tx+b&gt;1$，$w^Tx+b&lt;-1$设置为正负样本的阈值，为什么是正负1</strong><br>SVM最终学习到的是一个超平面，至于系数是$(w^T,b)$还是$(2w^T,2b)$都不重要，所以需要对$w,b$进行限制，这SVM对函数间隔进行限制为$y(w^Tx+b)=1$，这样在求得的解中$w,b$就是唯一的。当然$y(w^Tx+b)$也不一定为1，也可以为2，3或其他值</p>
</li>
<li><p><strong>SVM什么时候选择线性核函数，什么时候选择高斯核函数</strong><br>当样本量比较小，特征量比较大时使用线性核函数。因为此时特征空间已经很高维了，只是数据量不够，线性核函数足够了。如果用高斯核函数投影到高维容易出现过拟合。<br>当数据量比较大而特征量比较小时，使用高斯核函数，需要投影到高维特征空间。</p>
</li>
<li><p><strong>使用高斯核函数之前需要对数据进行处理吗</strong><br>需要对特征进行缩放，因为高斯核函数需要计算两个点之间的欧式距离，如果不特征缩放的话那些值特别大的特征将会对核函数的结果有决定性影响，而数据量小的特征将被忽略。</p>
</li>
<li><p><strong>SVM如何解决数据不均衡问题</strong><br>数据不均衡在SVM中导致的主要问题是数据量少的样本分布空间不如数据量多的样本。为了能够让分割超平面向数据量少的样本偏移，可以给样本少的分类更大的惩罚因为$C$,表示如果数据少的样本分错了将会有很大的惩罚，使模型更重视数量少的样本。</p>
</li>
<li><strong>SVM原始问题为什么要转化为对偶问题求解？</strong><ol>
<li>改变算法复杂度，对偶问题更容易求解。因为算法的复杂度与样本维度有关，在对偶问题下，算法复杂度和样本数量有关。如果是线性回归，样本维度小于样本数量，在原问题上求解就可以了。但如果是非线性回归，就涉及到升维，例如使用高斯核函数，将样本升到很高维，升维后的样本维度远远大于样本数量，这是显然在对偶问题下更好求解。</li>
<li>转化为对偶问题才能得到内积形式，引入核函数，进而推广到非线性分类问题中。</li>
</ol>
</li>
<li><strong>SVM为什么采用间隔最大化</strong><br>当数据线性可分时，有无穷多个超平面可以将样本划分开，利用间隔最大化可以求得最优的划分超平面。此时的划分超平面的分类结果是最鲁棒的，对未知数据的泛化能力最强。</li>
<li><strong>为什么SVM引入核函数</strong><br>当样本在原始空间中线性不可分时，通过核函数将样本映射到更高维的特征空间中，样本在这个特征空间中线性可分或近似线性可分。</li>
<li><strong>核函数也可以应用在别的分类模型中，为什么在逻辑回归中不用核函数呢？</strong><br>因为核函数将样本维度映射到很高维。在SVM中只有支持向量决定了划分超平面，只有少数样本参与核计算，在计算核函数时优势很大。但是逻辑回归是所有的样本都决定了划分超平面，如果采用核函数，那每个样本都参数核运算，则非常耗时。</li>
<li><p><strong>SVM为什么对缺失值敏感</strong><br>这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。</p>
</li>
<li><p><strong>SVM和逻辑回归的不同</strong></p>
<ol>
<li>SVM只有支持向量对模型有影响，即只有支持向量决定划分超平面。逻辑回归是所有点都影响划分超平面</li>
<li>损失函数不同。SVM采用hinge损失，逻辑回归采用对数损失<br>逻辑回归的目标函数 <script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]</script>SVM的目标函数<script type="math/tex; mode=display">\mathcal{L}(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)</script>逻辑回归是最大化似然，求得参数的值<br>SVM是最大化几何间隔，求得参数的值</li>
<li>输出不同。SVM输出0/1，逻辑回归输出样本属于正例的概率</li>
<li>处理非线性问题的能力不同。SVM通过核函数将非线性问题转化为线性问题。逻辑回归需要活动特征转换。</li>
<li>LR对异常值敏感；SVM相对不敏感，泛化能力好</li>
</ol>
</li>
<li><p>SVM的优缺点<br>优点：</p>
<ol>
<li>可以高效地解决高维特征的分类和回归</li>
<li>只依赖支持向量，无需全部样本</li>
<li>通过核函数可以处理线性不可分问题<br>缺点：</li>
<li>样本量巨大时，不适合用</li>
<li>SVM对缺失值敏感</li>
<li>对核函数没有选择的标准</li>
</ol>
</li>
</ul>
<h2><span id="37-决策树">3.7. 决策树</span></h2><p>决策树是一个非常见的机器学习算法，易于理解，可解释强，可以作为分类算法和回归算法。</p>
<p>决策树通常是递归的选择最优特征，并根据该特征对训练集进行划分，对训练即进行分类。</p>
<p>现将所有数据都放在根节点，然后选择一个最优特征，按照这一个特征将训练数据集分割成子集，分配到各个子节点上，如果子集中有些节点没有被正确分类，那在这个子集中再选择最优特征，继续分割，直到所有的训练数据集都被正确分类，或者没有合适的特征未知，最后每个子集都被分到叶子节点上，就变成了一棵决策树。</p>
<p>但如果决策树对训练数据集有非常好的分类能力，在测试数据集上可能会出现过拟合，就需要对已经生成的决策树进行由下而上的剪枝，使树变得简单。</p>
<p>决策树算法主要包括3个步骤：</p>
<ol>
<li>特征选择，选择哪个特征进行划分才能使数据能够很好的分类。</li>
<li>决策树的生成</li>
<li>决策树的剪枝</li>
</ol>
<p>决策树学习常用的算法有：ID3，C4.5，CART</p>
<p>在特征选择上，ID3使用信息增益，C4.5使用信息增益比，CART使用基尼指数</p>
<p>首先了解一个概念：<strong>信息熵</strong></p>
<p>一个事情发生的概率越大，这个事情所携带的信息熵越小，熵表示随机变量不确定的程度。假设$X$是一个随机变量，它的取值有$x_1,…x_n$，每个取值的概率为$p_i$，也就是$P(X=x_i)=p_i$<br>那么随机变量$X$的熵为：</p>
<script type="math/tex; mode=display">H(X)=-\sum_{i=1}^{i=n}p_ilogp_i</script><p>这里的$log$可以以2为底，也可以以$e$为底<br>熵越大，随机变量的不确定性越大。</p>
<p>例如一个人告诉你明天太阳从东方升起，那这个人相当于说了句废话，这个话的信息熵为0。</p>
<p><strong>条件熵</strong></p>
<p>有2个随机变量$X,Y$，条件熵就是在$X$给定的情况下随机变量$Y$的熵</p>
<script type="math/tex; mode=display">P(Y|X)=\sum_{i=1}^{i=n}p_iH(Y|X=x_i)</script><p>$p_i表示X为x_i$的概率</p>
<h3><span id="371-基础树id3c45cart">3.7.1. 基础树(ID3/C4.5/CART)</span></h3><h4><span id="3711-id3">3.7.1.1. ID3</span></h4><p>ID3根据信息增益进行划分，信息增益表示得知$X$时使得类$Y$的不确定性减少的程度。</p>
<p>信息增益是用来选择特征的一个指标，信息增益越大，说明这个特征带的信息越多，即已知这个特征的值让类$Y$的不确定性减少的越多，该特征越重要。</p>
<p>信息增益=信息熵-条件熵</p>
<p>特征$A$对训练数据集$D$的信息增益为$g(D,A)$<br>$g(D,A)=H(D)-H(D|A)$<br>原来数据集的信息熵为$H(D)$，知道特征$A$后数据集的信息熵为$H(D|A)$，信息熵减少了$H(D)-H(D|A)$就是信息增益</p>
<p>例如相亲问题中，女生根据男生会不会写代码这个条件下，来决定见或者不见。<br>如果男方会写代码，女方就见。不会写代码，女方就不见。说明这个特征对结果很重要，让分类结果变得确定，因此“会不会写代码”这个特征的信息增益很大。0&lt;=信息增益的取值&lt;=1</p>
<p>不同的特征有不同的信息增益<br>对数据集（整个数据集/子集）计算每个特征的信息增益，选择信息增益最大的特征进行划分数据集。</p>
<p>ID3算法步骤：</p>
<ol>
<li>从根节点开始，计算所有特征的信息增益，选择信息增益最大的特征作为划分特征，将根节点数据集划分成n类，n为特征的取值数</li>
<li>对n个子树再次计算所有特征的信息增益，再次划分子树</li>
<li>直到该节点上所有样本都为同一类时，或没有特征可以选，或信息增益都很小时结束划分。</li>
</ol>
<p>注意：当根节点选择特征1作为划分特征时，在子树中计算所有特征的信息增益不包括特征</p>
<p>缺点：</p>
<ul>
<li>信息增益对取值数据较多的特征有多偏好。例如编号，每个人的编号都不一样，编号的取值很多，则编号的信息增益接近1，形成只有2层的决策树。虽然信息增益很大，但是对于数据集的分割却没有意义。</li>
<li>信息增益只能处理离散的特征，不能处理连续特征</li>
<li>只能处理分类问题，不能处理回归问题</li>
<li>对缺失值敏感</li>
</ul>
<h4><span id="3712-c45">3.7.1.2. C4.5</span></h4><p>C4.5根据信息增益比进行划分</p>
<p>为了解决ID3对取值多的特征偏好这一缺点，引入信息增益比。</p>
<p>特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$<br>为信息增益$g(D,A)$与特征$A$的熵$H_A(D)$之比</p>
<script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</script><p>信息增益比就是对取值很多的特征进行惩罚。举个例子，假设有100个样本，对这100个样本进行均分。如果特征1的取值有2个，则将样本分成50,50的子集。如果特征2的取值有4个，则将样本分成25,25,25,25的子集。可以看出特征的取值越多，就会生成越多的小子集。但是如果小子集越多的话，就会出现过拟合的问题。所以需要对这样的特征添加个惩罚项。这个特征的取值个数越多，惩罚越大。</p>
<script type="math/tex; mode=display">信息增益比=\frac{信息增益}{惩罚项}</script><p>ID3中如果一个特征取值取值越多，则信息增益越大，但是在C4.5中这个特征的惩罚项也就越大，就可以让信息增益比平衡。</p>
<ul>
<li><p>C4.5怎么处理特征的缺失值<br>分两步实现：<br>第一步，计算所有特征的信息增益或者信息增益率的时候，假设数据集一共10000个样本，特征A中缺失了5000个，则无视缺失值，在剩下的5000个特征中计算信息增益（或者信息增益率），最后乘以0.5，思想就是缺失值多的特征通过这种降低权重的方式来体现信息的缺失；</p>
<p>第二步，如果运气不好，正好这个A特征乘0.5之后得到的信息增益或者增益率还是最大的，那么就像西瓜书中提到的那样，存在缺失值的样板按照比例进入分裂之后的新的分支，假设根据特征A分裂得到两个新的分支，一个分支有2000个样本有2000个样本，一个分支有3000个样本，则按照比例2000个缺失值和3000个缺失值样本分别进入两个分支。<br>缺点：<br>虽然解决了特征取值多的问题，但是仍然有以下问题</p>
</li>
<li><p>需要计算信息增益，计算量大</p>
</li>
<li>只能处理分类问题，不能处理回归问题</li>
<li>对缺失值有处理</li>
</ul>
<h4><span id="3713-cart分类回归树">3.7.1.3. CART(分类回归树)</span></h4><p>Classification And Regression Tree</p>
<p>CART可以用来做回归和分类。如果用来做<strong>回归，需要生成回归树，用平方损失选择最优特征</strong>。如果用来做分类，需要生成<strong>分类树，根据基尼指数来选择最优特征</strong>。</p>
<p>训练数据集$D$的基尼指数为：<br>$Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2$<br>$C_k$是$D$中属于第$k$类的样本子集<br>若是二分类，则可以简写为$Gini(D)=2p(1-p)$</p>
<p>如果要按照特征$A$来划分子集的话，需要先计算在特征$A$的条件下，集合$D$的基尼指数为：<br>$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$<br>特征$A=a$将数据集$D$划分为$D_1和D_2$<br>基尼指数$Gini(D)$表示集合$D$的不确定性。基尼指数$Gini(D,A)$表示特征$A$分割后集合$D$的不确定性。基尼指数越大，集合的不确定性也越大。划分时选择基尼指数小的特征进行划分。</p>
<ul>
<li><p>分类树建立<br>如果数据集有4个特征，每个特征有3个取值，则需要计算每个特征每个取值的基尼指数，即$Gini(D,A1=1),Gini(D,A1=2),Gini(D,A1=3)$，在特征1的3个基尼指数中找一个最小的，然后再分别计算特征2,3,4的3个基尼指数，分别找到一个最小的，然后再比较这4个最小的基尼指数，再次找到一个最小的，假设为$Gini(D,A3=2)$，则将根节点按照特征3=2进行划分成2个子集，等于2的划分到一个子集中，不等于2的划分到一个子集中。</p>
</li>
<li><p>回归树建立<br><strong>与分类树使用Gini指数分割特征不同，分类树使用平方和误差来分割特征</strong>。比如特征A的按照阈值a1和按照阈值a2，分别将数据集D划分成D1和D2，计算D1的平方和误差、D2的平方和误差、D1+D2的平方和误差，让这3个值同时最小，然后选择使用a1还是a2</p>
</li>
<li><p>回归树和分类树的区别：<br>当CART建立好之后，在给定测试集做预测时，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。回归树输出不是类别，采用叶子节点的均值或者中位数来预测输出结果。</p>
</li>
<li><p>CART划分的树是二叉树，因此CART每次分类时会将这个特征的特征值分成两堆，划分成左子树和右子树。然后在子树中还会用到这个特征计算基尼指数，因此CART中特征会被重复利用。但是在ID3和C4.5中，选中一个特征后，会划分为多个子树，例如这个特征有5个取值，就会划分成5个子树，在下一层中就不会计算这个特征的信息增益和信息增益比，因此特征只计算一次。</p>
</li>
<li><p><strong>CART怎么处理缺失值</strong><br>首先计算所有特征的Gini，对于有缺失值的特征，只用有值的部分计算Gini，然后再乘上非缺失值占的比重。如果最终选择的最优特征恰好有缺失值，则选择其他代理特征，有2种情况：</p>
<p>1、首先，如果某个存在缺失值的特征恰好是当前的分裂增益最大的特征，那么我们需要遍历剩余的特征，剩余的特征中如果有也存在缺失值的特征，那么这些特征忽略，仅仅在完全没有缺失值的特征上进行选择，我们选择其中能够与最佳增益的缺失特征分裂之后增益最接近的特征进行分裂。</p>
<p>2、如果我们事先设置了一定的标准仅仅选择差异性在一定范围内的特征作为代理特征进行分裂而导致了没有特征和最佳缺失特征的差异性满足要求，或者所有特征都存在缺失值的情况下，缺失样本默认进入个数最大的叶子节点。</p>
<p>显然这种缺失值的处理方式的计算量是非常大的，我们需要遍历其它的特征来进行代理特征选择，这个在数据量很大的情况下开销太大，而带来的性能提升确很有限，所以后来就不怎么用这种处理方式，xgb和lgb中就是直接将缺失值划分到增益大的节点里，这样在处理上要快速的多，而且在gbm的框架下一点点的误差其实影响不大。</p>
</li>
<li><p>可以处理连续值，因此既可以分类，也可以回归。</p>
</li>
<li>对缺失值有处理</li>
</ul>
<h4><span id="3714-总结">3.7.1.4. 总结</span></h4><p>ID3：信息增益<br>C4.5：信息增益比<br>CART：最小基尼指数</p>
<p>以上3个指标都是描述特征的。</p>
<p>以上三种方法都是对数据划分的方法，通过选择特征将数据划分为更小的子集。</p>
<p>ID3选择信息增益大的特征来划分，但是会偏好特征取值多的特征，引入C4.5，使用信息增益比，对取值多的特征添加惩罚项。但是以上2种方法都只能处理分类问题，后来引入CART，计算基尼指数，选择基尼指数最小的特征进行划分。</p>
<p><img src="/2020/06/19/百面机器学习概述/决策树.jpg" alt=""></p>
<h3><span id="372-树的剪枝">3.7.2. 树的剪枝</span></h3><p>一棵完全生长的决策树会出现过拟合的问题。例如如果按照“编号”进行划分树，则每个节点只包含1个样本，在测试集上的效果将会很差，出现过拟合现象。为了解决这个问题，对决策树进行剪枝，减掉一些枝叶。剪枝方法有2种方式：预剪枝、后剪枝</p>
<p>决策树的剪枝往往通过极小化决策树损失函数来实现。设决策树$T$中叶子节点个数为$|T|$，$t$为树$T$的叶子节点，每个叶子节点上有$N_t$个样本，这$N_t$个样本中，真实属于第$k$类样本有$N_{tk}$个</p>
<p>$C(T)=\sum_{t=1}^{t=T}N_tH_t(T)+\alpha|T|$<br>$H_t(T)=\sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$<br>即所有叶子节点的样本数*该节点的信息熵+正则化</p>
<p>$\alpha|T|$是正则项，用来控制决策树的复杂程度，防止过拟合。</p>
<p>剪枝就是在$\alpha$确定时，最小化损失函数。可以看出决策树生成只考虑了提高信息增益或信息增益比来对训练数据更好的拟合，而决策树剪枝通过优化损失函数还考虑了树的复杂程度。决策树生成只考虑局部信息，决策树剪枝考虑全局信息。</p>
<ul>
<li><p>预剪枝<br>在生成决策树的时候进行剪枝。在按照某个特征划分之前，先计算当前的划分是否能够带来模型泛化性能的提升，如果不能，则不再生成子树。此时该节点中包含的样本可能属于不同的类别，按照多数投票的原则，将该节点判为多数类别。怎么判断不再划分树：</p>
<ol>
<li>当树的深度达到一定深度</li>
<li>当节点样本个数小于某个阈值</li>
<li><p>计算每次分裂对测试集的准确率是否提升，当提升小于某个阈值时，停止划分</p>
<p>缺点：<br>可能会出现欠拟合的问题，应为当前划分可能造成准确率变小，但是可能后续会让准确率变大，但是树的划分已经止步于此了。</p>
</li>
</ol>
</li>
<li><p>后剪枝<br>先生成一个完整的决策树，然后自底向上的进行剪枝。通过比较在损失函数，如果剪枝后损失函数更小，则剪枝该子树。<br>后剪枝的欠拟合风险很小，泛化能力优于预剪枝，但是开销比较大。</p>
</li>
</ul>
<h2><span id="38-k近邻knn">3.8. K近邻(KNN)</span></h2><h3><span id="381-knn原理">3.8.1. KNN原理</span></h3><p>KNN是一种监督学习方法，一种基本的分类和回归模型。<br>KNN思想：给定一个训练集，其中的样本类别已确定，然后给定<strong>测试样本</strong>，基于某种距离度量在训练集中找出与其最靠近的k个样本，然后基于这k个训练集邻居，对测试集进行预测。通常在决策时：</p>
<ul>
<li>分类问题：k个邻居中出现最多的类别作为测试样本的类别</li>
<li>回归问题中，使用k个邻居的平均作为训练样本的值</li>
</ul>
<p>K近邻学习的特点是：没有显示的训练过程，属于懒惰学习。在训练阶段仅仅将样本保存起来，训练时间为0，等到有测试样本时才开始处理。</p>
<p>KNN的三个主要问题：</p>
<ul>
<li>K的选择</li>
<li>选择哪种距离度量方式</li>
<li>使用哪种决策手段：（1）多数投票，（2）加权投票，即距离近的样本权重大</li>
</ul>
<p><strong>K的选择</strong></p>
<p>如果选择较小的k，就相当于用较少的邻居来预测，如果邻居是噪声，预测就会出错。较小的k会让模型变得复杂，容易出现过拟合。</p>
<p>如果选择较大的k，邻居变多，与测试集较远的样本也会影响预测效果，使预测发生UC哦呜。较大的k会让模型变得简单。</p>
<p>在应用中，k一般取比较小的数值，通常采用交叉验证来选择最优的k</p>
<p><strong>距离度量方式</strong></p>
<ol>
<li>欧氏距离</li>
<li>曼哈顿距离</li>
</ol>
<p><strong>分类决策</strong></p>
<ol>
<li>多数投票</li>
<li>加权投票</li>
</ol>
<p><img src="/2020/06/19/百面机器学习概述/knn1.png" alt=""></p>
<p>KNN的优点：</p>
<ol>
<li>思想简单</li>
<li>可分类，也可回归</li>
<li>可用于非线性分类</li>
<li>训练时间段，仅为O(n)</li>
<li>对异常点不敏感</li>
<li></li>
</ol>
<p>KNN的缺点：</p>
<ol>
<li>计算量大</li>
<li>样本不均衡时，对稀有类别的预测准确率低</li>
<li>相比决策树，KNN解释性不强</li>
</ol>
<h3><span id="382-kd树">3.8.2. KD树</span></h3><p>当训练样本特别大或样本维度特别大时，测试样本需要计算与每个训练样本的距离，然后排序找出前k个样本，时间复杂度大。</p>
<p>为了提高k近邻搜索的效率，可以通过减少N值，即在计算距离时，不用计算与所有训练样本的距离，只计算一部分，其中一个应用就是KD树。</p>
<p>KD树算法分为2步：构造KD树、搜索KD树</p>
<ol>
<li>构造KD树<br>KD树是对训练集进行存储，以便快速查询。假设训练样本有k维，KD数是二叉树，对k维空间的一个划分，KD树相当于不断地用垂直于坐标轴的超平面将k维空间切分。样本有k维特征，先找出第1维的特征值，找出其中的中位数，超平面经过该样本点将空间划分为左右2部分，然后在左右区域中，再根据第2维特征的中位数划分。然后再根据第3维特征的中位数划分。重复以上操作，知道空间中只包含1个样本点。如果有6个样本，中位数取右中位数。</li>
</ol>
<p><strong>常见问题</strong></p>
<ol>
<li><p>不平衡的样本给KNN造成什么影响，怎么解决</p>
<p><img src="/2020/06/19/百面机器学习概述/knn2.png" alt=""><br>l<br>例如上图中样本点y直观上看属于红色类别。但是使用KNN预测时，由于蓝色样本比较多，所以在k个邻居中蓝色点也占多数，导致y分类错误。<br>为了解决以上问题，可以采用加权投票的方式，与测试样本点距离小的样本权重大。</p>
</li>
<li>为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下原理<br>先将样本按照距离分组，获得每个组的中心。给定一个测试样本，找出与其最近的那个簇，在这个簇上进行KNN。</li>
<li><p>KD树在对维度进行划分时，划分顺序是否可以优化？<br>可以先求每一维的方差，方差大说明数据越分散，可以获得比较好的划分效果。</p>
</li>
<li><p>KD树在划分时，需要找某个特征上的中位数，计算量大，怎么优化<br>在构建KD树之前，对每一维进行排序，在之后的划分中，直接使用。</p>
</li>
</ol>
<h1><span id="4-聚类">4. 聚类</span></h1><p>聚类是无监督学习，即不知道样本的标签。</p>
<h2><span id="41-k-means聚类">4.1. K-Means聚类</span></h2><p>K-Means聚类的目标函数为：最小化平方误差和，即SSE</p>
<script type="math/tex; mode=display">SSE=\sum_{i=1}^{k}\sum_{p \in C_i}|p-m_i|^2</script><p>$m_i$表示第$i$个簇的中心，使得每个样本点到中心的距离之间最小</p>
<p><strong>算法步骤:</strong></p>
<ol>
<li>数据预处理，如归一化，离群点处理等</li>
<li>随机选k个点，作为聚类中心。这k个点不一定是样本点，可以是任意一点</li>
<li>计算每个点到k个聚类中心的距离，然后将这个点分到最近的类中</li>
<li>重新计算每个簇的中心（簇中心为所有点的均值）</li>
<li>重复2~4，直到聚类中心不再发生改变或达到迭代次数</li>
</ol>
<ol>
<li><p><strong>K-Means常用的距离度量有哪些</strong></p>
<ul>
<li>曼哈顿距离<br>$d12=|x_1-x_2|+|y_1-y_2|$ </li>
<li>欧式距离<br>$d12=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$<br>欧式距离体现2个向量距离上的绝对差异</li>
<li>余弦相似度<br>$con(A,B)=\frac{AB}{||A||*||B||}$<br>相比欧式距离，余弦距离更关注2个向量在方向上的相对差异。2个向量方向越接近，越可能被聚为一类。</li>
</ul>
</li>
<li><p><strong>K-Means中k如何选择</strong></p>
<ul>
<li>根据场景选择</li>
<li>手肘法<br>计算指标SSE（误差平方和）<script type="math/tex; mode=display">SSE=\sum_{i=1}^{k}\sum_{p \in C_i}|p-m_i|^2</script>一共有k个簇，$C_i$为第$i$个簇，$p$是第$i$个簇的样本，$m_i$是第$i$第个簇的中心。$SSE$表示所有样本的误差平方和，表示聚类结果的好坏。核心思想：随着$k$的增大，每个簇中的样本越来越小，那每个簇越聚集，所有样本点的误差平方和$SSE$逐渐变小。假设理想情况下聚成$k’$类。我们从小到大遍历$k$，当$k&lt;k’$时，随着$k$的增加每个簇的聚合程度会迅速增加，那$SSE$会大幅度下降。当$k=k’$时，接下来再增加$k$，$SSE$下降的幅度会变小，此时不同的$k$和$SSE$的关系图就像手肘的形状。例如下图所示，手肘位置所对应的$k$就是最佳的$k$值。</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/kmeans1.png" alt=""></p>
<ul>
<li>轮廓系数法<br>首先求一个样本点的轮廓系数，然后将所有点的轮廓系数求平均得到整个样本的轮廓系数。<br>怎么求一个样本的轮廓系数？<br>$S_i=\frac{b-a}{max(a,b)}$<br>$a$表示样本$x_i$与相同簇的样本的平均距离。$b$为样本$x_i$与最近簇中样本的平均距离。 因此$a$表示簇内的聚合程度，$b$表示簇间的分离程度。下图中显示了$x_i$的轮廓系数计算方法。平均轮廓系数范围[-1,1]，轮廓系数越大，聚类效果越好，说明簇内越紧密(a越小)，簇间越分离(b越大)。</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/kmeans2.png" alt=""></p>
<p> 从小到大遍历$k$，然后计算所有样本的平均轮廓系数，找出最大轮廓系数对应的$k$.下图给出$k$与轮廓系数的关系：</p>
<p> <img src="/2020/06/19/百面机器学习概述/kmeans3.png" alt=""></p>
<p> 可以看出$k=2$时聚类效果最好。但是$K=2$时SSE却不是最小的，SSE依然很大。猜测是因为：SSE大说明样本间的误差很大。轮廓系数大说明不一定是$a$小，也有可能是$a$很大，但是$b$更大，导致SSE就很大。所以在选择$k$时需要同时看轮廓法和SSE法。</p>
</li>
</ol>
<ul>
<li><strong>确定了k，k个初始点怎么选</strong><br>（1）则k个初始点要尽可能远<br>（2）可以先对数据进行层次聚类，将层次聚类得到的k个聚类中心作为k-means的初始聚类中心</li>
<li><strong>K-Means怎么处理超大数据量的聚类</strong><br>K-Means在大数据量时，因为需要计算每个点到质心的距离，非常耗时。一种解决方案是Mini Batch K-Means小批量K-Means，即在计算距离时不必计算所有的样本点，而是从不同类别的样本中抽出一部分计算。这种方法会减少运行时间，但是会造成准确度下降。<br>步骤：<br>（1）给出k个初始质心<br>（2）从每个簇中选出一部分样本计算与k个质心的距离，分到最近的那个簇中<br>（3）根据小批量样本计算新的质心<br>（4）重复2~4，直到质心不再变化或迭代达到上界</li>
<li><p><strong>K-Means之前需要对数据标准化吗</strong><br>需要，K-Means是建立在距离度量上，如果维度之间的数据量差别太大，则在计算距离时会造成数值大的特征影响特别大。</p>
</li>
<li><p>优点<br>（1）原理简单，易于实现<br>（2）可解释强<br>缺点：<br>（1）容易受初始中心的影响，可能会陷入局部最优<br>（2）由于随机初始化中心点，聚类的结果不稳定<br>（3）k的选取会影响聚类结果<br>（4）对噪声和异常点敏感<br>（5）不适合环形数据，适用与球状数据<br>（6）时间复杂度大O(Nkt),N是样本个数，k是簇个数，t是迭代次数，不适用于大数据量</p>
</li>
</ul>
<p>K-Means手写代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomCenter</span><span class="params">(samples,k)</span>:</span></span><br><span class="line">    <span class="string">"""根据samples随机生成k个簇中心</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        samples (array): 待分类数据集</span></span><br><span class="line"><span class="string">        k (int): 聚类个数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        [array]: shape (K,n)随机生成的簇中心</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#将随机范围控制在样本的分布范围内</span></span><br><span class="line">    m,n = samples.shape</span><br><span class="line">    centers = np.mat(np.zeros((k,n)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):<span class="comment">#遍历每一个特征</span></span><br><span class="line">        max_i = np.max(samples[:,i])</span><br><span class="line">        min_i = np.min(samples[:,i])</span><br><span class="line">        range_i = max_i - min_i</span><br><span class="line">        <span class="comment">#生成K行1列的特征值，random 返回随机的浮点数，在半开区间 [0.0, 1.0)</span></span><br><span class="line">        centers[:,i] = np.mat(min_i+range_i*np.random((k,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> centers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calDistance</span><span class="params">(vecA,vecB)</span>:</span></span><br><span class="line">    <span class="string">"""计算2个向量的欧式距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vecA (array): shape:(n,)</span></span><br><span class="line"><span class="string">        vecB (array): shape:(n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        [float]: 2个向量的欧式距离</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.square(vecA-vecB)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KMeans</span><span class="params">(dataset,K)</span>:</span></span><br><span class="line">    <span class="string">"""对dataset聚成K类</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset (np.array): 待聚类数据集</span></span><br><span class="line"><span class="string">        K (int): 聚类个数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        [tupe]: 输出K个簇中心，输出(m个样本属于哪个簇，m个样本到簇重新的距离)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#m：样本个数，n：特征个数</span></span><br><span class="line">    m,n = dataset.shape</span><br><span class="line">    <span class="comment">#返回结果一共2维，表示样本所属类别、到簇中心的距离</span></span><br><span class="line">    res = np.zeros((m,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    centers = randomCenter(dataset,K)</span><br><span class="line">    clusterChange = <span class="keyword">True</span><span class="comment">#当簇中心不再变化时，停止迭代</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> clusterChange:</span><br><span class="line">        clusterChange = <span class="keyword">False</span></span><br><span class="line">        <span class="comment">#遍历所有样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            min_distance = float(<span class="string">'inf'</span>)<span class="comment">#当前样本距离簇的最小欧式距离</span></span><br><span class="line">            min_cluster = <span class="number">-1</span><span class="comment">#当前样本所属的簇下标</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):<span class="comment">#遍历K个簇</span></span><br><span class="line">                <span class="comment">#计算当前样本到K个簇中心的距离</span></span><br><span class="line">                dis = calDistance(dataset[i,:],centers[j,:])</span><br><span class="line">                <span class="keyword">if</span> dis &lt; min_distance:</span><br><span class="line">                    min_distance = dis</span><br><span class="line">                    min_cluster = j</span><br><span class="line">            <span class="keyword">if</span> res[i,<span class="number">0</span>] != min_cluster:<span class="comment">#如果样本i所属类别改变</span></span><br><span class="line">                clusterChange = <span class="keyword">True</span></span><br><span class="line">            <span class="comment">#更新样本i的聚类结果</span></span><br><span class="line">            res[i,:] = min_cluster,min_distance</span><br><span class="line"></span><br><span class="line">        <span class="comment">#更新簇中心</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">            <span class="comment">#找出属于第i个簇的样本,(z,n)</span></span><br><span class="line">            i_samples = dataset[np.nonzero(res[:,<span class="number">0</span>]==i)[<span class="number">0</span>]]</span><br><span class="line">            centers[i,:]=np.mean(i_samples,axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> centers,res</span><br></pre></td></tr></table></figure>
<h2><span id="42-k-means的优化">4.2. K-Means的优化</span></h2><p>K-Means的优化有以下几方面：</p>
<ol>
<li>数据归一化和离群点处理<br>K-Means是基于距离度量的，聚类之前如果没有归一化，则距离由那些数值大的特征控制，所以需要将数据集归一化到统一量纲。同时离群点和噪声也会对均值产生较大的影响，导致质心偏离，因此需要对数据做预处理。</li>
<li>选择合适K值<br>手肘法画出不同的K和SSE的关系图，但是这种方法不够自动化，可以采用Gap Statistic法</li>
<li>采用核函数<br>K-Means要求数据是球状数据，但是实际情况中并不常见。面对非凸的数据分布，需要适用核函数优化，将样本映射到高维空间，在高维空间进行聚类。</li>
</ol>
<h3><span id="421-k-means">4.2.1. K-Means++</span></h3><p>K-Means++是K-Means的优化模型，用来改进初始化中心的选择。原始K-Means随机选择k个点作为聚类中心。K-Means++的核心思想是：初始的聚类中心之间的相互距离要尽可能的远。K-Means++采用如下方式选取K个中心点：首先随机选第1个中心点，然后选择和第1个中心点距离最远的点作为2个中心点。在选择第3个中心时，计算每个样本点和最近的中心的距离（中心1或中心2），然后选择距离最大的点作为第3个中心点。以此类推，直到找到k个中心点。</p>
<h3><span id="422-isodata">4.2.2. ISODATA</span></h3><p>用来确定K值。ISODATA思想很简单：当属于某个类别的样本过少时，删除该类别。当属于某个类别的样本过多时，将该类别分成2个子类别。ISODATA的缺点是指定参数比较多。下面介绍什么时候分裂，什么时候合并</p>
<ul>
<li>当2个簇的中心距离小于某个阈值时，合并这2个簇</li>
<li>当1个簇的方差大于某个阈值时，说明这个簇已经很分散了，将这个簇分成2个簇</li>
<li>当一个簇中样本个数少于某个阈值时，不再分裂该簇。</li>
</ul>
<h2><span id="43-聚类评价指标">4.3. 聚类评价指标</span></h2><ol>
<li>SSE误差平方和<script type="math/tex; mode=display">SSE=\sum_{i=1}^{k}\sum_{p \in C_i}|p-m_i|^2</script>计算每个样本点与簇中心的距离，SSE越小，聚类效果越好。但是SSE值考虑簇内的紧密程度，没有考虑簇间的分离程度</li>
<li>轮廓系数<br>描述簇内的聚合程度和簇间的分离程度。<br>$s_i=\frac{b-a}{max(b,a)}$<br>表示样本$x_i$的轮廓系数，每个样本点都计算一次轮廓系数。$a$表示样本$x_i$与本簇其他样本的平均距离，表示簇内聚合程度；$b$是$x_i$与最近的簇中所有样本点的平均距离，表示簇间分离程度。$a$越小，$b$越大，说明聚类效果越好。所有样本点都有一个轮廓系数，将其求平均，得到当前聚类的平均轮廓系数。</li>
<li>R方<script type="math/tex; mode=display">RS=\frac{\sum_{x \in D}||x-c||^2-\sum_{i}\sum_{x \in C_i}||x-c_i||^2}{\sum_{x \in D}||x-c||^2}</script>其中$D$表示所有样本，$c$表示所有样本的中心点。$\sum_{x \in D}||x-c||^2$表示将所有样本作为1个簇，计算所有样本点与中心点的平方误差和，然后对样本进行聚类，$\sum_{i}\sum_{x \in C_i}||x-c_i||^2$表示聚类之后的样本误差和。$RS$表示聚类之后的结果和聚类之前的结果相比，平方误差和的改进幅度。</li>
</ol>
<h1><span id="5-贝叶斯分类">5. 贝叶斯分类</span></h1><h2><span id="51-朴素贝叶斯分类器">5.1. 朴素贝叶斯分类器</span></h2><p>先明确几个概念：<br><strong>先验概率</strong>：P(瓜熟)<br><strong>似然</strong>：P(瓜蒂脱落|瓜熟)<br><strong>后验概率</strong>：P(瓜熟|瓜蒂脱落)<br><strong>联合概率</strong>：P(瓜熟，瓜蒂脱落)<br><strong>全概率公式</strong>：P(瓜蒂脱落)=P(瓜蒂脱落|瓜熟)P(瓜熟)+P(瓜蒂脱落|瓜生)P(瓜生)</p>
<p><strong>贝叶斯定理</strong></p>
<script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}</script><p>贝叶斯定理打通了从$P(B|A)$到$P(A|B)$的道路。</p>
<p>如果有多个特征，并且多个特征之间相互独立，就是朴素贝叶斯分类器。<br>例如瓜蒂：脱落/未脱，形状：圆尖，颜色：深绿/浅绿/青。已知10个样本如下图所示，现在有一个瓜瓜蒂脱落，形状圆，颜色青色，判断这个瓜是生还是熟。</p>
<p><img src="/2020/06/19/百面机器学习概述/贝叶斯1.png" alt=""></p>
<p><strong>这个问题就是朴素贝叶斯分类器：有一些已知样本，样本有n个特征，这n个特征之间相互独立，现在给一个测试样本，知道测试样本这n个特征的值，判断这个测试样本属于哪一类。就需要计算这个样本属于每一类的概率，然后取概率最大的那一类作为测试样本最终的类别。朴素贝叶斯是基于贝叶斯定理和特征条件独立的分类方法。</strong></p>
<p>问题的关键是：怎么计算测试样本属于每一类的概率？</p>
<script type="math/tex; mode=display">P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}</script><p>对于每一类来说分母是一样的，只需要在不同的$y_i$中比较分子哪个大就可以了。由于所有的特征相互独立，所以有下式成立：</p>
<p>$P(x|y_i)P(y_i)=P(x_1|y_i)P(x_2|y_i)…P(x_n|y_i)P(y_i)$</p>
<p>由于$P(x_i|y)$也称为“似然”，所以朴素贝叶斯分类器常常和最大化似然联系在一起。</p>
<p>朴素贝叶斯分类的流程如下图所示：</p>
<p><img src="/2020/06/19/百面机器学习概述/贝叶斯2.png" alt=""></p>
<p>朴素贝叶斯分类器有3个阶段：</p>
<ol>
<li>准备工作阶段：获取训练数据</li>
<li>分类器训练阶段：这里并不是真正的训练，只是求先验概率和条件概率，为后续的分类做准备</li>
<li>分类阶段：给定测试样本，对其进行分类。</li>
</ol>
<p>朴素贝叶斯的优点</p>
<ol>
<li>算法简单，坚实的数学基础</li>
<li>在小规模数据上表现好</li>
<li>能处理多分类任务</li>
<li>适用于文本分类任务</li>
</ol>
<p>朴素贝叶斯的缺点：</p>
<ol>
<li>实际应用中，特征往往不是相互独立</li>
<li>对输入数据的格式很敏感（离散、连续）</li>
<li>需要知道先验概率，但先验概率很多时候是基于假设的，会造成分类错误</li>
</ol>
<p>常见问题</p>
<ol>
<li><strong>为什么要引入特征条件独立假设</strong><br>为了避免贝叶斯定理求解时面临的组合爆炸的问题。<br>$P(x|y_i)=P(x_1,x_2,…x_n|y_i)$<br>$x_1,…x_n$表示n个特征，每个特征$s_i$个，$y$的取值有$k$个，则参数个数有$k\prod s_i$，导致条件概率分布的参数数量为指数级别。</li>
</ol>
<ul>
<li><strong>在估计条件概率$P(X|Y)$时出现概率为0怎么办？</strong><br>例如在计算<script type="math/tex; mode=display">P(瓜熟|脱落，圆形，浅绿)=\frac{P(脱落|瓜熟)P(圆形|瓜熟)P(浅绿|瓜熟)P(瓜熟)}{P(脱落，圆形，浅绿)}</script>的概率时，如果$P(浅绿|瓜熟)=0$,这样会造成分子为0，即使其他属性看起来是瓜熟，分类结果都是瓜熟的概率为0。为了避免这一情况，通常使用拉普拉斯修正，在分子分母上加上一个值，避免出现为0的情况<br>修改先验概率公式为：<br>$P(瓜熟)=\frac{瓜熟个数+1}{所有样本+N}$<br>$N$表示类别个数，这里只有2个类别：瓜熟和瓜生，所以N=2<br>修改条件概率公式为：<br>$P(浅绿|瓜熟)=\frac{浅绿且瓜熟个数+1}{瓜熟个数+N_i}$<br>$N_i$表示属性颜色的取值个数，例如这里颜色一共有三个取值：浅绿，深绿，青色，所以$N_i=3$<br>总结一下就是不管是先验概率还是条件概率在分子上都是加1，分母上，先验概率加类别个数，条件概率加属性取值个数。拉普拉斯平滑作用在所有的先验概率和条件概率上，并不是只有为0的概率上。</li>
<li><strong>朴素贝叶斯分类器和逻辑回归的区别</strong><ol>
<li>朴素贝叶斯分类器是生成模型，需要先计算出先验概率和联合概率。逻辑回归是判别式模型，直接求出条件概率。</li>
<li>朴素贝叶斯分类器基于条件独立假设，逻辑回归没有此要求</li>
<li>朴素贝叶斯适用于数据量少的情景，逻辑回归适用于大规模数据集。</li>
</ol>
</li>
<li><p><strong>在贝叶斯定理中，由于特征相互独立，所以会出现连乘的情况，由于概率值很小，连乘之后趋向于0，怎么解决？</strong><br>对乘积取对数，将连乘变成连加：对每个条件概率取对数$P(x_i|y)$，然后再把所有的对数相加</p>
</li>
<li><p><strong>朴素贝叶斯要求特征相互独立，但很多时候并不满足，为什么朴素贝叶斯仍然取得不错的效果？</strong><br>对于分类任务来说，只要各个条件概率的大小排序相对正确，就可以通过比较大小找到正确的类别，并不需要准确的概率值，因为朴素贝叶斯是找后验概率最大的类，不需要知道精确概率值。</p>
</li>
<li><p><strong>朴素贝叶斯有没有超参数</strong><br>没有</p>
</li>
<li><strong>朴素贝叶斯分类器的应用</strong><br>朴素贝叶斯分类器广泛应用在文本分类上，例如垃圾邮件分类，情感分类等</li>
<li><strong>朴素贝叶斯分类器对异常值敏感吗</strong><br>对异常值不敏感。所以在数据处理时，不用去除异常值。</li>
</ul>
<h2><span id="52-半朴素贝叶斯分类器">5.2. 半朴素贝叶斯分类器</span></h2><p>朴素贝叶斯分类器假设属性条件相互独立，但在实际情况中往往不满足这个假设，于是尝试对这个假设进行放松，生成“半朴素贝叶斯分类器”。<br>在贝叶斯定理中，朴素贝叶斯分类器计算分子公式为</p>
<script type="math/tex; mode=display">P(x|y)=P(y)P(x_1|y)P(x_2|y)...P(x_n|y)</script><p>半朴素分类器的基本思想是考虑一部分属性之间的关系，并不是完全独立的。半朴素贝叶斯分类器采用“独依赖估计”策略，即每个属性除了类别之外，最多依赖1个其他属性，因此在计算分子时，变成:</p>
<script type="math/tex; mode=display">P(x|y)=P(y)P(x_1|y,x_j)P(x_2|y,x_j)...P(x_n|y,x_j)</script><p>这里将每个属性依赖的那个属性称为”父属性”。如果知道每个属性的父属性，可以直接计算上述公式，但如果不知道，问题就转化为怎么确定每个属性的父属性，不同的做法产生不同的独依赖分类器（One-Dependent Estimator ODE）。</p>
<p>最直接的做法是所有的属性都依赖同一个父属性，叫做超父ODE（SPODE）</p>
<p>还有一种是计算任意2个属性的相关性，为每个属性选择最大相关性的那个属性作为父属性（TAN ODE）</p>
<h1><span id="6-生成模型和判别模型">6. 生成模型和判别模型</span></h1><p>生成模型是由训练数据学习联合概率$P(X,Y)$，然后求得后验 概率分布$P(Y|X)$，选择后验概率大的那个类作为$X$所属的类。因为对于生成模型，必然要考虑</p>
<script type="math/tex; mode=display">P(Y|X)=\frac{P(X,Y)}{P(X)}</script><p>判别模式是直接求$P(Y|X)$。</p>
<p>生成模型：朴素贝叶斯分类器、贝叶斯网络、隐马尔科夫模型</p>
<p>判别模型：决策树、SVM、神经网络</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>打赏</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Echo 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Echo 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/" rel="next" title="How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey">
                <i class="fa fa-chevron-left"></i> How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/21/python输入怎么写/" rel="prev" title="python输入怎么写">
                python输入怎么写 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/touxiang.jpg" alt="Echo">
            
              <p class="site-author-name" itemprop="name">Echo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiaohuangrenlll@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Echohhhhhh" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">1. 特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.1. 特征归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.2. 类别型特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.3. 文本表示模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.4. Word2Vec</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.5. 图像不足时处理方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">2. 模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.1. 常见评估指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.2. P-R曲线和ROC曲线</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.3. 余弦距离</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.4. A/B测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.5. 模型评估方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.6. 过拟合和欠拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.7. 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.8. 梯度消失和梯度爆炸</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">3. 经典算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.1. 回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.1.1. 单变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.1.2. 多变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.1.3. 带有激活函数的反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.1.4. L1正则和L2正则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.1.5. 面试问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.2. 逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.2.1. 逻辑回归推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.2.2. 逻辑回归常见问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.3. Softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.4. 回归和分类总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.5. 数据不平衡问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.5.1. SMOTE算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.6. SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.6.1. 线性可分SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.6.2. 近似线性SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.6.3. 核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.6.4. 面试问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.7. 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.7.1. 基础树(ID3/C4.5/CART)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">3.7.1.1. ID3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">3.7.1.2. C4.5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">3.7.1.3. CART(分类回归树)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">3.7.1.4. 总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.7.2. 树的剪枝</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.8. K近邻(KNN)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.8.1. KNN原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">3.8.2. KD树</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">4. 聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.1. K-Means聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.2. K-Means的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.2.1. K-Means++</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.2.2. ISODATA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.3. 聚类评价指标</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">5. 贝叶斯分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">5.1. 朴素贝叶斯分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">5.2. 半朴素贝叶斯分类器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">6. 生成模型和判别模型</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Echo</span>

  
</div>









        
<div class="busuanzi-count">
  <!-- <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "topRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
