<!DOCTYPE html>




<html class="theme-next pisces" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png?v=5.1.4">






  <meta name="keywords" content="machine learning,">










<meta name="description" content="总结《百面机器学习》有关知识点。">
<meta name="keywords" content="machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="百面机器学习概述">
<meta property="og:url" content="http://yoursite.com/2020/06/19/百面机器学习概述/index.html">
<meta property="og:site_name" content="Echo&#39;s blog">
<meta property="og:description" content="总结《百面机器学习》有关知识点。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/PR曲线和ROC曲线.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/百面机器学习概述/linear.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/百面机器学习概述/linear1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L1_1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L1_2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L1_3.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/L2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/logistic.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/sigmoid2.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/softmax.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/softmax1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/softmax.jpg">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/多分类.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM3.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM4.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM5.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM6.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM7.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/SVM8.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/kmeans1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/kmeans2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/kmeans3.png">
<meta property="og:updated_time" content="2020-07-02T16:21:27.746Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="百面机器学习概述">
<meta name="twitter:description" content="总结《百面机器学习》有关知识点。">
<meta name="twitter:image" content="http://yoursite.com/2020/06/19/百面机器学习概述/PR曲线和ROC曲线.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/19/百面机器学习概述/">





  <title>百面机器学习概述 | Echo's blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b759ac2a7fa45129e3ef060bf68259f0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Echo's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">远方到底有多远</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/19/百面机器学习概述/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Echo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Echo's blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">百面机器学习概述</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-19T23:12:34+08:00">
                2020-06-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  14.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  52
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>总结《百面机器学习》有关知识点。</p>
<a id="more"></a>
<!-- TOC -->
<ul>
<li><a href="#1-特征工程">1. 特征工程</a><ul>
<li><a href="#11-特征归一化">1.1. 特征归一化</a></li>
<li><a href="#12-类别型特征">1.2. 类别型特征</a></li>
<li><a href="#13-文本表示模型">1.3. 文本表示模型</a></li>
<li><a href="#14-word2vec">1.4. Word2Vec</a></li>
<li><a href="#15-图像不足时处理方法">1.5. 图像不足时处理方法</a></li>
</ul>
</li>
<li><a href="#2-模型评估">2. 模型评估</a><ul>
<li><a href="#21-常见评估指标">2.1. 常见评估指标</a></li>
<li><a href="#22-p-r曲线和roc曲线">2.2. P-R曲线和ROC曲线</a></li>
<li><a href="#23-余弦距离">2.3. 余弦距离</a></li>
<li><a href="#24-ab测试">2.4. A/B测试</a></li>
<li><a href="#25-模型评估方法">2.5. 模型评估方法</a></li>
<li><a href="#26-过拟合和欠拟合">2.6. 过拟合和欠拟合</a></li>
<li><a href="#27-正则化">2.7. 正则化</a></li>
</ul>
</li>
<li><a href="#3-统计学">3. 统计学</a><ul>
<li><a href="#31-参数估计">3.1. 参数估计</a></li>
<li><a href="#32-散度">3.2. 散度</a></li>
<li><a href="#33-优化方法">3.3. 优化方法</a></li>
<li><a href="#34-贝叶斯">3.4. 贝叶斯</a></li>
</ul>
</li>
<li><a href="#4-经典算法">4. 经典算法</a><ul>
<li><a href="#41-回归模型">4.1. 回归模型</a><ul>
<li><a href="#411-单变量线性回归">4.1.1. 单变量线性回归</a></li>
<li><a href="#412-多变量线性回归">4.1.2. 多变量线性回归</a></li>
<li><a href="#413-l1正则和l2正则">4.1.3. L1正则和L2正则</a></li>
<li><a href="#414-面试问题">4.1.4. 面试问题</a></li>
</ul>
</li>
<li><a href="#42-逻辑回归">4.2. 逻辑回归</a><ul>
<li><a href="#421-逻辑回归推导">4.2.1. 逻辑回归推导</a></li>
<li><a href="#422-逻辑回归常见问题">4.2.2. 逻辑回归常见问题</a></li>
</ul>
</li>
<li><a href="#43-softmax回归">4.3. Softmax回归</a></li>
<li><a href="#44-数据不平衡问题">4.4. 数据不平衡问题</a></li>
<li><a href="#45-svm">4.5. SVM</a><ul>
<li><a href="#451-线性可分svm">4.5.1. 线性可分SVM</a></li>
<li><a href="#452-近似线性svm">4.5.2. 近似线性SVM</a></li>
<li><a href="#453-核函数">4.5.3. 核函数</a></li>
<li><a href="#454-面试问题">4.5.4. 面试问题</a></li>
</ul>
</li>
<li><a href="#46-决策树">4.6. 决策树</a><ul>
<li><a href="#461-基础树id3c45cart">4.6.1. 基础树(ID3/C4.5/CART)</a><ul>
<li><a href="#4611-id3">4.6.1.1. ID3</a></li>
<li><a href="#4612-c45">4.6.1.2. C4.5</a></li>
<li><a href="#4613-cart分类回归树">4.6.1.3. CART(分类回归树)</a></li>
<li><a href="#4614-总结">4.6.1.4. 总结</a></li>
</ul>
</li>
<li><a href="#462-树的剪枝">4.6.2. 树的剪枝</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5-聚类">5. 聚类</a><ul>
<li><a href="#51-k-means聚类">5.1. K-Means聚类</a></li>
<li><a href="#52-k-means的优化">5.2. K-Means的优化</a></li>
<li><a href="#53-knn">5.3. KNN</a></li>
<li><a href="#54-聚类评价指标">5.4. 聚类评价指标</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-特征工程">1. 特征工程</span></h1><h2><span id="11-特征归一化">1.1. 特征归一化</span></h2><ul>
<li>目的：消除特征之间量纲的影响，防止学习到的结果向数值大的特征倾斜。</li>
<li>Max-Min归一化：将数据归一化到[0,1]之间，等比缩放。</li>
<li>Z-Score归一化（零均值归一化）：将数据归一化到均值为0，标准差为1的分布上。</li>
<li>需要归一化：线性回归、逻辑回归、SVM、深度神经网络。不需要归一化：基于决策树模型。</li>
</ul>
<h2><span id="12-类别型特征">1.2. 类别型特征</span></h2><p>将类别特征—&gt;数值特征，有以下方法</p>
<ul>
<li>序号编码：有大小关系的，比如低中高，可以编码成1,2,3</li>
<li>one-hot编码：没有大小关系，问题：高维稀疏，需要将其嵌入成低维稠密向量</li>
<li>二进制编码：先编码再转换成二进制</li>
</ul>
<h2><span id="13-文本表示模型">1.3. 文本表示模型</span></h2><ul>
<li>词袋模型：将文章按照词分割，使用每个词的权重来表示文章</li>
<li>TF-IDF：用来计算一个词的权重，TF=一个词w在文章d中出现的频率，IDF：特属词的特征，逆文档频率</li>
<li>N-gram模型：将连续的n个词作为一个特征放在向量中。</li>
<li>词嵌入模型：将词映射成低维稠密向量(50~300维度)，Word2Vec是最常用的词嵌入模型。</li>
</ul>
<h2><span id="14-word2vec">1.4. Word2Vec</span></h2><p>Word2Vec有2种网络结构：CBOW，skip-gram</p>
<ul>
<li>CBOW：多预测一</li>
<li>skip-gram：一预测多</li>
</ul>
<h2><span id="15-图像不足时处理方法">1.5. 图像不足时处理方法</span></h2><p>图像不足易出现过拟合问题。解决图像不足有2种方法：</p>
<ul>
<li>基于模型：采用一些措施降低过拟合。例如简化模型，L1L2正则化、Dropout，集成学习。</li>
<li>基于数据：通过一些方法增加数据。平移、旋转、裁剪、修改图片亮度，锐度、添加噪声等方法。</li>
</ul>
<h1><span id="2-模型评估">2. 模型评估</span></h1><h2><span id="21-常见评估指标">2.1. 常见评估指标</span></h2><ul>
<li>准确率(Accuracy)</li>
<li>精确率(Precision)</li>
<li>召回率(Recall)</li>
</ul>
<p>其中这些指标都有一定的局限性。对于一个排序模型，怎么评估排序模型的好坏：</p>
<ul>
<li>设置不同的N，计算P@N,R@N</li>
<li>设置更综合的评价指标，PR曲线，ROC曲线，F1值</li>
</ul>
<p>RMSE：回归问题的评价指标。如果在一个问题上，RMSE非常高，但是观察预测值和真实值，发现90%的预测值都很接近真实值，为什么RMSE还是这么高。分析由于剩下的10%存在非常大的异常值，即使90%预测很准，但是这10%导致最终的RMSE差别很大。解决方案：</p>
<ul>
<li>如果10%是噪声数据，提前去除噪声</li>
<li>如果10%不是噪声数据，需要对异常数据进行建模</li>
<li>换一个对异常值不敏感的指标，例如MAPE</li>
</ul>
<h2><span id="22-p-r曲线和roc曲线">2.2. P-R曲线和ROC曲线</span></h2><ul>
<li>P-R曲线</li>
</ul>
<p>P-R曲线横坐标是召回率，纵坐标是准确率。<br>为什么要有P-R曲线？因为精确率precision和召回率recall指标都有一定的局限性，所以使用P-R曲线可以综合地评估一个模型的效果。<br>在P-R曲线中，通过改变正负样本间的阈值来改变precision和recall。当判定为正样本的阈值很大时，说明选出的正样本都是很有把握的，precision较大，recall较小。当判定为正样本的阈值很小时，即尽可能不漏掉正样本，导致precision降低，recall变大。</p>
<ul>
<li>ROC曲线<br>横坐标是假阳率FPR，纵坐标是真阳率TPR。</li>
</ul>
<script type="math/tex; mode=display">假阳率=\frac{负例被判为正例}{真正的负例}\quad
真阳率=\frac{正例被判为正例}{真正的正例}</script><p>可以看出真阳率也就是召回率。<br>同P-R曲线类似，ROC曲线也是通过不断改变正负样本的阈值生成的。</p>
<ul>
<li>P-R曲线和ROC曲线有什么不同</li>
</ul>
<p>当测试集中的负样本数量增加10倍时，P-R曲线发生了明显的变化，ROC曲线几乎不变。ROC曲线能够尽量降低测试集带来的干扰，适用于正负样本不均衡的数据集中。ROC适用的场景更多，被广泛应用在排序，推荐，广告等领域。</p>
<p><img src="/2020/06/19/百面机器学习概述/PR曲线和ROC曲线.png" alt=""></p>
<h2><span id="23-余弦距离">2.3. 余弦距离</span></h2><p>余弦相似度：</p>
<script type="math/tex; mode=display">\cos (A, B)=\frac{A \cdot B}{\|A\|_{2}\|B\|_{2}}</script><p>余弦距离体现方向上的相对差异，欧式距离体现数值上的绝对差异。</p>
<h2><span id="24-ab测试">2.4. A/B测试</span></h2><p>划分实验组和对照组<br>实验组和对照组：选取样本时要求独立性和无偏性。</p>
<h2><span id="25-模型评估方法">2.5. 模型评估方法</span></h2><p>通常将数据划分为训练集和测试集，但在<strong>样本划分</strong>和<strong>模型验证</strong>的过程中，存在着不同的方法。</p>
<ol>
<li>Holdout验证<br>随机将数据划分为训练集和测试集，在测试集上进行模型验证。<br>缺点：模型的效果取决于样本划分，具有随机性</li>
<li>k折交叉验证<br>将样本划分为k个大小相等的子集，一个子集作为测试集，k-1个子集作为训练集。将k次评估结果平均作为该模型最终的结果。<br>优点：（1）解决当数据量较小时，模型评估不准确的问题（2）消除数据对评估结果的随机性影响（3）可以用来选择超参数<br>缺点：耗时，需要训练多个模型求评估结果的平均值</li>
<li>自助法<br>如果数据比较小，holdout和k折交叉验证都需要划分数据集，导致训练集变小。自助发采用随机抽样，在全部样本中进行n次有放回的抽样，得到大小为n的训练集。这n个样本中有的样本是重复的，有的样本没有被抽到，这些没有抽到的样本作为测试集。</li>
</ol>
<h2><span id="26-过拟合和欠拟合">2.6. 过拟合和欠拟合</span></h2><ul>
<li>过拟合<br>模型在训练数据上loss很小，在测试集上loss大</li>
<li>欠拟合<br>模型在训练集和测试集上loss都很大，效果都不好</li>
<li><p>降低过拟合的方法</p>
<ol>
<li>增加数据集，减少噪声的影响。如果没有这么多数据，可以生成一些数据。例如图像可以通过平移，旋转等生成数据</li>
<li>正则化，在loss中添加L1或L2正则，防止参数过大</li>
<li>集成学习，将多个模型集成在一起，降低单一模型的过拟合风险</li>
<li>降低模型复杂度，例如减少网络参数，神经元个数等。</li>
<li>早停，当模型在验证机上的loss连续N次没有提升时，停止训练</li>
<li>Dropout，在训练过程中按照给定的概率随机删除隐藏层的一些神经单元。由于模型训练的随机性，减轻了不同特征之间的协同效应。<strong>Dropout只能在训练时使用，在测试集上不能使用</strong></li>
</ol>
</li>
<li><p>降低欠拟合的方法</p>
<ol>
<li>添加新的特征，当特征少时容易出现欠拟合，模型对训练数据的拟合程度不好。</li>
<li>增加模型复杂度</li>
<li>减少正则化系数</li>
</ol>
</li>
</ul>
<h2><span id="27-正则化">2.7. 正则化</span></h2><p>正则化为了避免过拟合，即在损失函数后加上一个正则项（惩罚项），模型越复杂，正则化值就越大。</p>
<script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><p>第一项是损失值，第二项是正则化项，$\lambda$调整两者的权重。下面拿回归问题举例，损失值为平方损失。</p>
<ul>
<li>L1正则<br>惩罚项为权重绝对值的和</li>
</ul>
<script type="math/tex; mode=display">L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\lambda\|w\|_{1}</script><ul>
<li>L2正则</li>
</ul>
<p>惩罚项为权重的平方和</p>
<script type="math/tex; mode=display">L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\frac{\lambda}{2}\|w\|^{2}</script><h1><span id="3-统计学">3. 统计学</span></h1><h2><span id="31-参数估计">3.1. 参数估计</span></h2><h2><span id="32-散度">3.2. 散度</span></h2><h2><span id="33-优化方法">3.3. 优化方法</span></h2><h2><span id="34-贝叶斯">3.4. 贝叶斯</span></h2><h1><span id="4-经典算法">4. 经典算法</span></h1><h2><span id="41-回归模型">4.1. 回归模型</span></h2><p>回归模型分为线性回归和非线性回归。</p>
<p>线性回归又分为单变量线性回归和多变量线性回归。</p>
<h3><span id="411-单变量线性回归">4.1.1. 单变量线性回归</span></h3><p>使用房子面积和房价举例。训练集中x表示房子面积，y表示房价。使用$(x,y)$表示一个训练样，其中$(x^{(i)},y^{(i)})$表示第$i$个训练样本。。</p>
<ul>
<li>$h(x)=w_0+w_1x$，通过训练集学习$w_0,w_1$，使得$x$可以得到对应的$y$。因为这里没有激活函数，$x和y$是线性关系，所以成为单变量线性回归模型，因为输入的特征$x$只有1个特征（房子面积）</li>
<li>损失函数：$Loss = \frac{1}{2m}\sum_{i=1}^{i=m}(h(x^{i})-y^{i})^2$<br>求得是平均损失，有m个样本，所以前面损失除以m，这里的1/2是为了求导简化添加的。</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/百面机器学习/../百面机器学习概述/linear.jpg" alt=""></p>
<h3><span id="412-多变量线性回归">4.1.2. 多变量线性回归</span></h3><p>输入的x有多个特征，上面单变量回归问题中，x只表示房子面积，在多变量线性回归中，x1=房子面积，x2=卧室个数，x3=房子楼层，x4=房子年龄</p>
<ul>
<li>$h(x)=w_0+w_1x_1+w_2x_2+w_3x_3+w_4x_4$<br>令$x=[x_1,x_2,x_3,x_4]$为列向量，$w=[w_1,w_2,w_3,w_4]$为列向量，则$h(x)=w^Tx$</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/百面机器学习/../百面机器学习概述/linear1.jpg" alt=""></p>
<h3><span id="413-l1正则和l2正则">4.1.3. L1正则和L2正则</span></h3><p>L1正则和L2正则都是为了避免过拟合设计的。使用L1正则化的叫做Lasso回归，使用L2正则化的叫做岭回归。<br>这两种回归为了<strong>解决线性回归出现过拟合</strong>的问题。通过<strong>在损失函数中引入正则化项</strong>来解决</p>
<ul>
<li><p>Lasso回归损失函数<br> 添加L1正则项</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}|w_j|</script></li>
<li><p>岭回归损失函数<br> 添加L2正则项</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}w_j^2</script></li>
</ul>
<p>我们从梯度下降的角度分析下为什么加入正则化可以避免过拟合。下面以L2正则化为例。L2正则化的梯度下降公式为</p>
<script type="math/tex; mode=display">w_j = w_j - \alpha * \frac{1}{m}\sum_{i=1}^{i=m}(h(x^i)-y^i)x^i_j-2\lambda w_j</script><p>当惩罚项越大，即$\lambda$越大时，更新后得到的$w_j$也就越小，模型复杂度变小，模型更简单。</p>
<p>下面以线性回归为例，模型只包含2个参数。下面等高线图表示$w_1,w_2$和损失的关系。当$w_1,w_2$取值为图像中最里面紫色圆圈上的点时，损失最小。</p>
<p><img src="/2020/06/19/百面机器学习概述/L1_1.png" alt=""></p>
<p>当加上L1正则化后，目标函数图像为</p>
<p><img src="/2020/06/19/百面机器学习概述/L1_2.png" alt=""></p>
<p>原先损失函数的第一项就是等高线，L1正则化损失函数的第二项就是菱形上的点。图中的菱形函数为$\sum_{j=1}^{j=2}w_j=F$。我们现在求解的目标是不仅要让第一项小，即$w_1,w_2$的值靠近紫色的圆圈，还要让第二项小，即$|w_1|+|w_2|=F$小，即$F$比较小。如下图所示，如果让菱形靠近紫色的圆圈，虽然损失函数第一项小了，但是第二项会变大。因此我们要取到一个恰好的值，让第一项的值+第二项的值最小。<br>我们发现对于同一个等高线来说，即损失函数的第一项相同，当菱形和等高线相切（只有一个交点）时，菱形的边长最小，即$w_1+w_2$最小，也就是相加得到的损失最小。</p>
<p>由上面的说明可以看出，L1正则化后的解一定是某个菱形和某个等高线的切点。经过观察发现，对于图中的每条等高线，与这个等高线相切的菱形的切点一般出现在坐标轴上(x轴或y轴)，例如上面的解为$(0,y)$，这也是说L1正则化更容易得到稀疏解(解向量中0比较多)的原因。</p>
<p><img src="/2020/06/19/百面机器学习概述/L1_3.png" alt=""></p>
<p>当加入L2正则化后，目标函数图像为</p>
<p><img src="/2020/06/19/百面机器学习概述/L2.png" alt=""></p>
<p>菱形变成了圆，同样求等高线和圆的切点作为最终的解。与L1正则化相比，切点不容易出现在坐标轴上，然后仍然比较靠近坐标轴。因此L2正则化得到的解比较小（靠近0），但是比较平滑（不等于0）。所以通过添加L2正则化向，模型不会得到特别大的权重，偏向于学习比较小的权重。因此L2正则化又叫做权重衰减。</p>
<p><strong>总结</strong></p>
<p>L1正则化使权重稀疏，L2正则化使权重平滑。即L1正则化趋向于使用更少的特征，没用的特征权重为0，而L2正则化趋向于使用更多的特征，但这些特征的权重都接近0。实际中最常用L2正则化。</p>
<h3><span id="414-面试问题">4.1.4. 面试问题</span></h3><ol>
<li><p>简单介绍以下线性回归</p>
<ul>
<li>线性：输入x和输出y的关系是线性的，即图像是直线</li>
<li>非线性：输入x和输出y的关系不是一次函数，图像不是直线</li>
<li>线性回归就是利用已有的样本，通过监督学习，学习由x到y的映射，然后利用学到的映射函数对未知的x进行预测。由于预测的值是连续值，所以是回归问题。</li>
</ul>
</li>
<li><p>线性回归的损失函数</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2</script></li>
</ol>
<ol>
<li><p>简述岭回归和Lasso回归<br>这两种回归为了<strong>解决线性回归出现过拟合</strong>的问题。通过<strong>在损失函数中引入正则化项</strong>来解决</p>
<ul>
<li><p>岭回归损失函数<br>添加L2正则项</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}w_j^2</script></li>
<li><p>Lasso回归损失函数<br>添加L1正则项</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^{i=m}(h(x^i)-y^i)^2+\lambda\sum_{j=1}^{j=n}|w_j|</script></li>
</ul>
</li>
<li><p>线性回归的假设<br>线性回归假设因变量y符合正太分布   </p>
</li>
</ol>
<h2><span id="42-逻辑回归">4.2. 逻辑回归</span></h2><p>逻辑回归是分类模型，虽然名字中有回归二字，但却是分类模型，用来二分类任务。<br>二分类的y有正负样本，即$y\in{(0,1)}$，$y$只有2个取值，一般将我们想要找的样本作为正样本。例如垃圾邮件分类中，想要找垃圾邮件，所以将垃圾邮件划分为正样本，非垃圾邮件为负样本。肿瘤良性判断中将恶性肿瘤设置为正样本，良性肿瘤设置为负样本。</p>
<ul>
<li><p>如何用连续的数组预测离散的y</p>
<p>线性回归输出的是连续值，而分类问题的标签y是离散值，属于{0,1}。怎么用回归模型预测离散的标签呢？<br>一个直观的办法是设定一个阈值，比如0，如果预测的值&gt;0，则属于正样本，否则属于负样本。<br>另一种方法是不去直接预测标签，而是预测样本属于正样本的概率。概率是连续值，并且在[0,1]之间。但是回归问题的输出值并不是在[0,1]之间，为了限制回归问题的值域，使用sigmoid函数，又成为logistic函数。因为sigmoid函数可以将输入$x\in[-\infty,\infty]$映射到$[0,1]$之间，输出的$h(x)$正好可以作为样本属于正例的概率。这种方法成为逻辑回归模型=logistic函数+回归模型</p>
</li>
</ul>
<h3><span id="421-逻辑回归推导">4.2.1. 逻辑回归推导</span></h3><p>sigmoid函数：</p>
<script type="math/tex; mode=display">g(x)=\frac{1}{1+e^{-x}}</script><p><img src="/2020/06/19/百面机器学习概述/sigmoid.png" alt=""></p>
<p>输出值在<code>0~1</code>之间。原先的回归问题的假设函数简化为$h(x)=w^Tx$，为了让$h(x)$的输出在0~1之间，在外面套上sigmoid函数。</p>
<script type="math/tex; mode=display">h(x)=\frac{1}{1+e^{-w^{T}x}}</script><p>求得的$h(x)$表示样本$x$被判为正样本的概率，例如$h(x)=0.7$表示邮件是垃圾邮件的概率是0.7。因为真实标签值$y$只能为0和1，所以需要将$h(x)$和$y$对应起来。<br>如果$h(x)&gt;=0.5$，也就是$w^Tx&gt;=0$，则预测$y=1$<br>如果$h(x)&lt;0.5$，也就是$w^Tx&lt;0$，则预测$y=0$</p>
<ul>
<li>逻辑回归损失函数</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/logistic.png" alt=""></p>
<ol>
<li><p>求单个样本预测正确的概率<br>$h(x)$表示样本为正例$(y=1)$的概率$P(y=1|x,w)=h(x)$<br>则样本为负例的概率为$P(y=0|x,w)=1-h(x)$</p>
</li>
<li><p>将上述公式整合</p>
<script type="math/tex; mode=display">P(y \mid {x})=\left\{\begin{array}{r}
h(x), y=1 \\
1-h(x), y=0
\end{array}\right.</script><p>将上述2种情况整合成一个公式为</p>
<script type="math/tex; mode=display">P(y^i|x^i)=h(x^i)^{y_i}*(1-h(x^i))^{1-y^i}</script><p>对于样本$(x^i,y^i)$，如果$y^i=1$，则概率为$h(x^i)$，如果$y^i=0$，则概率为$1-h(x^i)$</p>
</li>
<li><p>求$m$个样本的似然函数</p>
<blockquote>
<p>极大似然估计：利用已知的样本结果，反推最有可能（最大概率）导致这些样本结果出现的模型参数，即在模型已知的情况下，求参数。</p>
</blockquote>
<p>如果有$m$个样本，分别为$(x^1,y^1),(x^2,y^2),…,(x^m,y^m)$，这m个样本假设相互独立，组合概率为每个样本概率的乘积，即最大似然估计为：</p>
<script type="math/tex; mode=display">P_总=P(y^1|x^1)P(y^2|x^2)...P(y^m|x^m)\\
=\prod_{i=1}^{i=m}h(x^i)^{y_i}*(1-h(x^i))^{1-y^i}</script><p>其中<script type="math/tex">h(x)=\frac{1}{1+e^{-w^Tx}}</script></p>
</li>
<li><p>求对数似然<br>模型需要做的就是该概率最大，即连乘的乘积最大，但是连乘很复杂，通过<strong>对两边取对数将连乘变成累加的形式</strong></p>
<script type="math/tex; mode=display">log(P_总)=log(\prod_{i=1}^{i=m}h(x^i)^{y_i}*(1-h(x^i))^{1-y^i})\\
=\sum_{i=1}^{i=m}log(h(x^i)^{y_i}*(1-h(x^i))^{1-y^i})\\=\sum_{i=1}^{i=m}(y^ilog(h(x^i))+(1-y^i)log(1-h(x^i)))</script></li>
<li><p>求逻辑回归损失函数<br>上面的最大化$P_总$其实是我们的目标函数，但是如果在最大化目标函数时，对参数$w$进行求导时，非常复杂，所以就先取对数，将连乘换成累加。然后为了迎合一般都是最小化损失函数，所以加上一个符号。<br>模型最好的效果是让$log(P_总)$越大越好，但是损失函数却是越小越好，所以我们将其取负数作为损失函数，应为损失函数求的是平均误差，所以需要除以样本个数$m$,即逻辑回归的损失函数为交叉熵损失函数</p>
<script type="math/tex; mode=display">Loss(w)=-\frac{1}{m}\sum_{i=1}^{i=m}(y^ilog(h(x^i))+(1-y^i)log(1-h(x^i)))</script><ul>
<li><p>为什么可以用似然函数<br>因为逻辑回归的目标是让预测为正的概率最大，且预测为负的概率最大，即每个样本都要保证得到最大的概率，将所有样本预测后的概率相乘就最大，即得到似然函数。</p>
</li>
<li><p>为什么损失函数要取对数</p>
<ul>
<li>线性回归模型的平方损失函数对sigmoid函数求导无法保证是凸函数，在优化求$w$的过程中，求得的解有可能是局部最优，而不是全局最优</li>
<li>取对数后，方便后续的求导</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>梯度下降</p>
<p>首先先看sigmoid的导数</p>
<script type="math/tex; mode=display">g(x)=\frac{1}{1+e^{-x}}</script><p><img src="/2020/06/19/百面机器学习概述/sigmoid.jpg" alt=""></p>
<p><img src="/2020/06/19/百面机器学习概述/sigmoid1.jpg" alt=""></p>
<p>得到导数之后，更新参数$w_j$<br><img src="/2020/06/19/百面机器学习概述/sigmoid2.jpg" alt=""></p>
<ol>
<li>梯度下降公式中的$m$如果是样本总数，则每次更新参数时需要考虑所有的样本，称为批量梯度下降(BGD)。这种方法容易求得全局最优解，但是由于样本个数太多，训练过程非常慢。</li>
<li>如果$m=1$，即每次更新参数时只考虑一个样本，称为随机梯度下降(SGD)。这种方法训练速度快，但是准确率下降，并不是全局最优。</li>
<li>综上所述，当m为所有样本的一小部分时，比如m=32，即每次更新参数时只考虑一小部分样本，称为小批量梯度下降(MBGD)。它克服了上述两种方法的缺点又兼顾它们的优点，在实际中最常使用。</li>
</ol>
</li>
</ul>
<h3><span id="422-逻辑回归常见问题">4.2.2. 逻辑回归常见问题</span></h3><ol>
<li><p>用一句话概括逻辑回归<br>逻辑回归假设数据服从伯努利分布，通过极大化似然函数，使用梯度下降求解参数，达到二分类的目的。</p>
</li>
<li><p>逻辑回归的目的<br>进行二分类<br>逻辑回归作为回归，输出值是连续的，怎么应用在分类上呢？这里的y确实是一个连续的值，但是它的输出值在[0,1]之间，可以选定一个阈值来进行划分，如果输出值大于0.5，则判定为正样本，否则判定为负样本。</p>
</li>
<li><p>逻辑回归的基本假设</p>
<ul>
<li><p>逻辑回归假设数据服从伯努利分布。伯努利分布就是抛硬币，正面的概率为$p$，反面的概率为$1-p$<br>在逻辑回归中，样本被判定为正例的概率为$h(x)$，则被判为负例的概率为$1-h(x)$</p>
</li>
<li><p>第二个假设是样本为正的概率是</p>
</li>
</ul>
<script type="math/tex; mode=display">p=\frac{1}{1+e^{-w^Tx}}</script></li>
<li><p>逻辑回归的损失函数<br>逻辑回归的损失函数是它的极大对数似然函数的相反数</p>
<script type="math/tex; mode=display">Loss(w)=-\frac{1}{m}\sum_{i=1}^{i=m}(y^ilog(h(x^i))+(1-y^i)log(1-h(x^i)))</script></li>
<li><p>随机梯度下降、批量梯度下降、小批量梯度下降的优缺点</p>
<ul>
<li>梯度下降公式中的$m$如果是样本总数，则每次更新参数时需要考虑所有的样本，称为批量梯度下降(BGD)。这种方法容易求得全局最优解，但是由于样本个数太多，训练过程非常慢。</li>
<li>如果$m=1$，即每次更新参数时只考虑一个样本，称为随机梯度下降(SGD)。这种方法训练速度快，但是准确率下降，并不是全局最优。</li>
<li>综上所述，当m为所有样本的一小部分时，比如m=32，即每次更新参数时只考虑一小部分样本，称为小批量梯度下降(MBGD)。它克服了上述两种方法的缺点又兼顾它们的优点，在实际中最常使用。</li>
</ul>
</li>
<li><p>逻辑回归的优缺点<br>优点：</p>
<ul>
<li>形式简单，模型可解释性好。模型的权重$w$表示不同特征对最终结果的影响，如果$w_j$大，说明第$j$个特征的权重比较高，对最终的结果影响较大</li>
<li>模型效果不错。在工程上可以接受，如果特征工作做的好，效果不会太差。</li>
<li>训练速度快。在分类时，计算两仅仅和特征的数目有关</li>
<li>资源占用小，尤其是内存。因为只需要存储各个维度的特征值</li>
<li>方便输出结果。逻辑回归可以很方便的得到最后的分类结果，因为输出的结果表示每个样本属于正例的概率，可以很容易的对概率进行划分阈值。<br>缺点</li>
<li>准确率不是很高，因为形式简单</li>
<li>很难处理不平衡的数据。距离：如果正负样本比例1:1000，则将所有的样本都预测为负样本，模型的损失值就很小，但是这样的模型对于正样本的召回率并不高</li>
<li>处理非线性数据较麻烦。逻辑回归一般只处理线性可分的数据，一般用于二分类</li>
<li>无法筛选特征，需要提前做特征工程。</li>
</ul>
</li>
</ol>
<ul>
<li><p>逻辑回归的输出是真实概率吗<br>如果数据满足以上的2个假设，则输出的数据表示样本属于正例的概率。但是这2个假设并不是那么容易满足，所以很多情况下，逻辑回归的输出值无法作为真实的概率，只能看做置信度。</p>
</li>
<li><p>使用逻辑回归怎么进行多分类<br>可以将多分类问题转换成二分类问题</p>
</li>
<li><p>逻辑回归和线性回归的区别</p>
<ol>
<li>线性回归阈值$[-\infty,\infty]$,逻辑回归的阈值$[0,1]$</li>
<li>拟合函数不同，线性回归$h(x)=w^Tx$，逻辑回归$h(x)=\frac{1}{1+e^{-w^Tx}}$</li>
<li>损失函数形式不同。线性回归是最小二乘法（平方损失），逻辑回归是极大似然估计</li>
</ol>
</li>
</ul>
<h2><span id="43-softmax回归">4.3. Softmax回归</span></h2><p>逻辑回归用来解决二分类问题，softmax回归用来解决多分类问题。</p>
<p>假设对一个2*2的图像进行分类，判断图像是狗、猫、鸡的哪一种。即输入x有4个特征$x_1,x_2,x_3,x_4$，真实标签$y=1,2,3$。</p>
<p>softmax回归和线性回归一样，也是一个单层全连接层。输入有4个，输出有3个，所以参数有12个。</p>
<p><img src="/2020/06/19/百面机器学习概述/softmax.png" alt=""></p>
<p>像上面的线性回归，输出的y值是连续的，我们可以把输出的y值看做样本属于某一类值置信度。例如$o_1=o_3=10,o_2=1000$，则图片被预测为猫。但是由于输出的值不稳定，并且连续的预测值和真实的离散值误差难以衡量。<br>为了解决这个问题，这里用到softmax运算。softmax将原先输出的连续值$o$转换为概率。</p>
<p><img src="/2020/06/19/百面机器学习概述/softmax1.png" alt=""></p>
<p>最终softmax回归模型函数为</p>
<p>$o^i=W^Tx+b$<br>$y^i=softmax(o^i)$</p>
<p>逻辑回归和softmax回归的不同</p>
<ol>
<li>逻辑回归最后的输出层只有1个神经元，输出的结果表示样本属于正例的概率。softmax回归最后的输出层有n个神经元(n为类别数)，输出样本属于每个类别的概率。即sigmoid输出的是一个数，softmax输出的是一个向量。</li>
<li>逻辑回归最后输出用sigmoid将连续值转换为属于正例的概率。softmax回归最后输出用softmax将连续值转换为属于各类的概率。都是将连续值转换为概率。</li>
</ol>
<ul>
<li>softmax回归是逻辑回归在多分类的拓展</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/softmax.jpg" alt=""></p>
<ul>
<li><p>逻辑回归怎么用来解决多分类问题<br>将多分类问题拆分成多个二分类问题。具体怎么拆分有3种方法：</p>
<ol>
<li>一对一。假设多分类中一共有k类，则将任意2类进行组合，一共形成k(k-1)/2个组合。将一个样本输入到这k(k-1)/2个分类器中，得到k(k-1)/2个分类结果，然后取个数最多的类别作为这个样本最终预测类别</li>
<li>一对多。假设多分类中共有k类，一共形成k个分类器，第一个分类器将k1作为正例，其余类作为负例。第二个分类器将k2作为正例，其余类作为负例，以此类推。一个样本输入到这k个分类器中，输出k个分类结果，如果只有一个正例，则该样本属于这个类，如果输出多个正例，则看置信度。</li>
<li>多对多。每次将多个类作为正例，多个类作为负例。</li>
</ol>
<p><img src="/2020/06/19/百面机器学习概述/多分类.png" alt=""></p>
</li>
<li><p>如果一个样本有多个类别标签，怎么办？</p>
<p>这就不是一个多分类问题了，而是多标签学习。假设一共有k类，训练k个分类器。第$i$个分类器判断每个样本是否归为第$i$类。训练分类器时，需要将标签重新整理为属于第$i$类和不属于第$i$类。</p>
</li>
</ul>
<h2><span id="44-数据不平衡问题">4.4. 数据不平衡问题</span></h2><p>在分类问题中可能会遇到类别不平衡问题。例如1000个样本中，2个正例，998个负例。这样模型在预测值偏向于将样本预测为负例，但是在实际中却没有价值，因为它检测不到正例。</p>
<p>解决正负样本不均衡问题，有以下3个方法</p>
<ol>
<li>欠采样<br>对负样本进行负采样，使得正负样本均衡。欠采样需要丢弃一些负样本，但也不能随意丢弃，否则会丢失一些重要信息。代表算法为EasyEnsemble利用集成学习，将负样本划分为若干个集合进行训练，这样每个模型的数据都进行了欠采样。</li>
<li>过采样<br>生成一些正样本，使得正负样本均衡。但是过采样不能将原先的正样本复制n份，这样会造成严重的过拟合。过采样代表算法为SMOTE通过对训练集中的正样本进行插值生成一些正样本。</li>
<li>调整阈值<br>在逻辑回归中，我们通常将$h(x)&gt;0.5$判定为正例，否则为负例。这说明正例和负例出现的可能性相同。但是如果正负样本不平衡，正样本出现非常小，可能阈值就需要变小。假设正样本:负样本=2:10，则将阈值变成0.2，例$h(x)&gt;0.2$我们就判定为正例，否则为负例。</li>
</ol>
<h2><span id="45-svm">4.5. SVM</span></h2><p>SVM用来做二分类，主要分为三部分</p>
<ol>
<li>数据线性可分：最优划分超平面/硬间隔SVM/线性可分SVM</li>
<li>数据近似线性可分：软间隔SVM/近似线性可分SVM</li>
<li>数据线性不可分：核方法</li>
</ol>
<h3><span id="451-线性可分svm">4.5.1. 线性可分SVM</span></h3><p>SVM就是解决二分类问题，分类学习最基本的想法就是找到一个划分超平面，将不同类别的数据集分开。目的就是找到一个最优的分割线将两类分开。</p>
<p>分类问题有线性可分和线性不可分。线性可分就是在二维空间中能找到一条直线将2类样本划分开。线性不可分就是不能找到一条直线。对于线性不可分的样本通常是通过高斯核函数将样本映射到高维空间，然后转换为高维空间线性可分的问题。<br>下图中的直线都可以将2类样本分开，但是哪个才是好的分割线呢？分类器的价值不在于它多么擅长分割训练样本，而是对于哪些未知的样本它的分类效果怎么样。下图中中间较粗的那条线，在正确划分训练样本的前提下，尽可能地同时远离两个聚类。而其他的线都是有些“倾斜”，一头远离一类，另一头靠近另一类。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM1.png" alt=""></p>
<p>假设划分超平面的公式为</p>
<script type="math/tex; mode=display">w^Tx+b=0</script><p>这就是我们初中学的直线公式$ax+b=0$</p>
<p>理想中超平面进行划分时，如果结果大于0就划分为正例，否则划分为负例。下面就是逻辑回归的划分方式：</p>
<script type="math/tex; mode=display">\left\{\begin{array}{ll}
\omega^{T} x_{i}+b>0, & y_{i}=+1 \\
\omega^{T} x_{i}+b<0, & y_{i}=-1
\end{array}\right.</script><p>而SVM是逻辑回归的强化，SVM设置更严格的划分条件，变成：</p>
<script type="math/tex; mode=display">\left\{\begin{array}{l}
w^{T} x+b \geq 1 \quad y=1 \\
w^{T} x+b \leq-1 \quad y=-1
\end{array}\right.</script><p>上面我们简单的介绍下SVM和逻辑回归的不同，下面我们将详细介绍SVM</p>
<p>假设SVM中的分割超平面为<script type="math/tex">w^Tx+b=0</script></p>
<p>$w=(w_1,w_2,…w_d)$是有个列向量，拆分开就是$w_1x_1+w_2x_2+…+w_dx_d+b=0$</p>
<p>超平面有以下几个性质</p>
<p><strong>性质1</strong>：等比例缩放$w,b$，超平面不变，仍然是同一条直线或超平面<br><strong>性质2</strong>：点$x=(x_1,x_2,…,x_d)$到超平面的距离为</p>
<script type="math/tex; mode=display">\frac{|w_1x_1+w_2x_2+...+w_dx_d+b|}{\sqrt{w_1^2+w_2^2+...+w_d^2}}
=\frac{|w^Tx+b|}{||w||}</script><p>下面介绍2个概念：<strong>函数间隔、几何间隔</strong></p>
<blockquote>
<p>二维平面中点$(x,y)$到$AX+By+c=0$的距离为$\frac{|Ax+By+c|}{\sqrt{A^2+B^2}}$</p>
</blockquote>
<p>  <strong>超平面</strong>：$w^Tx+b=0$<br>  <strong>函数间隔</strong>：$\hat{\gamma}=y(w^Tx+b)=yf(x)=|f(x)|$<br>  函数间隔就是标签$y$乘上$f(x)$的值，函数间隔永远为正。但是由于SVM中$y$的取值只能为1和-1，所以函数间隔的值就是$|f(x)|$<br>  几何间隔表示点到超平面的距离，在SVM中点到超平面的距离为$\frac{|w^Tx+b|}{||w||}=\frac{|f(x)|}{||w||}=\frac{\hat{\gamma}}{||w||}$<br>  <strong>几何间隔</strong>：$\gamma=\frac{|w^Tx+b|}{||w||}=\frac{y(w^Tx+b)}{||w||}=\frac{\hat{\gamma}}{||w||}$</p>
<p>  函数间隔$y(w^Tx+b)$可以表示分类预测的准确性和置信度，值越大越好，说明模型将样本分正确的概率越大，但函数间隔并不表示点到超平面的距离，因为假如将$w,b$成比例变成$2w,2b$，超平面的位置没有变，但是$f(x)$却变成原来的2倍，即函数间隔变成原来的2倍。在实际中，定义点到直线的距离时，用的是几何间隔。$w,b$成倍数增加时，几何间隔不变。函数间隔是几何间隔没有除以$||w||$的表示，几何间隔是函数间隔归一化的结果。函数间隔是我们自己定义的，而几何间隔是客观存在的，无论$w,b$扩大几倍，对几何间隔没有影响。</p>
<p><strong>SVM的目标</strong></p>
<p>SVM的目标是找到一个最优的划分超平面。那怎么定义最优呢？在SVM中评价指标就是几何间隔。在下图中，我们直观的感觉图a的分类效果比b和c好，因为在图a中，与分割线最近的样本点相比b和c来说，这个样本点距离分割线最远。SVM正是遵循这一思想，在众多分割超平面中，找样本点（超平面附近的点）与分割超平面距离最大的那个超平面，即最小几何间隔中，最大的那个。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM3.png" alt=""></p>
<p>SVM的最佳划分超平面需要满足2个条件：（1）能够将所有的正负样本划分正确，即函数间隔大于0，（2）离超平面最近的样本点与超平面的几何距离最大</p>
<p>第一个条件公式为$\hat{\gamma}=y(w^Tx+b)&gt;0$<br>第二个条件为首先找到最近的样本点，即最小的函数间距，然后再最大化函数间距。</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min\tilde{\gamma}^{i}=\min \frac{y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)}{\| w \mid}=\frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m \\
\max _{w, b} \frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m
\end{array}</script><p>结合上面的2个条件，求解最优划分超平面的公式为</p>
<script type="math/tex; mode=display">\begin{aligned}
\max _{w, b} \frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m \\
\text { subject to } y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)>0
\end{aligned}</script><p>但是上面这个目标函数太复杂了，需要对其进行简化。这里就需要用到超平面的性质1：对$w,b$进行缩放，超平面不变。如果在求解的过程中，继续保持这个性质的话，就算上面的公式可以求得最优的划分超平面，但是$w,b$的取值却有无数个，即解不唯一。但是我们要求一个固定的超平面，对应的$w,b$是唯一的，就需要添加限制条件。限制条件可以有多种选择：</p>
<p>选择1：限制$||w||=1$，即超平面的法向量模长为1，这样就消除了等比缩放的影响。但是添加了这个限制对上面公式的简化没有什么帮助<br>选择2：$miny^i(w^Tx^i+b)=1$，限制最小的函数间隔等于1，这也是SVM所采取的方式。如果$\hat{\gamma}=1$，假设将$w,b$放大为$kw,kb$，理论上来说缩放后的函数间隔$\hat{\gamma}_{i+1}=k\hat{\gamma}$，但是我们限制了函数间隔只能为1，即$\hat{\gamma}_{i+1}=\hat{\gamma}=1$，那$k$也只能为1，即不存在等比缩放的问题。</p>
<blockquote>
<p>注意：SVM中将$miny^i(w^Tx^i+b)=1$，其实将值限制为2,3,4…，限制为多少都没关系，都可以消除等比缩放带来的影响，但是不可以不限制。</p>
</blockquote>
<p>有了这个限制条件，原先的目标就需要变了</p>
<p><strong>原先的目标函数</strong>：</p>
<script type="math/tex; mode=display">\begin{aligned}
\max _{w, b} \frac{1}{\|w\|} \min y^{i}\left(w^{T} \mathbf{x}^{i}+b\right), i=1,2, \ldots, m \\
\text { subject to } y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)>0
\end{aligned}</script><p>增加了$min y^i(w^Tx^i+b)=1$的限制，也就是说$y^i(w^Tx^i+b)&gt;=1$，第一个限制条件变成$\max _{w, b} \frac{1}{|w|}$，最大化$\frac{1}{||w||}$其实就是最小化$||w||$，也就是$min \frac{1}{2}||w||^2$，</p>
<p><strong>最终SVM的目标函数</strong></p>
<script type="math/tex; mode=display">\begin{aligned}
\min _{w, b} \frac{1}{2}||w||^2 , i=1,2, \ldots, m \\
\text { subject to } y^{i}\left(w^{T} \mathbf{x}^{i}+b\right)>=1
\end{aligned}</script><p>于是我们得到了3个平面：最优划分超平面$w^Tx+b=0$，与划分超平面间隔平行的2个超平面$w^Tx+b=1$和$w^Tx+b=-1$</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM2.png" alt=""></p>
<p>这是一个凸二次规划的问题，对于一个优化问题，通常可以从2个角度考虑：主问题和对偶问题。常常利用拉格朗日对偶性将主问题转换为对偶问题，通过求解对偶问题的解来得到原始问题的解，这是因为对偶问题的复杂度往往低于原始问题。</p>
<blockquote>
<p>拉格朗日乘子法知识点<br>拉格朗日乘子法将原问题转换为对偶问题进行求解，主问题有等式约束和不等式约束<br>主问题：</p>
<script type="math/tex; mode=display">\begin{array}{c}
\min _{x} f(x)\\
s . t . h_{i}(x)=0(i=1, \ldots, m)\\g_{j}(x) \leq 0(j=1, \ldots, n)
\end{array}</script><p>拉格朗日函数为</p>
<script type="math/tex; mode=display">L(x, \lambda, \mu)=f(x)+\sum_{i=1}^{m} \lambda_{i} h_{i}(x)+\sum_{j=1}^{n} \mu_{j} g_{j}(x)</script><p>由不等式约束引入的KKT条件</p>
<script type="math/tex; mode=display">\left\{\begin{array}{l}
g_{j}(x) \leq 0 \\
\mu_{j} \geq 0 \\
\mu_{j} g_{j}(x)=0
\end{array}\right.</script><p>其中$\lambda=\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)^{T} \text { 和 } \mu=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{n}\right)^{T}$是拉格朗日乘子</p>
</blockquote>
<p>根据上面的知识，我们得到<strong>SVM的拉格朗日函数</strong></p>
<p>[<br>L(\omega, b, \alpha)=\frac{1}{2}|\omega|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\omega^{T} x_{i}+b\right)\right)<br>]<br>其中 $\alpha=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right)$,拉格朗日乘子 $\alpha_{i} \geq 0$</p>
<p>最终求得的$w,b$只和支持向量有关。因此SVM的一个重要性质是：SVM训练完之后，大部分的训练样本不需要保留，最终模型只和支持向量有关。</p>
<h3><span id="452-近似线性svm">4.5.2. 近似线性SVM</span></h3><p>在实际应用中，完全线性可分是很少的，例如下图中，没有一条直线可以将2类完全分开。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM4.png" alt=""></p>
<p>于是就有了软间隔，与线性可分的硬间隔相比，条件没有那么苛刻，我们允许个别样本点出现在隔离带里面，例如下图</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM5.png" alt=""></p>
<p>硬间隔要求所有样本点都满足</p>
<script type="math/tex; mode=display">y^i(w^Tx^i+b)>=1</script><p>软间隔引入松弛变量，允许部分样本点满足</p>
<script type="math/tex; mode=display">y^i(w^Tx^i+b)+\xi_{i}>=1</script><p>当$\xi_{i}=0$时，样本分类正确<br>当0&lt;$\xi_{i}<1$时，样本分类正确 当$\xi_{i}="">=1$时，样本分类正确</1$时，样本分类正确></p>
<p>增加松弛变量后，SVM的目标变成：</p>
<script type="math/tex; mode=display">\begin{aligned}
&\min _{w} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \xi_{i}\\
&\text {s.t.} \quad g_{i}(w, b)=1-y_{i}\left(w^{T} x_{i}+b\right)-\xi_{i} \leq 0, \quad \xi_{i} \geq 0, \quad i=1,2, \ldots, n
\end{aligned}</script><p>松弛变量通过学习得到，且要惩罚大的松弛变量。<br>$C$是一个大于0的数，表示错误样本的惩罚程度，当$C$无穷大时，表示对错误样本的惩罚无穷大，不允许分错样本，这样$\xi_i$就无穷小，就是线性可分SVM。当$C$为有限值时，才会允许部分样本分错。</p>
<p>注意：在间隔内的那部分样本点是不是支持向量？是</p>
<h3><span id="453-核函数">4.5.3. 核函数</span></h3><p>上面讨论的硬间隔和软间隔都是样本完全线性可分或者近似线性可分的，但是也会遇到样本不是线性可分的，例如下图：</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM6.png" alt=""></p>
<p>解决方案：将样本使用核函数，向高维空间转化，使得在高维空间中线性可分。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM7.png" alt=""></p>
<p>核函数的好坏对SVM至关重要。若核函数选择不合适，将样本映射到一个不合适的特征空间，很有可能分类效果不佳。</p>
<p><img src="/2020/06/19/百面机器学习概述/SVM8.png" alt=""></p>
<h3><span id="454-面试问题">4.5.4. 面试问题</span></h3><ul>
<li><p><strong>介绍函数间隔和几何间隔</strong></p>
<blockquote>
<p>二维平面中点$(x,y)$到$AX+By+c=0$的距离为$\frac{|Ax+By+c|}{\sqrt{A^2+B^2}}$</p>
</blockquote>
<p><strong>超平面</strong>：$w^Tx+b=0$<br><strong>函数间隔</strong>：$\hat{\gamma}=y(w^Tx+b)=yf(x)=|f(x)|$<br>函数间隔就是标签$y$乘上$f(x)$的值，函数间隔永远为正。但是由于SVM中$y$的取值只能为1和-1，所以函数间隔的值就是$|f(x)|$<br>几何间隔表示点到超平面的距离，在SVM中点到超平面的距离为$\frac{|w^Tx+b|}{||w||}=\frac{|f(x)|}{||w||}=\frac{\hat{\gamma}}{||w||}$<br><strong>几何间隔</strong>：$\gamma=\frac{|w^Tx+b|}{||w||}=\frac{y(w^Tx+b)}{||w||}=\frac{\hat{\gamma}}{||w||}$</p>
<p>函数间隔$y(w^Tx+b)$可以表示分类预测的准确性和置信度，值越大越好，说明模型将样本分正确的概率越大，但函数间隔并不表示点到超平面的距离，因为假如将$w,b$成比例变成$2w,2b$，超平面的位置没有变，但是$f(x)$却变成原来的2倍，即函数间隔变成原来的2倍。在实际中，定义点到直线的距离时，用的是几何间隔。$w,b$成倍数增加时，几何间隔不变。函数间隔是几何间隔没有除以$||w||$的表示，几何间隔是函数间隔归一化的结果。函数间隔是我们自己定义的，而几何间隔是客观存在的，无论$w,b$扩大几倍，对几何间隔没有影响。</p>
</li>
<li><p><strong>SVM中$w^Tx+b&gt;1$，$w^Tx+b&lt;-1$设置为正负样本的阈值，为什么是正负1</strong><br>SVM最终学习到的是一个超平面，至于系数是$(w^T,b)$还是$(2w^T,2b)$都不重要，所以需要对$w,b$进行限制，这SVM对函数间隔进行限制为$y(w^Tx+b)=1$，这样在求得的解中$w,b$就是唯一的。当然$y(w^Tx+b)$也不一定为1，也可以为2，3或其他值</p>
</li>
<li><p><strong>SVM什么时候选择线性核函数，什么时候选择高斯核函数</strong><br>当样本量比较小，特征量比较大时使用线性核函数。因为此时特征空间已经很高维了，只是数据量不够，线性核函数足够了。如果用高斯核函数投影到高维容易出现过拟合。<br>当数据量比较大而特征量比较小时，使用高斯核函数，需要投影到高维特征空间。</p>
</li>
<li><p><strong>使用高斯核函数之前需要对数据进行处理吗</strong><br>需要对特征进行缩放，因为高斯核函数需要计算两个点之间的欧式距离，如果不特征缩放的话那些值特别大的特征将会对核函数的结果有决定性影响，而数据量小的特征将被忽略。</p>
</li>
<li><p><strong>SVM如何解决数据不均衡问题</strong><br>数据不均衡在SVM中导致的主要问题是数据量少的样本分布空间不如数据量多的样本。为了能够让分割超平面向数据量少的样本偏移，可以给样本少的分类更大的惩罚因为$C$,表示如果数据少的样本分错了将会有很大的惩罚，使模型更重视数量少的样本。</p>
</li>
<li><strong>SVM原始问题为什么要转化为对偶问题求解？</strong><ol>
<li>改变算法复杂度，对偶问题更容易求解。因为算法的复杂度与样本维度有关，在对偶问题下，算法复杂度和样本数量有关。如果是线性回归，样本维度小于样本数量，在原问题上求解就可以了。但如果是非线性回归，就涉及到升维，例如使用高斯核函数，将样本升到很高维，升维后的样本维度远远大于样本数量，这是显然在对偶问题下更好求解。</li>
<li>转化为对偶问题才能得到内积形式，引入核函数，进而推广到非线性分类问题中。</li>
</ol>
</li>
<li><strong>SVM为什么采用间隔最大化</strong><br>当数据线性可分时，有无穷多个超平面可以将样本划分开，利用间隔最大化可以求得最优的划分超平面。此时的划分超平面的分类结果是最鲁棒的，对未知数据的泛化能力最强。</li>
<li><strong>为什么SVM引入核函数</strong><br>当样本在原始空间中线性不可分时，通过核函数将样本映射到更高维的特征空间中，样本在这个特征空间中线性可分或近似线性可分。</li>
<li><strong>核函数也可以应用在别的分类模型中，为什么在逻辑回归中不用核函数呢？</strong><br>因为核函数将样本维度映射到很高维。在SVM中只有支持向量决定了划分超平面，只有少数样本参与核计算，在计算核函数时优势很大。但是逻辑回归是所有的样本都决定了划分超平面，如果采用核函数，那每个样本都参数核运算，则非常耗时。</li>
<li><strong>SVM为什么对缺失值敏感</strong><br>这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。</li>
<li><strong>SVM和逻辑回归的不同</strong><ol>
<li>SVM只有支持向量对模型有影响，即只有支持向量决定划分超平面。逻辑回归是所有点都影响划分超平面</li>
<li>损失函数不同。SVM采用hinge损失，逻辑回归采用对数损失<br>逻辑回归的目标函数 <script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]</script>SVM的目标函数<script type="math/tex; mode=display">\mathcal{L}(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)</script>逻辑回归是最大化似然，求得参数的值<br>SVM是最大化几何间隔，求得参数的值</li>
<li>输出不同。SVM输出0/1，逻辑回归输出样本属于正例的概率</li>
<li>处理非线性问题的能力不同。SVM通过核函数将非线性问题转化为线性问题。逻辑回归需要活动特征转换。</li>
<li>LR对异常值敏感；SVM相对不敏感，泛化能力好</li>
</ol>
</li>
<li><p>SVM的优缺点<br>优点：</p>
<ol>
<li>可以高效地解决高维特征的分类和回归</li>
<li>只依赖支持向量，无需全部样本</li>
<li>通过核函数可以处理线性不可分问题<br>缺点：</li>
<li>样本量巨大时，不适合用</li>
<li>SVM对缺失值敏感</li>
<li>对核函数没有选择的标准</li>
</ol>
</li>
</ul>
<h2><span id="46-决策树">4.6. 决策树</span></h2><p>决策树是一个非常常见的机器学习算法，易于理解，可解释强，可以作为分类算法和回归算法。</p>
<p>首先了解一个概念：信息熵</p>
<p>一个事情发生的概率越大，这个事情所携带的信息熵越小。<br>例如一个人告诉你明天太阳从东方升起，那这个人相当于说了句废话，这个话的信息熵为0。</p>
<h3><span id="461-基础树id3c45cart">4.6.1. 基础树(ID3/C4.5/CART)</span></h3><h4><span id="4611-id3">4.6.1.1. ID3</span></h4><p>ID3根据信息增益进行划分</p>
<p>信息增益是用来选择特征的一个指标，信息增益越大，说明这个特征带的信息越多，该特征越重要。</p>
<p>信息增益=信息熵-条件熵<br>表示在一个条件下，事情不确定性减少的程度。<br>例如相亲问题中，在南方会不会写代码这个条件下，女方见不见。<br>如果男方会写代码，女方就见。不会写代码，女方就不见。说明这个特征对结果很重要，信息增益很大。0&lt;=信息增益的取值&lt;=1</p>
<p>缺点：</p>
<ul>
<li>信息增益对取值数据较多的特征有多偏好。例如编号，每个人的编号都不一样，编号的取值很多，则编号的信息增益接近1，形成只有2层的决策树。虽然信息增益很大，但是对于数据集的分割却没有意义。</li>
<li>信息增益只能处理离散的特征，不能处理连续特征</li>
<li>只能处理分类问题，不能处理回归问题</li>
<li>对缺失值敏感</li>
</ul>
<h4><span id="4612-c45">4.6.1.2. C4.5</span></h4><p>C4.5根据信息增益比进行划分</p>
<p>为了解决ID3对取值多的特征偏好这一缺点，引入信息增益比。<br>信息增益比就是对取值很多的特征进行惩罚。举个例子，假设有100个样本，对这100个样本进行均分。如果特征1的取值有2个，则将样本分成50,50的子集。如果特征2的取值有4个，则将样本分成25,25,25,25的子集。可以看出特征的取值越多，就会生成越多的小子集。但是如果小子集越多的话，就会出现过拟合的问题。所以需要对这样的特征添加个惩罚项。这个特征的取值个数越多，惩罚越大。</p>
<script type="math/tex; mode=display">信息增益比=\frac{信息增益}{惩罚项}</script><p>ID3中如果特征取值个数很多，则信息增益越大，但是这个的惩罚项也就越大，就可以让信息增益比平衡。</p>
<p>缺点：<br>虽然解决了特征取值多的问题，但是仍然有以下问题</p>
<ul>
<li>只能处理离散的特征，不能处理连续特征</li>
<li>需要计算信息增益，计算量大</li>
<li>只能处理分类问题，不能处理回归问题</li>
<li>对缺失值有处理</li>
</ul>
<h4><span id="4613-cart分类回归树">4.6.1.3. CART(分类回归树)</span></h4><p>CART根据基尼指数(Gini)进行划分</p>
<p>基尼指数描述分类结果的不确定性，和信息熵类似。划分时选择基尼指数小的特征进行划分。0&lt;=基尼指数&lt;=1，</p>
<ul>
<li>CART划分的树是二叉树，因此CART每次分类时会将这个特征的特征值分成两堆，划分成左子树和右子树。然后在子树中还会用到这个特征计算基尼指数，因此CART中特征会被重复利用。但是在ID3和C4.5中，选中一个特征后，会划分为多个子树，例如这个特征有5个取值，就会划分成5个子树，在下一层中就不会计算这个特征的信息增益和信息增益比，因此特征只计算一次。</li>
<li>可以处理连续值，因此既可以分类，也可以回归。</li>
<li>对缺失值有处理</li>
</ul>
<h4><span id="4614-总结">4.6.1.4. 总结</span></h4><p>ID3：信息增益<br>C4.5：信息增益比<br>CART：最小基尼指数</p>
<p>以上3个指标都是描述特征的。</p>
<p>以上三种方法都是对数据划分的方法，通过选择特征将数据划分为更小的子集。</p>
<p>ID3选择信息增益大的特征来划分，但是会偏好特征取值多的特征，引入C4.5，使用信息增益比，对取值多的特征添加惩罚项。但是以上2种方法都只能处理分类问题，后来引入CART，计算基尼指数，选择基尼指数最小的特征进行划分。</p>
<h3><span id="462-树的剪枝">4.6.2. 树的剪枝</span></h3><p>一棵完全生长的决策树会出现过拟合的问题。例如如果按照“编号”进行划分树，则每个节点只包含1个样本，在测试集上的效果将会很差，出现过拟合现象。为了解决这个问题，对决策树进行剪枝，减掉一些枝叶。剪枝方法有2种方式：预剪枝、后剪枝</p>
<ul>
<li><p>预剪枝<br>在生成决策树的时候进行剪枝。在按照某个特征划分之前，先计算当前的划分是否能够带来模型泛化性能的提升，如果不能，则不再生成子树。此时该节点中包含的样本可能属于不同的类别，按照多数投票的原则，将该节点判为多数类别。怎么判断不再划分树：</p>
<ol>
<li>当树的深度达到一定深度</li>
<li>当节点样本个数小于某个阈值</li>
<li><p>计算每次分裂对测试集的准确率是否提升，当提升小于某个阈值时，停止划分</p>
<p>缺点：<br>可能会出现欠拟合的问题，应为当前划分可能造成准确率变小，但是可能后续会让准确率变大，但是树的划分已经止步于此了。</p>
</li>
</ol>
</li>
<li><p>后剪枝<br>先生成一个完整的决策树，然后自底向上的进行剪枝。通过在测试集上的准确率是否提升来判断是否剪枝该子树。<br>后剪枝的欠拟合风险很小，泛化能力优于预剪枝，但是开销比较大。</p>
</li>
</ul>
<h1><span id="5-聚类">5. 聚类</span></h1><p>聚类是无监督学习，即不知道样本的标签。</p>
<h2><span id="51-k-means聚类">5.1. K-Means聚类</span></h2><p>算法步骤:</p>
<ol>
<li>随机选k个点，作为聚类中心。这k个点不一定是样本点，可以是任意一点</li>
<li>计算每个点到k个聚类中心的距离，然后将这个点分到最近的类中</li>
<li>重新计算每个簇的中心（簇中心为所有点的均值）</li>
<li>重复2~4，直到聚类中心不再发生改变或达到迭代次数</li>
</ol>
<p>常见问题</p>
<ol>
<li><p><strong>K-Means常用的距离度量有哪些</strong></p>
<ul>
<li>曼哈顿距离<br>$d12=|x_1-x_2|+|y_1-y_2|$ </li>
<li>欧式距离<br>$d12=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$<br>欧式距离体现2个向量距离上的绝对差异</li>
<li>余弦相似度<br>$con(A,B)=\frac{AB}{||A||*||B||}$<br>相比欧式距离，余弦距离更关注2个向量在方向上的相对差异。2个向量夹角越小，余弦距离越大</li>
</ul>
</li>
<li><p>K-Means中k如何选择</p>
<ul>
<li>根据场景选择</li>
<li>手肘法<br>计算指标SSE（误差平方和）<script type="math/tex; mode=display">SSE=\sum_{i=1}^{k}\sum_{p \in C_i}|p-m_i|^2</script>一共有k个簇，$C_i$为第$i$个簇，$p$是第$i$个簇的样本，$m_i$是第$i$第个簇的中心。$SSE$表示所有样本的误差平方和，表示聚类结果的好坏。核心思想：随着$k$的增大，每个簇中的样本越来越小，那每个簇越聚集，所有样本点的误差平方和$SSE$逐渐变小。假设理想情况下聚成$k’$类。我们从小到大遍历$k$，当$k&lt;k’$时，随着$k$的增加每个簇的聚合程度会迅速增加，那$SSE$会大幅度下降。当$k=k’$时，接下来再增加$k$，$SSE$下降的幅度会变小，此时不同的$k$和$SSE$的关系图就像手肘的形状。例如下图所示，手肘位置所对应的$k$就是最佳的$k$值。</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/kmeans1.png" alt=""></p>
<ul>
<li>轮廓系数法<br>首先求一个样本点的轮廓系数，然后将所有点的轮廓系数求平均得到整个样本的轮廓系数。<br>怎么求一个样本的轮廓系数？<br>$S_i=\frac{b-a}{max(a,b)}$<br>$a$表示样本$x_i$与相同簇的样本的平均距离。$b$为样本$x_i$与最近簇中样本的平均距离。 因此$a$表示簇内的聚合程度，$b$表示簇间的分离程度。下图中显示了$x_i$的轮廓系数计算方法。平均轮廓系数范围[-1,1]，轮廓系数越大，聚类效果越好，说明簇内越紧密(a越小)，簇间越分离(b越大)。</li>
</ul>
<p><img src="/2020/06/19/百面机器学习概述/kmeans2.png" alt=""></p>
<p> 从小到大遍历$k$，然后计算所有样本的平均轮廓系数，找出最大轮廓系数对应的$k$.下图给出$k$与轮廓系数的关系：</p>
<p> <img src="/2020/06/19/百面机器学习概述/kmeans3.png" alt=""></p>
<p> 可以看出$k=2$时聚类效果最好。但是$K=2$时SSE却不是最小的，SSE依然很大。猜测是因为：SSE大说明样本间的误差很大。轮廓系数大说明不一定是$a$小，也有可能是$a$很大，但是$b$更大，导致SSE就很大。所以在选择$k$时需要同时看轮廓法和SSE法。</p>
</li>
</ol>
<ul>
<li>确定了k，k个初始点怎么选<br>（1）则k个初始点要尽可能远<br>（2）可以先对数据进行层次聚类，将层次聚类得到的k个聚类中心作为k-means的初始聚类中心</li>
<li>K-Means怎么处理超大数据量的聚类<br>K-Means在大数据量时，因为需要计算每个点到质心的距离，非常耗时。一种解决方案是Mini Batch K-Means小批量K-Means，即在计算距离时不必计算所有的样本点，而是从不同类别的样本中抽出一部分计算。这种方法会减少运行时间，但是会造成准确度下降。<br>步骤：<br>（1）给出k个初始质心<br>（2）从每个簇中选出一部分样本计算与k个质心的距离，分到最近的那个簇中<br>（3）根据小批量样本计算新的质心<br>（4）重复2~4，直到质心不再变化或迭代达到上界</li>
</ul>
<h2><span id="52-k-means的优化">5.2. K-Means的优化</span></h2><h2><span id="53-knn">5.3. KNN</span></h2><ul>
<li>KNN和K-Means有什么不同</li>
<li></li>
</ul>
<ol>
<li>K-Means优缺点<br>优点<br>（1）原理简单，易于实现<br>（2）可解释强<br>缺点：<br>（1）容易受初始中心的影响，可能会陷入局部最优<br>（2）k的选取会影响聚类结果<br>（3）对噪声和异常点敏感<br>（4）不适合环形数据</li>
</ol>
<p>手写K-Means算法</p>
<p><a href="https://zhuanlan.zhihu.com/p/114047758" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/114047758</a></p>
<h2><span id="54-聚类评价指标">5.4. 聚类评价指标</span></h2><p><a href="https://www.dataapplab.com/machine-learning-interview-questions/" target="_blank" rel="noopener">https://www.dataapplab.com/machine-learning-interview-questions/</a></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>打赏</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Echo 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Echo 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/07/Leetcode之位运算/" rel="next" title="Leetcode之位运算">
                <i class="fa fa-chevron-left"></i> Leetcode之位运算
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/21/python输入怎么写/" rel="prev" title="python输入怎么写">
                python输入怎么写 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/touxiang.jpg" alt="Echo">
            
              <p class="site-author-name" itemprop="name">Echo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">62</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiaohuangrenlll@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Echohhhhhh" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">1. 特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.1. 特征归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.2. 类别型特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.3. 文本表示模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.4. Word2Vec</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">1.5. 图像不足时处理方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">2. 模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.1. 常见评估指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.2. P-R曲线和ROC曲线</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.3. 余弦距离</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.4. A/B测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.5. 模型评估方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.6. 过拟合和欠拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">2.7. 正则化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">3. 统计学</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.1. 参数估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.2. 散度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.3. 优化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">3.4. 贝叶斯</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">4. 经典算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.1. 回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.1.1. 单变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.1.2. 多变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.1.3. L1正则和L2正则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.1.4. 面试问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.2. 逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.2.1. 逻辑回归推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.2.2. 逻辑回归常见问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.3. Softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.4. 数据不平衡问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.5. SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.5.1. 线性可分SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.5.2. 近似线性SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.5.3. 核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.5.4. 面试问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">4.6. 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.6.1. 基础树(ID3/C4.5/CART)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">4.6.1.1. ID3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">4.6.1.2. C4.5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">4.6.1.3. CART(分类回归树)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-text">4.6.1.4. 总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#undefined"><span class="nav-text">4.6.2. 树的剪枝</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-text">5. 聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">5.1. K-Means聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">5.2. K-Means的优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">5.3. KNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text">5.4. 聚类评价指标</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Echo</span>

  
</div>









        
<div class="busuanzi-count">
  <!-- <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "topRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
