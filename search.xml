<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>openpai</title>
    <url>/2020/01/10/openpai/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;最近实验室安装了openpai平台，可以在上面提交程序运行。下面记录怎么使用OpenPai提交NNI程序，进行调参。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e4%bd%bf%e7%94%a8%e6%ad%a5%e9%aa%a4">1. 使用步骤</a><ul>
<li><a href="#11-%e7%bc%96%e5%86%99%e7%a8%8b%e5%ba%8f">1.1. 编写程序</a></li>
<li><a href="#12-%e5%87%86%e5%a4%87%e9%95%9c%e5%83%8f">1.2. 准备镜像</a></li>
<li><a href="#13-%e7%bc%96%e5%86%99nni%e7%9a%84yml%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">1.3. 编写NNI的yml配置文件</a></li>
<li><a href="#14-%e5%ae%89%e8%a3%85nni">1.4. 安装NNI</a></li>
<li><a href="#15-%e5%90%af%e5%8a%a8nni">1.5. 启动NNI</a></li>
<li><a href="#16-nni%e6%b5%8f%e8%a7%88%e5%99%a8%e6%9f%a5%e7%9c%8b">1.6. NNI浏览器查看</a></li>
<li><a href="#17-%e5%9c%a8%e6%b5%8f%e8%a7%88%e5%99%a8%e4%b8%ad%e6%9f%a5%e7%9c%8bopenpai">1.7. 在浏览器中查看OpenPai</a></li>
<li><a href="#18-%e5%85%b3%e9%97%adnni">1.8. 关闭NNI</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-使用步骤">1. 使用步骤</span></h1><h2><span id="11-编写程序">1.1. 编写程序</span></h2><p>   先在VSCode中完成代码，先在VSCode的虚拟环境中运行，如果可以运行，再使用OpemPai运行。<br>   <strong>注：在OpenPai上运行程序，不需要指定使用哪块GPU，因为OpenPai会自动申请需要使用的GPU。即以下代码注释掉</strong></p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "1,2,3"</span></span><br></pre></td></tr></table></figure>
<h2><span id="12-准备镜像">1.2. 准备镜像</span></h2><p>   准备一个包含hdfs的镜像，将需要用的镜像push到实验室服务器的仓库。<br>   下面是我本人的镜像：</p>
<ul>
<li>运行环境mxnet<br> <code>lin-ai-27:5000/wangbeibei/mxnet:cu100_hdfs</code></li>
<li>运行环境是pytorch<br> <code>172.31.246.45:5000/dlspree:hdfs_pyg</code></li>
</ul>
<h2><span id="13-编写nni的yml配置文件">1.3. 编写NNI的yml配置文件</span></h2><p>   使用<code>pip install nni==1.2</code>安装1.2版本的nni，如果不指定版本，默认安装最新版，目前最新是1.3，1.3版本的nni其yml配置文件和1.2有所区别<br>   <a href="https://nni.readthedocs.io/zh/latest/Tutorial/ExperimentConfig.html#openpai" target="_blank" rel="noopener">OpenPai模式</a><br>   1.3版本的nni的yml配置文件和1.2有所不同,<a href="https://nni.readthedocs.io/zh/latest/TrainingService/PaiMode.html" target="_blank" rel="noopener">最新版本的配置文件</a>，其中多了<code>nniManagerNFSMountPath,containerNFSMountPath,paiStoragePlugin</code>三个必填的键。<br>   下面使用的是1.2版本的nni配置文件</p>
   <figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">authorName:</span> <span class="string">wangbeibei</span></span><br><span class="line"><span class="attr">experimentName:</span><span class="string">hetero_convlstm_grid_experiment</span></span><br><span class="line"><span class="comment">#并发尝试任务的最大数量，有1张gpu卡就是1，有2gpu卡就是2</span></span><br><span class="line"><span class="attr">trialConcurrency:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">maxExecDuration:</span> <span class="number">100</span><span class="string">h</span></span><br><span class="line"><span class="comment">#Trial 任务的最大数量，成功和失败的都计算在内</span></span><br><span class="line"><span class="attr">maxTrialNum:</span> <span class="number">600</span></span><br><span class="line"><span class="comment">#choice: local, remote, pai</span></span><br><span class="line"><span class="attr">trainingServicePlatform:</span> <span class="string">pai</span></span><br><span class="line"><span class="comment"># 指定nni管理器ip 为29号服务器</span></span><br><span class="line"><span class="attr">nniManagerIp:</span> <span class="number">202.205</span><span class="number">.99</span><span class="number">.174</span></span><br><span class="line"><span class="attr">searchSpacePath:</span><span class="string">hetero_convlstm_search_space.json</span></span><br><span class="line"><span class="comment">#choice: true, false</span></span><br><span class="line"><span class="attr">useAnnotation:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># 存储日志和数据的目录, 默认值是 /dataWangBeibei/nni/  experiments</span></span><br><span class="line"><span class="comment">#如果在虚拟环境中运行该代码，logDir是服务器下绝对路径，/data/ WangBeibei/graduation/Codenni_save_logs</span></span><br><span class="line"><span class="comment">#如果在docker下运行该代码，logDir是容器下的绝路径，/root/ Code/nni_save_logs/</span></span><br><span class="line"><span class="attr">logDir:</span> <span class="string">/data/WangBeibei/graduation/Codenni_save_logs</span></span><br><span class="line"><span class="attr">tuner:</span></span><br><span class="line">  <span class="comment">#choice: TPE, Random, Anneal, Evolution,BatchTuner,  MetisTuner, GPTuner</span></span><br><span class="line">  <span class="comment">#SMAC (SMAC should be installed throughnnictl)</span></span><br><span class="line"><span class="attr">  builtinTunerName:</span> <span class="string">TPE</span></span><br><span class="line"><span class="attr">  classArgs:</span></span><br><span class="line">    <span class="comment">#choice: maximize, minimize</span></span><br><span class="line"><span class="attr">    optimize_mode:</span> <span class="string">minimize</span></span><br><span class="line"><span class="attr">trial:</span></span><br><span class="line">  <span class="comment"># 指定了运行 Trial 进程的命令行</span></span><br><span class="line"><span class="attr">  command:</span> <span class="string">cd</span> <span class="string">baseline</span> <span class="string">&amp;&amp;</span> <span class="string">python3</span>  <span class="string">hetero_convlstm_baseline.py</span></span><br><span class="line">  <span class="comment">#指定了 Trial 代码文件的目录,../会进入到Cod目录下</span></span><br><span class="line"><span class="attr">  codeDir:</span> <span class="string">../</span></span><br><span class="line"><span class="attr">  gpuNum:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  cpuNum:</span> <span class="number">8</span></span><br><span class="line"><span class="attr">  memoryMB:</span> <span class="number">14000</span></span><br><span class="line">  <span class="comment"># docker 镜像地址</span></span><br><span class="line">  <span class="comment">#pytorch镜像：172.31.246.45:5000dlspree:hdfs_pyg</span></span><br><span class="line">  <span class="comment">#mxnet镜像：lin-ai-27:5000/wangbeibeimxnet:cu100_hdfs</span></span><br><span class="line"><span class="attr">  image:</span> <span class="number">172.31</span><span class="number">.246</span><span class="number">.45</span><span class="string">:5000/dlspree:hdfs_pyg</span></span><br><span class="line"><span class="comment"># 配置访问的 OpenPAI 集群</span></span><br><span class="line"><span class="attr">paiConfig:</span></span><br><span class="line">  <span class="comment">#OpenPai网页的用户名和密码，也是53号服务器用户名和密码</span></span><br><span class="line"><span class="attr">  userName:</span> <span class="string">user</span></span><br><span class="line">  <span class="comment"># 密码如果是全数字需要 ""</span></span><br><span class="line"><span class="attr">  passWord:</span> <span class="string">psw</span></span><br><span class="line">  <span class="comment">#OpenPai集群的主节点</span></span><br><span class="line"><span class="attr">  host:</span> <span class="number">172.31</span><span class="number">.246</span><span class="number">.52</span></span><br></pre></td></tr></table></figure>
<p>   <strong>这里资源的配置都是针对一个trail的，memoryMB也是针对一个trail的。</strong><br>   <strong>注意：</strong> pai 模式下，NNIManager 会启动 RESTful 服务，监听端口为 NNI 网页服务器的端口加1。 例如，如果网页端口为<code>8080</code>，那么 RESTful 服务器会监听在 <code>8081</code>端口，来接收运行在 Kubernetes 中的 Trial 作业的指标。 因此，需要在防火墙中启用端口 <code>8081</code> 的 TCP 协议，以允许传入流量。</p>
<p>通常在服务器中8080端口无法使用，我们需要在启动NNI管理器时手动通过 —port 指定端口。</p>
<h2><span id="14-安装nni">1.4. 安装NNI</span></h2><p>   由于NNI并不依赖于任何环境，因此当我们使用OpenPAI提交NNI任务时，为了方便（需要解决ip和端口映射问题），<strong>不需要在docker中启动NNI，直接在服务器环境下安装NNI，启动即可</strong>。<br>   使用<code>pip install nni==1.2</code>安装nni</p>
<h2><span id="15-启动nni">1.5. 启动NNI</span></h2><p>   使用<code>nnictl create --port 6688 --config xxx.yml</code>来启动一个Experiment,如果端口被占用，换别的端口</p>
<h2><span id="16-nni浏览器查看">1.6. NNI浏览器查看</span></h2><p>   在浏览器中输入<code>服务器ip:6688</code></p>
<h2><span id="17-在浏览器中查看openpai">1.7. 在浏览器中查看OpenPai</span></h2><p>   在浏览器中登录OpenPai，可以查看启动的trail，在代码中的print输出的内容在stdout中查看。<br>   <strong>注：有时候print语句输出的内容在stdout显示不出来，添加<code>flush=True</code>就可以了</strong></p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"************进入main函数"</span>,flush=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>   <img src="/2020/01/10/openpai/logs.png" alt=""></p>
<h2><span id="18-关闭nni">1.8. 关闭NNI</span></h2><p>直接在服务器中使用<code>nnictl stop</code>即可关闭nni的Experiment</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>openpai</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch 学习</title>
    <url>/2020/01/06/pytorch-%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>介绍Pytorch的一些使用方法<br><a id="more"></a></p>
<h1><span id="gpu">GPU</span></h1><p><a href="https://zhuanlan.zhihu.com/p/71566775" target="_blank" rel="noopener">转载出处</a></p>
<p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.6_use-gpu" target="_blank" rel="noopener">GPU计算</a></p>
<h2><span id="查看-gpu-信息">查看 GPU 信息</span></h2><p>更多接口，参考 <a href="https://pytorch.org/docs/stable/cuda.html" target="_blank" rel="noopener">torch.cuda</a></p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.cuda</span><span class="selector-class">.is_available</span>()       # 判断 <span class="selector-tag">GPU</span> 是否可用</span><br><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.cuda</span><span class="selector-class">.device_count</span>()       # 判断有多少 <span class="selector-tag">GPU</span></span><br><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.cuda</span><span class="selector-class">.get_device_name</span>(0)   # 返回 <span class="selector-tag">gpu</span> 名字，设备索引默认从 0 开始</span><br><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.cuda</span><span class="selector-class">.current_device</span>()     # 返回当前设备索引</span><br></pre></td></tr></table></figure>
<h2><span id="torchdevice">torch.device</span></h2><p><code>torch.device</code> 表示 <code>torch.Tensor</code> 分配到的设备的对象。其包含一个设备类型（<code>cpu</code> 或 <code>cuda</code>），以及可选的设备序号。如果设备序号不存在，则为当前设备，即 <code>torch.cuda.current_device()</code> 的返回结果。</p>
<p>可以通过如下方式创建 <code>torch.device</code> 对象：</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过字符串</span></span><br><span class="line"><span class="attr">device</span> = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"><span class="attr">device</span> = torch.device(<span class="string">'cuda:1'</span>)  # 指定类型及编号。注意，代码不会检查编号是否合法</span><br><span class="line"><span class="attr">device</span> = torch.device(<span class="string">'cuda'</span>)    # 默认为当前设备，如果是多GPU，默认使用全部GPU</span><br></pre></td></tr></table></figure>
<p>还可以通过设备类型加上编号，来创建 <code>device</code> 对象：</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">device</span> = torch.device(<span class="string">'cuda'</span>, <span class="number">0</span>)</span><br><span class="line"><span class="attr">device</span> = torch.device(<span class="string">'cpu'</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="配置-cuda-访问限制">配置 CUDA 访问限制</span></h2><p>可以通过如下方式，设置当前 <code>Python</code> 脚本可见的 <code>GPU</code>。</p>
<h3><span id="在命令行设置">在命令行设置</span></h3><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">CUDA_VISIBLE_DEVICES</span>=<span class="number">1</span> python my_script.py</span><br></pre></td></tr></table></figure>
<p><strong>实例</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Environment Variable Syntax      Results</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">1</span>           Only device <span class="number">1</span> will be seen</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>         Devices <span class="number">0</span> <span class="keyword">and</span> <span class="number">1</span> will be visible</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0,1"</span>       Same as above, quotation marks are optional</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>       Devices <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span> will be visible; device <span class="number">1</span> <span class="keyword">is</span> masked</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">""</span>          No GPU will be visible</span><br></pre></td></tr></table></figure>
<h3><span id="在-python-代码中设置">在 Python 代码中设置</span></h3><figure class="highlight moonscript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="built_in">os</span></span><br><span class="line"><span class="built_in">os</span>.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0, 2"</span></span><br></pre></td></tr></table></figure>
<h3><span id="使用函数-set_device">使用函数 set_device</span></h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">torch<span class="selector-class">.cuda</span><span class="selector-class">.set_device</span>(id)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>官方建议使用 <code>CUDA_VISIBLE_DEVICES</code>，不建议使用 <code>set_device</code> 函数。</p>
</blockquote>
<h2><span id="用-gpu-训练">用 GPU 训练</span></h2><p>默认情况下，使用 <code>CPU</code> 训练模型。可以通过如下方式，通过 <code>GPU</code> 进行训练。<strong>使用 GPU 时，模型和输入必须位于同一张 GPU 上。</strong></p>
<p><code>.to(device)</code> 和 <code>.cuda()</code> 的区别如下：</p>
<p><a href="https://stackoom.com/question/3bltP/Pytorch-%E5%9C%A8CUDA%E8%AE%BE%E5%A4%87%E4%B8%8A%E6%9C%89%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%E5%8F%AF%E4%BB%A5%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F-%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E5%90%97" target="_blank" rel="noopener">to和cuda的区别</a><br><img src="/2020/01/06/pytorch-学习/to和cuda区别.png" alt="to和cuda区别"></p>
<ol>
<li><code>.to()</code> 中的参数必不可少</li>
<li>对于 <code>module</code> 而言，<code>.to()</code> 是 <code>inplace</code> 的，而 <code>.cuda()</code> 不是；而对于 <code>tensor</code> 而言，两者一致。</li>
</ol>
<blockquote>
<p><strong>注</strong>：实测，两者时间消耗持平。推荐使用<code>.to()函数</code></p>
</blockquote>
<p><strong>方式 1 ：使用cuda</strong></p>
<figure class="highlight nix"><table><tr><td class="code"><pre><span class="line"><span class="attr">device</span> = torch.device(<span class="string">"cuda:1"</span>)   <span class="comment"># 指定模型训练所在 GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 CPU 转移至 GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available() <span class="literal">and</span> use_gpu:</span><br><span class="line">    <span class="attr">net</span> = net.cuda(device)    <span class="comment"># 默认在第一块 GPU 上训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时将数据转移至 GPU,注意cuda有返回值</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available() <span class="literal">and</span> use_gpu:</span><br><span class="line">    <span class="attr">inputs</span> = inputs.cuda(device)</span><br><span class="line">    <span class="attr">labels</span> = labels.cuda(device)</span><br></pre></td></tr></table></figure>
<p><strong>方法 2 ：使用to</strong></p>
<figure class="highlight nix"><table><tr><td class="code"><pre><span class="line"><span class="attr">device</span> = torch.device(<span class="string">"cuda:1"</span>)   <span class="comment"># 指定模型训练所在 GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 CPU 转移至 GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available() <span class="literal">and</span> use_gpu:</span><br><span class="line">    <span class="attr">net</span> = net.to(device)    <span class="comment"># 默认在第一块 GPU 上训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时将数据转移至 GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available() <span class="literal">and</span> use_gpu:</span><br><span class="line">    <span class="attr">inputs</span> = inputs.to(device)</span><br><span class="line">    <span class="attr">labels</span> = labels.to(device)</span><br></pre></td></tr></table></figure>
<h2><span id="存在的问题">存在的问题</span></h2><h3><span id="batch-size-太大">batch size 太大</span></h3><p>当想要用大批量进行训练，但是 <code>GPU</code> 资源有限，此时可以通过<strong>梯度累加</strong>（<code>accumulating gradients</code>）的方式进行。</p>
<p>梯度累加的基本思想在于，在优化器更新参数前，也就是执行 <code>optimizer.step()</code> 前，进行多次反向传播，使得梯度累计值自动保存在 <code>parameter.grad</code> 中，最后使用累加的梯度进行参数更新。</p>
<p>这个在 <code>PyTorch</code> 中特别容易实现，因为 <code>PyTorch</code> 中，梯度值本身会保留，除非我们调用 <code>model.zero_grad()</code> 或 <code>optimizer.zero_grad()</code>。</p>
<p>修改后的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.zero_grad() <span class="comment"># 重置保存梯度值的张量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(training_set):</span><br><span class="line">    predictions = model(inputs)<span class="comment"># 前向计算</span></span><br><span class="line">    loss = loss_function(predictions, labels)<span class="comment"># 计算损失函数</span></span><br><span class="line">    loss.backward()<span class="comment"># 计算梯度</span></span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:<span class="comment">#重复多次前面的过程</span></span><br><span class="line">        optimizer.step()<span class="comment">#更新梯度</span></span><br><span class="line">        model.zero_grad()<span class="comment">#重置梯度</span></span><br></pre></td></tr></table></figure>
<h3><span id="model-太大">model 太大</span></h3><p>当模型本身太大，以至于不能放置于一个 <code>GPU</code> 中时，可以通过<strong>梯度检查点</strong> (<code>gradient-checkpoingting</code>) 的方式进行处理。</p>
<p>梯度检查点的基本思想是<strong>以计算换内存</strong>。具体来说就是，在反向传播的过程中，把梯度切分成几部分，分别对网络上的部分参数进行更新。如下图所示：</p>
<p><img src="http://tankzhou.cn/images/%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5%E7%82%B9.gif" alt=""></p>
<p>梯度检查点图示</p>
<p>这种方法速度很慢，但在某些例子上很有用，比如训练长序列的 RNN 模型等。</p>
<p>具体可参考：<a href="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a" target="_blank" rel="noopener">From zero to research — An introduction to Meta-learning</a></p>
<p>单机多卡训练，即<strong>并行训练</strong>。并行训练又分为<strong>数据并行</strong> (<code>Data Parallelism</code>) 和<strong>模型并行</strong>两种。</p>
<p>数据并行指的是，多张 <code>GPU</code> 使用相同的模型副本，但是使用不同的数据批进行训练。而模型并行指的是，多张<code>GPU</code> 分别训练模型的不同部分，使用同一批数据。</p>
<p>两者对比如下图所示：</p>
<p><img src="/2020/01/06/pytorch-学习/多GPU.jpg" alt=""></p>
<p>模型并行 VS 数据并行</p>
<h2><span id="数据并行">数据并行</span></h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">Pytorch多GPU官方实例</a></p>
<h3><span id="pytorch-api">Pytorch API</span></h3><p>【<strong>Class 原型</strong>】</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">torch.nn.DataParallel(module, <span class="attribute">device_ids</span>=None, <span class="attribute">output_device</span>=None, <span class="attribute">dim</span>=0)</span><br></pre></td></tr></table></figure>
<p>【<strong>参数</strong>】</p>
<ul>
<li><strong>module</strong> ：要进行并行的 <code>module</code>。这里隐含了一点 ，即网络中的某一层也是可以进行数据并行的，但是一般不会这么使用。</li>
<li><strong>device_ids</strong> : <code>CUDA</code> 列表，可以为 <code>torch.device</code> 类型，也可以是编号组成的 <code>int</code> 列表。<strong>默认使用全部 GPU</strong></li>
<li><strong>output_device</strong> : 某一 <code>GPU</code> 编号或 <code>torch.device</code> 。指定输出的 <code>GPU</code>，默认为第一个，即 <code>device_ids[0]</code></li>
</ul>
<p>【<strong>返回值</strong>】</p>
<p>要进行并行的模型。</p>
<p>【<strong>基本使用方式</strong>】</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; net = torch.nn.DataParallel(model, device_ids=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; output = net(input_var)  <span class="comment"># input_var can be on any device, including CP</span></span><br></pre></td></tr></table></figure>
<h3><span id="数据并行的原理">数据并行的原理</span></h3><p>数据并行的具体原理流程为：</p>
<p><img src="/2020/01/06/pytorch-学习/数据并行.png" alt=""></p>
<ol>
<li><p>将模型加载至主设备上，作为 <code>controller</code>，一般设置为 <code>cuda:0</code></p>
</li>
<li><p>在每次迭代时，执行如下操作：</p>
<ol>
<li><p>将 <code>controller</code> 模型复制（<code>broadcast</code>）到每一个指定的 <code>GPU</code> 上</p>
</li>
<li><p>将总输入的数据 <code>batch</code>，进行均分，分别作为各对应副本的输入 (<code>scatter</code>)</p>
</li>
<li><p>每个副本独立进行前向传播，并进行反向传播，但只是求取梯度，每个GPU上的loss都要进行<code>loss.backward()</code>,得到各自的梯度</p>
</li>
<li><p>将各副本的梯度汇总（<code>gather</code>）到 <code>controller</code> 设备，并进行求和 (<code>reduced add</code>)</p>
<blockquote>
<p>During the backwards pass, gradients from each replica are summed into the original module.</p>
</blockquote>
</li>
<li><p>更具总体度，更新 <code>controller</code> 设备上的参数</p>
</li>
</ol>
</li>
</ol>
<h3><span id="注意事项">注意事项</span></h3><p>【<strong>警告 1</strong>】</p>
<ul>
<li>设置的 <code>batch size</code> 为总的批量尺寸，其必须大于 <code>GPU</code> 数量。</li>
<li>在 <code>parallelized module</code> 运行之前，必须保证其在 <code>controller</code> 设备上，存在参数和 <code>buffers</code>。</li>
<li>并行的 <code>GPU</code> 列表中，必须包含主 <code>GPU</code></li>
<li>当 <code>forward()</code> 中，<code>module</code> 返回一个标量，那么并行的结果将返回一个 <code>vector</code>，其长度等于 <code>device</code> 的数量，对应于各个设备的结果。</li>
</ul>
<p>【<strong>警告 2</strong>】</p>
<p>在每次前向传播过程中，<code>module</code> 都先会被复制到每一个 <code>device</code> 上。因此，在前向传播中，任何对该运行的 <code>module</code> 的副本的更新，在此后都将会丢失。</p>
<p>比方说，如果 <code>module</code> 有一个 <code>counter</code> 属性，每次前向传播都会进行累加，则它将会保持为初始值。因为更新是发生在模型的副本（在其他 <code>device</code> 上的副本）上的，并且这些更新在前向传播结束之后将会被销毁。</p>
<p>然而，<code>DataParallel</code> 保证 <code>controller</code> 设备上的副本的参数和 <code>buffers</code> 与其他并行的 <code>modules</code> 之间共享存储。因此，如若对 <code>controller device</code> 的 参数和 <code>buffers</code> 的更改，将会被记录。例如，<code>BatchNorm2d</code> 和 <code>spectral_norm()</code> 依赖于这种行为来更新 <code>buffers</code>。</p>
<p>【<strong>警告 3</strong>】</p>
<p>定义于 <code>module</code> 及其子 <code>module</code> 上的前向传播和反向传播 <code>hooks</code>，将会被调用 <code>len(device_ids)</code> 次，每个设备对应一次。</p>
<p>具体来说，<code>hooks</code> 只能保证按照正确的顺序执行对应设备上的操作，即在对应设备上的 <code>forward()</code> 调用之前执行，但是不能保证，在所有 <code>forward)()</code> 执行之前，通过 <code>register_forward_pre_hook()</code> 执行完成所有的 <code>hooks</code>。</p>
<p>【<strong>警告 4</strong>】</p>
<p>任何位置和关键字 (<code>positional and keyword</code>) 输入都可以传递给 <code>DataParallel</code>，处理一些需要特殊处理的类型。</p>
<p><code>tensors</code> 将会在指定维度（默认为 <code>0</code>）上被 <code>scattered</code>。 <code>tuple</code>， <code>list</code> 和 <code>dict</code> 类型则会被浅拷贝。其他类型则会在不同的线程之间进行共享，且在模型前向传播过程中，如果进行写入，则可被打断。</p>
<p>【<strong>警告 5</strong>】</p>
<p>当对 <code>pack sequence -&gt; recurrent network -&gt; unpack sequence</code> 模式的 <code>module</code> 使用 <code>DataParallel</code> 或 <code>data_parallel</code> 时，有一些小的问题。</p>
<p>每个设备上的 <code>forward</code> 的对应输入，将仅仅是整个输入的一部分。因为默认的 <code>unpack</code> 操作 <code>torch.nn.utils.rnn.pad_packed_sequence()</code> 只会将该设备上的输入 <code>padding</code> 成该设备上的最长的输入长度，因此，将所有设备的结构进行汇总时，可能会发生长度的不匹配的情况。</p>
<p>因此，可以利用 <code>pad_packed_sequence()</code> 的 <code>total_length</code> 参数来保证 <code>forward()</code> 调用返回的序列长度一致。代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># ... __init__, other methods, etc.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># padded_input is of shape [B x T x *] (batch_first mode) and contains</span></span><br><span class="line">    <span class="comment"># the sequences sorted by lengths</span></span><br><span class="line">    <span class="comment">#   B is the batch size</span></span><br><span class="line">    <span class="comment">#   T is max sequence length</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, padded_input, input_lengths)</span>:</span></span><br><span class="line">        total_length = padded_input.size(<span class="number">1</span>)  <span class="comment"># get the max sequence length</span></span><br><span class="line">        packed_input = pack_padded_sequence(padded_input, input_lengths,</span><br><span class="line">                                            batch_first=<span class="keyword">True</span>)</span><br><span class="line">        packed_output, _ = self.my_lstm(packed_input)</span><br><span class="line">        output, _ = pad_packed_sequence(packed_output, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                        total_length=total_length)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">m = MyModule().cuda()        <span class="comment"># 设置 controller 模型</span></span><br><span class="line">dp_m = nn.DataParallel(m)    <span class="comment"># 进行副本拷贝</span></span><br></pre></td></tr></table></figure>
<h3><span id="示例程序">示例程序</span></h3><p>下面是使用 <code>DataParrel</code> 的核心代码，其余部分与一般的训练流程一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置当前脚本可见的 GPU 列表</span></span><br><span class="line"><span class="comment"># 这里设置 0 号和 1 号 GPU 对当前脚本可见。</span></span><br><span class="line"><span class="comment"># 此时，若 DataParallel 中指定使用其他 GPU 资源，额外的编号将会被忽略</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0, 1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用数据并行</span></span><br><span class="line"><span class="comment"># 1. 将 model 转移到某 GPU 上 -- net.cuda()</span></span><br><span class="line"><span class="comment"># 2. 指定并行训练要用到的 GPU -- device_ids=[0, 1]</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)</span><br><span class="line">    net = nn.DataParallel(net.cuda(), device_ids=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转移到 controller 所在 GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">and</span> use_gpu:</span><br><span class="line">    inputs = inputs.cuda(device)</span><br><span class="line">    labels = labels.cuda(device)</span><br></pre></td></tr></table></figure>
<h3><span id="模型的加载">模型的加载</span></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Single2Parallel</span><span class="params">(self, origin_state)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将串行的权值参数转换为并行的权值参数</span></span><br><span class="line"><span class="string">    :param origin_state : 原始串行权值参数</span></span><br><span class="line"><span class="string">    :return             : 并行的权值参数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  	converted = OrderedDict()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> origin_state.items():</span><br><span class="line">      name = <span class="string">"module."</span> + k</span><br><span class="line">      converted[name] = v</span><br><span class="line">  </span><br><span class="line">  	<span class="keyword">return</span> converted</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Parallel2Single</span><span class="params">(self, origin_state)</span>:</span></span><br><span class="line">  	<span class="string">"""</span></span><br><span class="line"><span class="string">  	将并行的权值参数转换为串行的权值参数</span></span><br><span class="line"><span class="string">  	:param origin_state : 原始串行权值参数</span></span><br><span class="line"><span class="string">  	:return             : 并行的权值参数</span></span><br><span class="line"><span class="string">  	"""</span></span><br><span class="line">    </span><br><span class="line">    converted = OrderedDict()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> origin_state.items():</span><br><span class="line">      name = k[<span class="number">7</span>:]</span><br><span class="line">      converted[name] = v</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">return</span> converted</span><br></pre></td></tr></table></figure>
<h2><span id="模型并行">模型并行</span></h2><p>如果模型本身较大，一张 <code>GPU</code> 放置不下时，要通过模型并行来处理。模型并行指的是，将模型的不同部分，分别放置于不同的 <code>GPU</code> 上，并将中间结果在 <code>GPU</code> 之间进行传递。</p>
<p>尽管从执行时间上来看，将模型的不同部分部署在不同设备上确实有好处，但是它通常是出于避免内存限制才使用。具有特别多参数的模型会受益于这种并行策略，因为这类模型需要很高的内存占用，很难适应到单个系统。</p>
<h3><span id="基本使用">基本使用</span></h3><p>下面，我们以一个 <code>toy</code> 模型为例，讲解模型并行。模型并行的实现方式如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features_1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),  <span class="comment"># 30</span></span><br><span class="line">            ......</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),  <span class="comment"># 12</span></span><br><span class="line">        ).to(<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line">        self.features_2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),  <span class="comment"># 5</span></span><br><span class="line">            ......).to(<span class="string">'cuda:1'</span>)  <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            ......</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, class_num)).to(<span class="string">'cuda:1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.features_1(x.to(<span class="string">'cuda:0'</span>))</span><br><span class="line">        out = self.features_2(out.to(<span class="string">'cuda:1'</span>))</span><br><span class="line">        out = out.view(<span class="number">-1</span>, <span class="number">384</span>)</span><br><span class="line">        out = self.classifier(out)</span><br><span class="line">        out = F.softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>上面的 <code>toy</code> 模型看起来和在单个 <code>GPU</code> 上运行的模型没什么区别，只不过用 <code>to(device)</code> 来将模型内的不同层分散到不同的 <code>GPU</code> 上进行运行，并且将中间结果转移到对应的 <code>GPU</code> 上即可。</p>
<p><code>backward()</code> 和 <code>torch.optim</code> 将会自动考虑梯度，与在一个 <code>GPU</code> 上没有区别。</p>
<blockquote>
<p><strong>注意</strong>：在调用 <code>loss</code> 函数时，<code>labels</code> 与 <code>output</code> 必须在同一个 <code>GPU</code> 上。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此时，不在此需要使用 model = model.cuda()</span></span><br><span class="line">model = ToyModel()</span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> trainloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 要处理的部分</span></span><br><span class="line">    images = images.to(<span class="string">'cuda:0'</span>)</span><br><span class="line">    labels = labels.to(<span class="string">'cuda:1'</span>)   <span class="comment"># 必须与输出所在 GPU 一致</span></span><br><span class="line">    </span><br><span class="line">    outputs = net(images)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h3><span id="模型并行的性能分析">模型并行的性能分析</span></h3><p>以上的实现解决了单个模型太大，不能存放于一个 <code>GPU</code> 的情况。然而，需要注意的是，相较于在单个 <code>GPU</code> 上运行，其速度更慢。因为任何时候，只有一个 <code>GPU</code> 在工作，而另一个则闲置。而当中间结果在 <code>GPU</code> 之间进行转移时，速度会进一步下降。</p>
<p>下面同时实例分析。以 <code>resnet50</code> 为例，用随机生成的数据输入，比较两个版本的运行时间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models.resnet <span class="keyword">import</span> ResNet, Bottleneck</span><br><span class="line"></span><br><span class="line">num_classes = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelParallelResNet50</span><span class="params">(ResNet)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        super(ModelParallelResNet50, self).__init__(</span><br><span class="line">            Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.seq1 = nn.Sequential(</span><br><span class="line">            self.conv1,</span><br><span class="line">            self.bn1,</span><br><span class="line">            self.relu,</span><br><span class="line">            self.maxpool,</span><br><span class="line"></span><br><span class="line">            self.layer1,</span><br><span class="line">            self.layer2</span><br><span class="line">        ).to(<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line">        self.seq2 = nn.Sequential(</span><br><span class="line">            self.layer3,</span><br><span class="line">            self.layer4,</span><br><span class="line">            self.avgpool,</span><br><span class="line">        ).to(<span class="string">'cuda:1'</span>)</span><br><span class="line"></span><br><span class="line">        self.fc.to(<span class="string">'cuda:1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.seq2(self.seq1(x).to(<span class="string">'cuda:1'</span>))</span><br><span class="line">        <span class="keyword">return</span> self.fc(x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line">num_batches = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">120</span></span><br><span class="line">image_w = <span class="number">128</span></span><br><span class="line">image_h = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.train(<span class="keyword">True</span>)</span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    one_hot_indices = torch.LongTensor(batch_size) \</span><br><span class="line">                           .random_(<span class="number">0</span>, num_classes) \</span><br><span class="line">                           .view(batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        <span class="comment"># generate random inputs and labels</span></span><br><span class="line">        inputs = torch.randn(batch_size, <span class="number">3</span>, image_w, image_h)</span><br><span class="line">        labels = torch.zeros(batch_size, num_classes) \</span><br><span class="line">                      .scatter_(<span class="number">1</span>, one_hot_indices, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># run forward pass</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs.to(<span class="string">'cuda:0'</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># run backward pass</span></span><br><span class="line">        labels = labels.to(outputs.device)</span><br><span class="line">        loss_fn(outputs, labels).backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.switch_backend(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line">num_repeat = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">stmt = <span class="string">"train(model)"</span></span><br><span class="line"></span><br><span class="line">setup = <span class="string">"model = ModelParallelResNet50()"</span></span><br><span class="line"><span class="comment"># globals arg is only available in Python 3. In Python 2, use the following</span></span><br><span class="line"><span class="comment"># import __builtin__</span></span><br><span class="line"><span class="comment"># __builtin__.__dict__.update(locals())</span></span><br><span class="line">mp_run_times = timeit.repeat(</span><br><span class="line">    stmt, setup, number=<span class="number">1</span>, repeat=num_repeat, globals=globals())</span><br><span class="line">mp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)</span><br><span class="line"></span><br><span class="line">setup = <span class="string">"import torchvision.models as models;"</span> + \</span><br><span class="line">        <span class="string">"model = models.resnet50(num_classes=num_classes).to('cuda:0')"</span></span><br><span class="line">rn_run_times = timeit.repeat(</span><br><span class="line">    stmt, setup, number=<span class="number">1</span>, repeat=num_repeat, globals=globals())</span><br><span class="line">rn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(means, stds, labels, fig_name)</span>:</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    ax.bar(np.arange(len(means)), means, yerr=stds,</span><br><span class="line">           align=<span class="string">'center'</span>, alpha=<span class="number">0.5</span>, ecolor=<span class="string">'red'</span>, capsize=<span class="number">10</span>, width=<span class="number">0.6</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'ResNet50 Execution Time (Second)'</span>)</span><br><span class="line">    ax.set_xticks(np.arange(len(means)))</span><br><span class="line">    ax.set_xticklabels(labels)</span><br><span class="line">    ax.yaxis.grid(<span class="keyword">True</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(fig_name)</span><br><span class="line">    plt.close(fig)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot([mp_mean, rn_mean],</span><br><span class="line">     [mp_std, rn_std],</span><br><span class="line">     [<span class="string">'Model Parallel'</span>, <span class="string">'Single GPU'</span>],</span><br><span class="line">     <span class="string">'mp_vs_rn.png'</span>)</span><br></pre></td></tr></table></figure>
<p>结果如下所示。模型并行相较于单 <code>GPU</code> 训练的模型，训练时间开销多出 <code>4.02/3.75-1=7%</code> 左右。当然，这存在优化空间，因为多 <code>GPU</code> 中，每一时刻只有一个 <code>GPU</code> 进行训练，其他闲置。而在中间数据转移过程中，又消耗一定的时间。</p>
<p><img src="/2020/01/06/pytorch-学习/模型并行.jpg" alt=""></p>
<p>模型并行 VS 单 GPU</p>
<h3><span id="输入流水线">输入流水线</span></h3><p>解决上面的问题的最直接的方式就是使用流水线技术，即 <code>GPU-0</code> 输出到 <code>GPU-1</code> 之后，在 <code>GPU-1</code> 训练的同时，<code>GPU-0</code> 接收下一批数据，这样就可以多 <code>GPU</code> 同时执行了。</p>
<p>下面，我们将 <code>120</code> 个样本的 <code>batch</code> 再次细分，分为 <code>20</code> 张样本每份的小 <code>batch</code>。由于 <code>Pytorch</code> 同步启动 <code>CUDA</code> 操作，因此，该操作不需要使用额外的多线程来处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipelineParallelResNet50</span><span class="params">(ModelParallelResNet50)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, split_size=<span class="number">20</span>, *args, **kwargs)</span>:</span></span><br><span class="line">        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)</span><br><span class="line">        self.split_size = split_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        splits = iter(x.split(self.split_size, dim=<span class="number">0</span>))</span><br><span class="line">        s_next = next(splits)</span><br><span class="line">        s_prev = self.seq1(s_next).to(<span class="string">'cuda:1'</span>)</span><br><span class="line">        ret = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s_next <span class="keyword">in</span> splits:</span><br><span class="line">            <span class="comment"># A. s_prev runs on cuda:1</span></span><br><span class="line">            s_prev = self.seq2(s_prev)</span><br><span class="line">            ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># B. s_next runs on cuda:0, which can run concurrently with A</span></span><br><span class="line">            s_prev = self.seq1(s_next).to(<span class="string">'cuda:1'</span>)</span><br><span class="line"></span><br><span class="line">        s_prev = self.seq2(s_prev)</span><br><span class="line">        ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat(ret)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">setup = <span class="string">"model = PipelineParallelResNet50()"</span></span><br><span class="line">pp_run_times = timeit.repeat(</span><br><span class="line">    stmt, setup, number=<span class="number">1</span>, repeat=num_repeat, globals=globals())</span><br><span class="line">pp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)</span><br><span class="line"></span><br><span class="line">plot([mp_mean, rn_mean, pp_mean],</span><br><span class="line">     [mp_std, rn_std, pp_std],</span><br><span class="line">     [<span class="string">'Model Parallel'</span>, <span class="string">'Single GPU'</span>, <span class="string">'Pipelining Model Parallel'</span>],</span><br><span class="line">     <span class="string">'mp_vs_rn_vs_pp.png'</span>)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，<code>device-to-device</code> 的 <code>tensor copy</code> 操作是同步的。如果创建多个数据流，则需要保证 <code>copy</code> 操作以合适的同步方式进行。</p>
<p>在完成 <code>tensor</code> 拷贝之前，对 <code>source tensor</code> 进行写入，或者对 <code>target tensor</code> 进行读写，都可能会导致不可预期的行为。上面的实现中，在源和目标设备中，均只使用了默认的 <code>stream</code>，因此无需额外的强化同步操作。</p>
<p><img src="/2020/01/06/pytorch-学习/模型并行2.jpg" alt=""><br>模型并行 VS 单 GPU VS 流水线模型并行</p>
<p>如上图所示，流水线输入确实加速了训练进程，大约 <code>3.75/2.51-1=49%</code>，但距离 <code>100%</code> 的加速相去甚远。由于我们在流水线并行实现中，引入了一个新的参数 <code>split_sizes</code>，但是并不知晓其对训练时间的影响。</p>
<p>直觉上来说，使用一个小的 <code>split_sizes</code> 将会导致许多微小的 <code>CUDA</code> 内核的启动，而使用较大的 <code>split_sizes</code>，则会导致较长的空闲时间。下面是一个搜索最佳 <code>split_sizes</code> 的实验。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">means = []</span><br><span class="line">stds = []</span><br><span class="line">split_sizes = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">60</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> split_size <span class="keyword">in</span> split_sizes:</span><br><span class="line">    setup = <span class="string">"model = PipelineParallelResNet50(split_size=%d)"</span> % split_size</span><br><span class="line">    pp_run_times = timeit.repeat(</span><br><span class="line">        stmt, setup, number=<span class="number">1</span>, repeat=num_repeat, globals=globals())</span><br><span class="line">    means.append(np.mean(pp_run_times))</span><br><span class="line">    stds.append(np.std(pp_run_times))</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(split_sizes, means)</span><br><span class="line">ax.errorbar(split_sizes, means, yerr=stds, ecolor=<span class="string">'red'</span>, fmt=<span class="string">'ro'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'ResNet50 Execution Time (Second)'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Pipeline Split Size'</span>)</span><br><span class="line">ax.set_xticks(split_sizes)</span><br><span class="line">ax.yaxis.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">"split_size_tradeoff.png"</span>)</span><br><span class="line">plt.close(fig)</span><br></pre></td></tr></table></figure>
<p>实验结果如下所示：</p>
<p><img src="/2020/01/06/pytorch-学习/pipeline.jpg" alt=""></p>
<p>流水线输入分割份数</p>
<p>如上图所示，最佳的参数为 <code>12</code>，其将导致 <code>3.75/2.43-1=54%</code> 的加速。但这仍存在加速的可能。例如，所有在 <code>cuda:0</code> 上的操作放在默认的 <code>stream</code> 上。这意味着，在下一个 <code>split</code> 上的计算，不能与上一个 <code>split</code> 的 <code>copy</code> 操作进行重叠。然而，由于 <code>next_split</code> 和 <code>prev_plit</code> 是不同的 <code>tensor</code>，因此这不存在问题。</p>
<p>该实现需要在每个 <code>GPU</code> 上使用多个 <code>stream</code>，并且模型中不同的子网络需要使用不同的 <code>stream</code> 管理策略。</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>运行GPU程序</title>
    <url>/2019/12/29/%E8%BF%90%E8%A1%8CGPU%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<p>平时主要使用mxnet和pytorch，下面记录下在代码中怎么使用GPU<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#1-mxnet">1. mxnet</a><ul>
<li><a href="#11-%e5%8d%95gpu">1.1. 单GPU</a></li>
<li><a href="#12-%e5%a4%9agpu">1.2. 多GPU</a></li>
</ul>
</li>
<li><a href="#2-pytorch">2. pytorch</a><ul>
<li><a href="#21-%e5%8d%95gpu">2.1. 单GPU</a></li>
<li><a href="#22-%e5%a4%9agpu">2.2. 多GPU</a></li>
</ul>
</li>
<li><a href="#3-mxnet%e5%92%8cpytorch%e5%8c%ba%e5%88%ab">3. mxnet和pytorch区别</a><ul>
<li><a href="#31-ndarray%e5%92%8ctensor">3.1. NDArray和Tensor</a><ul>
<li><a href="#311-%e5%92%8cnumpy%e8%bd%ac%e6%8d%a2">3.1.1. 和numpy转换</a></li>
<li><a href="#312-%e8%bd%ac%e6%8d%a2%e4%b8%ba%e6%a0%87%e9%87%8f">3.1.2. 转换为标量</a></li>
<li><a href="#313-%e6%94%b9%e5%8f%98%e6%95%b0%e6%8d%ae%e5%bd%a2%e7%8a%b6">3.1.3. 改变数据形状</a></li>
<li><a href="#314-%e6%95%b0%e6%8d%ae%e8%bd%ac%e5%88%b0gpu%e4%b8%8a">3.1.4. 数据转到GPU上</a></li>
</ul>
</li>
<li><a href="#32-loss%e8%ae%a1%e7%ae%97">3.2. loss计算</a></li>
<li><a href="#33-rnn%e8%be%93%e5%85%a5%e7%bb%b4%e5%ba%a6">3.3. RNN输入维度</a></li>
<li><a href="#34-%e5%a4%9agpu%e8%bf%90%e8%a1%8c">3.4. 多GPU运行</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-mxnet">1. mxnet</span></h1><h2><span id="11-单gpu">1.1. 单GPU</span></h2><p><a href="http://zh.gluon.ai/chapter_deep-learning-computation/use-gpu.html" target="_blank" rel="noopener">GPU计算</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定使用哪块GPU</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"3"</span></span><br><span class="line"><span class="comment">#被指定的GPU编号默认为0</span></span><br><span class="line">ctx = mx.gpu(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#模型、数据都需要拷贝到GPU中</span></span><br><span class="line">nd.array(data,ctx)</span><br><span class="line">data.as_in_context(ctx)</span><br></pre></td></tr></table></figure>
<h2><span id="12-多gpu">1.2. 多GPU</span></h2><p><a href="http://zh.gluon.ai/chapter_computational-performance/multiple-gpus-gluon.html" target="_blank" rel="noopener">多GPU计算</a><br><a href="http://mxnet.incubator.apache.org/api/faq/multi_device" target="_blank" rel="noopener">Run MXNet on Multiple CPU/GPUs with Data Parallelism</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">ctx_id = <span class="string">'1,2,3,4'</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = ctx_id</span><br><span class="line">num_gpus = len(ctx_id.split(<span class="string">','</span>))</span><br><span class="line">ctx = [mx.gpu(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_gpus)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用split_and_load()将数据分配到多个GPU上</span></span><br></pre></td></tr></table></figure>
<h1><span id="2-pytorch">2. pytorch</span></h1><h2><span id="21-单gpu">2.1. 单GPU</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment">#使用to()将数据拷贝到GPU上</span></span><br><span class="line">train_feature.to(device)</span><br></pre></td></tr></table></figure>
<h2><span id="22-多gpu">2.2. 多GPU</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1,2,3"</span>  </span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="comment">#指定所有的GPU</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line">    all_devices = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment">#pytorch在使用多GPU时，需要先将数据和模型拷贝到GPU-0上，</span></span><br><span class="line"><span class="comment">#使用多GPU的关键代码</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>,flush=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment">#数据并行</span></span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<hr>
<p>2020.2.11更新</p>
<h1><span id="3-mxnet和pytorch区别">3. mxnet和pytorch区别</span></h1><p>在写程序时，主要用到<code>mxnet和pytorch</code>框架，这里针对2者在代码上的不同做个总结，下面的不同都是我自己在写程序遇到的，仅仅是一部分，仅供参考。</p>
<h2><span id="31-ndarray和tensor">3.1. NDArray和Tensor</span></h2><h3><span id="311-和numpy转换">3.1.1. 和numpy转换</span></h3><ul>
<li><p>mxnet</p>
<ul>
<li><strong>Numpy—&gt;NDArray</strong><br><code>nd.array(a)</code></li>
<li><strong>NDArray—&gt;Numpy</strong><br><code>D.asnumpy()</code></li>
</ul>
</li>
<li><p>pytorch</p>
<ul>
<li><strong>Numpy—&gt;Tensor</strong><br><code>D = torch.from_numpy(a)</code></li>
<li><strong>Tensor—&gt;Numpy</strong><br><code>a = D.numpy()</code></li>
</ul>
</li>
</ul>
<h3><span id="312-转换为标量">3.1.2. 转换为标量</span></h3><ul>
<li>mxnet<br><code>asscalar()</code>将函数结果转换成Python的标量<br><code>X.sum().asscalar()</code></li>
<li>pytorch<br><code>item()</code>将函数结果转换成Python的标量<br><code>X.sum().item()</code></li>
</ul>
<h3><span id="313-改变数据形状">3.1.3. 改变数据形状</span></h3><ul>
<li><p>mxnet</p>
<ul>
<li><p><strong>reshape()</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = x.reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">X = x.reshape((<span class="number">-1</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>pytorch</p>
<ul>
<li><p><strong>view()</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x.view(<span class="number">15</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">5</span>)  <span class="comment"># -1所指的维度可以根据其他维度的值推出来</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3><span id="314-数据转到gpu上">3.1.4. 数据转到GPU上</span></h3><ul>
<li><p>mxnet<br>使用<code>as_in_context()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在gpu上创建NDArray</span></span><br><span class="line">B = nd.random.uniform(shape=(<span class="number">2</span>, <span class="number">3</span>), ctx=mx.gpu(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">z = x.as_in_context(mx.gpu())</span><br></pre></td></tr></table></figure>
</li>
<li><p>pytorch<br>使用<code>to()</code>函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>) <span class="comment"># GPU</span></span><br><span class="line">y = torch.ones_like(x, device=device)  <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">x = x.to(device)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2><span id="32-loss计算">3.2. loss计算</span></h2><ul>
<li>maxnet</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = L2Loss()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record()：</span><br><span class="line">            <span class="comment">#l维度(batch_size,)</span></span><br><span class="line">            l = loss(net(X),y)</span><br><span class="line">        <span class="comment">#等价于l.sum().backward()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#step对参数的梯度进行更新，传入batch_size是因为计算得到的梯度是一个batch样本的梯度和，需要除以batch_size得到梯度的平均值</span></span><br><span class="line">        trainer.step(batch_size)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注1：在使用<code>y.backward()</code>自动求梯度时，如果<code>y</code>不是一个标量，mxnet将默认先对<code>y</code>中元素求和得到新的变量，在求该变量关于<code>x</code>的梯度 </p>
<p>注2：mxnet的<code>L2Loss()</code>返回值维度<code>(batch_size,)</code>，即batch中每个样本的loss。需要在<code>step()</code>中传入<code>batch_size</code>参数</p>
</blockquote>
<ul>
<li>pytorch</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = MSELoss()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment">#l维度(1,)</span></span><br><span class="line">        l = loss(net(X),y)</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#step对参数的梯度进行更新，不需要传入batch_size参数</span></span><br><span class="line">        trainer.step()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注1：在<code>y.backward()</code>时，如果<code>y</code>是标量，则<code>backward()</code>不需要传入任何参数，否则，需要传入与<code>y</code>同形的Tensor</p>
<p>注2：grad在反向传播过程中是累加的，这意味着每一个batch运行反向传播，梯度都会累加之前batch的梯度，所以一般在反向传播传播之前需要把梯度清零。</p>
<p>注3：pytorch的<code>MSELoss()</code>返回值维度<code>(1,)</code>，Tensor中只有1个数，也称作<code>scalar:零维的张量</code>。因为<code>MSELoss()</code>返回值是batch中所有样本loss的平均值。所以在<code>step()</code>中不需要传入<code>batch_size</code>参数</p>
</blockquote>
<h2><span id="33-rnn输入维度">3.3. RNN输入维度</span></h2><p>RNN有2个数据：所有时间步的隐藏状态output、最后一个时间步的隐藏状态out_state</p>
<ul>
<li>mxnet<br>输入数据维度默认<code>(T,batch_size,input_size)</code><br>output输出数据维度<code>(T,batch_size,hidden_size)</code></li>
<li>pytorch<br>输入数据维度默认<code>(T,batch_size,input_size)</code><br>output输出数据维度<code>(T,batch_size,hidden_size)</code></li>
</ul>
<h2><span id="34-多gpu运行">3.4. 多GPU运行</span></h2><ul>
<li><p>mxnet<br><code>split_and_load()</code>返回值的list，里面每个元素是<code>NDArray</code>,经过<code>split_and_load()</code>对一个batch数据进行分割，得到的<code>X,y</code>的shape变成<code>(batch_size/n,*)</code>,经过模型输出得到的predicted维度也是<code>(batch_size/n,*)</code><br>计算得到的loss也是list类型，里面存储一个batch在多个GPU上计算的loss，对每个loss分别反向传播求梯度，然后再使用<code>step()</code>更新参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        gpu_Xs = gutils.split_and_load(X,ctx)</span><br><span class="line">        gpu_ys = gutils.split_and_load(y,ctx)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            <span class="comment">#ls是list，里面有n个NDArray，n是GPU的个数</span></span><br><span class="line">            ls = [loss(net(gpu_X),gpu_y <span class="keyword">for</span> gpu_X,gpu_y <span class="keyword">in</span> zip(gpu_Xs,gpu_ys)</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> ls:</span><br><span class="line">            l.backward()</span><br><span class="line"></span><br><span class="line">        train.step(batch_size)</span><br></pre></td></tr></table></figure>
</li>
<li><p>pytorch<br><a href="https://echohhhhhh.github.io/2019/01/31/%E8%BF%90%E8%A1%8CGPU%E7%A8%8B%E5%BA%8F/" target="_blank" rel="noopener">Pytorch学习</a>  </p>
<p>pytorch使用<code>DataParallel()</code>来进行数据并行，不需要手动将数据划分到多个GPU上，即<code>train_feature</code>的维度是<code>(batch_size,*)</code>，经过模型输出的<code>predicted</code>的维度也是<code>(batch_size,*)</code>，这一点和<code>mxnet</code>不同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用多GPU的关键代码</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">    transformer_model = nn.DataParallel(transformer_model)<span class="comment">#默认全部GPU</span></span><br><span class="line">transformer_model.to(device0)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,training_epoch+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> train_feature,train_label <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment">#train_label:(tabch_size,*)</span></span><br><span class="line">        predicted = transformer_model(train_feature)</span><br><span class="line">        l = loss(predicted,train_label)<span class="comment">#l的shape：(1,)</span></span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Mxnet</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode踩坑</title>
    <url>/2019/12/26/leecode-tirck/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;最近开始刷Leecode，使用Python语言，踩了很多坑，在此记录。</p>
<a id="more"></a>
<!-- TOC -->
<ul>
<li><a href="#1-%e4%b8%80%e4%b8%aa%e8%90%9d%e5%8d%9c%e4%b8%80%e4%b8%aa%e5%9d%91">1. 一个萝卜一个坑</a><ul>
<li><a href="#11-int%e5%87%bd%e6%95%b0">1.1. int函数</a></li>
<li><a href="#12-bin%e5%87%bd%e6%95%b0">1.2. bin函数</a></li>
<li><a href="#13-zip%e5%87%bd%e6%95%b0">1.3. zip函数</a></li>
<li><a href="#14-%e4%ba%8c%e5%88%86%e6%b3%95%e8%ae%b2%e8%a7%a3">1.4. 二分法讲解</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-一个萝卜一个坑">1. 一个萝卜一个坑</span></h1><h2><span id="11-int函数">1.1. int函数</span></h2><p>int()函数用于将一个字符串或数字转换为整型。<br><code>int(x,base=10)</code>:x—字符串或数字，base—进制数，默认十进制。 如果显示的指定base参数，x必须为str。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">int(<span class="number">3</span>) = <span class="number">3</span></span><br><span class="line">int(<span class="number">3.6</span>) = <span class="number">3</span></span><br><span class="line">int(<span class="string">'12'</span>,<span class="number">16</span>) = <span class="number">18</span></span><br><span class="line">int(<span class="string">'11'</span>,<span class="number">2</span>) = <span class="number">3</span><span class="comment">#将字符串解析为2进制</span></span><br></pre></td></tr></table></figure>
<h2><span id="12-bin函数">1.2. bin函数</span></h2><p>bin()返回一个整数int的二进制表示，返回类型str<br><code>bin(x)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#返回的结果前2个字符是固定的'0b'，后面才是真正的值。</span></span><br><span class="line">bin(<span class="number">10</span>) = <span class="string">'0b1010'</span></span><br><span class="line">bin(<span class="number">20</span>) = <span class="string">'0b10100'</span></span><br></pre></td></tr></table></figure>
<h2><span id="13-zip函数">1.3. zip函数</span></h2><p>zip()接受一系列(多个，个数不固定)可迭代对象(最常用list,tuple)作为参数，将多个对象中，对应位置的元素打包成一个个tuple，然后返回由这些tuple组成的list。若传入参数中长度不一样，则返回liist的长度和参数中最短的相同。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = [<span class="string">'wang'</span>,<span class="string">'bei'</span>]</span><br><span class="line">y = [<span class="string">'lei'</span>,<span class="string">'xiao'</span>,<span class="string">'kang'</span>]</span><br><span class="line">list(zip(x,y))</span><br><span class="line">输出：[(<span class="string">'wang'</span>,<span class="string">'lei'</span>),(<span class="string">'bei'</span>,<span class="string">'xiao'</span>)]</span><br><span class="line"></span><br><span class="line">x = <span class="string">'wang'</span></span><br><span class="line">y = <span class="string">'lei'</span></span><br><span class="line">list(zip(x,y))</span><br><span class="line">输出：[(<span class="string">'w'</span>,<span class="string">'l'</span>),(<span class="string">'a'</span>,<span class="string">'e'</span>),(<span class="string">'n'</span>,<span class="string">'i'</span>)]  </span><br><span class="line"></span><br><span class="line"><span class="comment">#假设zip传入的参数是x，y，zip依次将(x[0],y[0]),(x[1],y[1])..</span></span><br><span class="line"><span class="comment">#以可迭代对象返回，可强制转换为list或tuple</span></span><br></pre></td></tr></table></figure>
<p>zip(<em>)传入的参数是zip()的返回值类型，从<zip\>类型中每一个tuple中，取出第0个元素组成一个list，再取出第1个元素组成一个list，即zip(\</zip\></em>)返回值是一个大list，里面有2个tuple</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">b = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</span><br><span class="line">zipped = list(zip(a,b))</span><br><span class="line"><span class="comment">#zipped = [(1,'a'),(2,'b'),(3,'c')]</span></span><br><span class="line">c = list(zip(*zipped))</span><br><span class="line"><span class="comment">#c = [(1,2,3),('a','b','c')]</span></span><br></pre></td></tr></table></figure>
<h2><span id="14-二分法讲解">1.4. 二分法讲解</span></h2><p><a href="https://leetcode-cn.com/problems/search-insert-position/solution/te-bie-hao-yong-de-er-fen-cha-fa-fa-mo-ban-python-/" target="_blank" rel="noopener">参考资料</a></p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>nni</title>
    <url>/2019/11/25/nni/</url>
    <content><![CDATA[<p>NN是微软官方出的一个自动调参的第三方库。非常好用。但是用起来还是有一些需要注意的地方。这里记录一下。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e6%9c%af%e8%af%ad">1. 术语</a><ul>
<li><a href="#11-%e4%bd%bf%e7%94%a8%e6%ad%a5%e9%aa%a4">1.1. 使用步骤</a></li>
</ul>
</li>
<li><a href="#2-nni%e5%91%bd%e4%bb%a4">2. NNI命令</a></li>
</ul>
<!-- /TOC -->
<p><a href="https://nni.readthedocs.io/zh/latest/Tutorial/QuickStart.html" target="_blank" rel="noopener">NNi官方文档</a></p>
<h1><span id="1-术语">1. 术语</span></h1><p>主要有2个术语experiment和trial。<br>experiment：如果需要调整LSTM的超参数，则需要指定每个超参数的可选项，然后运行程序，让nni自动调参。运行的这个调参程序就是experiment。<br>trial：上面运行的程序中，有很多的参数组合，每一个超参数组合是trial。<br>每一个experiment有一个ID，experiment中的每一个trial也有一个ID。在网页中可以看到</p>
<p><img src="/2019/11/25/nni/experiment_id.png" alt=""></p>
<p><img src="/2019/11/25/nni/trial_id.png" alt=""></p>
<h2><span id="11-使用步骤">1.1. 使用步骤</span></h2><ol>
<li><p>创建搜索空间json文件<br>这里定义需要调整的超参数</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> <span class="attr">"num_layer"</span>:&#123;<span class="attr">"_type"</span>:<span class="string">"choice"</span>,<span class="attr">"_value"</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]&#125;,</span><br><span class="line"> <span class="attr">"hidden_size"</span>:&#123;<span class="attr">"_type"</span>:<span class="string">"choice"</span>,<span class="attr">"_value"</span>:[<span class="number">64</span>,<span class="number">128</span>,<span class="number">256</span>,<span class="number">512</span>]&#125;,</span><br><span class="line"> <span class="attr">"batch_size"</span>: &#123;<span class="attr">"_type"</span>:<span class="string">"choice"</span>, <span class="attr">"_value"</span>: [<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>,<span class="number">64</span>,<span class="number">128</span>,<span class="number">256</span>]&#125;,</span><br><span class="line"> <span class="attr">"learning_rate"</span>:&#123;<span class="attr">"_type"</span>:<span class="string">"uniform"</span>,<span class="attr">"_value"</span>:[<span class="number">0.0001</span>,<span class="number">0.001</span>]&#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>首先在程序中import nni </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nni  </span><br><span class="line"><span class="keyword">import</span> os  </span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"2"</span><span class="comment">#单GPU</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,train_epoch):</span><br><span class="line">    每一个batch，在测试集上训练，并反向传播  </span><br><span class="line">    每一个epoch，计算验证集/测试集的loss</span><br><span class="line">    每一个epoch，计算验证集/测试集的评价指标(mae,rmse)</span><br><span class="line">    <span class="comment">#将验证集的评价指标加入到nni中</span></span><br><span class="line">    <span class="keyword">if</span> epoch &lt; train_epoch:</span><br><span class="line">         nni.report_intermediate_result(valid_mae)</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         nni.report_final_result(valid_mae)</span><br></pre></td></tr></table></figure>
<p>在下面的yml文件中，需要指定gpuNum,即需要使用多少张GPU卡。但是nni会自动申请gpu，你也不知道会申请到哪个gpu。为了解决这个问题，需要指定程序只能看见哪些卡，那么就会只申请看见的卡。<br>通过以下代码指定：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"2"</span><span class="comment">#单GPU</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1,2"</span><span class="comment">#多GPU</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：如果只指定能看见第2张卡，但是程序中，会给第2张卡编号的为0，即通过ctx=mx.gpu(0)来获取。如果指定能看见1和2卡，那么程序中会分别编号0和1。</strong></p>
</li>
<li><p>nni的配置文件config.yml</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">authorName:</span> <span class="string">wangbeibei</span></span><br><span class="line"><span class="attr">experimentName:</span> <span class="string">gru_grid_experiment</span></span><br><span class="line"><span class="comment">#并发尝试任务的最大数量，有1张gpu卡就是1，有2张gpu卡就是2</span></span><br><span class="line"><span class="attr">trialConcurrency:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">maxExecDuration:</span> <span class="number">100</span><span class="string">h</span></span><br><span class="line"><span class="comment">#Trial 任务的最大数量，成功和失败的都计算在内</span></span><br><span class="line"><span class="attr">maxTrialNum:</span> <span class="number">10</span></span><br><span class="line"><span class="comment">#choice: local, remote, pai,在虚拟环境和docker运行时，都写local</span></span><br><span class="line"><span class="attr">trainingServicePlatform:</span> <span class="string">local</span></span><br><span class="line"><span class="comment">#在第一步创建的json文件的路径，这里需要写相对路径，因为当前的yml文件和json文件在同一文件夹下</span></span><br><span class="line"><span class="attr">searchSpacePath:</span> <span class="string">lstm_search_space.json</span></span><br><span class="line"><span class="comment">#choice: true, false</span></span><br><span class="line"><span class="attr">useAnnotation:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># 存储日志和数据的目录, 默认值是 /data/WangBeibei/nni/experiments</span></span><br><span class="line"><span class="comment">#如果在虚拟环境中运行该代码，logDir是服务器下的绝对路径，/data/WangBeibei/graduation/Code/nni_save_logs</span></span><br><span class="line"><span class="comment">#如果在docker下运行该代码，logDir是容器下的绝对路径，/root/Code/nni_save_logs/</span></span><br><span class="line"><span class="attr">logDir:</span> <span class="string">/root/Code/nni_save_logs/</span></span><br><span class="line"><span class="attr">tuner:</span></span><br><span class="line">    <span class="comment">#choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner, GPTuner</span></span><br><span class="line">    <span class="comment">#SMAC (SMAC should be installed through nnictl)</span></span><br><span class="line"><span class="attr">    builtinTunerName:</span> <span class="string">TPE</span></span><br><span class="line"><span class="attr">    classArgs:</span></span><br><span class="line">        <span class="comment">#choice: maximize, minimize</span></span><br><span class="line">        <span class="comment">#mae、rmse、mse都是最小化</span></span><br><span class="line"><span class="attr">        optimize_mode:</span> <span class="string">minimize</span></span><br><span class="line"><span class="attr">trial:</span></span><br><span class="line"><span class="comment"># 指定了运行 Trial 进程的命令行</span></span><br><span class="line"><span class="attr">command:</span> <span class="string">cd</span> <span class="string">baseline</span> <span class="string">&amp;&amp;</span> <span class="string">python3</span> <span class="string">lstm_baseline.py</span></span><br><span class="line"><span class="comment">#指定了 Trial 代码文件的目录</span></span><br><span class="line"><span class="comment">#首先从yml目录下，进入到代码的根目录下</span></span><br><span class="line"><span class="attr">codeDir:</span> <span class="string">../</span></span><br><span class="line"><span class="comment">#指定需要使用</span></span><br><span class="line"><span class="attr">gpuNum:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动容器<br>nni网页的默认端口是8080，所以需要和本地映射一下，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -it -p 7000:8080 -v $PWD:/root --runtime=nvidia --rm -u 1018 --name wbbnni lin-ai-27:5000/wangbeibei/mxnet:cu_100  </span><br><span class="line"></span><br><span class="line">docker run -it -p 7000:8080 -v $PWD:/root --runtime=nvidia --rm -u 1018 --name wbbnni lin-ai-27:5000/wangbeibei/pytorch_nni</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动一个nni的experiment<br>进入到yml所在的目录，使用以下命令，启动一个experiment实例，然后就可以在网页上查看<br>使用<code>nnictl create --config config.yml</code><br>出现以下提示，说明启动成功。</p>
<p><img src="/2019/11/25/nni/create.png" alt=""></p>
</li>
<li><p>在网页上访问<br>在网页上打开<code>服务器ip:7000</code></p>
<p><img src="/2019/11/25/nni/success.png" alt=""></p>
</li>
<li><p>错误日志<br>如果提交的任务都失败了，可以去看日志文件。日志文件的位置在下图中。在容器中进入到下面的目录中。</p>
<p><img src="/2019/11/25/nni/error1.png" alt=""></p>
<p><img src="/2019/11/25/nni/error2.png" alt=""></p>
<p>在容器中进入到日志目录中，找到对应id的文件夹。</p>
<p><img src="/2019/11/25/nni/log.png" alt=""></p>
</li>
<li><p>成功日志<br>如果trial成功运行，那么关于这次trial的</p>
<p><img src="/2019/11/25/nni/success1.png" alt=""></p>
</li>
</ol>
<h1><span id="2-nni命令">2. NNI命令</span></h1><p><a href="https://github.com/microsoft/nni/blob/master/docs/zh_CN/Tutorial/Nnictl.md" target="_blank" rel="noopener">NNI命令参考文档</a></p>
<ol>
<li>nnictl create<br>（1）在默认端口8080上创建一个新的Experiment<br><code>nnictl create --config nni/examples/trials/mnist/config.yml</code><br>（2）在指定的端口上8088创建新的Experiment<br><code>nnictl create --config nni/examples/trials/mnist/config.yml --port 8088</code></li>
<li>nnictl stop<br>停止正在运行的单个或多个Experiment<br>（1）没有指定id，停止所有正在运行的Experiment<br><code>nnictl stop</code><br>（2）停止指定id的Experiment<br><code>nnictl stop [experiment_id]</code></li>
<li>nnictl update<br>更新 Experiment 的搜索空间，其中<code>experiment_id</code>是可选的参数，后面的<code>--filename</code>是必填的参数。<br>先使用vscode修改搜索空间的json文件，然后再使用下面的命令来更新搜索空间文件。<br><code>nnictl update searchspace [experiment_id] --filename examples/trials/mnist/search_space.json</code></li>
<li><p>nnictl view<br>如果使用stop结束调参程序，以后还想看一下网页上的调参结果，使用该命令。这个命令只是在前端展示调参的结果，调参程序不会再次启动。  </p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">nnictl <span class="keyword">view</span> [<span class="keyword">OPTIONS</span>]</span><br><span class="line">其中experiment_id是必填的，port是选填的</span><br><span class="line">nnictl <span class="keyword">view</span> [experiment_id] <span class="comment">--port 8088</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>nnictl top<br>查看正在运行的Experiment</p>
</li>
<li>nnictl experiment show<br>显示Experiment的信息</li>
<li>nnictl experiment status<br>显示Experiment的状态</li>
<li>nnictl experiment list<br>显示正在运行的Experiment的信息</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning for Spatio-Temporal Data Mining: A Survey</title>
    <url>/2019/11/08/Deep-Learning-for-Spatio-Temporal-Data-Mining-A-Survey/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;<a href="https://arxiv.org/pdf/1906.04928.pdf" target="_blank" rel="noopener">Deep Learning for Spatio-Temporal Data Mining: A Survey</a><br>这篇论文是时空领域的综述论文，介绍了近几年时空领域的发展。<br><a id="more"></a></p>
<p>一些比较好的GitHub<br><a href="https://github.com/xuehaouwa/Awesome-Trajectory-Prediction" target="_blank" rel="noopener">https://github.com/xuehaouwa/Awesome-Trajectory-Prediction</a></p>
<h1><span id="引言">引言</span></h1><p>时空领域的数据的应用很广泛，包括环境和气候预测（风预测，降雨预测等），公共安全预测（crime预测），智能交通预测（交通流量预测），人类活动（人类轨迹模式挖掘）。本文将时空数据的类型，和广泛应用的深度学习模型，以及现有的研究进展.</p>
<h1><span id="时空数据分类">时空数据分类</span></h1><ol>
<li>Even data<br>事件数据由离散的事件组成，有location和time信息，例如cirme事件和traffic accident事件，疾病爆发事件，社会事件。</li>
<li>Trajectory data<br>轨迹数据是一系列随着时间变化的经纬度序列组成。有人的轨迹和车的轨迹。  </li>
<li>Point reference data<br>一般都是气象数据，测量一个区域的温度，植被等。  </li>
<li>Raster data（栅格数据）<br>有固定的m个区域，每个区域有n个时间段，可以用一个$R^{m \times n}$来表示。例如traffic flow数据。</li>
<li>Vedio data<br>视频数据和时空数据类似，相邻的像素可以表示相似的RGB，在时间上，特征变化平缓。可以用三维张量表示。  </li>
</ol>
<p>以上的5类数据可以用下图的方式表示。</p>
<p><img src="/2019/11/08/Deep-Learning-for-Spatio-Temporal-Data-Mining-A-Survey/stdata.png" alt=""></p>
<p>(1)轨迹和时间序列都可以表示成序列的形式。<br>(2)有时轨迹也可以表示成2维矩阵，矩阵的行和列是网格的长和宽，矩阵的值表示轨迹走的网格。这个表示形式通常使用CNN模型。例如：<a href="https://arxiv.org/pdf/1705.09436.pdf" target="_blank" rel="noopener">（2017NIPS）Human Trajectory Prediction using Spatially aware Deep Attention Models</a><br>(3)空间地图可以表示为Graph和2维矩阵。至于使用图还是2D矩阵根据应用而定。例如在城市交通流预测，交通网络中的交通数据使用Graph表示，例如<a href="https://arxiv.org/pdf/1707.01926.pdf" target="_blank" rel="noopener">(2018ICLR)Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting
</a><br>(4)网格数据，通常使用2D或3D张量表示，如果是2D矩阵，行和列分别表示区域和时间。如果是3D张量，分别表示区域的行和列，时间。</p>
<p><img src="/2019/11/08/Deep-Learning-for-Spatio-Temporal-Data-Mining-A-Survey/model.png" alt=""><br>未完待续。。。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
      </tags>
  </entry>
  <entry>
    <title>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title>
    <url>/2019/11/05/Enhancing-the-Locality-and-Breaking-the-Memory-Bottleneck-of-Transformer-on-Time-Series-Forecasting/</url>
    <content><![CDATA[<p>2019NIPS的一篇论文，<br><a href="https://arxiv.org/pdf/1907.00235.pdf" target="_blank" rel="noopener">论文地址</a><br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-introduction">1. Introduction</a></li>
<li><a href="#2-%e6%a8%a1%e5%9e%8b">2. 模型</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-introduction">1. Introduction</span></h1><p>本文说Transformer有2个缺点，（1）locality-agnostics：局部不可知性，原先只针对一个点，计算该点和其余所有点的相关性，没有考虑到子序列和子序列的attention。（2）memory bottleneck：内存瓶颈，空间复杂度和输入序列的长度L有关。对于捕获长时间序列不方便。</p>
<h1><span id="2-模型">2. 模型</span></h1><p>本文针对以上2个问题，提出2种解决方案，（1）传统Transformer—&gt;Conv Transformer,（2）使用LogSparse来减少内存的使用。</p>
<p><img src="/2019/11/05/Enhancing-the-Locality-and-Breaking-the-Memory-Bottleneck-of-Transformer-on-Time-Series-Forecasting/conv.png" alt=""></p>
<p>图b是传统的Transformer，在计算Attention时，计算的是一个点和其余所有点的Attention，在本文中使用一维卷积来生成query和key，在计算Attention时，使用的是子序列之间的Attention，捕获了local上下文信息。</p>
<p><img src="/2019/11/05/Enhancing-the-Locality-and-Breaking-the-Memory-Bottleneck-of-Transformer-on-Time-Series-Forecasting/log.png" alt=""></p>
<p>在传统Transformer在计算Atterntion时，使用的因果卷积，在第t步捕获前面所有历史时间步的信息，但是这样空间复杂度较大。所以本文提出了LogSparse，即在计算Attention时，对历史时间步的信息使用log函数进行抽样，然后多堆叠几层，也可以捕获前面所有时间步的信息。减少了空间复杂度，又可以捕获历史时间步的所有信息。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Time Series</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>DGL</title>
    <url>/2019/11/04/DGL/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;Deep Graph Library(DGL),一款面向图神经网络以及图机器学习的全新框架。DGL基于主流框架进行开发。用户可以使用他们偏爱的框架编写常见的CNN和注意力层，而当遇到图相关的计算时可以切换到DGL。用户和DGL的交互主要通过自定义函数UDF（user-defined function）。目前DGL支持Pytorch和MXNet/Gluon作为系统后端。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-dgl">1. DGL</a></li>
<li><a href="#2-%e5%87%bd%e6%95%b0%e4%bb%8b%e7%bb%8d">2. 函数介绍</a><ul>
<li><a href="#21-%e5%88%9b%e5%bb%ba%e5%9b%be">2.1. 创建图</a></li>
<li><a href="#22-%e8%8e%b7%e5%8f%96%e8%8a%82%e7%82%b9%e5%92%8c%e8%be%b9%e7%9a%84%e4%b8%aa%e6%95%b0">2.2. 获取节点和边的个数</a></li>
<li><a href="#23-%e5%88%86%e9%85%8d%e8%8a%82%e7%82%b9%e5%92%8c%e8%be%b9%e7%9a%84%e7%89%b9%e5%be%81">2.3. 分配节点和边的特征</a></li>
<li><a href="#24-%e5%88%a0%e9%99%a4%e8%8a%82%e7%82%b9%e5%92%8c%e8%be%b9%e7%89%b9%e5%be%81">2.4. 删除节点和边特征</a></li>
<li><a href="#25-%e8%87%aa%e5%ae%9a%e4%b9%89message%e5%87%bd%e6%95%b0">2.5. 自定义message函数</a></li>
<li><a href="#26-%e8%87%aa%e5%ae%9a%e4%b9%89reduce%e5%87%bd%e6%95%b0">2.6. 自定义reduce函数</a></li>
<li><a href="#27-%e6%b3%a8%e5%86%8cmessage%e5%92%8creduce%e5%87%bd%e6%95%b0">2.7. 注册message和reduce函数</a></li>
<li><a href="#28-updateall%e6%9b%b4%e6%96%b0%e8%8a%82%e7%82%b9%e7%89%b9%e5%be%81">2.8. update_all更新节点特征</a></li>
<li><a href="#29-%e9%ab%98%e7%ba%a7%e7%94%a8%e6%b3%95">2.9. 高级用法</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://archwalker.github.io/blog/2019/07/07/GNN-Framework-DGL-GCN.html" target="_blank" rel="noopener">GNN 教程：DGL框架-消息和GCN的实现</a>     </p>
<h1><span id="1-dgl">1. DGL</span></h1><p>DGL是基于<strong>消息传递message passing</strong>的编程模型。原因在于图上的计算往往可以表示为2步：</p>
<ol>
<li>发送节点：根据自身的特征计算需要向外分发的消息。</li>
<li>接收节点：对收到的消息进行累加并更新自身的特征。<br>用户需要自定义<strong>消息分发函数</strong>和<strong>消息聚合函数</strong>，来构造新的模型。</li>
</ol>
<ul>
<li>消息分发函数（message function）：将结点自身的消息传递传递给其邻居。因为对每条边来说，每个源节点将会将自身的Embedding(e.src.data)和边的Embedding(edge.data)传递给目的节点。对于每个目的节点来说，它可能会收到多个源节点传过来的消息，它会将这些消息存储在mailbox中。</li>
<li>消息聚合函数（reduce function）：聚合函数的目的是根据邻居传过来的消息更新自身节点Embedding，对每个节点来说，它先从邮箱(v.mailbox[‘m’])中汇聚消息函数所传递过来的消息(message)，并清空邮箱(v.mailbox[‘m’])内的消息；然后该节点结合汇聚后的结果和该节点原Embedding，更新节点Embedding。</li>
</ul>
<p><img src="/2019/11/04/DGL/dgl.png" alt=""></p>
<p>GCN的公式如下所示：</p>
<p><img src="/2019/11/04/DGL/gcn.png" alt=""></p>
<p>上面的数学描述可以利用<strong>消息传递</strong>的机制实现：<br>（1）在GCN中，每个节点都有属于自己的表示$h_i$<br>（2）根据消息传递（message passing），每个节点将会收到邻居节点发来的Embedding<br>（3）每个节点将聚合邻居节点的Embedding，得到中间表示$\hat{h_i}$<br>（4）对中间节点表示$\hat{h_i}$进行线性变换，然后利用非线性函数$f$进行计算：$h^{new}_u = f(W_u\hat{h}_u)$<br>（5）利用新的节点表示$h^{new}_u$对该节点的表示$h_u$进行更新。</p>
<h1><span id="2-函数介绍">2. 函数介绍</span></h1><p><a href="https://docs.dgl.ai/en/latest/api/python/graph.html" target="_blank" rel="noopener">官方文档</a><br><a href="https://docs.dgl.ai/en/latest/tutorials/basics/1_first.html" target="_blank" rel="noopener">实现GCN例子</a></p>
<h2><span id="21-创建图">2.1. 创建图</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line">g = dgl.DGLGraph()</span><br><span class="line"><span class="comment">#为图添加节点和边</span></span><br><span class="line">g.add_node(<span class="number">34</span>)<span class="comment">#添加34个节点</span></span><br><span class="line"><span class="comment">#一共有78条边</span></span><br><span class="line">edge_list = [(<span class="number">1</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">0</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">0</span>), (<span class="number">5</span>, <span class="number">0</span>), (<span class="number">6</span>, <span class="number">0</span>), (<span class="number">6</span>, <span class="number">4</span>), (<span class="number">6</span>, <span class="number">5</span>), (<span class="number">7</span>, <span class="number">0</span>), (<span class="number">7</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">2</span>), (<span class="number">7</span>, <span class="number">3</span>), (<span class="number">8</span>, <span class="number">0</span>), (<span class="number">8</span>, <span class="number">2</span>), (<span class="number">9</span>, <span class="number">2</span>), (<span class="number">10</span>, <span class="number">0</span>), (<span class="number">10</span>, <span class="number">4</span>),</span><br><span class="line">        (<span class="number">10</span>, <span class="number">5</span>), (<span class="number">11</span>, <span class="number">0</span>), (<span class="number">12</span>, <span class="number">0</span>), (<span class="number">12</span>, <span class="number">3</span>), (<span class="number">13</span>, <span class="number">0</span>), (<span class="number">13</span>, <span class="number">1</span>), (<span class="number">13</span>, <span class="number">2</span>),</span><br><span class="line">        (<span class="number">13</span>, <span class="number">3</span>), (<span class="number">16</span>, <span class="number">5</span>), (<span class="number">16</span>, <span class="number">6</span>), (<span class="number">17</span>, <span class="number">0</span>), (<span class="number">17</span>, <span class="number">1</span>), (<span class="number">19</span>, <span class="number">0</span>), (<span class="number">19</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="number">21</span>, <span class="number">0</span>), (<span class="number">21</span>, <span class="number">1</span>), (<span class="number">25</span>, <span class="number">23</span>), (<span class="number">25</span>, <span class="number">24</span>), (<span class="number">27</span>, <span class="number">2</span>), (<span class="number">27</span>, <span class="number">23</span>),</span><br><span class="line">        (<span class="number">27</span>, <span class="number">24</span>), (<span class="number">28</span>, <span class="number">2</span>), (<span class="number">29</span>, <span class="number">23</span>), (<span class="number">29</span>, <span class="number">26</span>), (<span class="number">30</span>, <span class="number">1</span>), (<span class="number">30</span>, <span class="number">8</span>),</span><br><span class="line">        (<span class="number">31</span>, <span class="number">0</span>), (<span class="number">31</span>, <span class="number">24</span>), (<span class="number">31</span>, <span class="number">25</span>), (<span class="number">31</span>, <span class="number">28</span>), (<span class="number">32</span>, <span class="number">2</span>), (<span class="number">32</span>, <span class="number">8</span>),</span><br><span class="line">        (<span class="number">32</span>, <span class="number">14</span>), (<span class="number">32</span>, <span class="number">15</span>), (<span class="number">32</span>, <span class="number">18</span>), (<span class="number">32</span>, <span class="number">20</span>), (<span class="number">32</span>, <span class="number">22</span>), (<span class="number">32</span>, <span class="number">23</span>),</span><br><span class="line">        (<span class="number">32</span>, <span class="number">29</span>), (<span class="number">32</span>, <span class="number">30</span>), (<span class="number">32</span>, <span class="number">31</span>), (<span class="number">33</span>, <span class="number">8</span>), (<span class="number">33</span>, <span class="number">9</span>), (<span class="number">33</span>, <span class="number">13</span>),</span><br><span class="line">        (<span class="number">33</span>, <span class="number">14</span>), (<span class="number">33</span>, <span class="number">15</span>), (<span class="number">33</span>, <span class="number">18</span>), (<span class="number">33</span>, <span class="number">19</span>), (<span class="number">33</span>, <span class="number">20</span>), (<span class="number">33</span>, <span class="number">22</span>),</span><br><span class="line">        (<span class="number">33</span>, <span class="number">23</span>), (<span class="number">33</span>, <span class="number">26</span>), (<span class="number">33</span>, <span class="number">27</span>), (<span class="number">33</span>, <span class="number">28</span>), (<span class="number">33</span>, <span class="number">29</span>), (<span class="number">33</span>, <span class="number">30</span>),</span><br><span class="line">        (<span class="number">33</span>, <span class="number">31</span>), (<span class="number">33</span>, <span class="number">32</span>)]</span><br><span class="line"><span class="comment">#添加边的源节点和目的节点  </span></span><br><span class="line">drc,dst = tuple(zip(*edge_list))</span><br><span class="line">g.add_edges(src,dst)</span><br><span class="line"><span class="comment">#边是双向的</span></span><br><span class="line">g.add_edges(dst,src)</span><br></pre></td></tr></table></figure>
<h2><span id="22-获取节点和边的个数">2.2. 获取节点和边的个数</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看节点和边个数</span></span><br><span class="line">g.number_of_nodes()</span><br><span class="line">g.number_of_edges()  </span><br><span class="line"></span><br><span class="line"><span class="comment">#查看节点和边类型</span></span><br><span class="line">g.node_attr_schemes()</span><br></pre></td></tr></table></figure>
<h2><span id="23-分配节点和边的特征">2.3. 分配节点和边的特征</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="comment">#分配节点特征</span></span><br><span class="line">g.ndata[<span class="string">'feature'</span>] = torch.eye(<span class="number">34</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取某个节点的特征  </span></span><br><span class="line">G.nodes[<span class="number">2</span>].data[<span class="string">'feature'</span>]</span><br><span class="line">G.nodes[[<span class="number">10</span>,<span class="number">11</span>]].data[<span class="string">'feature'</span>]  </span><br><span class="line"></span><br><span class="line"><span class="comment">#分配边特征,9条边，每条边特征有2个</span></span><br><span class="line">g.edata[<span class="string">'edge_feature'</span>] = torch.randn(<span class="number">9</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#单独为每条边赋值</span></span><br><span class="line">g.edata[<span class="number">1</span>].data[<span class="string">'edge_feature'</span>] = torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">g.edata[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]].data[<span class="string">'edge_feature'</span>] = torch.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#同时指定起点和终点</span></span><br><span class="line">g.edata[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]].data[<span class="string">'edge_feature'</span>] = torch.randn(<span class="number">3</span>,<span class="number">2</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment">#查看图的节点特征和边特征</span></span><br><span class="line">g.ndata,g.edata</span><br></pre></td></tr></table></figure>
<h2><span id="24-删除节点和边特征">2.4. 删除节点和边特征</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g.ndata.pop(<span class="string">'feature'</span>)</span><br><span class="line">g.edata.pop(<span class="string">'edge_feature'</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="25-自定义message函数">2.5. 自定义message函数</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">message_func</span><span class="params">(edges)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    在该函数中，接收一个参数edges，edges有3个成员变量：</span></span><br><span class="line"><span class="string">    edges.src:获取源节点</span></span><br><span class="line"><span class="string">    edges.dst:获取目的节点</span></span><br><span class="line"><span class="string">    edges.data:获取边</span></span><br><span class="line"><span class="string">    主要是向目的节点传递消息，返回的格式是dict</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> &#123; <span class="string">'alpha'</span>: alpha, <span class="string">'state'</span>: edge.src[<span class="string">'state'</span>] &#125;</span><br></pre></td></tr></table></figure>
<h2><span id="26-自定义reduce函数">2.6. 自定义reduce函数</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_func</span><span class="params">(nodes)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    源节点通过message函数，将消息发送给目的节点，目的节点接收多个邻居发来的消息，并存储在mailbox中，reduce函数聚合多个邻居发来的消息,并以dict的形式返回。</span></span><br><span class="line"><span class="string">    reduce函数，接收一个参数nodes，nodes有2个成员变量</span></span><br><span class="line"><span class="string">    nodes.data:获取节点的特征</span></span><br><span class="line"><span class="string">    nodes.mailbox:获取message函数返回的值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    state = nodes.mailbox[<span class="string">'state'</span>]</span><br><span class="line">    alpha = nodes.mailbox[<span class="string">'alpha'</span>]</span><br><span class="line">    alpha = nd.softmax(alpha, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    new_state = nd.relu(nd.sum(alpha * state, axis=<span class="number">1</span>))  </span><br><span class="line">    <span class="keyword">return</span> &#123; <span class="string">'new_state'</span>: new_state &#125;</span><br></pre></td></tr></table></figure>
<h2><span id="27-注册message和reduce函数">2.7. 注册message和reduce函数</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#自定了message和reduce函数，在graph中注册，以便后续使用 </span></span><br><span class="line">g.register_message_func(message_func)</span><br><span class="line">g.register_reduce_func(reduce_func)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pagerank_batch</span><span class="params">(g)</span>:</span></span><br><span class="line">    g.send(g.edges())</span><br><span class="line">    g.recv(g.nodes())</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果没有将自定义的message和reduce函数注册，使用以下语句</span></span><br><span class="line">    g.send(g.edges(),message_func)</span><br><span class="line">    g.recv(g.nodes(),reduce_func)</span><br></pre></td></tr></table></figure>
<h2><span id="28-update_all更新节点特征">2.8. update_all更新节点特征</span></h2><p>该方法是上面方法的高级版本<br><code>DGLGraph.update_all(message_func=&#39;default&#39;, reduce_func=&#39;default&#39;, apply_node_func=&#39;default&#39;)</code><br>传入的参数是message函数名，reduce函数名，UDF函数名,如果不传入，使用默认值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pagerank_level2</span><span class="params">(g)</span>:</span></span><br><span class="line">    <span class="comment"># g.update_all()</span></span><br><span class="line">    g.update_all(self.message_func,self.reduce_func)</span><br></pre></td></tr></table></figure>
<h2><span id="29-高级用法">2.9. 高级用法</span></h2><p><a href="https://docs.dgl.ai/en/latest/tutorials/basics/3_pagerank.html" target="_blank" rel="noopener">PageRank实现</a></p>
<ul>
<li><p><code>dgl.function.copy_src(src, out)</code>:需要指定源节点的名称，和message的key值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line">message_func = dgl.function.copy_src(<span class="string">'feature'</span>, <span class="string">'state'</span>)</span><br><span class="line">等价于  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">message_func</span><span class="params">(edges)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'state'</span>: edges.src[<span class="string">'feature'</span>]&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>dgl.function.sum(msg, out)</code>:用于对目的节点的mailbox进行求和，需要指定message的key值和输出的名称。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line">reduce_func = dgl.function.sum(<span class="string">'state'</span>, <span class="string">'new_state'</span>)</span><br><span class="line">等价于</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_func</span><span class="params">(nodes)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'new_state'</span>: torch.sum(nodes.mailbox[<span class="string">'state'</span>], dim=<span class="number">1</span>)&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dgl.function <span class="keyword">as</span> fn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pagerank_builtin</span><span class="params">(g)</span>:</span></span><br><span class="line">    N = <span class="number">100</span>  <span class="comment"># number of nodes</span></span><br><span class="line">    DAMP = <span class="number">0.85</span>  <span class="comment"># damping factor</span></span><br><span class="line">    g.ndata[<span class="string">'pv'</span>] = g.ndata[<span class="string">'pv'</span>] / g.ndata[<span class="string">'deg'</span>]</span><br><span class="line">    g.update_all(message_func=fn.copy_src(src=<span class="string">'pv'</span>, out=<span class="string">'m'</span>,</span><br><span class="line">                reduce_func=fn.sum(msg=<span class="string">'m'</span>,out=<span class="string">'m_sum'</span>))</span><br><span class="line">    g.ndata[<span class="string">'pv'</span>] = (<span class="number">1</span> - DAMP) / N + DAMP * g.ndata[<span class="string">'m_sum'</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>GCN</tag>
        <tag>DGL</tag>
      </tags>
  </entry>
  <entry>
    <title>XGBoost</title>
    <url>/2019/11/02/XGBoost/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;最近使用XGBoost做一个二分类的任务。记录XGBoost的主要参数和调参过程。<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#1-xgboost">1. XGBoost</a><ul>
<li><a href="#11-xgboost%e7%9a%84%e4%bc%98%e5%8a%bf">1.1. XGBoost的优势</a></li>
<li><a href="#12-%e5%8f%82%e6%95%b0">1.2. 参数</a></li>
<li><a href="#13-%e8%b0%83%e5%8f%82">1.3. 调参</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-xgboost">1. XGBoost</span></h1><p>&ensp;&ensp;&ensp;&ensp;XGBoost是一种十分精致的算法，可以处理各种不规则的数据。<br>构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难，因为涉及到很多参数。所以为了提高模型的表现，参数的调整十分必要。</p>
<h2><span id="11-xgboost的优势">1.1. XGBoost的优势</span></h2><ul>
<li>正则化<br>正则化防止过拟合，实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。</li>
<li>缺失值处理<br>XGBoost内置处理缺失值的规则。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值的处理方法</li>
</ul>
<h2><span id="12-参数">1.2. 参数</span></h2><p>&ensp;&ensp;&ensp;&ensp;XGBoost实际上是很多CART树堆叠起来。传入的特征可以含有None值。XGBoost有很多参数，使用GridSearchCV进行网格搜索时比较耗时。  </p>
<p>使用pip install xgboost安装</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">XGBClassifier(</span><br><span class="line">        base_score=<span class="number">0.5</span>, </span><br><span class="line">        booster=<span class="string">'gbtree'</span>, </span><br><span class="line">        colsample_bylevel=<span class="number">1</span>,</span><br><span class="line">        colsample_bytree=<span class="number">1</span>, </span><br><span class="line">        gamma=<span class="number">0</span>, </span><br><span class="line">        learning_rate=<span class="number">1</span>, </span><br><span class="line">        max_delta_step=<span class="number">0</span>,</span><br><span class="line">        max_depth=<span class="number">2</span>, </span><br><span class="line">        min_child_weight=<span class="number">1</span>, </span><br><span class="line">        missing=<span class="keyword">None</span>, </span><br><span class="line">        n_estimators=<span class="number">2</span>,</span><br><span class="line">        n_jobs=<span class="number">1</span>, </span><br><span class="line">        nthread=<span class="keyword">None</span>, objective=<span class="string">'binary:logistic'</span>, random_state=<span class="number">0</span>,</span><br><span class="line">        reg_alpha=<span class="number">0</span>, </span><br><span class="line">        reg_lambda=<span class="number">1</span>, </span><br><span class="line">        scale_pos_weight=<span class="number">1</span>, </span><br><span class="line">        seed=<span class="keyword">None</span>,</span><br><span class="line">        silent=<span class="keyword">True</span>, </span><br><span class="line">        subsample=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>XGBoost参数有3类：<br><a href="https://www.cnblogs.com/wanglei5205/p/8579244.html" target="_blank" rel="noopener">https://www.cnblogs.com/wanglei5205/p/8579244.html</a><br>（1）通用类别：不需要调整，默认就好：</p>
<ul>
<li>booster：[默认gbtree]<br>选择每次迭代的模型，有两种选择：<br>gbtree：基于树的模型<br>gbliner：线性模型</li>
<li>silent[默认0]<br>当这个参数值为1时，静默模式开启，不会输出任何信息。<br>一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</li>
<li>nthread[默认值为最大可能的线程数]<br>这个参数用来进行多线程控制，应当输入系统的核数。<br>如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。</li>
</ul>
<p>（2）学习目标参数：与任务有关</p>
<ul>
<li>objective:损失函数，支持分类/回归<br>[默认reg:linear]，这个参数定义需要被最小化的损失函数。最常用的值有：<br>binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。<br>multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。<br>在这种情况下，你还需要多设一个参数：num_class(类别数目)。<br>multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</li>
<li><p>eval_metric：评价函数，对于回归问题，默认值是rmse，对于分类问题，默认值是error。<br>典型值有：<br>rmse 均方根误差<br>logloss 负对数似然函数值<br>error 二分类错误率(阈值为0.5)<br>merror 多分类错误率<br>mlogloss 多分类logloss损失函数<br>auc 曲线下面积</p>
</li>
<li><p>seed：随机数的种子，默认为0<br>设置它可以复现随机数据的结果，也可以用于调整参数</p>
</li>
</ul>
<p>（3）booster参数：弱学习器参数，需要仔细调整，会影响模型性能<br>学习率和n_estimators具有相反的关系，建议学习率设小，通过交叉验证确定n_estimators</p>
<ul>
<li>eta[默认0.3]，和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。</li>
</ul>
<p><strong>和树有关的参数</strong></p>
<ul>
<li>min_child_weight[默认1]，最小样本权重的和，用于避免过拟合。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</li>
<li>max_depth[默认6]，树的最大深度。 用来避免过拟合的。max_depth越大，模型越复杂，学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10</li>
<li>gamma[默认0]，Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。   </li>
<li>subsample[默认1]<br>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。<br>典型值：0.5-1  </li>
<li>colsample_bytree[默认1]<br>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。<br>典型值：0.5-1  </li>
</ul>
<p><strong>和正则化有关的参数</strong></p>
<ul>
<li>lambda[默认1]<br>权重的L2正则化项。(和Ridge regression类似)。<br>这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。  </li>
<li><p>alpha[默认1]<br>权重的L1正则化项。(和Lasso regression类似)。<br>可以应用在很高维度的情况下，使得算法的速度更快。<br><strong>样本不均衡</strong></p>
</li>
<li><p>scale_pos_weight[默认1]<br>正样本占的比重，为1时表示正负样例比重是一样的。当正样本较少时，正样本:负样本=1:9，将scale_pos_weight设置为9，scale_pos_weight=负样本个数/正样本个数。在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。</p>
</li>
</ul>
<h2><span id="13-调参">1.3. 调参</span></h2><ol>
<li>先给定一个较高的学习率(learning rate)，一般情况下，学习率为0.1，但是对于不同的问题，理想的学习率在0.05~0.3之间波动。先调节决策树的数量n_estimators</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">20</span>,<span class="number">40</span>, <span class="number">60</span>, <span class="number">80</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">model = XGBClassifier(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">5</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'参数的最佳取值：&#123;0&#125;'</span>.format(optimized_GBM.best_params_))</span><br><span class="line">print(<span class="string">'最佳模型得分:&#123;0&#125;'</span>.format(optimized_GBM.best_score_))</span><br><span class="line">display(pd.DataFrame(optimized_GBM.cv_results_).T)</span><br></pre></td></tr></table></figure>
<ol>
<li>在给定的learning rate和n_eatimators情况下，对决策树特定参数调优(max_depth,min_child_weight,gamma,subsample,colsample_bytree) </li>
<li>max_depth和min_child_weight参数调优</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'max_depth'</span>: list(range(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)), <span class="string">'min_child_weight'</span>: list(range(<span class="number">1</span>,<span class="number">7</span>,<span class="number">1</span>))&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">60</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>gamma参数调优<br>Gamma参数取值范围可以很大，我这里把取值范围设置为5了。你其实也可以取更精确的gamma值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'gamma'</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">350</span>, <span class="string">'max_depth'</span>: <span class="number">3</span>, <span class="string">'min_child_weight'</span>: <span class="number">5</span>, </span><br><span class="line">              <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>subsamplehe colsample_bytree参数,相当于每个树的样本和特征个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> cv_params = &#123;  </span><br><span class="line">  <span class="string">'subsample'</span>: [i / <span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>, <span class="number">10</span>)],  </span><br><span class="line">  <span class="string">'colsample_bytree'</span>: [i / <span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>, <span class="number">10</span>)]  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>正则化参数调优<br>下一步应用正则化来降低过拟合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'reg_alpha'</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">'reg_lambda'</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>学习率调优<br>最后使用较低的学习率</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2019/11/02/Transformer/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;《Attention is all you need》是Google Brain在2017年发表在NIPS的一篇文章。虽然在这篇文章之前，也在用Attention，但在这篇文章中，正式提出了Attention的概念，从此Attention在各个领域得到了广泛的应用。</p>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">论文地址</a>  </p>
<p><a href="https://zhuanlan.zhihu.com/p/102591791" target="_blank" rel="noopener">你应该知道的transformer</a></p>
<a id="more"></a>
<!-- TOC -->
<ul>
<li><a href="#1-transformer%e8%ae%b2%e8%a7%a31">1. Transformer讲解1</a><ul>
<li><a href="#11-%e4%bd%8d%e7%bd%ae%e5%b5%8c%e5%85%a5">1.1. 位置嵌入</a></li>
<li><a href="#12-self-attention">1.2. self-attention</a></li>
<li><a href="#13-%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e5%92%8clayernorm">1.3. 残差连接和LayerNorm</a></li>
<li><a href="#14-transformer%e7%9a%84%e6%95%b4%e4%bd%93%e7%bb%93%e6%9e%84">1.4. transformer的整体结构</a></li>
</ul>
</li>
<li><a href="#2-tramsformer%e8%ae%b2%e8%a7%a32">2. Tramsformer讲解2</a><ul>
<li><a href="#21-multi-head">2.1. multi-head</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650411699&amp;idx=3&amp;sn=83286bfa620ebe7297759fb78c31286c&amp;chksm=becd94e989ba1dff4f933b69d5c74b6cc5c41026e3bed6e5067361a91863b7b6169335ff3326&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">参考资料</a></p>
<p>Latex公式在VSCode中显示还正常，博客上显示错乱。。。，凑合看吧</p>
<h1><span id="1-transformer讲解1">1. Transformer讲解1</span></h1><p><a href="https://spaces.ac.cn/archives/4765" target="_blank" rel="noopener">Attention is All You Need浅读（简介+代码）</a></p>
<p><a href="https://github.com/aespresso/a_journey_into_math_of_ml/blob/master/03_transformer_tutorial_1st_part/transformer_1.ipynb" target="_blank" rel="noopener">transformer教程</a></p>
<p><a href="https://www.bilibili.com/video/av58239477/" target="_blank" rel="noopener">transformer视频讲解</a></p>
<p>transformer和LSTM的最大区别是：LSTM是迭代的，是一个接一个字，当这个字过完LSTM单元，才可以进下一个字。Transformer的训练是并行的，就是所有的字是全部同时训练的，加快了训练效率。但是这样字之间的顺序是丢失的，transformer引入position embedding来捕获字与字之间的位置关系。<br>transformer的结构分为编码器和解码器：</p>
<p><img src="/2019/11/02/Transformer/transformer.png" alt=""></p>
<p>先把一个句子输入到编码器，得到一个隐藏层，把隐藏层输入到解码器，得到输出的序列。例如在机器翻译中，输入是why do you work？通过编码器得到一个隐藏层，输入到解码器中，先给解码器一个start符，开始翻译，解码器输出翻译的第一个字‘为’，将‘为’输入到解码器中，输出‘什’，然后再输入到解码器中，直到解码器输出‘结束符’停止。</p>
<p><img src="/2019/11/02/Transformer/transformer1.png" alt=""></p>
<p>Transformer的编码器分为4部分，分别是：</p>
<ol>
<li>位置嵌入</li>
<li>多头注意力机制</li>
<li>残差</li>
<li>Positionwise FFN</li>
</ol>
<p><img src="/2019/11/02/Transformer/transformer-all1.png" alt=""></p>
<h2><span id="11-位置嵌入">1.1. 位置嵌入</span></h2><p><img src="/2019/11/02/Transformer/PE.png" alt=""></p>
<p>其中<br>$pos$：字在句子中的位置，比如一句话有10个字，pos从0~9<br>$i$：embedding dimension的维度，比如一个字的字向量有256维，则$i$的取值为0~255。<br>$d_{model}$：总共字向量的维度，即256.<br>如果一句话中有10个字，pos从0至9，一个字向量维度是256，从0~255.则<br>第0个字的position embedding为$PE_{(0,0)},PE_{(0,1)},…,PE_{(0,254)},PE_{(0,255)}$.如果字向量维度的下标是偶数使用$sin$，为奇数使用$cos$。使用以上公式可以区分字与字之间的位置信息。<br>在输入的时候将每个字的词嵌入和位置嵌入相加，作为总体的输入。</p>
<h2><span id="12-self-attention">1.2. self-attention</span></h2><ol>
<li><p>经过Embedding-lookup查表得到字向量，和position embedding得到位置嵌入，两者相加，得到一个字的最终嵌入表示，输入的维度为$X \in R^{batch_size \quad <em> \quad seq_len \quad </em> \quad embedding_dim}$<br>比如一个batch中有32个句子，每个句子有10个单词，embedding_dim=100，则输入的维度为$X \in R^{32<em>10</em>100}$</p>
</li>
<li><p>然后将$X_{embedding}$进行3次线性变换，$W_QX_{embedding},W_KX_{embedding},W_VX_{embedding}$得到$Q,K,V$,维度都是$R^{batch_size \quad <em> \quad seq_len \quad </em> \quad embedding_dim}$  </p>
</li>
<li>对$Q,K,V$进行分割，即多头注意力机制，其中head的个数是一个超参数，注意：$embedding \quad dimension必须能够整除head$,即一个矩阵变成$h$个矩阵，分割之后$Q,K,V$的维度为$[batch_size,seq_len,h,embedding_dimension/h]$,之后把$Q,K,V$的$seq_len,h$的维度进行转置，为了方面后面的计算，转置之后的$Q,K,V$的维度为$[batch_size,h,seq_len,embedding_dimension/h]$  </li>
<li>拿出一个$head$，即$Q*K^T$,得到的维度是$[batch_size,h,seq_len,seq_len]$,每个字与每个字之间的注意力，每一行表示当前这个字和所有字的关系。如果2个字之间的意思越相近，得到的注意力也越大。对每一行做softmax归一化，即每一行的和为1，得到归一化之后的注意力矩阵。</li>
<li>将注意力矩阵给$V$加权，即让所有字的信息融入到当前字中，得到当前字的一个加权表示。最终让每一个字都融合所有字的信息。得到的$V$的维度为$[batch_size,h,seq_len,embedding_dimension/h]$.</li>
<li>在训练的时候，通常多句话进行计算，即形成一个mini-batch。mini-batch中的句子的长度不一样，找出句子的最大长度max_seq_len，将短的句子补成和最大句子一样的长度，使用0padding。假设max_seq_len=10，一个句子的长度为7，即得到的attention矩阵，下面3行和右边3列都是0。在对attention矩阵做softmax会出问题，因为softmax计算涉及到指数计算，$e^0=1$,即经过softmax计算attention不为0，让无效的部分参与了计算。为了解决这个问题，需要用一个mask让这些无效的区域不参与计算，一般给无效的区域加一个很大的负数的偏置，$Z_illegal$表示无效的区域，加上一个很大的负数，变成负数，即通过softmax指数计算结果还是0。  </li>
<li>不同的head得到的结果concat起来，才能恢复到原来的维度[batch_size,seq_len,embedding_dimension]。不同的head关注的点不一样，可能有的head关注的local的关系，有的head关注的是global的关系。  </li>
<li>最后一步是PositionwiseFFN，其实就是2层全连接，对输出$[batch_size,seq_len,embedding_dimension]$，经过2次线性变换和一个Relu激活函数。Relu=max(0,x)</li>
</ol>
<p><img src="/2019/11/02/Transformer/FFN.png" alt=""></p>
<p><img src="/2019/11/02/Transformer/multi.png" alt=""></p>
<p><img src="/2019/11/02/Transformer/multi1.png" alt=""></p>
<p><img src="/2019/11/02/Transformer/multi2.png" alt=""></p>
<p>Attention的总体架构如下：给出Q，求Q和所有K的attention，然后使用attention和value加权求和，得到加权的value。</p>
<p><img src="/2019/11/02/Transformer/qkv.png" alt=""></p>
<h2><span id="13-残差连接和layernorm">1.3. 残差连接和LayerNorm</span></h2><p><img src="/2019/11/02/Transformer/res1.png" alt=""></p>
<h2><span id="14-transformer的整体结构">1.4. transformer的整体结构</span></h2><p><img src="/2019/11/02/Transformer/transformer-all.png" alt=""></p>
<h1><span id="2-tramsformer讲解2">2. Tramsformer讲解2</span></h1><p><a href="https://www.bilibili.com/video/av56239558?from=search&amp;seid=9460464372943296837" target="_blank" rel="noopener">李宏毅视频讲解</a></p>
<p>其中<br>$x1,x2,x3,x4表示4个词向量，组成一个sequence,$$每一个a分别乘上3个矩阵，得到q,k,v，然后使用每一个q对每个k做attention$。</p>
<p><img src="/2019/11/02/Transformer/a1.png" alt=""></p>
<p>将每一个q对k做attention，然后将结果进行softmax归一化，相加为1.</p>
<p><img src="/2019/11/02/Transformer/a2.png" alt=""></p>
<p>其中得到的$\hat{a}_{1,1},…\hat{a}_{1,4}$表示每个词对词1的attention，然后将attention和v做点积，再相加得到词1新的加权词向量b1，即b1包含了所有词对词1的影响，产生b1的时候已经看到了所有的词，即global attention，再远的词都可以看到。</p>
<p><img src="/2019/11/02/Transformer/a3.png" alt=""></p>
<p>同理可以算出b2</p>
<p><img src="/2019/11/02/Transformer/a4.png" alt=""></p>
<p>即一个序列a1…a4，通过一个self-attention layer得到一个新的序列b1…b4。此时b1…b4可以并行计算。</p>
<p><img src="/2019/11/02/Transformer/a5.png" alt=""></p>
<p>self-attention的输入看做矩阵$I$,经过self-attention layer变成矩阵$O$。</p>
<p><img src="/2019/11/02/Transformer/a6.png" alt=""></p>
<h2><span id="21-multi-head">2.1. multi-head</span></h2><p>如果head=2，生成2个$q,k,v$，维度也是原来的一半。经过self-attention操作得到$b^{i,1}和b^{i,2}$,将$b^{i,1}和b^{i,2}进行concat，得到b^i，或者concat之后乘上矩阵W得到b^i.$  </p>
<p><img src="/2019/11/02/Transformer/a7.png" alt=""></p>
<p><img src="/2019/11/02/Transformer/a8.png" alt=""></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>评价指标</title>
    <url>/2019/10/18/metrics/</url>
    <content><![CDATA[<p>介绍在回归问题中的主要评价指标，以及各自的特点</p>
<a id="more"></a>
<!-- TOC -->
<ul>
<li><a href="#1-%e9%97%ae%e9%a2%98">1. 问题</a></li>
<li><a href="#2-%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98">2. 回归问题</a><ul>
<li><a href="#21-mae%e5%b9%b3%e5%9d%87%e7%bb%9d%e5%af%b9%e8%af%af%e5%b7%ae">2.1. MAE(平均绝对误差)</a></li>
<li><a href="#22-mse%e8%af%af%e5%b7%ae%e5%b9%b3%e6%96%b9%e5%b9%b3%e5%9d%87%e5%80%bc">2.2. MSE(误差平方平均值)</a></li>
<li><a href="#23-rmse%e5%9d%87%e6%96%b9%e6%a0%b9%e8%af%af%e5%b7%ae">2.3. RMSE(均方根误差)</a></li>
<li><a href="#24-mape%e5%b9%b3%e5%9d%87%e7%bb%9d%e5%af%b9%e7%99%be%e5%88%86%e6%af%94%e8%af%af%e5%b7%ae">2.4. MAPE(平均绝对百分比误差)</a></li>
<li><a href="#25-%e6%80%bb%e7%bb%93">2.5. 总结</a></li>
</ul>
</li>
<li><a href="#3-%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98">3. 分类问题</a><ul>
<li><a href="#31-micro-f1">3.1. Micro-F1</a></li>
<li><a href="#32-macro-f1">3.2. Macro-F1</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-问题">1. 问题</span></h1><p>在回归问题中，标签y的分布不均衡，范围在[1,88]，其中70%的值都在1左右。解决的方法：从损失函数入手，设计不同的评价指标，让其更关注一些大的y值。</p>
<h1><span id="2-回归问题">2. 回归问题</span></h1><p><a href="https://zhuanlan.zhihu.com/p/74627482" target="_blank" rel="noopener">Metircs参考资料</a></p>
<h2><span id="21-mae平均绝对误差">2.1. MAE(平均绝对误差)</span></h2><p><img src="/2019/10/18/metrics/mae.png" alt=""></p>
<p>绝对误差的平均值。</p>
<ol>
<li>范围在$[0,\infin]$</li>
<li>单看MAE并不能看出这个模型的好坏，因为不知道y的平均值。比如MAE=10,y的平均值为1000，则这个模型还不错，但是如果y的平均值为1，那这个模型就非常不好。</li>
<li>改进：$MAE/y_{mean}$</li>
</ol>
<h2><span id="22-mse误差平方平均值">2.2. MSE(误差平方平均值)</span></h2><p><img src="/2019/10/18/metrics/mse.png" alt=""></p>
<ol>
<li>范围在$[0,\infin]$,  </li>
<li>很多算法的loss函数都是基于MSE的，因为MSE计算速度快，比RMSE更容易操作。但是我们很少把MAE作为最终的评价指标。</li>
<li>更关注一些y比较大的值，但是它的代价是对异常点过于敏感。如果预测出的y很不合理，则它的误差比较大，从而对RMSE的值有很大的影响。</li>
</ol>
<h2><span id="23-rmse均方根误差">2.3. RMSE(均方根误差)</span></h2><p><img src="/2019/10/18/metrics/rmse.png" alt=""></p>
<p>在MSE上加了根号，误差的结果和数据是一个级别，在数量级上更直观，如果RMSE=10，可以认为回归问题效果与真实结果平均相差10。</p>
<ol>
<li>范围在$[0,\infin]$</li>
<li>RMSE把更大的注意力放在y更大的值上，只有更大的y值预测准确了，模型的效果才会好。</li>
</ol>
<h2><span id="24-mape平均绝对百分比误差">2.4. MAPE(平均绝对百分比误差)</span></h2><p><img src="/2019/10/18/metrics/mape.png" alt=""></p>
<ol>
<li>范围$[0,\infin]$，当MAPE=0%表示完美模型，MAPE大于100%表示劣质模型。</li>
<li>当真实值有数据等于0时，存在分母为0的情况，该公式不可用</li>
<li>比如将y1预测为1.5，和100预测为100.5，差值是一样的，。即1的MAPE比100的MAPE大很多。以MAPE作为loss函数时，更加关注y比较小的值。</li>
<li>单看MAPE的大小是没有意义的，因为MAPE是个相对值，而不是绝对值。MAPE只能用来对不同模型同一组数据的评估。比如对于同一组数据，模型A的MAPE比模型B小，可以说明模型A比模型B好。但是如果说MAPE=10%，并不能判断这个模型好还是不好</li>
</ol>
<h2><span id="25-总结">2.5. 总结</span></h2><p>综上，在选用评价指标时，需要考虑</p>
<ol>
<li><p>数据中是否有0，如果有0值就不能用MPE、MAPE之类的指标；</p>
</li>
<li><p>数据的分布如何，如果是长尾分布可以选择带对数变换的指标，中位数指标比平均数指标更好；</p>
</li>
<li><p>是否存在极端值，诸如MAE、MSE、RMSE之类容易受到极端值影响的指标就不要选用；</p>
</li>
<li><p>得到的指标是否依赖于量纲(即绝对度量，而不是相对度量)，如果指标依赖量纲那么不同模型之间可能因为量纲不同而无法比较；</p>
</li>
</ol>
<h1><span id="3-分类问题">3. 分类问题</span></h1><p>在二分类任务中，使用Precison，Recall和F1值来评价分类的效果。<br><img src="/2019/10/18/metrics/hun.png" alt=""></p>
<p><img src="/2019/10/18/metrics/f1.png" alt=""></p>
<p>F1是针对二分类的，对于多分类，有2个常用的指标，Marco-F1和Micro-F1.</p>
<h2><span id="31-micro-f1">3.1. Micro-F1</span></h2><p><img src="/2019/10/18/metrics/micro.png" alt=""></p>
<p>假设对于一个多分类问题，有三个类，分别是1，2，3</p>
<p>$TP_i$表示分类$i$的TP<br>$FP_i$表示分类$i$的FP<br>$TN_i$表示分类$i$的TN<br>$FN_i$表示分类$i$的FN<br>接下来，我们来计算Micro的Precison</p>
<p><img src="/2019/10/18/metrics/precison-mi.png" alt=""></p>
<p>以及Micro的Recall</p>
<p><img src="/2019/10/18/metrics/recall-mi.png" alt=""></p>
<p>然后计算Micro-F1</p>
<p><img src="/2019/10/18/metrics/f1-mi.png" alt=""></p>
<h2><span id="32-macro-f1">3.2. Macro-F1</span></h2><p>先计算每个类的Precison和Rcall，从而计算出每个类的F1，然后将所有类的F1值平均得到Macro-F1。<br>如果数据集中各个类的分布不均衡的话，建议使用Micro-F1。<br>Macro-F1平等的看待各个类别，它的值更容易受少类别的影响Micro则更容易受常见类别的影响。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>keras基础知识</title>
    <url>/2019/09/07/keras%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>最近看到keras容易上手，封装比较好，学习一下<br><a id="more"></a></p>
<h2><span id="11-基础层的介绍">1.1. 基础层的介绍</span></h2><h3><span id="111-flatten">1.1.1. Flatten</span></h3><p>&ensp;&ensp;&ensp;&ensp;Flatten用来将输入“压平”，把多维的输入变成一维。常用在从卷积层到全连接层的过渡，特征全都变成1维就可以输入到全连接层中。Flatten不影响batch的大小。<br>通过<code>keras.layers.Flatten</code>来引入。<br>比如通过卷积层的输出形状为(batch_size,output_channel,width,height),例如(16,64,32,32),通过<code>Flatten</code>层之后变换成(16,64<em>32\</em>32)=(16,65536)，不改变batch_size的大小。</p>
<h3><span id="112-dense">1.1.2. Dense</span></h3><p>Dense全连接层。<br>如果Dense是在第一层的话，需要指定output_dim和input_dim，即<code>Dense(output_dim=64,input_dim=44)</code>。在Dense中的API中，看到其实没有input_dim或者input_shape这个参数，查看源码看到，input_dim和input_shape是在**kwargs中，因为并不是所有的Dense层都需要传入输入的形状，只有第一层需要。可以输入<code>input_dim=44</code>,也可以是<code>input_shape=(44,)</code>。一般都是用input_shape</p>
<p><strong>注意：在input_shape中不包含batch的大小</strong></p>
<p><img src="/2019/09/07/keras基础知识/dense.png" alt=""></p>
<p>如果Dense不是在第一层，则只需要指定output_dim，而input_dim则默认为上一层的输出维度。即<code>Dense(output_dim=64)</code><br>如果需要激活函数，则需要再指定激活函数，例如<br><code>Dense(64,avtivation=&#39;relu&#39;)</code></p>
<h3><span id="113-embedding">1.1.3. Embedding</span></h3><p>Embedding只能作为模型的第一层。基本上用户自然语言处理方面，用来做词嵌入。</p>
<h3><span id="114-lstm">1.1.4. LSTM</span></h3><p>LSTM传入的参数<br><code>LSTM(batch_input_dim=(batch_size,time_steps,input_size),output_dim=cell_size,return_sequences=True,stateful=True)</code></p>
<p>其中</p>
<ul>
<li>input_dim是(batch_size，时间步的个数，每个时间步的特征个数)</li>
<li>output_dim是LSTM中隐藏层单元的个数  </li>
<li>return_sequences默认是False，表示一个时间步长为T的序列，只在最后一个时间步输出一个结果，True表示每个时间步都输出一个结果，保留起来。 </li>
<li>stateful默认是False，表示这个batch与batch之间是不是有联系的，表示这个batch和下一个batch的状态是不是要连起来。</li>
</ul>
<h3><span id="115-merge层">1.1.5. Merge层</span></h3><p>Merge层提供了用于融合<strong>两个层或两个张量</strong>的方法，如果方法以大写字母开头，例如<code>Add()</code>表示融合两个层，如果以小写字母开头，例如<code>add()</code>表示融合两个张量。<br>通过以下来调用<br><code>keras.layers.Add()或keras.layers.add()</code><br>例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">input1 = keras.layers.Input(shape=(<span class="number">16</span>,))</span><br><span class="line">x1 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input1)</span><br><span class="line">input2 = keras.layers.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">x2 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input2)  </span><br><span class="line"><span class="comment"># 相当于added = keras.layers.add([x1, x2])</span></span><br><span class="line">added = keras.layers.Add()([x1, x2])  </span><br><span class="line"></span><br><span class="line">out = keras.layers.Dense(<span class="number">4</span>)(added)</span><br><span class="line">model = keras.models.Model(inputs=[input1, input2], outputs=out)</span><br></pre></td></tr></table></figure>
<h2><span id="12-训练">1.2. 训练</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add()</span><br><span class="line">....</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,loss=<span class="string">'mean_squared_error'</span>,metric=[<span class="string">'mse'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_x,train_y,epochs=<span class="number">1000</span>,batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对测试集进行验证loss或metric</span></span><br><span class="line">model.evaluate(test_x,test_y,batch_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对测试集输出预测的结果。</span></span><br><span class="line">model.predict(test_x)</span><br></pre></td></tr></table></figure>
<p>在训练的时候，有以下3种方法：fit，train_on_batch,fit_gen</p>
<ol>
<li>fit()<br>当使用fit()函数时，首先要保证2个条件：（1）训练数据可以完成的放在内存中，（2）数据已经不需要再做任何处理了，可以直接训练。</li>
<li>train_on_batch()<br>train_on_batch()函数接收一个batch的输入和标签，然后反向传播，更新参数。大部分情况下都不需要用到train_on_batch()，例如<code>cost = train_on_batch(train_x,train_y)</code>,返回值是误差  </li>
</ol>
<h2><span id="13-模型构建">1.3. 模型构建</span></h2><p>在keras中，一些简单的组件可以直接使用def来实现，在这个输入是input，直接使用某些层，得到输出,不使用model。<br>在实现整个模型的框架时，使用<code>model = Model(input=input,output=output)</code>,然后<code>return model</code><br>对于有些层，这个层在<code>keras.layers</code>中没有，这时候就需要自己定义一个层，这个层中的参数需要自己定义。自定义层中的参数也是需要学习的。</p>
<h2><span id="14-callbacks">1.4. Callbacks</span></h2><p>传入fit()函数中的callbacks必须是一个list，里面是一个或多个callback实例。</p>
<h3><span id="141-modelcheckpoint">1.4.1. ModelCheckpoint</span></h3><p>回调函数<code>Callbacks</code>是一组在<strong>训练阶段</strong>被调用的函数集，使用回调函数来查看训练过程中网络内部的状态和统计信息。在模型上调用<code>fit()</code>函数时，可以将<code>ModelCheckpoint</code>传递给训练过程。<br>训练深度学习模型时，<code>Checkpoint</code>是模型的权重。<code>ModelCheckpoint</code>回调类运行定义检查模型权重的位置，文件应如何命名，</p>
<h3><span id="142-earlystopping">1.4.2. EarlyStopping</span></h3><p>EarlyStopping是Callbacks的一种，用于提前停止训练。停止训练的标准是当val_loss或者val_root_mean_square_error不再减少，或者val_acc不在增加。我们在训练模型时，主要目的是获得最好的泛化性能。模型的泛化能力通常使用验证集来评估。模型在训练的时候，模型在训练集上loss一直在变小，但是在验证集上的loss却是先变小后变大。说明出现了过拟合。解决过拟合的方法有2种方法：权重衰减和早停法。早停法就是模型在验证集上的表现开始下降时，停止训练。这样就可以避免过拟合的问题。</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/2019/08/21/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;最近项目中需要用到正则表达式对文件名进行匹配。在上传文件时，如果文件名中含有连续的相同字母或数字，则不允许上传。并且文件名中必须含有设备型号。<br><a id="more"></a><br>正则表达式描述了字符串的匹配模型。</p>
<ul>
<li>+：表示前面的字符必须出现至少一次（1次或多次）<br>eg：runoo+b可以匹配runoob、runooob、runoooob</li>
<li><em>：表示前面的字符可以不出现，也可以出现一次或多次（0次，1次或多次）<br>eg：runoo\</em>b可以匹配runob、runoob、runooob</li>
<li>？：表示前面的字符出现0次或1次。<br>eg：do(es)?可以匹配”do”或”does”<br>eg：colou?r 可以匹配 color 或者 colour</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title>时空领域论文总结</title>
    <url>/2019/08/02/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;这学期看了很多论文，下面把看过的论文总结一下。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e6%80%bb%e7%bb%93">1. 总结</a></li>
<li><a href="#2-poi%e6%8e%a8%e8%8d%90">2. POI推荐</a><ul>
<li><a href="#21-point-of-interest-recommendation-exploiting-self-attentive-autoencoders-with-neighbor-aware-influence-cikm2018">2.1. Point-of-Interest Recommendation Exploiting Self-Attentive Autoencoders with Neighbor-Aware Influence   (CIKM2018)</a></li>
<li><a href="#22-hst-lstm-a-hierarchical-spatial-temporal-long-short-term-memory-network-for-location-prediction2018ijcai">2.2. HST-LSTM: A Hierarchical Spatial-Temporal Long-Short Term Memory Network for Location Prediction（2018IJCAI）</a></li>
</ul>
</li>
<li><a href="#3-%e6%97%b6%e7%a9%ba%e6%95%b0%e6%8d%ae%e9%a2%84%e6%b5%8b">3. 时空数据预测</a><ul>
<li><a href="#31-hyperst-net-hypernetworks-for-spatio-temporal-forecasting2019aaai">3.1. HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting（2019AAAI）</a></li>
<li><a href="#32-restful-resolution-aware-forecasting-of-behavioral-time-series-data2018cikm">3.2. RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data（2018CIKM）</a></li>
<li><a href="#33-spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand2019aaai">3.3. Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand（2019AAAI）</a></li>
<li><a href="#34-revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-predictionaaai2019">3.4. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction（AAAI2019）</a></li>
<li><a href="#35-deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-predictionaaai2017">3.5. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction（AAAI2017）</a></li>
<li><a href="#36-attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting2019aaai">3.6. Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting（2019AAAI）</a></li>
<li><a href="#37-urbanfm-inferring-fine-grained-urban-flows2019kdd">3.7. UrbanFM: Inferring Fine-Grained Urban Flows（2019KDD）</a></li>
<li><a href="#38-deep-multi-view-spatial-temporal-network-for-taxi-demand-prediction2018aaai">3.8. Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction（2018AAAI）</a></li>
<li><a href="#39-urban-traffic-prediction-from-spatio-temporal-data-using-deep-meta-learning2019kdd%e9%83%91%e5%ae%87">3.9. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning（2019KDD郑宇）</a></li>
</ul>
</li>
<li><a href="#4-%e5%9b%be%e5%8d%b7%e7%a7%af">4. 图卷积</a><ul>
<li><a href="#41-semi-supervised-classification-with-graph-convolutional-networks2017iclr">4.1. Semi-Supervised Classification with Graph Convolutional Networks（2017ICLR）</a></li>
<li><a href="#42-diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting2018iclr">4.2. Diffusion Convolutional Recurrent Neural Network Data-Driven Traffic Forecasting（2018ICLR）</a></li>
<li><a href="#43-graph-attention-networks2018iclr">4.3. Graph Attention Networks（2018ICLR）</a></li>
<li><a href="#44-deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning2018aaai">4.4. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning（2018AAAI）</a></li>
</ul>
</li>
<li><a href="#5-time-series-forecasting">5. Time Series Forecasting</a><ul>
<li><a href="#51-multi-horizon-time-series-forecasting-with-temporal-attention-learning2019kdd">5.1. Multi-Horizon Time Series Forecasting with Temporal Attention Learning（2019KDD）</a></li>
<li><a href="#52-enhancing-the-locality-and-breaking-the-memory-bottleneck-of-transformer-on-time-series-forecasting2019nips">5.2. Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting（2019NIPS）</a></li>
</ul>
</li>
<li><a href="#6-traffic-accident%e9%a2%84%e6%b5%8b">6. traffic accident预测</a><ul>
<li><a href="#61-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">6.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference（2016AAAI）</a></li>
<li><a href="#62-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">6.2. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction（2018IEEE-ITSC）</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-总结">1. 总结</span></h1><ul>
<li>在POI推荐上，考虑考虑用户对不同POI的喜爱程度，对每个POI分配不同的权重，同时对一个POI也要考虑这个POI在不同方面的权重（比如饭店在食物和环境的权重）</li>
<li>在时空建模上，时间上考虑不同的方面，比如recent，periodic等方面，在空间上考虑neighbor，function similarity等方面，其中功能相似一般通过POI来度量</li>
<li>考虑不同区域之间的相似性，可以使用2个区域之间的traffic flow，2个区域之间的traffic flow越大，说明这2个区域关联性越强。还可以使用2个区域的POI，2个区域之间的POI越相似，这2个区域的function越相似。</li>
<li>不管time interval是多少，一般只说预测将来多少个时间段的数据，而不是说预测将来多长时间的数据。比如说预测将来5个时间段的数据。如果time interval=30min，则预测的是将来1.5h的数据，如果time interval=1h，则预测的是将来5h的数据。</li>
<li>如果是将城市划分成网格，每个网格都有空间特征，比如这个网格的flow或者speed。使用CNN获取这个区域的空间特征，所有区域的形成的矩阵形状为(F,I,J)，其中F表示每个区域的空间特征个数，I和J表示网格的高和宽。如果将不同时间段的网格图拼接起来，比如将t个时间段的网格按照时间拼接起来，则形成的形状为(F*t,I,J),然后通过卷积来捕获空间特征。但是这样有一个问题，将时间拼接起来形成通道，会损失通道信息。如果预测是所有区域下一个时间段的inflow和outflow，则输出为(2,I,J)。如果预测的是所有区域接下来p个时间段的速度，则输出为(p,I,J)。</li>
<li>比如说网格数据有I*J个网格，每个网格有F个特征，这F个特征都是关于这个网格的特征。一般像外部因素不会放在网格中。因为外部因素，像温度，天气等一般不放在网格中。但也有放在网格中的，比较少。</li>
<li>比如预测第t天的flow，用到该预测天前hour，day，week数据，同时还要考虑外部因素，这里使用的外部因素只考虑被预测当天的外部因素。</li>
<li>对于预测flow问题，如果只预测一个区域的flow，在构造样本的时候对于每一个区域都构造一个以该区域为目标区域的样本。训练模型是用所有区域的样本来训练，预测的时候输出一个区域的flow。并不是预测一个区域，只用这个区域的历史数据来训练，而是用所有的区域来训练。</li>
<li>《2018[AAAI] When will you arrive estimating travel time based on deep neural networks》用户的轨迹数据本来是一个时间序列数据，每个轨迹点使用&lt;经度，纬度&gt;表示，可以使用嵌入将用户的轨迹转换成一个矩阵，使用1D卷积捕获空间关系，然后再送入LSTM中，捕获时间关系。</li>
</ul>
<h1><span id="2-poi推荐">2. POI推荐</span></h1><h2><span id="21-point-of-interest-recommendation-exploiting-self-attentive-autoencoders-with-neighbor-aware-influence-cikm2018">2.1. Point-of-Interest Recommendation Exploiting Self-Attentive Autoencoders with Neighbor-Aware Influence   (CIKM2018)</span></h2><p><a href="http://delivery.acm.org/10.1145/3280000/3271733/p697-ma.pdf?ip=218.247.253.241&amp;id=3271733&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1564715146_1e8dd5a9fd9356658c3d093a628ac7c5" target="_blank" rel="noopener">论文地址</a></p>
<p>论文题目：邻居感知的自注意自编码器的POI推荐</p>
<ul>
<li>挑战<br>（1）建模用户POI之间的非线性关系，原先都是所有的POI权重一样；<br>（2）结合上下文信息，例如POI地理坐标。<br>（3）用户去过的POI是一小部分，而所有的POI非常多，使得POI矩阵变得非常稀疏</li>
<li>模型<br><strong>self-attentive encoder and a neighbor-aware decoder（SAE-NAD）</strong><br>&ensp;&ensp;&ensp;&ensp;通过self-attentive encoder区分用户对访问过的POI的喜好程度，用户的访问POI向量中每个POI的权重不同，这样可以学到更好的user hidden representation。<br>&ensp;&ensp;&ensp;&ensp;通过neighbor-aware decoder结合地理上下文信息，使得用户之前到达区域的附近或相似的区域可达性变大。将访问的POI嵌入和未访问的POI嵌入做内积，基于RBF（2个POI的点对点距离） kernel，来计算访问过的POI对未访问POI的影响。<br>&ensp;&ensp;&ensp;&ensp;为了建模稀疏矩阵，我们给未访问的POI分配相同的小权重，给访问过的POI通过访问频率分配不同的大权重。这样对于每个用户就可以区分未访问，少访问，经常访问的POI   </li>
<li>贡献<br><strong>第一篇使用基于attention的自编码器在POI推荐上</strong></li>
<li><strong>目标</strong><br><strong>根据用户check-in的记录，向用户推荐一系列从未去过的POIS</strong></li>
<li><p>Definitoin<br>POI(类型，经纬度)<br>Check-in(用户id，POI ID，时间)</p>
<p><img src="/2019/08/02/论文总结/1.png" alt=""></p>
</li>
</ul>
<p>&ensp;&ensp;&ensp;&ensp;使用stack autoencoder（堆叠自编码器）学习用户隐藏表示。输出是一个用户去过和没去过的POI，一个n维的0/1向量，其中的$l_2,l_4,l_6$表示去过的POI下标，根据去过POI的下标在POI嵌入矩阵$W^{(1)}$中取出对应的POI向量组成矩阵$W^{(1)_{[L_u]}}$，得到用户已经去过区域的POI嵌入，这n个POI有些POI更能表示用户的喜好，用到self-attentive机制，为$W^{(1)_{[L_u]}}$中的每个POI嵌入学习不同的权重，来构成user hidden representation。一般的attention给每个POI嵌入学习一个分数，这个分数只能反映POI在一方面的重要性。例如饭店，在味道方面这个用户喜欢这家饭店，但是在环境方面用户不喜欢，为了从不同方面捕获用户的喜好，使用multiple-dimension attention，分别对不同方面进行打分。</p>
<p><img src="/2019/08/02/论文总结/2.png" alt=""><br><img src="/2019/08/02/论文总结/3.png" alt=""></p>
<p>这n个POI嵌入向量，从$d_a$个方面进行打分，得到的$A_u \in R^{d_a \times n}$ ,然后把n个POI嵌入乘上分数再相加，得到第u个用户隐藏表示$Z^{(1)}_u \in R^{d_a  times H_1}$，从$d_a$个方面来表示这个用户，为了让这个矩阵输入到encoder中，通过全连接将$d_a$个方面整合成一个方面，从一个矩阵变成一个向量。 然后经过2个encoder得到$z^{(3)}_u$<br><img src="/2019/08/02/论文总结/4.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;用户访问过的POI会对没访问过的POI有影响，影响程度有着2个POI的相似性和距离决定。和已访问过的POI相似或邻近的POI用户访问的概率比较大。使用内积的方式求2个POI的相似性，但是这没有考虑到2个POI之间的距离，我们采用RBF kernel根据2个POI之间的距离计算2个POI之间的相关性，得到一个N*N的矩阵。然后把相似性和距离相关性相乘，得到2个POI的最终区域相关性，<br><img src="/2019/08/02/论文总结/6.png" alt=""></p>
<p>解码器阶段：将用户隐藏进行解码。其中$z^{(3)}_u$表示根据用户去过的POI得到的用户表示，$p_u$表示去过的POI对未去过的POI的影响。<br><img src="/2019/08/02/论文总结/7.png" alt=""></p>
<ul>
<li>总结<br>根据一个用户之前去过的POI对这个用户进行表示，不同的POI有不同的权重，同一个POI在不同的方面也有不同的权重，得到一个user hidden representation，将用户表示经过2层encoder，然后在解码的时候，用到邻居信息，去过的POI对未去过的POI有影响，影响大小根据这2个POI之间的相似性和距离决定。</li>
</ul>
<h2><span id="22-hst-lstm-a-hierarchical-spatial-temporal-long-short-term-memory-network-for-location-prediction2018ijcai">2.2. HST-LSTM: A Hierarchical Spatial-Temporal Long-Short Term Memory Network for Location Prediction（2018IJCAI）</span></h2><p>这篇论文没怎么看懂<br>弱实时预测，向用户推荐下一分钟或小时要去的地点。 在LSTM中使用时空信息。<br><img src="/2019/08/02/论文总结/8.png" alt=""></p>
<ul>
<li>贡献<br>提出HST-LSTM结合时空影响到LSTM中，来解决位置预测中数据稀疏的问题。<br>HST-LSTM建模用户的历史访问序列，使用encoder-decoder的方式来提高预测性能。</li>
<li>模型<br><strong>HST-LSTM model</strong><br>AOI:具有一种功能的区域，例如购物中心，工作区<br>Visit Record：用户在一段时间内（几周或几个月）访问的所有AOIS<br>Visit Session：一个用户在在一个时间段（一天）访问的AOI序列，在一个session中的AOI有强烈的相关性，揭示了用户的运动模式。<br>Visit Session Sequence：一个用户连续的visit sessions，可以作为上下文信息预测下一个AOI。<br>多个AOI组成一个visit session，多个visit session组成visit record，</li>
<li>目标<br>用户在一段时间内访问了N个AOI，这N个AOI按照时间排序（AOI可能有重复），给定前j个用户去过的AOI，预测接下来用户要去的N-j个AOI，是一个多对多的预测。</li>
</ul>
<p><img src="/2019/08/02/论文总结/9.png" alt=""><br>在LSTM中3个门控机制中，输入门，遗忘门，输出门加入时空因素。其中s和q都是d维的向量，分别表示空间和时间的影响因素。<br><img src="/2019/08/02/论文总结/10.png" alt=""><br>基于提出的ST-LSTM，对每个visit session建模。使用STLSTM，每一个时间步输入的信息是一个visit session中的AOI嵌入，使用一个STLSTM对一个session进行建模，输出最后一个时间步的隐藏状态$h^i_e$作为第$i$个session的表示。对$n-1$个session进行建模得到n-1个隐藏状态，将这n-1个隐藏状态使用Contextual LSTM建模长期的visit sequence，在global context encoding阶段，每个时间步输入的是上一个STLSTM中session的表示。<br><img src="/2019/08/02/论文总结/11.png" alt=""><br><img src="/2019/08/02/论文总结/12.png" alt=""><br>在Decoding阶段，使用前i-1个session的推断接下来要去的AOI。</p>
<ul>
<li>总结<br>在LSTM阶段加入时空信息，提出STLSTM。<br>使用encoder-decoder来实现POI推荐，encoder和decoder都是LSTM</li>
</ul>
<h1><span id="3-时空数据预测">3. 时空数据预测</span></h1><h2><span id="31-hyperst-net-hypernetworks-for-spatio-temporal-forecasting2019aaai">3.1. HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting（2019AAAI）</span></h2><p>题目：超时空网络预测<br>以前的方法分别对时间和空间分别建模，没有考虑到时间和空间内在的因果关系。空间的属性（POI或路网）影响空间的特征（工作区或居民区），从而影响时间特征（inflow trend）<br><img src="/2019/08/02/论文总结/13.png" alt=""></p>
<ul>
<li>目标<br>预测一个区域。根据这个区域的空间和时间特征，预测ST数据，例如空气质量预测，交通流量预测。<br>本篇论文提出一个框架，包含3部分：空间模块，时间模块，演绎模块（deduction module）。<br><strong>这是第一篇考虑空间和时间特征内在因果关系的框架。</strong><br>使用spatial module从spatial attribute建模spatial characteristic，然后使用deduction module从spatial characteristic建模temporal characteristic.<br><img src="/2019/08/02/论文总结/14.png" alt=""><br>Spatial module：两阶段模块，在第一阶段，将spatial attribute建模成spatial characteristic。在第二阶段，生成多个独立的因素，deduction module使用它们来建模时间模块中对应神经网络的参数。例如a—》A，b—》B，c—》C。空间模块像一个hypernetwork。<br>Temporal module：应用不同的HyperST层，HyperST层的参数由deduction module计算得到，可以被看做object的时间characteristic。<br>Deduction module：连接空间和时间模块，空间和时间的内在因果关系被考虑进去。<br><img src="/2019/08/02/论文总结/15.png" alt=""> </li>
</ul>
<h2><span id="32-restful-resolution-aware-forecasting-of-behavioral-time-series-data2018cikm">3.2. RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data（2018CIKM）</span></h2><p>基于不同粒度的行为时间序列数据预测<br>行为数据，例如购买行为，邮件行为。<br>现在的预测方法经常仅仅使用一种时间粒度（天或周），然而现在的行为时间数据经常有多重时间粒度模式，每种时间模式之间相互依赖。本篇论文提出RESolution-aware Time series Forecasting（RESTFul），使用循环神经网络来编码不同粒度的时间模式成一个低维表示。不同时间粒度的表示在融合阶段，使用卷积融合框架。最终学到的conclusive embedding向量输入到MLP中用来预测行为时间序列数据。<br><strong>这是第一篇使用多时间粒度来预测时间序列数据。</strong><br>不同粒度的时间序列长度是一样的，比如为5，就表示最近3天，5周。</p>
<ul>
<li>定义<ol>
<li>Behavioral Time Series：表示一段时间段内的行为数据，$X=[x_1,x_2…x_t,…x_T]，其中t \in [1,2,…T],x_t是一个标量，数值或离散值，表示第t个时间段的行为数据$</li>
<li>Behavioral Time Series Forecasting：给定历史行为时间序列数据，给定前k个时间段的历史数据$[x_T-k,…x_T],预测x_{T’},其中T’ &gt;= T$  </li>
<li>Interval Resolution $\alpha$:$s_t和s_{t+1}之间的时间差距$，如果为1天，表示1天测量一次，如果为1周，表示1周测量一次。</li>
<li>Remporal Resolution $\beta$:如果$\beta=week$表示一次测量1周，如果为1day，表示一次测量1天。</li>
<li>限制$\alpha &gt;= \beta$  如果$\alpha=1week,\beta=1day,表示1周测量1次，1次测1天。 \alpha=1week,\beta=1week,表示1周测量1次，1次测1周。其中x_t = g(x_t,x_{t+1}…x_{x+\beta})的聚合值，例如一周的平均值或最大值$ </li>
</ol>
</li>
<li>模型<br>RESTFul有2个阶段，第一个阶段，使用循环神经网络来编码不同时间粒度的时间模式。第2个阶段，使用卷积融合模块来融合不同时间粒度的表示。<br><img src="/2019/08/02/论文总结/16.png" alt="">  </li>
</ul>
<p>$\alpha,\beta \in {day:1,week:7},这样&lt;\alpha,\beta&gt;就有3种组合，分别是<1,1>,<7,1>,<7,7>$，对于每种组合都有一组行为时间序列数据，对于每组时间序列数据，都使用GRU来这个序列进行时间建模，得到最后一个时间步的隐藏状态向量。将每种组合得到的隐藏状态拼接起来，最终得到一个张量，维度是$R^{|\alpha|\times|\beta|\times d_s}$。然后使用卷积操作，先使用2次2*2的卷积，同时使用padding保证得到的结果大小不变，只改变通道的大小，最终得到的结果是$R^{|\alpha|\times|\beta|\times d_s/4}$,然后展开得到一个$|\alpha|\times|\beta|\times d_s/4$的向量，输入到MLP中，最终可以用来预测回归任务和分类任务。回归任务的损失函数是均方差，分类任务的损失函数是交叉熵。</7,7></7,1></1,1></p>
<ul>
<li>总结：<br>考虑不同时间粒度，对不同时间粒度的序列使用GRU建模，将最后一个时间步的隐藏状态拼接起来使用CNN。使用2维卷积对时间数据进行建模，不太合适，可以考虑时间3维卷积。</li>
</ul>
<h2><span id="33-spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand2019aaai">3.3. Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand（2019AAAI）</span></h2><p><a href="https://echohhhhhh.github.io/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/" target="_blank" rel="noopener">论文总结</a><br>预测出租车流量，对一个区域在空间上考虑neighbor，function similarity，road connectivity。   </p>
<h2><span id="34-revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-predictionaaai2019">3.4. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction（AAAI2019）</span></h2><p>&ensp;&ensp;&ensp;&ensp;</p>
<ul>
<li>挑战：空间依赖性时动态的，随着时间变化，比如早上居住区和工作区的联系强烈，晚上联系较弱。时间上不是严格的周期性，存在dynamic temporal shifting。比如早高峰在7点值9点，每天可能不一样。</li>
<li>模型：Spatial-Temporal Dynamic Network（STDN），流量门控机制学习location之间动态相似性，periodically shifting attention机制捕获长期周期时间shifting。</li>
<li>将一个城市划分成a*b=n个网格，将一个时间段（eg.一个月）划分成m个长度相等的时间段。<br>traffic volume：区域$i$的start流量$y^s_{i,t}$：在第$t$个时间段离开这个区域的trip个数，区域$i$的end流量$y^e_{i,t}$：在第$t$个时间段到达这个区域的trip个数。<br>traffic flow：从在第$t$个时间段从区域$i$出发，在第$\tau$个时间段到达$j$区域的traffic flow使用$f^{j,\tau}_{i,t}$表示。</li>
<li>目标：给定时间段t及其之前的traffic volume和traffic flow，预测第$t+1$个时间段的start and end traffic volume。</li>
<li>模型：使用Local CNN和LSTM捕获时间和空间关系。<br><img src="/2019/08/02/论文总结/STDN.png" alt=""><br>在提出本文的组件之前，先介绍一下2个base model。</li>
<li>空间 Local CNN<br>使用traffic volume来获取空间相关性，使用local CNN得到区域表示。<br><img src="/2019/08/02/论文总结/volume.png" alt=""></li>
<li>Short-term 时间依赖，短期比如说预测今天9:00~9:30的traffic volume，输入是今天7:00~8:30。<br>使用LSTM来获取短期时间依赖。<br><img src="/2019/08/02/论文总结/short-term.png" alt="">  </li>
<li>下面提取本文的改进，<br><strong>Local CNN—》Flow Gate Mechanism<br>Short-term temporal—》Periodically Shifted Attention Mechanism</strong></li>
</ul>
<p><strong>Spatial Dynamic Similarity: Flow Gating Mechanism(空间动态相似性)</strong><br>在local CNN中，local spatial dependency主要是traffic volume。Y表示traffic volume。  但是traffic volume是静态的，不能完全反映目标区域和周围邻居的关系，traffic flow可以更加直接的反应区域之间的联系。两个区域之间的flow越多表示2个区域联系越强（eg.这2个区域越相似）。设计Flow Gating Mechanism(FGM)捕获区域间的dynamic spatial dependency。<br>traffic flow分为2种：inflow和outflow。<br>给定一个目标区域$i$，获取该区域历史$l$个时间段的traffic flow(从$t-l+1到t$),将历史$l$个时间段的inflow和outflow拼接在一起，形成一个三维张量$F_{i,t} \in \mathbb{R}^{S \times S \times 2l}$，其中$S$表示邻近区域，使用CNN建模区域之间的空间相关性。其中$F_{i,j}$作为第一层的输入。  </p>
<p><img src="/2019/08/02/论文总结/cnn.png" alt=""><br>在每一个卷积层，<strong>使用traffic flow信息来捕获区域之间的动态相似性</strong>，通过一个流量门来限制空间信息。每一层的输出是空间表示$Y^{i,k}_t$，受流量门调整。<br>即对上式的traffic volume，通过traffic flow来控制。 $\sigma$的取值是[0,1]，对traffic volume起到门控机制。  </p>
<p><img src="/2019/08/02/论文总结/gate.png" alt=""><br><strong>Temporal Dynamic Similarity：Periodically Shifted Attention Mechanism(时间动态相似性)</strong><br>以前的LSTM没有考虑长期依赖(例如：周期)，比如预测第$t$天9点的volume，考虑昨天或前天这个时间段的数据。但是traffic volume并不是严格周期的，在时间上会有平移，图a显示了在天之间的时间平移，图b显示了在周之间的时间平移。<br><img src="/2019/08/02/论文总结/shifting.png" alt=""><br>因为时间具有shifting，因此设计了Periodically Shifted Attention Mechanism（PSAM），这里只考虑天周期性，不考虑周周期。从前P天对应时间段的数据来预测，为了解决time shifting，获取每一天的$Q$个时间段,假设预测的时间段是9:00~9:30，$Q=5$,则获取该时间段前后1个小时的数据8:00~10:30。<br>即获取前P天的数据，并且从每天中获取Q个时间段。如模型图所示，对于每一天都有Q个时间段，可以获取每个时间段的traffic volume和traffic flow，然后使用图卷积，即可以得到一个区域每个时间段的表示。每一天都有一个自己的LSTM，每个LSTM都有Q个时间步，每个时间步都会得到一个隐藏状态向量，即会得到Q个隐藏状态，使用Attention，将Q个隐藏状态整合成一个隐藏状态，用$h^p_{i,t}$表示。其中attention中的$\alpha^{p,q}_{i,t}$表示在第$p$天，第$q$个时间段的重要性。$\alpha^{p,q}_{i,t}$根据长期的隐藏状态和被预测天的短期隐藏状态$h_{i,t}$计算得到。 </p>
<p><img src="/2019/08/02/论文总结/lstm.png" alt=""><br><img src="/2019/08/02/论文总结/attention.png" alt=""><br>经过attention之后，P天会得到P个隐藏状态，然后再经过一个LSTM来保存周期的序列信息，最终得到长期依赖表示$\hat{h}^p_{i,t}$。  </p>
<p><img src="/2019/08/02/论文总结/periodic.png" alt=""><br><strong>Joint Traning</strong><br>将短期表示$h_{i,t}$和长期依赖$\hat{h}^p_{i,t}$拼接得到$h^c_{i,t}$，送到一个全连接神经网络中，得到最终的输出，表示为$y^i_{s,t+1}$$y^i_{e,t+1}$作为start volume和end volume。<br><img src="/2019/08/02/论文总结/output.png" alt=""></p>
<h2><span id="35-deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-predictionaaai2017">3.5. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction（AAAI2017）</span></h2><p>预测flow of crowd，提出ST-ResNet，使用残差网络来建模traffic crowd的时间邻近，周期，区域属性，对每一种属性，设计一个残差卷积单元，建模traffic crowd的空间属性。ST-ResNet对3个残差神经网络的输出分配不同的权重，动态地结合3个输出，在整合3个输出的时候同时考虑外部因素，例如天气，day of week。在这篇论文中，预测2种crowd flow：inflow和outflow。inflow是在一个时间段内从其他区域进行到目标区域的crowds。outflow是在一个时间段内离开目标区域的corwds。inflow和outflow是行人数量、车的数据、公共交通系统上的人数量或者3个的总和。</p>
<ul>
<li>gloal：给定历史t个时间段所有区域的inflow和outflow，预测第t+1个时间段所有区域的inflow和outflow。<br>将一个city网格划分成$I<em>J$，下面定义inflow和outflow<br><img src="/2019/08/02/论文总结/inflow.png" alt=""><br>inflow：从其他区域进入到(i,j)<br>outflow：从(i,j)出发到其他区域<br>其中$Tr:g1—&gt;g2…—&gt;g_{|Tr|}$<br><img src="/2019/08/02/论文总结/X.png" alt=""><br>其中X是所有区域的inflow和outflow矩阵。<br><img src="/2019/08/02/论文总结/problem.png" alt=""><br><img src="/2019/08/02/论文总结/ST-ResNet.png" alt=""><br>这个模型由4个组件构成：temporal closeness，period，trend和external。<br>每个时间段内都有一个网格图，2通道，表示所有区域的inflow和outflow。多个时间段按照时间排列会有多个图。在时间段上划分为3部分：recnet、near、distant，分别送到3个模块中：closeness，period，trend，然后对三个模块的输出分配不同的权重融合，再和external信息融合送到Tanh中。<br><strong>Conv-ResNet</strong><br>前3个模块内部是相同的结构，由2部分组成：卷积和残差单元<br><img src="/2019/08/02/论文总结/conv-resnet.png" alt=""><br>拿closeness模块举例，首先使用Conv来捕获near和distant区域的关系。  将closeness的图拼接在一起，closeness一共有$l_c$个时间段，每个时间段有2个通道，将这$l_c$时间段的图拼接在一起，变成$2</em>l_c <em> I </em> J$的数据送入到第一层卷积层。<br><img src="/2019/08/02/论文总结/closeness.png" alt=""><br><img src="/2019/08/02/论文总结/resnet.png" alt=""><br>根据上述结构分别对period和trend进行编码<br><strong>External Component</strong><br>主要考虑以下的外部因素，使用2层全连接提取外部因素。第一层是嵌入层，第二层是转换低维到高维，和$X_t$的维度一样。<br><img src="/2019/08/02/论文总结/external.png" alt=""><br><strong>Fusion</strong><br>所有的区域都被closeness，period，trend影响，但是不同的区域影响程度不同，<br><img src="/2019/08/02/论文总结/fusion.png" alt=""><br><strong>Fusion the external component</strong><br>将3个closeness，period，trend的输出融合，然后再和被预测时间段t的外部因素融合。<br><img src="/2019/08/02/论文总结/fusion-external.png" alt=""> </li>
</ul>
<h2><span id="36-attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting2019aaai">3.6. Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting（2019AAAI）</span></h2><p>&ensp;&ensp;&ensp;&ensp;使用时空图卷积预测所有节点在未来n个时间步的traffic flow。将traffic flow分为3个时间粒度级别：recent，daily，weekly，3个时间粒度的数据使用3个相同的module来建模，每个module有2个submodule：时空Attention和时空GCN</p>
<h2><span id="37-urbanfm-inferring-fine-grained-urban-flows2019kdd">3.7. UrbanFM: Inferring Fine-Grained Urban Flows（2019KDD）</span></h2><p>从粗粒度级的flow推断细粒度级的flow。比如给出的是3<em>3区域的flow，需要推断6\</em>6区域的flow.大的区域称为superregion，划分的小区域称为subregion，同时考虑superregion和subregion的flow约束关系，加起来和superregion的flow相等。<br><img src="/2019/08/02/论文总结/fine-grain.png" alt=""><br>模型的总体框架如下：<br>主要分为2个部分：inference network和external factor subnet。其中推断网络由2个模块组成，特征提取模块和分布上采样模块。<br>在推断网络中，输入是I*J的flow，先经过卷积和M个残差块，捕获空间相关性。在分布上采样模块，每个网格区域需要划分为N<em>N个区域，所以分布上采样主要是改变特征图的大小，从原来的$F</em>I<em>J变成F</em>NI<em>NJ$，对于每个网格区域，经过分布上采样模块，输出的$N</em>N$的flow分布概率。原始的输入$X_c的维度是I<em>J$，经过近邻上采样，会将原始的输入$变成维度为NI</em>NJ$,就是将每个区域的$flow复制N<em>N份$，然后和分布上采样输出的概率相乘，得到每个细粒度区域的flow。<br>**需要注意外部因素，输入是一个向量，经过特征提取模块，输出也是一个向量，为了将外部因素和粗粒度级的flow和细粒度级的flow拼接，也需要将外部因素reshape成$I</em>J或NI<em>NJ$的形状。我原先以为是将外部因素复制$I</em>J或NJ<em>NJ$份，其实并不是，是使用reshape函数。*</em>  </p>
<p><img src="/2019/08/02/论文总结/urbanFM.png" alt=""></p>
<h2><span id="38-deep-multi-view-spatial-temporal-network-for-taxi-demand-prediction2018aaai">3.8. Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction（2018AAAI）</span></h2><p>将一个城市进行网格划分，时间段：30min。预测一个网格区域的taxi demand。<br><strong>注意：根据多个区域，多个时间段，预测一个区域，一个区域的taxi demand</strong><br>根据前$t-h,….t$个时间段的taxi demand和外部因素，预测第$t+1$个时间段的taxi demand。<br><img src="/2019/08/02/论文总结/taxi-demand.png" alt=""><br>文章的标题是multi-view分别是spatial view、temporal view和semantic view(城市功能)，其中spatial view考虑的是target的邻近区域，但是有些区域离target很远，但是城市功能（居民区、商业区）和target相似，通过semantic view来捕获。<br><strong>1. Spatial view：Local CNN</strong><br>仅考虑空间近邻的区域，邻居区域大小$S<em>S,例如7</em>7$，通道数为1，表示taxi demand，表示为$Y^{i,0}_t \in R^{S \times S \times 1}$，经过K个卷积层，输出变成$Y^{i,K}_t \in R^{S \times S \times \lambda}$，然后reshape成一个向量维度为$S^2\lambda$，输入到全连接$FC中，输出一个d维的向量$。时间段有$t-h,….t$，每个时间段的$S<em>S</em>1$的网格都输入到Conv中，然后再经过全连接$FC$,所以最终输出$t-h,….t$个时间步，每个时间步是$d维。$<br><strong>细节：对于城市的边界区域，使用0来填充邻居。</strong><br><strong>2. Temporal view：LSTM</strong><br>经过spatial view输出每个时间步的表示，再和每个时间步的外部信息(天气，hour of day，day of week)拼接，共同输入到LSTM中，最终输出最后一个时间步的隐藏状态。<br><strong>3. Semantic View：Structural Embedding</strong><br>根据区域之间的城市功能相似性来构建graph，图中的节点是所有的区域，共$L个$，边：2个区域之间的相似性。相似性的计算是通过$Dynamic \quad Time \quad Warping \quad (DTW)$。 下图中给出了2个区域相似性的计算公式。根据区域$i 和 j$在工作日的taxi demand的时间序列，计算2个时间序列的相似性，即2个区域的相似性。根据区域间的相关性构建了一个全连接图$G$,使用$Embed$嵌入层,本文使用$LINE对图中的节点进行嵌入$，得到每个节点的低维特征表示，然后再次送入全连接中。<br><strong>注意：构建的是一个全连接图，即任意2个节点之间都有边，因为任意2个节点都可以达到。</strong><br><img src="/2019/08/02/论文总结/DTW.png" alt=""><br><strong>4. Prediction Component</strong><br>将LSTM中最后一个时间步的隐藏状态和target区域的节点表示$m^i$拼接，送入到全连接中，经过$sigmoid函数，$最终输出的值在[0,1]之间，然后再反归一化得到真实的taxi demand。<br><img src="/2019/08/02/论文总结/prediction.png" alt=""></p>
<p><strong>5. Loss Function</strong><br><strong>注意：损失函数有参考意义。</strong>   </p>
<p>损失函数中包含2部分，一个是输出的taxi demand的均方差，一个是MAPE，前面更关注一些大的值，为了避免模型被一些大的值控制，后面加入MAPE。<br><img src="/2019/08/02/论文总结/loss.png" alt=""> </p>
<p><img src="/2019/08/02/论文总结/DMVST-Net.png" alt=""><br><strong>6. 数据集</strong><br>使用广州2个月的taxi数据，$区域划分20<em>20，每个区域700m</em>700m$，<br>（1）使用$Min-Max归一化为[0,1]之间，同时也对y进行归一化到[0,1]之间。$模型预测的输出也在$[0,1]之间，$然后对$y$使用反归一化得到真实的taxi demand。<br>（2）邻居大小设置为$9*9$<br>（3）时间段：30min，根据前8个时间段(4h)预测下一个时间段<br>（4）最后FC的激活函数是$Sigmoid$，其余FC的激活函数是$Relu$</p>
<h2><span id="39-urban-traffic-prediction-from-spatio-temporal-data-using-deep-meta-learning2019kdd郑宇">3.9. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning（2019KDD郑宇）</span></h2><p>通过时空数据，使用深度元学习，进行城市交通预测<br>论文代码：<a href="https://github.com/panzheyi/ST-MetaNet" target="_blank" rel="noopener">https://github.com/panzheyi/ST-MetaNet</a><br><strong>Abstract</strong><br>&ensp;&ensp;&ensp;&ensp;预测城市traffic有以下挑战：（1）复杂的时空相关性，（2）时空相关性的多样性，每个location的POI和路网信息都不一样。提出deep-meta-learning模型（深度元学习），叫做ST-MetaNet，同时预测所有location的traffic，使用seq2seq架构，包含encoder来学习历史信息，decoder来一步接一步的预测，encoder和decoder有相同的架构，都包含RNN来编码历史traffic数据，一个meta graph attention来捕获各种的空间关系，一个meta RNN来考虑各种的时间相关性。</p>
<p><strong>Introduction</strong></p>
<ol>
<li><p>ST相关性的Complex：<br>&ensp;&ensp;&ensp;&ensp;traffic随着location变化，不同的location，traffic也不同。同一个location，不同的时间点的traffic也不一样。构建一个geo-graph表示空间结构，节点：location，边：location之间的关系。<br>&ensp;&ensp;&ensp;&ensp;在空间上，一些location会相互影响，例如图1(a)中的$S3$发生了accident，那么$S1,S2,S4$可能会发生交通阻塞。<br>&ensp;&ensp;&ensp;&ensp;在时间上，一个location的traffic会受到recent或far时间的影响。例如$S4$举办一个演唱会，$S4$的inflow变大，并且会持续一段时间。  </p>
</li>
<li><p>ST相关性的Diversity：<br>&ensp;&ensp;&ensp;&ensp;在上面构建的geo-graph中，有节点特征和边特征。节点：location，节点特征：这个location的POI、路的密度。边：location之间的关系。边特征：location的连通性和距离。比如图1(b)和(c)中，$R1和R3$有相同的POI，都是商业区，$R2$是住宅区，所以它们的flow的时间模式也不一样。  </p>
</li>
</ol>
<p><img src="/2019/08/02/论文总结/Urban-Traffic-Prediction-from-Spatio-Temporal-Data-Using-Deep-Meta-Learning-1.png" alt="">     </p>
<p>&ensp;&ensp;&ensp;&ensp;为了解决以上的挑战，提出<strong>ST-MetaNet</strong>,首先从geo-graph中的节点和边的特征中提取meta knowledge，从中生成预测网络的权重。<br><img src="/2019/08/02/论文总结/Urban-Traffic-Prediction-from-Spatio-Temporal-Data-Using-Deep-Meta-Learning-2.png" alt=""><br>文章的贡献有4个：<br>（1）提出一个新颖的deep meta learning模型，预测城市traffic，ST-MetaNet利用从geo-graph中提取的meta knowledge，生成graph attention network和RNN seq2seq的权重。<br>（2）提出一个meta graph attention网络来建模空间相关性，Attention机制可以捕获location之间的动态关系，attention网络中的权重是从geo-graph的meta knowledge中提取出来的。<br>（3）提出meta gated RNN，生成<br>（4）在traffic flow和traffic speed做实验<br><strong>Preliminaries</strong><br>&ensp;&ensp;&ensp;&ensp;一共有$N_l个location，每个location有N_t个时间步，traffic一共有D_t类$<br>Ubran Traffic：可以表示为以下的张量</p>
<script type="math/tex; mode=display">
X=\left(X_{1}, \ldots, X_{N_{t}}\right) \in \mathbb{R}^{N_{t} \times N_{l} \times D_{t}}</script><p>其中$X_{t}=\left(x_{t}^{(1)}, \ldots, x_{t}^{\left(N_{l}\right)}\right)$表示在时间步$t$所有区域的traffic信息。<br>&ensp;&ensp;&ensp;&ensp;Geo-Graph 特征：分为节点特征和边特征，其中 $G=\{\mathcal{V}, \mathcal{E}\}$ 表示一个有向图，$\mathcal{V} = \{v^{(1)},\ldots,v^{(N_l)}\}$表示所有节点，$\mathcal{E} = \{e^{(ij)} | 1 \leq i, j \leq N_l\}$表示所有的边，使用$\mathcal{N}_i表示节点i的邻居。$<br>&ensp;&ensp;&ensp;&ensp;问题定义：给定前$\tau_{in}$个时间段的$\left(X_{t-\tau_{i n}+1}, \ldots, X_{t}\right)$所有location在所有时间段的traffic特征，和geo-graph特征$\mathcal{G}$，预测在接下来$\tau_{out}$个时间段所有节点的traffic信息，表示为$\left(\hat{Y}_{t+1}, \ldots, \hat{Y}_{t+\tau_{o u t}}\right)$。<br><strong>Methodologies</strong><br>&ensp;&ensp;&ensp;&ensp;ST-MetaNet是Seq2Seq结构，由encoder(蓝色)和decoder(绿色)组成, encoder编码输入序列$\left(X_{t-\tau_{i n}+1}, \ldots, X_{t}\right)$，生成隐藏状态$\{H_{RNN},H_{Meta-RNN}\}$,用来初始化decoder的状态，预测输出序列$\left(\hat{Y}_{t+1}, \ldots, \hat{Y}_{t+\tau_{o u t}}\right)$。<br>&ensp;&ensp;&ensp;&ensp;encoder和decoder有相同的网络架构，包含以下4个组件。<br>（1）RNN：使用RNN来对历史traffic进行嵌入，捕获长期的时间依赖。<br>（2）Meta-knowledge learner：使用2个全连接FCNs，分分别叫做NMK-Learner和EMK-Learner，从节点特征(POI和GPS位置)和边特征(location的道路连通性和距离)学习meta-knowledge，得到的meta-knowledge用来学习GAT和RNN的权重。<br>（3）Meta-GAT：由Meta-Learner和GAT组成，使用FCN作为Meta-Learner，它的输入是所有节点和边的meta knowledge，输出是GAT的权重。Meta-GAT可以捕获多样的空间相关性。<br>（4）Meta-RNN：由Meta-Learner和RNN组成，Meta-Learner是FCN，输入是所有节点的meta knowledge，输出是每一个节点在RNN的权重，Meta-RNN可以捕获多样的时间相关性。</p>
<p><img src="/2019/08/02/论文总结/Urban-Traffic-Prediction-from-Spatio-Temporal-Data-Using-Deep-Meta-Learning-3.png" alt=""></p>
<ol>
<li>RNN(GRU)组件<br>编码所有的location的traffic信息，RNN网络对所有的location共享相同的参数，每次GRU输入的是一个location所有时间步的traffic信息，输出这个location的隐藏状态，下一次再输入另一个location所有时间步的traffic，所有的location共享GRU的参数。GRU输出所有location的隐藏状态$H_{t}=\left(h_{t}^{(1)}, \ldots, h_{t}^{\left(N_{l}\right)}\right)$  <script type="math/tex; mode=display">
 h_{t}^{(i)}=\operatorname{GRU}\left(z_{t}^{(i)}, h_{t-1}^{(i)} | W_{\Omega}, U_{\Omega}, b_{\Omega}\right), \quad \forall i \in\left\{1, \ldots, N_{l}\right\}</script></li>
<li>Meta-Knowledge Learner<br>提出2个meta-knowledge learner：NMK-Learner和EMK-Learner，就是2个FCN，输入是一个节点或一条边的特征，输出是节点或边的向量嵌入表示，这些嵌入表示被用来生成GAT和RNN的权重，捕获时空相关性。使用NMK$(v^{(i)})$和EMK$(e^{(ij)})$表示节点和边的嵌入表示。</li>
</ol>
<h1><span id="4-图卷积">4. 图卷积</span></h1><p><a href="https://echohhhhhh.github.io/2019/03/03/%E5%9B%BE%E5%8D%B7%E7%A7%AF/" target="_blank" rel="noopener">图卷积总结</a></p>
<h2><span id="41-semi-supervised-classification-with-graph-convolutional-networks2017iclr">4.1. Semi-Supervised Classification with Graph Convolutional Networks（2017ICLR）</span></h2><h2><span id="42-diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting2018iclr">4.2. Diffusion Convolutional Recurrent Neural Network Data-Driven Traffic Forecasting（2018ICLR）</span></h2><h2><span id="43-graph-attention-networks2018iclr">4.3. Graph Attention Networks（2018ICLR）</span></h2><h2><span id="44-deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning2018aaai">4.4. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning（2018AAAI）</span></h2><h1><span id="5-time-series-forecasting">5. Time Series Forecasting</span></h1><h2><span id="51-multi-horizon-time-series-forecasting-with-temporal-attention-learning2019kdd">5.1. Multi-Horizon Time Series Forecasting with Temporal Attention Learning（2019KDD）</span></h2><h2><span id="52-enhancing-the-locality-and-breaking-the-memory-bottleneck-of-transformer-on-time-series-forecasting2019nips">5.2. Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting（2019NIPS）</span></h2><h1><span id="6-traffic-accident预测">6. traffic accident预测</span></h1><h2><span id="61-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">6.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference（2016AAAI）</span></h2><p>&ensp;&ensp;&ensp;&ensp;在这篇论文中，使用数据：7个月的accident数据和1.6 million的用户GPS数据。使用堆叠降噪自编码器SDA来学习用户GPS的层次特征。这些特征被用来accidentrisk level的预测。这个模型一旦训练好，给定用户的移动轨迹，就可以模拟对应的accident risk地图。但是导致accident的因素很多。例如司机行为，天气，道路情况等。其他的研究尽管考虑到这些因素，但是没有揭示accidentrisk随着这些因素的变化。这篇论文的问题就是：能否通过实时的位置数据来评accident risk。商业和娱乐区有较高的accident risk，因为这些区域有较高的人流密度和人流量。<strong>accident因为受到很多因素的影响使得仅仅给定人类的移动情况，变得不好预测。因此我们推断一个accident的risk，而不是这次accident会不会发生。因为这是一个回归问题，而不是分类问题。</strong><br>&ensp;&ensp;&ensp;&ensp;我们的模型利用降噪自编码器来学习人类移动的层次特征表示。在预测accident risk任务中，经过自编码器学出来的人类移动特征比原始数据更有效。最终，根据人类移动数据的实时输入，我们的模型可以仿真大规模的accident risk地图。有high risk的区域会高亮显示。</p>
<p>这篇论文的贡献有3个：</p>
<ol>
<li>第一篇在城市级别上预测accident risk。  </li>
<li>构造深度学习框架  </li>
<li>在accident risk上的模拟是非常有效的。  </li>
</ol>
<p><strong>使用的数据：</strong></p>
<ul>
<li>traffic accident 数据。收集了三十万日本从2013.1.1~2013.7.31的traffic accident数据。每条记录包括事故发生的地点和小时，严重程度。其中严重程度被划分为3级，轻度受伤：1，重度受伤：2，致命：3</li>
<li>人类移动数据。收集了大约1.6million用户的GPS记录，在日本2013.1.1~2013.7.31。  </li>
</ul>
<p>accident的risk可以通过事故的频率和严重程度计算。定义risk level=每一个accident的严重程度的和。  时间划分：1个时间作为一个时间段，一天划分24个时间段。空间划分：每个区域500m*500m。时间索引t，空间索引r表示一个区域。即每1个小时统计一次risk level。 同时每小时统计一次该区域的人流密度density。risk level使用$g_{r,t}$表示，人流密度使用$d_{r,t}$表示。问题是通过$d_{r,t}$来预测$g_{r,t}$。每个区域每个时间段的(d,g)作为一个样本。  </p>
<p><strong>总结</strong> </p>
<ul>
<li>没有考虑时间和空间特征，没有考虑外部因素</li>
<li>使用的特征太单一，只考虑区域的人流密度</li>
</ul>
<h2><span id="62-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">6.2. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction（2018IEEE-ITSC）</span></h2><p>traffic risk受很多因素的影响。例如不同的区域有不同的accident rate，天气因素，交通量，时间因素。本文结合<br>accident，traffic flow，天气，空气质量的历史短期和周期特征本文提出的模型用来预测短期的accident risk。和AAAI2016一样，本文是回归问题，预测accident risk。将accident分为3级。模型输入的特征是最近的traffic accident，traffic flow，weather，和air quality。最近指的是前几个小时或者昨天或者上星期。<br>将城市网格划分，每个网格区域1000m*1000m，每个时间段是30min或者60min。  </p>
<p>这篇文章是预测一个区域未来n天的accident平均发生频率。输入有2个，第1个是这个区域历史n个时间段发生的accident的次数，第2个是这个区域的经纬度坐标。<br>这篇论文的前身《A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk》比这篇传入的特征更多，但是不明白为啥没中。<br>这篇文章说traffic accident具有day和week周期性。所以考虑了hour，day，week共3个级别的数据。这篇文章是预测1个区域的risk level，和上一篇不同，上一篇是预测frequency。这篇文章使用的特征有，accident risk，traffic flow，holiday，time period（处于1天的哪个时段，论文中将1天分为7个时段），weather，air quality。将这个区域的以上这6个特征拼接在一起，表示为$I_r(t)$。分别获取这个区域hour，day，week共3个级别的$I_r$,作为LSTM的时间步，每个时间步的特征个数是6个特征拼接起来形成的$I_r$。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
      </tags>
  </entry>
  <entry>
    <title>Traffic Accident相关论文</title>
    <url>/2019/07/21/traffic-accident/</url>
    <content><![CDATA[<p>以下是关于event prediction的相关论文，主要是traffic accident这一类的事件预测。<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#1-%e8%ae%ba%e6%96%87">1. 论文</a><ul>
<li><a href="#11-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">1.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference(2016AAAI)</a></li>
<li><a href="#12-combining-satellite-imagery-and-open-data-to-map-road-safety2017aaai">1.2. Combining Satellite Imagery and Open Data to Map Road Safety(2017AAAI)</a></li>
<li><a href="#13-a-deep-learning-approach-to-the-prediction-of-short-term-traffic-accident-risk%e6%9c%aa%e4%b8%ad">1.3. A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk(未中)</a></li>
<li><a href="#14-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">1.4. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction(2018IEEE-ITSC)</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-论文">1. 论文</span></h1><h2><span id="11-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">1.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference(2016AAAI)</span></h2><p>论文地址：<br><a href="https://shiba.iis.u-tokyo.ac.jp/song/wp-content/uploads/2017/02/AAAI2016.pdf" target="_blank" rel="noopener">Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference</a></p>
<p>目标：使用real-time GPS data 预测所有区域的traffic risk level，回归问题，预测risk的大小，而不是accident会不会发生 。<br><strong>数据集：</strong><br>Traffic accident data和Human mobility data throughout Japan from 2013.1.1~2013.7.31。<br><strong>模型：</strong></p>
<p><img src="/2019/07/21/traffic-accident/1.png" alt="traffic-accident/1.png"></p>
<p>对Japan的区域进行网格划分，获取每个网格的risk和mobility。使用Denoise Autoencoder模型对mobility进行编码representation，然后输入到Logistic regression层作为预测。<br><strong>总结：</strong></p>
<ul>
<li>第一个使用深度学习来预测traffic accident的模型，使用real-time GPS data作为输入。 </li>
<li>没有考虑到时间和空间的关系；</li>
<li>特征单一。只考虑了human mobility，可以考虑weather，POI，population，land use等信息。</li>
</ul>
<h2><span id="12-combining-satellite-imagery-and-open-data-to-map-road-safety2017aaai">1.2. Combining Satellite Imagery and Open Data to Map Road Safety(2017AAAI)</span></h2><p>论文地址：<br><a href="https://pdfs.semanticscholar.org/ef28/efaa43a05be548ed61d52a6bd590b88e7782.pdf" target="_blank" rel="noopener">Combining Satellite Imagery and Open Data to Map Road Safety</a></p>
<p>直接从原始的satellie image来预测road safety。相同的safety在图像视觉上有一些相同的特点，比如颜色(grey/greed)，路段等。所以图像的特点是road safety的一种体现。 </p>
<p><img src="/2019/07/21/traffic-accident/2.png" alt="traffic-accident/1.png"></p>
<p>traffic accident被分为3类：slight，heavy，fatal。<br><strong>数据集：</strong><br>NYC：收集了14000张卫星图像，每个图像图片的标签是3类accident中的一类<br>Denver：收集了21406张卫星图像，标签是3类中的一类。<br>每张图片是256*256，使用ConvNet来对图像进行分类。使用NYC的卫星图像训练模型，使用训练好的模型对NYC的测试卫星图像进行测试。<br>使用Denver的traffic accident映射到地图上，形成traffic 热力图。使用从NYC训练得到的模型，输入是Denver的卫星图片，输出区域的traffic accident severity。即可以生成地图来表示区域的risk。</p>
<p><strong>总结：</strong></p>
<ul>
<li>第一个使用satellite image来预测city-scale road safety的模型</li>
<li>仅仅使用satellite image来预测traffic accident，没有考虑时间和空间信息，外部信息。</li>
</ul>
<h2><span id="13-a-deep-learning-approach-to-the-prediction-of-short-term-traffic-accident-risk未中">1.3. A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk(未中)</span></h2><p>论文地址：<br><a href="https://www.researchgate.net/publication/320627131_A_Deep_Learning_Approach_to_the_Prediction_of_Short-term_Traffic_Accident_Risk" target="_blank" rel="noopener">A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk</a></p>
<p>这篇论文首先指出了《Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference》AAAI2016的缺点：（1）只考虑了human mobility data，像traffic flow，weather，air quality，regional characteristic这些重要的信息没有考虑。（2）没有考虑traffic的周期pattern。<br>本篇论文收集了big and heterogeneous data related traffic accident。<br><strong>数据集：</strong><br>traffic accident：北京2016年的accident数据，每条记录包含时间，地点，严重程度，分为三类，slight、heavy、fatal<br>traffic flow data：北京2016.8所有的taxi的GPS信息和speed信息。<br>air quality：北京的daily PM2.5信息。<br>weather information：cloudy，sunny…<br>每个区域的risk level是这个区域所有的accident severity的总和。<br>将traffic accident按照时间和空间划分，时间1h为一个slot，空间每个gird大小为1000m*1000m。<br>在给定时间t，定义所有区域的时间相关性，<br>从以上这些数据集提取出6个矩阵，分别是</p>
<p><img src="/2019/07/21/traffic-accident/4.png" alt="traffic-accident/1.png"></p>
<p>将这6个矩阵进行整合成一个矩阵，每个区域每个时间得到一个多源数据的表示。<br><strong>模型：Traffic Accident Risk Prediction Method based on LSTM (TARPML)</strong></p>
<p><img src="/2019/07/21/traffic-accident/5.png" alt="traffic-accident/1.png"></p>
<p>有2个input layer，隐藏层有4个LSTM layer和3个fully connected layer，1个output layer，输出risk level。<br>使用LSTM是因为LSTM可以捕捉periodic信息。<br>输入层中的Short-term features是预测时间槽t的前几个小时的特征I，Periodic feature是预测时间槽t的daily和weekly特征。将这三种特征拼接起来，输入到first input layer中。区域的经纬度信息输入到second input layer中， 直接和fully connected layer相连。<br>输入的短期特征是预测时间t前n个小时的特征，n=4.输入的周期特征是预测时间t昨天和上周该时间段前后3个小时的特征。所以输入的特征维度为$(n+2n_d+2n_w+2,6)$。对于一个区域预测时间t的risk level，需要输入的数据是$(n+2n_d+2n_w+2,6)$</p>
<p><img src="/2019/07/21/traffic-accident/7.png" alt="traffic-accident/1.png"></p>
<p><strong>整体架构</strong></p>
<p><img src="/2019/07/21/traffic-accident/3.png" alt="traffic-accident/1.png"></p>
<p><strong>总结</strong></p>
<ul>
<li>使用到的数据集是北京：traffic accident，traffic flow，weather，holiday，air quality数据</li>
<li>分三种时间模式，recent，daily，weekly，直接concatenate输入到LSTM中。并且把region的经纬度输入到全连接中，相当于位置embedding。</li>
<li>没有考虑不同区域之间的关系</li>
<li>想法：（1）回归问题，预测区域的risk level，但是是一个区域还是所有区域，没有想好（2）考虑recent，daily，weekly，使用Attention机制计算三种之间的重要性.（3）对区域进行embedding，（4）考虑不同区域之间的关系.</li>
</ul>
<h2><span id="14-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">1.4. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction(2018IEEE-ITSC)</span></h2><p>论文地址：<br><a href="https://arxiv.org/pdf/1710.09543.pdf" target="_blank" rel="noopener">A Deep Learning Approach to the Citywide Traffic<br>Accident Risk Prediction</a></p>
<p>这篇论文是上篇论文的修改版本。<br>和上文的改进之处是加入了很多图表对现象进行解释。解释了为什么预测traffic accident分类比回归要难的原因。<br>这里只使用了北京Traffic accident数据，没有使用其他外部数据。<br>在给定时间t，计算所有区域的空间相关性，然后再计算时空相关性。 计算下面2个公式，主要是为了说明traffic accident具有day周期性。所以在本论文中一个time slot=24h。<strong>计算一个区域每天的traffic accident发生的频率作为risk。</strong></p>
<p><img src="/2019/07/21/traffic-accident/8.png" alt="traffic-accident/1.png"></p>
<p><img src="/2019/07/21/traffic-accident/9.png" alt="traffic-accident/1.png"></p>
<p><img src="/2019/07/21/traffic-accident/10.png" alt="traffic-accident/1.png"></p>
<p>输入序列长度为100，输入每个区域100h的traffic accident frequency，输出是每个区域未来3天的mean frequency，使用所有的区域样本进行训练。测试时，输入是所有区域100h的traffic accident frequency，输出所有区域未来3天的平均frequency。</p>
<p><img src="/2019/07/21/traffic-accident/11.png" alt="traffic-accident/1.png"></p>
<p><strong>总结：</strong></p>
<ul>
<li>只使用了traffic accident数据，没有使用traffic flow，weather，road network等外部信息</li>
<li>没有考虑空间信息</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
        <tag>Traffic accident</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络踩坑</title>
    <url>/2019/07/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B8%A9%E5%9D%91/</url>
    <content><![CDATA[<p>参考资料:<br><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650729285&amp;idx=1&amp;sn=8f78edc716bbd2198cd7b14f62a93298&amp;chksm=871b2f3bb06ca62d60632da0faebbee63068405a5841934ebec300dc4bbace4b5e6f79daaeed&amp;mpshare=1&amp;scene=1&amp;srcid=07263lmMeeAcHLPSZpJXJI3L#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650729285&amp;idx=1&amp;sn=8f78edc716bbd2198cd7b14f62a93298&amp;chksm=871b2f3bb06ca62d60632da0faebbee63068405a5841934ebec300dc4bbace4b5e6f79daaeed&amp;mpshare=1&amp;scene=1&amp;srcid=07263lmMeeAcHLPSZpJXJI3L#rd</a>   </p>
<a id="more"></a>
<!-- TOC -->
<ul>
<li><a href="#1-suffle%e6%95%b0%e6%8d%ae%e9%9b%86">1. Suffle数据集</a></li>
<li><a href="#2-%e5%bd%92%e4%b8%80%e5%8c%96">2. 归一化</a></li>
<li><a href="#3-batchsize">3. batch_size</a></li>
<li><a href="#4-%e5%88%92%e5%88%86%e6%95%b0%e6%8d%ae%e9%9b%86">4. 划分数据集</a></li>
<li><a href="#5-%e9%aa%8c%e8%af%81%e9%9b%86%e7%9a%84%e4%bd%bf%e7%94%a8">5. 验证集的使用</a></li>
<li><a href="#6-%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81">6. 交叉验证</a></li>
<li><a href="#7-%e6%95%b0%e6%8d%ae%e8%be%93%e5%85%a5%e5%92%8c%e8%be%93%e5%87%ba">7. 数据输入和输出</a></li>
<li><a href="#8-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0">8. 激活函数</a></li>
<li><a href="#9-gpu%e8%bf%90%e8%a1%8c%e7%a8%8b%e5%ba%8f">9. GPU运行程序</a></li>
<li><a href="#10-%e4%bd%bf%e7%94%a8%e5%a4%9agpu%e8%bf%90%e8%a1%8c">10. 使用多GPU运行</a></li>
<li><a href="#11-ndarray%e5%92%8cnumpy">11. NDArray和numpy</a></li>
<li><a href="#12-tensorboard%e4%bd%bf%e7%94%a8">12. tensorboard使用</a></li>
<li><a href="#13-dropout%e7%9a%84%e4%bd%bf%e7%94%a8">13. Dropout的使用</a></li>
<li><a href="#14-%e8%b0%83%e5%8f%82%e7%bb%8f%e9%aa%8c">14. 调参经验</a></li>
<li><a href="#15-earlystopping">15. EarlyStopping</a></li>
<li><a href="#16-%e5%8d%b7%e7%a7%af%e5%b0%ba%e5%af%b8%e5%a4%a7%e5%b0%8f%e5%8f%98%e5%8c%96">16. 卷积尺寸大小变化</a></li>
<li><a href="#17-%e5%9b%ba%e5%ae%9a%e9%9a%8f%e6%9c%ba%e6%95%b0%e7%a7%8d%e5%ad%90">17. 固定随机数种子</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-suffle数据集">1. Suffle数据集</span></h1><p>   <strong>先划分数据集再shuffle</strong>。先将数据集划分成训练集、验证集、测试集。然后在DataLoader划分mini-batch时对训练集进行shuffle得到batch。<strong>对验证集和测试集不需要shuffle</strong>。不对训练集进行shuffle容易造成过拟合。<br>   <strong>只对train进行shuffle，对val和test不进行shuffle</strong></p>
<h1><span id="2-归一化">2. 归一化</span></h1><p>   先划分数据集，再归一化。将数据划分成训练集，验证集，测试集，然后计算<strong>训练集的平均值和标准差</strong>。使用训练集的平均值和标准差对验证集和测试集进行归一化。模型不应该知道关于测试集的任何信息，所以要用训练集的均值和标准差对训练集归一化。<br>   <strong>划分数据集—&gt;归一化—&gt;对训练集shuffle</strong></p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = <span class="number">0.7</span>) <span class="comment">#train 70%, test 30%</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(X_train)</span><br><span class="line"><span class="comment">#X_val_std = ss.transform(X_val)#如果有验证集</span></span><br><span class="line">X_test_std = ss.transform(X_test)</span><br></pre></td></tr></table></figure>
<p>   一般都是把数据归一化成[0,1]或者减去均值除以标准化。默认是对每一列进行归一化，即axis=0。很少用sklearn的标准化方法，都是自己写一个方法用来标准化。<br>   <img src="/2019/07/17/神经网络踩坑/norm.png" alt=""><br>   在实际中对train，val，test归一化有2种方法，<br>   方法1：同时传入train，val，test参数，返回归一化后的trian，val，test和训练集的mean、std。</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">    mean = x.mean(axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    std = x.std(axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (x - mean) / std  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalization</span><span class="params">(train, val, test)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">train, val, test: np.ndarray</span></span><br><span class="line"><span class="string">Returns</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">stats: dict, two keys: mean and std</span></span><br><span class="line"><span class="string">train_norm, val_norm, test_norm: np.ndarray,</span></span><br><span class="line"><span class="string">                                 shape is the same as original</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> train.shape[<span class="number">1</span>:] == val.shape[<span class="number">1</span>:] <span class="keyword">and</span> val.shape[<span class="number">1</span>:] == test.shape[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#求出训练集的mean和std</span></span><br><span class="line">    <span class="comment">#假设train的维度是(3,6,9),mean和std的维度为(1,6,9)</span></span><br><span class="line">    mean = train.mean(axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    std = train.std(axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x - mean) / std</span><br><span class="line"></span><br><span class="line">train_norm = normalize(train)</span><br><span class="line">val_norm = normalize(val)</span><br><span class="line">test_norm = normalize(test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;<span class="string">'mean'</span>: mean, <span class="string">'std'</span>: std&#125;, train_norm, val_norm, test_norm</span><br></pre></td></tr></table></figure>
<p>   方法2：<br>   如果使用(data-mean)/std进行标准化，需要计算train的平均值和标准差，但是怎么将train的平均值和标准差保留用在val和test上呢？下面自定义一个标准化的类</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Scaler</span>:</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line"><span class="comment">#计算train的平均值和标准差</span></span><br><span class="line">      <span class="comment">#假如data的维度是(2880, 1024, 2)</span></span><br><span class="line">      <span class="comment">#下面的平均值是mean所有的数相加/总个数</span></span><br><span class="line">    self.mean = np.mean(data)<span class="comment">#实数</span></span><br><span class="line">    self.std = np.std(data)<span class="comment">#实数</span></span><br><span class="line">   <span class="comment">#归一化：(数据-平均值)/标准差</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (data - self.mean) / self.std</span><br><span class="line">	</span><br><span class="line">   <span class="comment">#反归一化：(数据*标准差)+平均值</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">inverse_transform</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data * self.std + self.mean</span><br></pre></td></tr></table></figure>
<p>   然后在用到<code>Scaler</code>这个类时，<code>scaler = utils.Scaler(train)</code>,则scaler对象则保留了train的平均值和标准差，使用<code>scaler.mean和scaler.std</code>即可以获得train的平均值和标准差。</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scaler = utils.Scaler(train)</span><br><span class="line"><span class="comment">#对train，val，test进行标准化   </span></span><br><span class="line">train_new = scaler.transform(train)</span><br><span class="line">val_new = scaler.transform(val)</span><br><span class="line">test_new = scaler.transform(test)</span><br></pre></td></tr></table></figure>
<p>   在计算loss时，不需要反归一化。在计算评价指标时需要反归一化。在计算评价指标时，比如RMSE,MAE等，首先根据归一化后的test_new得到预测结果predict，然后将predict根据scaler的inverse_transform反归一化，然后使用真实量级的predict和label再计算评价指标。</p>
<h1><span id="3-batch_size">3. batch_size</span></h1><p>   当数据量较大时，向网络中传入所有的数据来计算loss和梯度，更新参数会造成内存溢出。所以每次向网络中值传入一个batch的数据，说过这一个batch的数据来更新权重，输出这个batch里面所有样本的平均loss。下次再使用另一个batch，更新网络参数，直到所有的数据全都输入，完成一个epoch。</p>
<h1><span id="4-划分数据集">4. 划分数据集</span></h1><p>   如果数据充足的情况下，通常采用均匀随机抽样的方法将数据集划分为3部分，训练集，验证集和测试集，这三个集合不能有交集，常见的比例是8:1:1，6:2:2。需要注意的是，通常都会给定训练集和测试集，而不会给验证集，一般的做法是从训练集中抽取一部分数据作为验证集。</p>
<h1><span id="5-验证集的使用">5. 验证集的使用</span></h1><p>   在训练时，仅使用训练集的数据进行训练，使用验证集评价模型。当选中最好的模型超参数之后，再使用训练集+验证集来训练模型，以充分利用所有的标注数据，然后再测试集上测试</p>
<p>   训练模型时，使用一个bacth来训练模型更新模型参数，记录下batch的loss。当训练完一个epcoh时，记录下模型的参数和梯度。并在验证集上计算验证集的误差，在测试集上计算测试集的MAE和MSE。<br>   <strong>在训练的时候，每个batch记录训练的时间，</strong><br>   <strong>使用 tensorboard，训练模型时，每一个batch记录一下train_loss，每一个epoch记录一下模型梯度，在验证集上的loss和评价指标，在测试集上的loss和评价指标，和模型的参数(只保存在val上效果最好的那组参数，其余的删掉)。至于使用val的loss还是评价指标来选择最好的模型，这个需要自己选择</strong><br>   在模型训练的时候记录训练集/验证集/测试集的loss，以及验证集/测试集的评价指标。<br>   为训练集，验证集，测试集创建3个SummaryWriter。<br>   mxboard中log文件夹下的目录结构为：<br>   —logs<br>   $\qquad$—时间1文件夹<br>   $\qquad\qquad$—train文件夹<br>   $\qquad\qquad$—valid文件夹<br>   $\qquad\qquad$—test文件夹<br>   $\qquad$—时间2文件夹<br>   $\qquad\qquad$—train文件夹<br>   $\qquad\qquad$—valid文件夹<br>   $\qquad\qquad$—test文件夹</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timestamp = datetime.now().strftime(<span class="string">"%Y%m%d%H%M%S"</span>)</span><br><span class="line"> mxboard_log = <span class="string">'./logs/%s_%s/'</span> % (<span class="string">'GRU'</span>,timestamp)</span><br><span class="line"> <span class="keyword">if</span> os.path.exists(mxboard_log):</span><br><span class="line">     shutil.rmtree(mxboard_log)</span><br><span class="line"> os.makedirs(mxboard_log)   </span><br><span class="line"></span><br><span class="line"> train_sw = SummaryWriter(logdir=mxboard_log+<span class="string">'/train'</span>,flush_secs=<span class="number">2</span>)</span><br><span class="line"> val_sw = SummaryWriter(logdir=mxboard_log+<span class="string">'/val'</span>,flush_secs=<span class="number">2</span>)</span><br><span class="line"> test_sw = SummaryWriter(logdir=mxboard_log+<span class="string">'/test'</span>,flush_secs=<span class="number">2</span>)  </span><br><span class="line"></span><br><span class="line"> <span class="comment">#为了让loss显示在一张图上，tag需要一样，但是使用不同的sw，即3个loss会分别写入train,valid,test文件夹中，但是在tensorboard网页上会显示在同一张图中，</span></span><br><span class="line"> train_sw.add_scalar(tag=<span class="string">'loss'</span>,value=training_loss,global_step=global_step)  </span><br><span class="line"> val_sw.add_scalar(tag=<span class="string">'loss'</span>,value=loss_mean,global_step=global_step)</span><br><span class="line"> test_sw.add_scalar(tag=<span class="string">'loss'</span>,value=loss_mean,global_step=global_step)  </span><br><span class="line"> <span class="comment">#评价指标的显示，同理。</span></span><br><span class="line"> val_sw.add_scalar(tag=<span class="string">'MAE'</span>,value=mae,global_step=global_step)</span><br><span class="line"> test_sw.add_scalar(tag=<span class="string">'MAE'</span>,value=mae,global_step=global_step)</span><br></pre></td></tr></table></figure>
<p>   <a href="https://github.com/panzheyi/ST-MetaNet" target="_blank" rel="noopener">ST-MetaNet</a>这篇论文的代码在使用验证集验证的过程是：</p>
<ol>
<li>在for循环中遍历所有的的epoch</li>
<li>在每个epoch中，使用训练集训练模型，使用该epoch训练的模型对val进行验证，记录当前模型在val的metric(eg. MAE,MSE)和该模型的参数。</li>
<li>进行下一个epoch，重复步骤2</li>
<li>等到所有的epoch都结束了，选出在val上MAE或MSE最好的那个epoch的模型参数，重新给model加载这个epoch的参数，对测试集进行测试，输出metrics。</li>
<li><p>即这篇的val是用来早停的，选出效果最好的epoch的模型参数。  </p>
<p><a href="https://github.com/Davidham3/ASTGCN" target="_blank" rel="noopener">ASTGCN</a>这篇论文的代码在使用没有选出最好效果的epoch，而是每个epoch在val上计算loss。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        start_time = time()</span><br><span class="line">        output = model(input)</span><br><span class="line">        loss = loss_funtion(output,label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        train_loss = loss.mean().asscalar()</span><br><span class="line">        <span class="comment">#一个batch,使用sw记录train_loss</span></span><br><span class="line">        sw.add_scalar(train_loss)</span><br><span class="line">        print(<span class="string">'每个batch需要的时间和train_loss'</span>)</span><br><span class="line">    <span class="comment">#一个epoch，使用sw记录model的梯度</span></span><br><span class="line">    sw.add_histogram(param.grad())</span><br><span class="line">    <span class="comment">#一个epoch，使用val进行验证，并使用sw记录val的loss</span></span><br><span class="line">    compute_val_loss(net, val_loader)</span><br><span class="line">    <span class="comment">#一个epoch，计算test的metric，并使用sw记录test的MAE等值</span></span><br><span class="line">    compute_metrics(net,test_loader) </span><br><span class="line">    <span class="comment">#一个epoch保存模型参数</span></span><br><span class="line">    net.save_param()</span><br></pre></td></tr></table></figure>
<p>完整代码  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">global_step = <span class="number">1</span></span><br><span class="line"> <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> train_w, train_d, train_r, train_t <span class="keyword">in</span> train_loader:</span><br><span class="line"></span><br><span class="line">         start_time = time()</span><br><span class="line"></span><br><span class="line">         <span class="keyword">with</span> autograd.record():</span><br><span class="line">             output = net([train_w, train_d, train_r])</span><br><span class="line">             l = loss_function(output, train_t)</span><br><span class="line">         l.backward()</span><br><span class="line">         trainer.step(train_t.shape[<span class="number">0</span>])</span><br><span class="line">         training_loss = l.mean().asscalar()</span><br><span class="line"></span><br><span class="line">         sw.add_scalar(tag=<span class="string">'training_loss'</span>,</span><br><span class="line">                       value=training_loss,</span><br><span class="line">                       global_step=global_step)</span><br><span class="line"></span><br><span class="line">         print(<span class="string">'global step: %s, training loss: %.2f, time: %.2fs'</span></span><br><span class="line">               % (global_step, training_loss, time() - start_time))</span><br><span class="line">         global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">     <span class="comment"># logging the gradients of parameters for checking convergence</span></span><br><span class="line">     <span class="keyword">for</span> name, param <span class="keyword">in</span> net.collect_params().items():</span><br><span class="line">         <span class="keyword">try</span>:</span><br><span class="line">             sw.add_histogram(tag=name + <span class="string">"_grad"</span>,</span><br><span class="line">                              values=param.grad(),</span><br><span class="line">                              global_step=global_step,</span><br><span class="line">                              bins=<span class="number">1000</span>)</span><br><span class="line">         <span class="keyword">except</span>:</span><br><span class="line">             print(<span class="string">"can't plot histogram of &#123;&#125;_grad"</span>.format(name))</span><br><span class="line"></span><br><span class="line">     <span class="comment"># compute validation loss</span></span><br><span class="line">     compute_val_loss(net, val_loader, loss_function, sw, epoch)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># evaluate the model on testing set</span></span><br><span class="line">     evaluate(net, test_loader, true_value, num_of_vertices, sw, epoch)</span><br><span class="line"></span><br><span class="line">     params_filename = os.path.join(params_path,</span><br><span class="line">                                    <span class="string">'%s_epoch_%s.params'</span> % (model_name,</span><br><span class="line">                                                            epoch))</span><br><span class="line">     net.save_parameters(params_filename)</span><br><span class="line">     print(<span class="string">'save parameters to file: %s'</span> % (params_filename))</span><br><span class="line"></span><br><span class="line"> <span class="comment"># close SummaryWriter</span></span><br><span class="line"> sw.close()</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> <span class="string">'prediction_filename'</span> <span class="keyword">in</span> training_config:</span><br><span class="line">     prediction_path = training_config[<span class="string">'prediction_filename'</span>]</span><br><span class="line"></span><br><span class="line">     prediction = predict(net, test_loader)</span><br><span class="line"></span><br><span class="line">     np.savez_compressed(</span><br><span class="line">         os.path.normpath(prediction_path),</span><br><span class="line">         prediction=prediction,</span><br><span class="line">         ground_truth=all_data[<span class="string">'test'</span>][<span class="string">'target'</span>]</span><br><span class="line">     )</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1><span id="6-交叉验证">6. 交叉验证</span></h1><p>   原先对交叉验证使用的数据集一直都理解错了。<br>   <a href="https://blog.csdn.net/qq_24753293/article/details/79970997" target="_blank" rel="noopener">参考资料</a><br>   交叉验证使用的数据集是训练集，而不是全部的数据集。在交叉验证的时候把训练集分成K个集合，其中K-1份用来训练，1份用来验证。<br>   <img src="/2019/07/17/神经网络踩坑/cross.png" alt=""><br>   比如使用5折交叉验证，使用不同的5个训练集和测试集，训练得到5个模型，但是我们最后使用的模型并不是这5个模型中的一个。我们仍然认为这5个模型是一个模型，虽然参数不同，只是它们的输入不同而已。交叉验证只是为了验证这个模型的性能，交叉验证的目的并不是为了得到最终的模型。<br>   假设我们有2个模型：线性回归和MLP。怎么说哪个模型更好呢？我们可以使用K折交叉验证来证明哪个模型更好，一旦我们选择了更好模型，例如MLP,那我们就用全部的数据来训练这个模型。<br>   先使用网格搜索选择超参数，然后使用交叉验证输出这个模型的预测结果。<br>   交叉验证有2个用处：</p>
<ul>
<li>准确的调整模型的超参数。超参数不同模型就不同。使用交叉验证来选出最好的超参数。</li>
<li>比如分类问题，有多个算法，逻辑回归，决策树，聚类等方法，不确定使用哪个方法时，可以使用交叉验证。</li>
</ul>
<h1><span id="7-数据输入和输出">7. 数据输入和输出</span></h1><ol>
<li>在gloun中Dense的输入是二维的，(batch_size,feature)，比如输入是(64,120)表示一个batch有64个样本，每个样本有120个特征。如果训练集中的X不是二维的，可以使用reshape()将X变换成(-1,全连接输入单元个数)</li>
<li>卷积神经网络，卷积的输入和输出形状是(batch_size,通道,高,宽)，如果后面接的是全连接，就要转换成二维(batch_size,每个样本特征=通道*高*宽)，但是不需要人去手动转换形状，Dense会自动转换。如果是keras，从卷积层到全连接层，形状不会自动转变，所以需要自己加一个<code>Flatten()</code>层。  </li>
<li>如果是一个分类问题，比如mnist数字识别，最后一层是一个神经单元个数为10的全连接层，然后把输送入到softmax，将每一行的10个值都变成在[0,1]之间小数。损失函数是交叉熵损失损失。在gluon中，最后一层Dense只需要指定输出神经单元个数即可，即<code>Dense(10)</code>，在预测的时候，输出predict，这时的predict并没有归一化到[0,1]的范围内，我们直接把predict和true_label输入到loss中，在loss函数中，才会对predict进行softmax计算，将predict归一化到[0,1]范围内。   </li>
<li><p>在keras中，和gluon不同，会在最后一层的输出指定softmax激活函数，即<code>Dense(10,activation=&#39;softmax&#39;)</code>。</p>
<p><img src="/2019/07/17/神经网络踩坑/conv.png" alt=""><br>（3）循环神经网络的输入形状为(时间步数，batch_size，特征个数)<br><a href="https://www.zhihu.com/question/41949741" target="_blank" rel="noopener">通俗易懂的RNN图解</a></p>
</li>
</ol>
<h1><span id="8-激活函数">8. 激活函数</span></h1><p>   在使用激活函数的时候，一般都是<br>   net.add(nn.Dense(10,activation=’relu’)),在定义层的时候直接加上activation，<br>   也可以使用,但是不常用<br>   net.add(nn.Dense(10),<br>           nn.Activation(‘relu’)<br>    )<br>    或者net.add(nn.Conv2D(channels=6, kernel_size=5, activation=’sigmoid’))<br>    只有当在该层和激活函数之间有其余的操作时，才会分开写，例如在卷积计算之后，激活函数之前加上批量归一化层，写成</p>
<pre><code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add(nn.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">    BatchNorm(),</span><br><span class="line">    nn.Activation(<span class="string">'sigmoid'</span>))  </span><br><span class="line">    或者  </span><br><span class="line">    n.Dense(<span class="number">120</span>),</span><br><span class="line">    BatchNorm(),</span><br><span class="line">    nn.Activation(<span class="string">'sigmoid'</span>)</span><br></pre></td></tr></table></figure>
</code></pre><ol>
<li><p>什么时候用激活函数</p>
<ul>
<li>如果是回归问题，最后一层不需要激活函数（当然，如果数据归一化，可以加激活函数，也可以不加）</li>
<li>如果是分类问题，最后一层的激活函数使用sigmoid(二分类)，softmax(多分类)</li>
<li><p>大部分问题上，使用Relu会得到较好的性能。现在已经很少使用sigmoid激活函数了，sigmoid函数的输出范围在[0,1]之间，x轴在[-5,5]之间的梯度非常高，当x在该范围之外时，梯度很好，接近于0，在反向传播时，容易出现梯度消失问题，无法完成深层网络的训练。</p>
<p><img src="/2019/07/17/神经网络踩坑/sigmoid.png" alt=""></p>
</li>
<li><p>由于梯度消失问题，尽量避免使用sigmoid和tanh激活函数</p>
</li>
<li>Relu是一个通用的激活函数，在大多数情况下都可以使用</li>
<li><strong>注意：Relu只能在隐藏层中使用，不可以在输出层使用</strong>  </li>
<li>使用softmax作为最后一层的激活函数时，前一层最好不要使用relu激活，而是使用tanh代替，否则最终的loss很可能变成nan</li>
</ul>
</li>
</ol>
<h1><span id="9-gpu运行程序">9. GPU运行程序</span></h1><p>   ctx=mx.gpu(2)，下标从0开始<br>   <strong>需要用到ctx的地方：</strong></p>
<ul>
<li><p>数据集需要放到gpu上。有2种方法。<br> （1）在创建数据的时候，指定ctx，在gpu上创建数据。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = gluon.data.DataLoader(</span><br><span class="line">                     gluon.data.ArrayDataset(</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'week'</span>], ctx=ctx),</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'day'</span>], ctx=ctx),</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'recent'</span>], ctx=ctx),</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'target'</span>], ctx=ctx)</span><br><span class="line">                     ),</span><br><span class="line">                     batch_size=batch_size,</span><br><span class="line">                     shuffle=<span class="keyword">True</span></span><br><span class="line"> )</span><br></pre></td></tr></table></figure>
<p> （2）在训练的时候，使用as_in_context()将train_loader,val_loader,test_loader,数据拷贝到gpu上<br> <img src="/2019/07/17/神经网络踩坑/gpu.png" alt="">         </p>
<ul>
<li><p>模型初始化的时候，通过ctx指定gpu设备，将模型参数初始化在gpu上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line">net.initialize(ctx=mx.gpu())</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1><span id="10-使用多gpu运行">10. 使用多GPU运行</span></h1><p>假设<code>ctx=[mx.gpu(1),mx.gpu(2)]</code>，则需要调整以下内容<br>(1)模型初始化，使用<br><code>net.initialize(init=init.Normal(sigma=0.01), ctx=ctx)</code><br>(2)split_and_load函数，将一个batch_size的数据再次划分成子集，并复制到各个GPU上，比如batch_size=6，有2个GPU，那么每个GPU上有3个样本，  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))  </span><br><span class="line">gpu_x = gutils.split_and_load(x, ctx)</span><br></pre></td></tr></table></figure>
<h1><span id="11-ndarray和numpy">11. NDArray和numpy</span></h1><p>使用gluon运行程序，gluon中的数据结构是NDArray，普通的python程序中的数据是numpy。什么时候用nd.array？什么时候用np.array?</p>
<ul>
<li><strong>nd.array</strong><ul>
<li>在模型内部的运算，使用的都是nd。比如模型的数据输入，在创建DataLoader时，数据需要转换成nd.array()类型。</li>
<li>自定义的compute_val_loss()计算验证集的loss时，传入的数据是val_loader，是nd.array类型，但是在返回loss的时候，需要转换成np.array()，</li>
<li>自定义的evaluate计算数据，返回的值是np.array()</li>
</ul>
</li>
<li><strong>np.array</strong><ul>
<li>在metrics.py中计算MSE，RMSE，MAE等指标时，输出和输出都是np.array类型。</li>
</ul>
</li>
<li><p><strong>nd.array和np.array转换</strong></p>
<ul>
<li><p>nd.array—&gt;np.array:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">10</span>)</span><br><span class="line">b = a.asnumpy()</span><br></pre></td></tr></table></figure>
</li>
<li><p>np.array—&gt;nd.array</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = nd.array(b)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1><span id="12-tensorboard使用">12. tensorboard使用</span></h1><ul>
<li>在训练集上每次epoch之后，验证模型在验证集上的平均loss，对验证集上的每个batch中的每个样本都求出一个loss，将所有样本的loss放在list中，最后求list的平均值得到验证集的平均loss。</li>
<li>在训练集上每次epoch之后，写一个evaluate函数，验证模型在测试集上的RMSE或MSE等指标。tensorboard中tag相同的会被显示在同一张图中。为了显示训练集，验证集和测试集的loss，tag都被设置为loss，但是SummaWriter的logdir不同</li>
</ul>
<h1><span id="13-dropout的使用">13. Dropout的使用</span></h1><p>丢弃层会将隐藏单元中的值以一定的概率丢弃，即被设置为0，起到正则化的作用，用来应对过拟合。在测试模型时，为了拿到更加确定的结果，一般不使用丢弃法，只在训练模型下才使用dropout。在训练模型时，将靠近输入层的丢弃概率设的小一点。dropout一般放在全连接层后面</p>
<h1><span id="14-调参经验">14. 调参经验</span></h1><p><a href="https://www.cnblogs.com/kamekin/p/10163743.html" target="_blank" rel="noopener">调参经验</a></p>
<p><a href="https://mp.weixin.qq.com/s/whbQ3b7NcA9Ifvb9aymkbQ" target="_blank" rel="noopener">33 个神经网络「炼丹」技巧</a></p>
<h1><span id="15-earlystopping">15. EarlyStopping</span></h1><p><a href="https://github.com/dmlc/dgl/tree/master/examples/mxnet/gat" target="_blank" rel="noopener">GAT官方实现EarlyStopping的完整代码</a></p>
<p>早停是在模型在val_loss，或者val_acc,val_mae等指标上进行。传入2个参数，patience和delta。</p>
<ul>
<li>如果val_loss在连续patience epoch内，val_loss都大于最好的val_loss，即val_loss在增大，模型出现过拟合。</li>
<li><p>当前val_loss&gt;最好的val_loss-delta，有2种情况</p>
<ul>
<li>当前val_loss上升，counter+1</li>
<li>val_loss虽然减少，但是减少很小，基本可以视为不变，counter+1</li>
</ul>
</li>
<li><p>当前val_loss &lt;= 最好的val_loss-delta,说明val_loss一直在下降，即更新最高的val_loss</p>
</li>
<li>总结：即val_loss在连续patience内，都没有显著下降(current_loss &lt;= best_loss - delta)，则停止训练</li>
</ul>
<h1><span id="16-卷积尺寸大小变化">16. 卷积尺寸大小变化</span></h1><ul>
<li>2D卷积，输入和输出形状一样：一般kernel_size=(3，3),padding=1,stride=1，输入和输出的形状一样</li>
<li>2D卷积，输入和输出高和宽减半：kernel_size=(3,3),padding=1,stride=2，输出的形状是输入一半</li>
<li>3D卷积，一般kernel_size=(3,3,3),padding=1,stride=1，输入和输出的形状一样</li>
<li>3D卷积，一般kernel_size=(1,1,1),padding=0,stride=1，输入和输出的形状一样   </li>
</ul>
<hr>
<p>2020.2.4 更新  </p>
<h1><span id="17-固定随机数种子">17. 固定随机数种子</span></h1><ul>
<li><p>mxnet版本</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">   seed = <span class="number">2020</span></span><br><span class="line">   mx.random.seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">random.seed(seed)</span><br></pre></td></tr></table></figure>
</li>
<li><p>pytorch版本</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seed = <span class="number">2020</span></span><br><span class="line">torch.manual_seed(seed) <span class="comment"># cpu</span></span><br><span class="line">torch.cuda.manual_seed(seed) <span class="comment">#gpu</span></span><br><span class="line">torch.backends.cudnn.deterministic=<span class="keyword">True</span><span class="comment">#cudn,cpu/gpu结果一致</span></span><br><span class="line">np.random.seed(seed)<span class="comment">#numpy</span></span><br><span class="line">random.seed(seed)<span class="comment">#ramdom</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>调参经验</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度爆炸和衰减</title>
    <url>/2019/07/16/%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E8%A1%B0%E5%87%8F/</url>
    <content><![CDATA[<p>记录梯度爆炸和衰减的相关内容<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-为什么使用梯度更新规则">1. 为什么使用梯度更新规则</a></li>
<li><a href="#2-激活函数">2. 激活函数</a></li>
<li><a href="#3-初始化缓解梯度消失和爆炸">3. 初始化缓解梯度消失和爆炸</a></li>
<li><a href="#4-如何判断出现梯度爆炸">4. 如何判断出现梯度爆炸</a></li>
<li><a href="#5-如何解决梯度爆炸">5. 如何解决梯度爆炸</a></li>
</ul>
<!-- /TOC -->
<p>参考资料：<br><a href="https://zhuanlan.zhihu.com/p/33006526" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33006526</a><br><a href="http://wangxin123.com/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#什么是梯度消失和梯度爆炸，分别会引发什么问题" target="_blank" rel="noopener">什么是梯度消失和梯度爆炸，分别会引发什么问题</a></p>
<h1><span id="1-为什么使用梯度更新规则">1. 为什么使用梯度更新规则</span></h1><p>&ensp;&ensp;&ensp;&ensp;现在，神经网络的参数都是通过反向传播更新的。深层神经网络由很多分线性层堆叠，每一个非线性层都可以看做是一个非线性函数。神经网络就是一个复合的非线性函数，将输入映射成输出。损失函数就是使关于真实值与预测值之间的误差。通过损失函数对参数求导，得到梯度，梯度的含义就是这个参数对整个网络的影响程度大小。使用梯度下降更新参数。<br>梯度决定了网络（参数）的学习速率。如果梯度出现异常，参数更新出现异常，即神经元失去了学习的能力。<br>如果权重的值都小于1，最终的输出很小，<br>如果权重的值都大于1，最终的输出很大，</p>
<ul>
<li>在反向求导时，假设对$W^1$求梯度，如果其他的权重都小于1，求得的梯度很小，出现梯度消失，使用$W-\alpha \Delta W$，权重更新的很慢，训练的难度大大增加。<strong>梯度消失比梯度爆炸更常见</strong> </li>
<li>在反向求导时，如果权重大于1，梯度大幅度更新，网络变得很不稳定。较好的情况是网络无法利用训练数据学习，最差的情况是梯度或权重增大溢出，变成网络无法更新的Nan值。<br>不用层的参数更新速率不一样，一般靠近网络输出层的参数更新速度加快，学习的情况较好。靠近网络输入层的参数更新速度慢，学习的很慢，有时候训练了很久，前几层的权重参数值和刚开始初始化的值差不多。因此，梯度消失和爆炸的根本原因在于反向传播法则。<br><img src="/2019/07/16/梯度爆炸和衰减/1.png" alt="">       </li>
</ul>
<h1><span id="2-激活函数">2. 激活函数</span></h1><p>（1）在权重更新的时候，需要计算前层的偏导信息，因此如果激活函数选择的不合适，比如sigmoid，梯度消失的很明显。因为sigmoid的导数不会超过0.25，经过链式求导，很容易出现梯度消失。<br><img src="/2019/07/16/梯度爆炸和衰减/sigmoid.png" alt=""><br>（2）tanh比sigmoid好一些，但是它的导数仍然小于1<br><img src="/2019/07/16/梯度爆炸和衰减/tanh.png" alt="">    </p>
<h1><span id="3-初始化缓解梯度消失和爆炸">3. 初始化缓解梯度消失和爆炸</span></h1><p>使用Xavier初始化：基本思想是通过网络层时，输出和输出的方差相同。Xavier在tanh中表现很好，但在Relu激活函数中表现很差。<br>何凯明提出了针对Relu的初始化方法<br>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification He, K. et al. (2015)<br>该方法集合He initialization，简单思想是：在Relu网络中，假定每一层有一半的神经元被激活，另一半为0，所有，要保持方差不变，只需要在Xavier的基础上再除以2。<br>针对Relu的激活函数，基本使用He initialization。</p>
<ul>
<li>Xavier初始化：tanh，sigmoid激活函数</li>
<li>He初始化：Relu激活函数</li>
</ul>
<h1><span id="4-如何判断出现梯度爆炸">4. 如何判断出现梯度爆炸</span></h1><p>当出现以下信号时，说明出现了梯度爆炸：</p>
<ol>
<li>训练过程中，每个节点和层的权重梯度连续大于1</li>
<li>模型不稳定，梯度显著变化，快速变大</li>
<li>训练过程中，权重变成了Nan</li>
<li>权重无法从训练数据中更新</li>
</ol>
<h1><span id="5-如何解决梯度爆炸">5. 如何解决梯度爆炸</span></h1><ol>
<li>梯度剪切<br>此方法针对梯度爆炸提出来的，基本思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过了这个阈值，就强制限制在这个范围之，这可以防止梯度爆炸。</li>
<li>权重正则化<br>比较常见的是L1正则化和L2正则化。正则化是通过对网络权重做正则化限制过拟合<br><img src="/2019/07/16/梯度爆炸和衰减/reg.png" alt="">    </li>
<li><p>Relu，LeakRelu激活函数<br>如果激活函数的导数为1，就不存在梯度爆炸的问题了。每层网络都可以得到相同的更新速度。在深层网络中使用relu激活函数不会导致梯度消失和爆炸的情况。<br><img src="/2019/07/16/梯度爆炸和衰减/relu.png" alt=""></p>
<p>relu的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练<br>同时也存在一些缺点：由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）；输出不是以0为中心的</li>
</ul>
</li>
<li>BatchNorm</li>
<li>RestNet</li>
<li>LSTM，可以解决梯度消失的问题</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>梯度</tag>
        <tag>反向传播</tag>
      </tags>
  </entry>
  <entry>
    <title>Dataset</title>
    <url>/2019/07/12/Dataset/</url>
    <content><![CDATA[<h1><span id="数据">数据</span></h1><a id="more"></a> 
<p>滴滴数据集：<a href="https://outreach.didichuxing.com/app-vue/dataList" target="_blank" rel="noopener">https://outreach.didichuxing.com/app-vue/dataList</a></p>
<p>京东城市计算：<a href="http://urban-computing.com/index-40.htm" target="_blank" rel="noopener">http://urban-computing.com/index-40.htm</a></p>
<p>NYC crash数据：<a href="https://data.cityofnewyork.us/Public-Safety/Vision-Zero-View-Data/v7f4-yzyg" target="_blank" rel="noopener">https://data.cityofnewyork.us/Public-Safety/Vision-Zero-View-Data/v7f4-yzyg</a></p>
]]></content>
      <categories>
        <category>Dataset</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction</title>
    <url>/2019/07/01/Deep-Spatio-Temporal-Residual-Networks-for-Citywide-Crowd-Flows-Prediction/</url>
    <content><![CDATA[<p>参考资料<br><a href="https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/</a><br>城市计算，郑宇发表的论文：<br><a href="https://www.microsoft.com/en-us/research/project/urban-computing/#!publications" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/project/urban-computing/#!publications</a><br><a id="more"></a></p>
<h1><span id="abstract">Abstract</span></h1><p>&ensp;&ensp;&ensp;&ensp;预测人群流量对于公共安全是非常重要的，同时也有挑战，因为涉及到很多复杂的因素。例如区域间交通、时间、以及天气。我们提出了一个基于深度学习的方法：ST-ResNet，预测城市中每个区域的inflow和outflow。基于时空数据，我们设计了端到端结构的ST-ResNet。我们应用残差神经网络框架来建模时间closeness、period和trend的属性。对每一个属性，设计一个残差卷积单元分支，每个残差卷积单元对空间进行建模。三个残差神经网络对每个区域分配不同的权重。残差神经网络的输出再和外部因素（天气）整合，最终预测每个区域的人流量。在北京和NYC2种数据集上做了实验。</p>
<h1><span id="instruction">Instruction</span></h1><p>&ensp;&ensp;&ensp;&ensp;在这篇论文中，预测2种流量：inflow和outflow。inflow：在给定一个时间间隔中，从其他区域进入到一个区域的所有流量。outflow：在给定时间间隔中，从这个区域离开的流量。这2种流量都标识人迁移的变化。inflow/outflow可以由行人的数量、周围路上车的数量、公共交通系统（bus,metro）上的人数量或者它们的总和（如果都可以获取到）。 例如图1中的(b)，可以通过手机信号来推测人inflow/outflow分别是(3,1)，使用车辆的GPS轨迹，推测处inflow/outflow分别是(0,3)。<br><img src="/2019/07/01/Deep-Spatio-Temporal-Residual-Networks-for-Citywide-Crowd-Flows-Prediction/figure1.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;预测城市中每个区域的inflow和outflow有以下3个复杂的因素：</p>
<ol>
<li>空间依赖。例如图1中(2)，r2的inflow会受到邻近区域(r1)和偏远区域的outflow影响，相反，r2的outflow也会影响其他区域的inflow。同时r2的inflow也会影响自身的outflow。</li>
<li>时间依赖。一个区域的流量受到邻近和较远时间段的影响。例如，在上午8点发生交通阻塞将会影响9点的traffic。并且，早上高峰时的交通情况可能其连续工作日的同一时刻相似。当温度变得越来越低，太阳升的越来越晚，人们也起的越来越晚。</li>
<li>外部因素影响。一些外部因素，像天气，事故可能会影响不同区域的flow。<br>&ensp;&ensp;&ensp;&ensp;为了处理这些挑战，提出ST-RestNet，模型的贡献主要如下：</li>
</ol>
<ul>
<li>ST-ResNet使用基于卷积的残差网络，来建模周围和较远任意2个地区的空间依赖。同时保证模型的准确率不包含在神经网络的深层架构中（？？？）  </li>
<li>将流量的时间属性总结为3类，分别是closeness、period、trend。ST-ResNet使用3个残差网络分别对这3个属性建模</li>
<li>ST-ResNet为不同的分支和区域分配不同的权重，动态整合以上3个网络的输出。然后再和外部因素进行整合。</li>
<li>使用北京出租车轨迹数据和NYC自行车轨迹数据来做实验。有6个baseline</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat</title>
    <url>/2019/07/01/Tomcat/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;<br>参考资料<a href="https://www.cnblogs.com/centos2017/p/9956432.html" target="_blank" rel="noopener">https://www.cnblogs.com/centos2017/p/9956432.html</a></p>
<a id="more"></a>
<h1><span id="内存溢出">内存溢出</span></h1><p>在Tomcat时，常常会遇到内存溢出的错误，主要是以下2种：</p>
<ul>
<li>java.lang.OutOfMemoryError: Java heap space</li>
<li>java.lang.OutOfMemoryError: PermGen space</li>
</ul>
<h1><span id="原理">原理</span></h1><ul>
<li>-Xms 为jvm启动时分配的初始内存      比如-Xms200m，表示分配200M</li>
<li>-Xmx 为jvm运行分配的最大内存        比如-Xms500m，表示jvm进程最多只能够占用500M内存</li>
<li>-Xss 每个线程堆栈的大小             一般情况下256K是足够了。影响了此进程中并发线程数大小</li>
<li>-XX  PermSize=64M JVM初始分配的非堆内存</li>
<li>-XX  MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配<br>首先了解一下JVM内存管理的机制，然后解释每个参数的含义。<br>按照官方的说法：Java虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在Java虚拟机启动时创建的。<br>在JVM中堆之外的内存称为非堆内存（Non-heap memory）。<br>简单来说，堆就是Java代码可及的内存，是留给开发人员使用的，非堆就是JVM留给自己用的。</li>
</ul>
<h2><span id="堆heap内存">堆Heap内存</span></h2><ul>
<li>JVM初始分配的堆内存由-Xms指定，默认是物理内存的1/64；</li>
<li>JVM最大分配的堆内存由-Xmx指定，默认是物理内存的1/4。<br>默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；<br>空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。<br><strong>因此服务器一般设置-Xms、-Xmx 相等，以避免在每次GC 后调整堆的大小</strong>。<br>说明：如果-Xmx 不指定或者指定偏小，应用可能会导致java.lang.OutOfMemoryError: Java heap space错误，此错误来自JVM，不是Throwable的，无法用try…catch捕捉</li>
</ul>
<h2><span id="非堆内存分配">非堆内存分配</span></h2><ul>
<li>JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；</li>
<li>由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。<br>XX:MaxPermSize设置过小会导致java.lang.OutOfMemoryError: PermGen space 就是内存益出。<br>为什么会内存益出：<br>（1）这一部分内存用于存放Class和Meta的信息，Class在被 Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同。<br>（2）GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS 的话,就很可能出现PermGen space错误。<br>这种错误常见在web服务器对JSP进行pre compile的时候。</li>
</ul>
<h1><span id="解决方案">解决方案</span></h1><p>(1) 进入到tomcat的/bin目录下<br>在bin目录下，创建一个新的文件，<br>如果是Linxu或Mac系统，创建setenv.sh<br>如果是Windows系统，创建setenv.bat<br>(2) 添加配置（Linux/Mac）<br>在这个文件中添加以下内容</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">CATALINA_OPTS</span>=<span class="string">"<span class="variable">$CATALINA_OPTS</span> -Xms2048m"</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">CATALINA_OPTS</span>=<span class="string">"<span class="variable">$CATALINA_OPTS</span> -Xmx2048m"</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">CATALINA_OPTS</span>=<span class="string">"<span class="variable">$CATALINA_OPTS</span> -XX:MaxPermSize=512m"</span></span><br></pre></td></tr></table></figure>
<p>如果是Windows系统，使用以下配置</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="builtin-name">set</span> <span class="string">"JAVA_OPTS=%JAVA_OPTS% -Xms2048m -Xmx2048m-XX:MaxPermSize=512m -server"</span></span><br></pre></td></tr></table></figure>
<p>(3) 完成以上配置后，启动Tomcat服务可以使用以下2种命令(Linux/Mac)：</p>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cd</span> apache/bin</span><br><span class="line">./catalina.<span class="keyword">sh</span> <span class="keyword">run</span>或者./startup.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<p>如果是Windows系统，使用catalina.bat启动Tomcat服务<br>(4) 查看log日志<br>在日志中，启动Tomcat时，可以看到刚刚配置的参数。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>MiST-A Multiview and Multimodal Spatial-Temporal Learning Framework for Citywide Abnormal Event Forecasting</title>
    <url>/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/</url>
    <content><![CDATA[<p><a href="http://delivery.acm.org/10.1145/3320000/3313730/p717-huang.pdf?ip=218.247.253.153&amp;id=3313730&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1561380383_40e3bd8d678088e9b04173b89f85c49c" target="_blank" rel="noopener">论文出处</a><br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e5%85%b3%e9%94%ae%e5%ad%97">1. 关键字</a></li>
<li><a href="#2-%e6%91%98%e8%a6%81">2. 摘要</a></li>
<li><a href="#3-%e4%bb%8b%e7%bb%8d">3. 介绍</a></li>
<li><a href="#4-%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0">4. 问题描述</a><ul>
<li><a href="#41-%e5%ae%9a%e4%b9%89">4.1. 定义</a></li>
<li><a href="#42-%e6%a8%a1%e5%9e%8b">4.2. 模型</a></li>
</ul>
</li>
<li><a href="#5-%e6%96%b9%e6%b3%95">5. 方法</a><ul>
<li><a href="#51-context-aware-recurrent-framework">5.1. Context-aware Recurrent Framework</a></li>
<li><a href="#52-multi-modal-pattern-fusion-module">5.2. Multi-Modal Pattern Fusion Module</a></li>
<li><a href="#53-conclusive-recurrent-network">5.3. Conclusive Recurrent Network</a></li>
<li><a href="#54-forecasting-and-model-inference">5.4. Forecasting and Model Inference</a></li>
</ul>
</li>
<li><a href="#6-%e8%af%84%e4%bb%b7">6. 评价</a><ul>
<li><a href="#61-%e6%95%b0%e6%8d%ae%e9%9b%86">6.1. 数据集</a><ul>
<li><a href="#611-%e6%95%b0%e6%8d%ae%e7%bb%9f%e8%ae%a1">6.1.1. 数据统计</a></li>
</ul>
</li>
<li><a href="#62-%e5%ae%9e%e9%aa%8c">6.2. 实验</a><ul>
<li><a href="#621-%e5%8f%82%e6%95%b0%e8%ae%be%e7%bd%ae">6.2.1. 参数设置</a></li>
<li><a href="#622-baseline">6.2.2. Baseline</a></li>
<li><a href="#623-%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87">6.2.3. 评价指标</a></li>
</ul>
</li>
<li><a href="#63-%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c">6.3. 实验结果</a><ul>
<li><a href="#631-overall-comparisonq1">6.3.1. Overall Comparison(Q1)</a></li>
<li><a href="#632-forecasting-accuracy-vs-time-periodq2">6.3.2. Forecasting Accuracy v.s Time Period(Q2)</a></li>
<li><a href="#633-forecasting-accuracy-vs-categoriesq3">6.3.3. Forecasting Accuracy v.s Categories(Q3)</a></li>
</ul>
</li>
<li><a href="#64-component-wise-evaluation-of-mistq4">6.4. Component-Wise Evaluation of MiST(Q4)</a></li>
<li><a href="#65-effect-of-spatial-and-temporal-scaleq5">6.5. Effect of Spatial and Temporal Scale(Q5)</a></li>
<li><a href="#66-hyperparameters-studiesq6">6.6. Hyperparameters Studies(Q6)</a></li>
<li><a href="#67-case-studyq7">6.7. Case Study(Q7)</a></li>
</ul>
</li>
<li><a href="#7-%e6%80%bb%e7%bb%93">7. 总结</a></li>
<li><a href="#8-%e8%af%a5%e4%bd%9c%e8%80%85%e5%85%b6%e4%bb%96%e8%ae%ba%e6%96%87">8. 该作者其他论文</a></li>
<li><a href="#9-anomaly%e6%a3%80%e6%b5%8b%e9%a2%86%e5%9f%9f%e7%9a%84%e5%85%b6%e4%bb%96%e8%ae%ba%e6%96%87">9. Anomaly检测领域的其他论文</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-关键字">1. 关键字</span></h1><p><strong>异常事件预测、深度神经网络、时空数据挖掘</strong></p>
<h1><span id="2-摘要">2. 摘要</span></h1><p>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[应用]</font>城市异常事件，比如犯罪、事故，如果不及时处理的话，会造成人员和财产的损失。如果异常事件能在发生之前自动被预测出来，对很多领域都有重要意义，比如公共秩序维护、灾难控制和人的活动建模。<font color="#FF0000">[挑战]</font>然而，预测不同类型的城市异常事件是非常有挑战的，因为它被很多复杂的因素影响。(i)区域内动态的时间关系；(ii)区域间复杂的空间关系；(iii)潜在的类别之间的关系。<font color="#FF0000">[模型]</font>在这篇论文中，我们研究了一个<strong>Multi-View and Multi-Modal Spatial-Temporal learning多视角和多模态的时空学习框架(MiST)</strong> 来解决以上的挑战，通过增强不同视角（空间、时间和语义）的相关性，和将多模态单元映射到相同的潜在空间。特别的，将多模态模式融合架构和分层循环框架进行整合，MiST可以保留多视角异常事件数据的潜在的结果信息，和自动地学习特定视角表示的重要性。在三个真实数据集上的实验，例如：犯罪数据和城市异常数据，表明我们MiST模型比其他先进的模型效果都好。</p>
<h1><span id="3-介绍">3. 介绍</span></h1><p>&ensp;&ensp;&ensp;&ensp;城市异常事件，比如犯罪(抢劫、袭击)和城市异常(道路封锁、噪声)如果不及时处理，对公共安全有很大的风险。据统计，异常造成了很大的损失，因为准确和可靠的预测异常事件是数据驱动的决策者用于减少人和经济的损失迫切的需求。<font color="#FF0000">[应用]</font>例如，在灾难控制中，通过预测未来的异常事件，当地政府可以设计更好的交通规划和移动管理策略来防止严重的社会骚乱。此外，在公共秩序维护上，了解城市每个区域的异常事件潜在的发生模式对人们活动建模和地方推荐任务是非常重要的。在这篇论文中，我们旨在提前预测城市中不同区域不同类型的异常事件，为社会福利给予重大的提高。</p>
<p>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[前人工作]</font>前人已经有一些研究关于使用时空数据检测地理异常。大部分这些研究都是通过分析被研究对象的历史轨迹和移动模式，使用统计和数据挖掘的方法来发现异常事件。然而，这些方法并不是预测将来的时间，而是在它们发生之后鉴定是不是异常事件，这会造成信息延迟和缺乏异常处理的提前准备。<br>&ensp;&ensp;&ensp;&ensp;从多个角度，我们确定了建模这种异常事件数据的三个挑战。<font color="#FF0000">[挑战]</font><br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[考虑空间关系]</font>第一，在城市中异常事件的分布是变化的，并且不同区域异常事件的分布是不同的。在这种情况下，异常事件的发生不再是区域独立的，在预测异常事件时，考虑不同区域的空间关系是非常重要的。并且，当建模动态空间关系对时，概率图模型将不再有效，由于概率图模型基于先验假设分布有很多的参数，涉及大量的计算。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[时间动态依赖]</font>第二，异常事件的发生模式经常涉及到随时间变化的潜在因素。例如，工作日的犯罪因果性和周末可能不同。传统的时间序列预测技术，像ARIMA和SVR被限制在线性模型，它仅依赖于单级周期模式。因此，这些方法很难在时间动态上预测异常事件。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[不同类别的异常事件间相互影响]</font>第三，不同类别的异常事件有着显示和隐示的影响。例如，一个区域的抢劫可能会引发该区域的交通堵塞，由于人群的聚集和巡逻的增加。因此，一种类别异常事件的发生不仅仅来源于不同区域之间的空间关系和时间槽之间的时间依赖，还可能来源于不同类别异常事件的相互影响。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[模型3个阶段]</font>受以上挑战的启发，该工作提出了一个通用且灵活的框架：Multi-View Deep Spatial-Temporal Networks(MiST)，从多视角异常事件数据的关系中学习预测结构。特别的，在第一阶段，我们提出了上下文感知context-aware的循环框架从不同的角度来捕获异常事件数据的时间动态性，并且自动提供了某个视角的表示。在第二阶段，为了将区域间的关系、不同类别间的影响和已编码的多维度数据的时间模式整合起来，我们基于attention机制提出了一个模式融合模块，来促进不同视角的融合，并且在预测模型的相应视角，自动地捕获关联区域、时间槽、类别的贡献。为了增强MisT模型的时间序列结构信息和非线性，在最后阶段设计了一个总结性的循环网络模块，对融合嵌入向量的序列模式进行建模。最终的总结潜在表示被喂入一个全连接神经网络来预测未来时间槽的异常事件。<br>&ensp;&ensp;&ensp;&ensp;综上所述，我们贡献主要是：</p>
<ul>
<li>我们引入了一个新的多视角和多模态时空学习框架MiST来预测一个城市每个区域不同类型的异常事件。MiST映射所有的空间时间和语义单元到一个潜在空间来保留它们跨模态的相关性。</li>
<li>我们提出了一个多模态融合模型，和分层循环框架，学习共享在多视角数据中潜在的区域-时间-类别关系，并且自动地调整每个视角中的相关性，以协助预测任务。</li>
<li>在三个真实世界异常事件数据集，从NYC和Chicago收集的数据集进行试验，MiST一直比其他state-of-the-art方法效果好。</li>
</ul>
<h1><span id="4-问题描述">4. 问题描述</span></h1><p>&ensp;&ensp;&ensp;&ensp;在这一节，首先引出preliminary和problem。</p>
<h2><span id="41-定义">4.1. 定义</span></h2><ul>
<li><p>定义1 Geographical Region(地理区域)<br>把城市进行网格分区。划分成$I \times J$,有$I$行$J$列，带有经纬度信息。每一个网格被视为一个地理区域，表示为$r_{i,j}$，其中$i和j$是分别是行和列的索引。在这篇论文中，我们使用区域作为最小单元来研究异常事件预测问题。<br>&ensp;&ensp;&ensp;&ensp;我们定义地理区域集合$R=(r_{1,1},…,r_{i,j},…,r_{I,J})$,并且假设有$L$个异常事件类别，$C=(c_1,…,c_l,…,c_L)$,其中$C$表示异常事件类别集合，下标为$l$。给定一个时间窗口$T$,我们分割$T$为不重叠且连续的时间槽$(T=(t_1,…,t_k,…,t_K))$,其中$K$表示时间槽的个数，索引是$k$.<br><strong><script type="math/tex">区域R是I \times J;
异常事件类别C，有L个值;
时间T，有K个时间槽</script></strong></p>
</li>
<li><p>定义2 Abnormal Event Data Source(异常事件数据源)<br>假设一个区域$r_{i,j}$，使用$Y_{i,j}=(y^1_{i,j},…,y^l_{i,j},…,y^L_{i,j}) \in \mathbb{R}^{L \times K}$来表示在区域$r_{i,j}$过去$K$个时间槽发生的所有类型的异常事件。对于$y^l_{i,j} \in \mathbb{R}^K$表示区域$r_{i,j}$在类别$c_l$上从时间$t_1到t_K$的值。在$y^l_{i,j}$中，每一个元素$y^{l,k}_{i,j}$为1如果在区域$r_{i,j}$在时间$t_k$中有类别$c_l$异常事件发生，否则为0。<br>即$Y_{i,j}$是一个矩阵，一共有$L行K列$，每一个元素非0即1，其中每一行表示一种类别，每一列表示一个时间段。</p>
</li>
<li><strong>Problem Statement</strong><br><font color="#FF0000">[任务]</font>给定一个城市区域$R$时间从$t1到t_K$,所有异常事件类别的数据源$Y$，其中$Y$有$I \times J$个矩阵，每个矩阵都是$\mathbb{R}^{L \times K}$。目标是学习一个预测框架来推断一个区域$r_{i,j}$在未来$h$个时间槽，异常事件类别$c_l$是否发生。即计算$(y^{l,(K+h)}_{i,j}|Y_{i,j}=(y^1_{i,j},…,y^L_{i,j}));i,j \in [1,…,I],[1,…,J]$。即给定一个区域历史$K$个时间槽所有类别异常事件发生地数据，来预测这个区域在未来第$K+h$个时间槽，类别$l$事件是否发生，即输出结果是0/1</li>
</ul>
<h2><span id="42-模型">4.2. 模型</span></h2><p>&ensp;&ensp;&ensp;&ensp;我们提出的MiST模型是一个多层表示学习框架，如figure1所示。在详细介绍模型之前，首先介绍一下模型的输入，然后详细介绍设计的动机。</p>
<ul>
<li>定义3 Event Context Tensor(事件上下文张量)<br>给定一个目标区域$r_{i,j}$，使用event context tensor$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$，对这个区域的邻近区域在时间段$t_k$中不同类别的异常事件进行建模。$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$，有3个维度，分别表示$I行J列L个类别$。给定一个时间槽$t_k,\mathcal{A}^k_{i,j,l}$为1如果？？？？？,</li>
<li><strong>Context-aware Recurrent Framework</strong><br>为了从时间角度，就异常事件分布的动态属性方面表示区域内的相关性，我们提出了基于LSTM的context encoder，将每个时间槽的$\mathcal{A}$展开形成的向量中的每个元素，学习一个潜在表示。从我们的LSTM encoder中学习到的表示，可以对异常事件的时间依赖特性建模，还可以捕获异常事件的局部时间上下文和多层周期模式。  </li>
<li><strong>Multi-Modal Pattern Fusion Mudule</strong><br>为了捕获异常事件分布，在区域间和不同类别的关系，我们提出了深度融合模块，用于同时对周围地理区域和不同类别的异常事件的固有发生模式进行建模。我们将$K(表示K个时间槽)$个张量$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$按照时间进行排序，然后对于每一个时间槽$t_k$都有一个张量$\mathcal{A}^k_{i,j}$,将它的隐藏向量表示，应用attention机制，从空间-类别视图生成summarized嵌入向量。</li>
<li><strong>Conclusive Recurrent Networks</strong><br>依赖从空间-时间-类别视图生成的隐藏表示，我们提出一个conclusive recurrent networks来有效地捕获位置、时间、类别多模态的序列模式。最终的spatial-temporal-categorical多视图序列表示被保存在conclusive recurrent network单元格的最终状态，在解码阶段为预测异常发生的概率提供了指导。</li>
</ul>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure1.png" alt=""></p>
<p>输入的数据A是非0即1的张量，表示目标区域和邻居发生异常事件的情况。先选中一个目标区域$r_{i,j}$，找出这个区域的邻居$r_{i\prime,j\prime} \in G(i,j)$。flatten得到的是目标区域和邻居的值，就是0或1值，然后选中一个区域，有K个时间段，得到一个区域在一个异常类别上的时间序列，例如0110…，然后输入到LSTM中，每一步都可以得到一个隐藏状态。所以第一个时间段第一个异常类别，会有很多个隐藏状态，假设目标区域和其邻域共有9个区域，则第一个时间步第一个类别会输出9个隐藏状态。这样每个区域在每个类别上，每个时间步上都会得到一个隐藏状态。只是对一个区域进行建模，没有涉及到邻居和类别。在第二步使用Attention，获取每个区域的得到，就是把这个区域所有的特征全都塞到一个全连接神经网络中，一个区域的特征有3个，LSTM输出的隐藏状态，这个区域的嵌入表示，异常类别的嵌入表示，根据这3个特征得到这个区域的得分，然后将每个区域的得分使用softmax归一化。然后将得分再乘上一个隐藏状态得到每个时间步的表示。再将每个时间步的表示作为一个序列传入到LSTM中，将最终的隐藏状态传入到MLP中。最后预测的值是一个概率，表示这个区域在这个时间段发生这个类别的异常事件的概率。</p>
<h1><span id="5-方法">5. 方法</span></h1><h2><span id="51-context-aware-recurrent-framework">5.1. Context-aware Recurrent Framework</span></h2><p>&ensp;&ensp;&ensp;&ensp;在MiST架构中，在异常事件在时间槽$t_1到t_k$的分布，我们首先采用LSTM网络来编码复杂的的区域内相关性。特别，LSTM包含1个记忆细胞状态和3个控制门通过分别执行写、读、重置操作来更新记忆细胞状态。用公式表示，区域$r_{i,j}$和异常类别$c_l$在第$t$个时间槽的隐藏状态$h^t_{i,j,l}$和记忆细胞状态$c^t_{i,j,l}$计算公式如下：</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/1.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;其中$W_<em> \in \mathbb{R}^{d_s \times d_s}$表示前一个状态$(i.e., c^{t-1}_{i,j,l} \quad and \quad h^{t-1}_{i,j,l})$到当前状态的转换矩阵，$V_</em> \in \mathbb{R}^{d_x \times d_s}$是从输入到当前状态的转换矩阵，$d_x和d_s$分别表示输入向量的维度和隐藏状态的维度，且$b_<em> \in \mathbb{R}^{d_s}$是偏置向量，$\sigma(.)和\phi(.)$分别表示sigmoid和tanh函数。$\odot$表示元素相乘。分别使用$i^t_{i,j,l},o^t_{i,j,l},f^t_{i,j,l}$表示输入门、输出门、遗忘门。为了简单起见，我们用$h^t_{i,j,l}=LSTM(</em>,c^{t-1}_{i,j,l},h^{t-1}_{i,j,l})$表示上面的式1。当然也存在RNN的一些变体，例如GRU。</p>
<h2><span id="52-multi-modal-pattern-fusion-module">5.2. Multi-Modal Pattern Fusion Module</span></h2><p>&ensp;&ensp;&ensp;&ensp;然后直接或间接地应用RNN来解决异常事件预测问题是直观的。一般的RNN不能处理来自其他地理区域和时间类别的影响因素。因此我们进一步使用attention机制来自适应地捕获空间和类别的动态相关性。Attention机制用来推断训练集不同部分的重要性，让学习算法更加关注重要的部分。Attention机制引入一个context vector建模相关性，让编码器-解码器摆脱定长的内部表示。并且，在融合过程中，为了区分区域和类别，用$e_{r_{i,j}} \in \mathbb{R}^{d_e}$表示区域嵌入，用$e_{c_j} \in \mathbb{R}^{d_e}$表示类别嵌入，这两种嵌入在attention机制中会用到。attention的计算公式如下：</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/2.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;在attention网络中将隐藏表示向量的大小作为attention dimensionality，用$S$表示，其中$d_s$表示LSTM中隐藏状态的维度。$W^k \in \mathbb{R}^{d_s \times S} \quad b^k \in \mathbb{R}^{d_s}$分别表示权重矩阵和偏置向量，将输入映射到隐藏层，得到$\eta^k_{i,j,l}$作为$h^k_{i,j,l}$的隐藏表示。然后我们度量了每个区域$r_{i,j}$每种类别$c_l$的隐藏表示$\eta^k_{i,j,l}$的重要性，归一化得到$\alpha^k_{i,j,l}$。attention中的权重由输入的空间-类别特征$e_{r_{i,j}} \in \mathbb{R}^{d_e},e_{c_j} \in \mathbb{R}^{d_e}$联合决定，在Context-LSTM编码器中编码历史隐藏状态$h^k_{i,j,k}$。在获取attention权重后，在时间段k的输出隐藏表示向量计算如下：</p>
<script type="math/tex; mode=display">q^k = \sum_{i,j \in G}\sum_{l=1}^{L} \alpha^k_{i,j,l}h^k_{i,j,l} \tag{3}</script><p>&ensp;&ensp;&ensp;&ensp;其中$q^k$是$h^k_{i,j,l}$的summarized拼接表示，描述了在区域$r_{i,j}$异常事件的发生，哪个因素更重要。在MiST的训练过程中，带有attention机制的深度融合模块被参数化为前向神经网络，和整个神经网络一起训练。我们提出的方法是非常通用的，可以自动学习不同视图的相关性权重。</p>
<h2><span id="53-conclusive-recurrent-network">5.3. Conclusive Recurrent Network</span></h2><p>&ensp;&ensp;&ensp;&ensp;目前为止，我们已经研究了MiST到的2个组件，(i)从temporal角度，使用context-LSTM建模区域内动态的相关性；(ii)从spatial-categorical角度，使用深度融合模块捕获复杂的区域间和类别见的相关性。经过以上步骤，得到了summarized representation $q^k$，从不同角度使用不同的权重$\alpha^k_{i,j,l}$计算组合表示。<br>&ensp;&ensp;&ensp;&ensp;为了将空间-类别的编码pattern和时间pattern整合在一起，我们提出了用循环神经网络编码多维模式，用潜在空间的表示建模location-time-category之间的关系。在这篇论文中，我们采用LSTM作为循环神经单元，公式如下：</p>
<script type="math/tex; mode=display">\xi_k = LSTM(q_{k-1},\xi_{k-1}) \tag{4}</script><p>&ensp;&ensp;&ensp;&ensp;联合嵌入$\xi$将所有的空间、时间、类别单元映射到一个共同的潜在空间中。提出的conclusive循环神经网络提供了一种灵活的方式让不同的视图彼此合作。将空间、类别上下文信号和时间状态结合，MiST框架可以预测将来异常事件，不仅仅根据时间序列关系，还根据区域间的空间关系和不同类别的共现关系。</p>
<h2><span id="54-forecasting-and-model-inference">5.4. Forecasting and Model Inference</span></h2><p>&ensp;&ensp;&ensp;&ensp;最终，我们利用MLP来解码异常事件出现的概率，通过捕获隐藏向量元素之间的非线性依赖。公式如下：</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/5.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;其中，$N$表示隐藏层的个数，对于层$\psi_n$，$W_n$和$b_n$表示权重矩阵和偏置向量。我们使用$ReLU,\phi(.)$为全连接层的激活函数。使用$\sigma(.)sigmoid$作为输出层的激活函数，值域在(0,1),输出异常事件发生的概率，在区域$r_{i,j}$时间槽$t_k$异常事件类别$c_l$，例如$y^{l,k}_{i,j}$。</p>
<p>&ensp;&ensp;&ensp;&ensp;综上所述，我们的异常事件发生预测可以被看做是一个分类问题。我们利用叫啥上作为损失函数。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/6.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;其中，$\hat{y}^{l,k}_{i,j}$表示预测的在区域$r_{i,j}$第$k$个时间段发生第$l$个异常类别事件的概率，$S$是训练集中异常事件的集合。使用Adam优化器来学习参数。<br>算法流程如下：</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/algo.png" alt=""></p>
<h1><span id="6-评价">6. 评价</span></h1><p>在三个真实异常事件数据集上做了实验，数据从NYC和Chicago收集，验证模型的有效性和准确率和其他baseline，通过实验回答以下几个问题</p>
<ul>
<li>Q1：和其他state-of-the-art预测方法，在预测全市犯罪和不同城市的异常情况时，MiST可以达到与之媲美的准确率吗？</li>
<li>Q2：在不同的时间段中，MiST一直比其他的算法表现好吗？</li>
<li>Q3：和其他state-of-the-art技术相比，MiST模型怎么预测不同种类的异常事件</li>
<li>Q4：MiST使用不同关键组件的组合形成的变体效果怎么样？</li>
<li>Q5：MiST在不同的空间和时间范围上表现怎么样？</li>
<li>Q6：不同的参数设置怎么影响MiST的预测效果？</li>
<li>Q7：当预测城市异常事件时，怎么解释MiST框架捕获的空间和类别维度的动态重要性权重？</li>
</ul>
<h2><span id="61-数据集">6.1. 数据集</span></h2><h3><span id="611-数据统计">6.1.1. 数据统计</span></h3><p>&ensp;&ensp;&ensp;&ensp;我们从NYC和Chicago收集了2种类型的3个异常事件数据，有2个犯罪数据和1个城市异常数据，通过做实验，预测城市的每个区域发生每种城市犯罪和异常事件的可能性。数据集基本统计如下：</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/table1.png" alt=""></p>
<p>在我们的实验中，我们重点关注了一些关键类别，把其他的类别看做外部类别。我们也给了不同类型和时间周期的异常事件在地理上的分布，如Figure2所示。</p>
<ul>
<li>NYC Crime Data(NYC-C)：这个数据集中有多个类别的犯罪记录。每一个犯罪记录有犯罪类别、经纬度、时间。时间跨度为2015.1~2015.12</li>
<li>NYC Urban Anomaly Data(NYC-A)：这个数据集时间跨度为2014.1~2014.12，从NYC311个非紧急服务中心收集来的，这里记录了不同类别的城市异常。每个记录都有异常类别、经纬度、时间。</li>
<li>Chicago Crime Data(CHI-C)：从芝加哥收集的2015.1~2015.12不同种类的犯罪记录，记录的个数和NYC类似。</li>
</ul>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure2.png" alt=""></p>
<h2><span id="62-实验">6.2. 实验</span></h2><h3><span id="621-参数设置">6.2.1. 参数设置</span></h3><p>&ensp;&ensp;&ensp;&ensp;在我们的试验中，利用Adam作为优化器，使用Tensorflow实现MiST架构。在LSTM中设置隐藏状态维度$d_s=32$，区域嵌入向量$e_{r_{i,j}}$和类别嵌入向量$e_{c_j}$的维度$d_e=32$，attention的维度$S=32$，MLP的层数为3。batch size=64，学习率=0.001。</p>
<h3><span id="622-baseline">6.2.2. Baseline</span></h3><p>(i)传统的时间序列预测方法：SVR、ARIMA<br>(ii)传统的有监督学习算法：LR<br>(iii)循环神经网络和它的变体for时空数据预测：ST-RNN 、GRU<br>(iv)先进的神经网络模型for 时间序列和序列模型：RDN、HRN、ARM</p>
<h3><span id="623-评价指标">6.2.3. 评价指标</span></h3><p>&ensp;&ensp;&ensp;&ensp;在实验中，按照时间顺序将数据集划分为训练集(6.5个月)、验证集(0.5个月)和测试集(1个月)。验证集被用来调整超参数，在测试集上进行性能比较。我们把NYC和Chicago划分为248和189个互不相交的区域，每个区域的大小$2km \times 2km$，根据区域划分的结果，我们可以映射每个异常事件(犯罪或城市异常)到一个地理区域中，作为MiST的输入。我们采用2种评价指标来衡量所有的方法。</p>
<ul>
<li>(i)使用Macro-F1 和Micro-F1来衡量不同种类犯罪的预测准确率。这2个指标表示了不同类别之间的整体效果。这2个指标的数据定义如下：</li>
</ul>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/micro.png" alt=""></p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/macro.png" alt=""></p>
<p>其中$J$是异常事件的种类数。这2个值越高效果越好</p>
<ul>
<li>(ii) 使用F1-score和$AUC$来衡量预测一个类别的异常事件发生的准确率。F1和AUC越高，说明预测效果越好。<br>&ensp;&ensp;&ensp;&ensp;为了确保所有方法的性能公平比较，在测试集中预测一段时间连续几天异常事件发生的概率。在评估结果中，一段时间所有天的平均性能作为最终的结果</li>
</ul>
<h2><span id="63-实验结果">6.3. 实验结果</span></h2><h3><span id="631-overall-comparisonq1">6.3.1. Overall Comparison(Q1)</span></h3><p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/table2.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;表2显示了不同城市犯罪和城市异常的预测准确率。总结以下3点：<br>&ensp;&ensp;&ensp;&ensp;第一：MiST比其他神经网络方法效果都好。例如，在预测Chicago犯罪时，MiST比最好的模型RDN Macro-F1和Micrl-F1高9.6%和30.9%。<br>&ensp;&ensp;&ensp;&ensp;第二：神经网络方法比传统的时间序列和有监督学习方法效果好。这是由于（1）传统的时间序列预测方法仅仅强调一个固定的时间模式，而不是时间依赖的演变。（2）神经网络方法使用非线性方法捕获多维空间-时间数据的内在结构，这非常有用。<br>&ensp;&ensp;&ensp;&ensp;第三：在循环神经网络中(ST-LSTM和GRU)和深度序列数据模型方法(RDN、HRN、ARM)效果不分上下。这再一次验证了仅仅考虑时间维度的数据依赖在预测犯罪和城市异常发生时不够的。相反，MiST动态关联潜在的空间、时间、类别的关系，表现了很好的灵活性和优越性。</p>
<h3><span id="632-forecasting-accuracy-vs-time-periodq2">6.3.2. Forecasting Accuracy v.s Time Period(Q2)</span></h3><p>&ensp;&ensp;&ensp;&ensp;对于MiST和其他的baseline，在不同的训练和测试时间段上做了实验。我们发现MiST在不同的测试时间段上一直保持最好的效果。并且也可以发现在MiST和起亚baseline相比，当滑动训练集和测试集的时间窗口时，MiST的效果更稳定，这说明MiST在学习随着时间动态的异常事件分布时更健壮。</p>
<h3><span id="633-forecasting-accuracy-vs-categoriesq3">6.3.3. Forecasting Accuracy v.s Categories(Q3)</span></h3><p>&ensp;&ensp;&ensp;&ensp;我们测试了MiST在预测单个异常类别事件的有效性，在NYC的犯罪和异常数据、Chicago的犯罪数据集上，结果如figure3和4所示。发现MiST在所有的类别上都取得了最好的效果。另一个发现是MiST在预测building/Use时效果比ST-RNN高了84.1%左右，这说明MiST在预测稀疏异常类别时表现也很好，解决了数据稀疏问题。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure3.png" alt=""></p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure4.png" alt=""></p>
<h2><span id="64-component-wise-evaluation-of-mistq4">6.4. Component-Wise Evaluation of MiST(Q4)</span></h2><p>为了更的理解MiST，对MiST的不同组件进行组合做了实验。</p>
<ul>
<li><strong>Spatial-View+Temporal View</strong> $MiST-st$<br>这个变体捕获了空间和时间依赖，不考虑类别的影响</li>
<li><strong>Category-view+Temporal View</strong>$MiST-ct$<br>这个变体考虑了累呗和时间依赖，不考虑区域间的空间相关性</li>
<li><strong>Temporal View</strong>$MiST-t$<br>这个变体仅仅使用LSTM和时间attention机制，不考虑空间和类别。</li>
</ul>
<p>&ensp;&ensp;&ensp;&ensp;结果显示使用全部的组件效果最好，这说明使用一个联合框架是很有必要的，同时捕获空间视图（区域间的空间相关性）、时间视图（区域内的时间相关性）、类别视图（类别间的依赖）。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure5.png" alt=""></p>
<h2><span id="65-effect-of-spatial-and-temporal-scaleq5">6.5. Effect of Spatial and Temporal Scale(Q5)</span></h2><p>&ensp;&ensp;&ensp;&ensp;进一步研究了空间和时间范围的影响。在event context tensor$\mathcal{A}$中，网格地图的地理范围$G=I \times J$，在我们的实验中$I=J$，循环框架中时间序列长度为$T$。 在十月份的Crime上做了实验，实验结果如图6所示：2个结论，（1） 随着I和J的增大，实验效果也变好。因为每个网格是$2km \times 2km$，I和J增大，说明考虑了更大的地理区域在学习表示时，当I和J为11时，准确率趋于稳定。另一个可能的原因是当考虑更大的地理区域时，需要学习更多的参数，训练MiST更加困难。（2）当时间序列长度$T$变大时，准确率也变得更好。当T=10时趋于稳定。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure6.png" alt=""></p>
<h2><span id="66-hyperparameters-studiesq6">6.6. Hyperparameters Studies(Q6)</span></h2><p>&ensp;&ensp;&ensp;&ensp; 为了检验MiST模型的健壮性，设置不同的超参数看预测效果。除了被测试的参数外，其余参数都被设置为默认值。 总体上，发现MiST在两个任务上（预测NYC犯罪和异常事件）对参数不敏感，并且都能达到很好的效果，说明MiST模型的健壮性。并且发现当表示的维度为32时，效果最好。这是因为刚开始，潜在表示的维度变大能够为循环框架和Attention框架提供一个更好的表示，随着参数的增加，可能会造成过拟合。在我们的实验中，为了权衡有效性和计算代价，将表示维度设置为32。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure7.png" alt=""></p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure8.png" alt=""></p>
<h2><span id="67-case-studyq7">6.7. Case Study(Q7)</span></h2><p>&ensp;&ensp;&ensp;&ensp;MiST除了有很好的预测性能，并且在预测一个区域特定类别的异常事件时，能很好的解释空间和类别相关性的重要性。为了说明这点，我们做了实验说明模型的可解释性，在预测NYC盗窃事件时，在一个$5\times 5$的网格中，中间的区域表示目标区域，将attention权重可视化。说明MiST能够动态建模目标区域和其他区域的相关性，并且可以动态建模目标区域的异常类别事件（盗窃）和其他类别的关系。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure9.png" alt=""></p>
<h1><span id="7-总结">7. 总结</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文提出了一个新的神经网络架构MiST，从空间-事件-类别维度对城市异常事件的动态模式进行建模。我们整合了循环神经网络和多模态融合模块来建模空间-事件的相关性。在不同的真实数据集上评测模型，结果显示MiST比其他baseline效果都好。<font color="#FF0000">[未来方向]</font>关于我们工作的未来方向。第一，检测不同类别的异常事件发生的因果关系，这对公共政策的制定有用。发现异常事件发生的潜在因素，以及不同类别的异常事件在时空上怎么传播。第二，由于数据的限制，我们只在3个真实数据集上做了实验，实际上，MiST通用且灵活，可以应用到其他多维且有时间戳的序列数据上。</p>
<h1><span id="8-该作者其他论文">8. 该作者其他论文</span></h1><p>除了这篇论文之外，作者还发表了3篇关于anomaly方向的论文：</p>
<ul>
<li><p>[2016 CIKM]《Crowdsourcing-based urbananomaly prediction system for smart cities》<br>数据集：311 is NYC’s non-emergency service platform.人们可以在这个平台上抱怨周围发生的事情，通过文字、电话或者app，在NYC OpenData可以获取到。<br>Crodsourcing-bases Urban Anomaly Prediction Scheme(CUAPS)给定一个区域，在异常发生之前进行预测。在crowdsourcing data中，结合空间和时间信息进行预测。首先使用贝叶斯推理模型，根据区域的异常分布来鉴别区域之间的依赖性。然后应用一个最优的异常状态预测方案来预测一个区域的异常事件，从这个区域本身的数据和它依赖的区域。</p>
<p><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/CUAPS.png" alt=""></p>
</li>
<li><p>[2017 ECML] 《Uapd: Predicting urban anomalies from spatial-temporal data》<br>数据集：311 is NYC’s non-emergency service platform（作者提供了整理好的数据和源码）<a href="https://bitbucket.org/xianwu9/uapd/src/master/" target="_blank" rel="noopener">https://bitbucket.org/xianwu9/uapd/src/master/</a><br>和Pittsburgh OpenData portal<br>挑战：时间动态，多维相关，空间、时间、类别。提出模型Urban Anomaly PreDection(UAPD)。首先提出一个概率模型，模型参数通过马尔科夫连推导出来，来检测历史异常记录的变化点，然后最相关的记录被用来预测将来的异常。在第二阶段，从被检测出的变化点开始，使用3维张量建模异常数据，每一维表示区域、时间、类别。然后，分解张量，将每个维度之间的潜在关系合并到张量对应的固有因子上。随后，预测下一时间段的异常变成了一个时间序列预测问题。在第三阶段，利用向量自回归来捕获多个时间序列之间的相互依赖性，从而生成预测结果。</p>
</li>
<li>[2018 CIKM] 《DeepCrime:Attentive Hierarchical Recurrent Networks for Crime Prediction》和郑宇联合发表<br>数据集：NYC的Crime记录，<br>DeepCrime，a deep neural network architecture。编码空间、时间、类别到隐藏向量表示中。通过分层循环神经网络捕获异常的动态信息。</li>
</ul>
<h1><span id="9-anomaly检测领域的其他论文">9. Anomaly检测领域的其他论文</span></h1><ul>
<li>[2013 Ubicomp] 《Flead: Online frequency<br>likelihood estimation anomaly detection for mobile sensing》<br>数据集：手机收集的数据</li>
<li>[2015 CIKM] 《Profiling pedestrian distribution and anomaly detection in a dynamic environment》<br>数据集：没有说</li>
<li>[2015 SIGSPATIAL] 郑宇《Detecting collective anomalies from multiple spatio-temporal datasets across different domains》<br>提供了数据集和代码<a href="https://www.microsoft.com/en-us/research/publication/detecting-collective-anomalies-from-multiple-spatio-temporal-datasets-across-different-domains/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2F%3Fid%3D255670" target="_blank" rel="noopener">链接</a><br>数据描述：<br>（1）POI数据：NYC有24031个POI，共14中类别<br>（2）Road network data：在NYC的862个区域中的路段，每个路段有2个终点和一些中间点，还有一些属性，比如级别，速度限制等<br>（3）311data：NYC<br>（4）Taxicab data：在NYC的14000个出租车产生的数据，包括费用和行程数据，行程数据包括：上下车地点和时间，行程的距离和持续时间，出租车ID，乘客个数等。<br>（5）Bike tenting data：自行车租赁数据，NYC的340个自行车站点，大约7000辆车，每一条记录包括时间，车辆ID，站点ID，返还记录。</li>
<li>[2017 CIKM] 《Spatiotemporal event forecasting from incomplete hyper-local price data》<br>数据集：有6个数据集，来自6个不同的城市，其中2个是商品价格数据，数据从<a href="https://www.premise.com/" target="_blank" rel="noopener">https://www.premise.com/</a>获取。其中4个是美国4个城市房地产短租的价格数据，数据从Airbnb获取</li>
<li>[2017 KDD] 《Contextual spatial outlier detection with metric learning》<br>一部分数据来源：<a href="http://archive.ics.uci.edu/ml/index.php" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/index.php</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/2019/06/12/Docker/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖到一个可移植的镜像中，然后发布到Linux或Window系统中，也可以实现虚拟化。容器是完全使用沙箱机制，相互之前不会有任何接口。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-docker">1. Docker</a></li>
<li><a href="#2-%e9%95%9c%e5%83%8f%e5%ae%b9%e5%99%a8">2. 镜像&amp;容器</a><ul>
<li><a href="#21-%e9%95%9c%e5%83%8f">2.1. 镜像</a></li>
<li><a href="#22-%e9%95%9c%e5%83%8f%e5%91%bd%e4%bb%a4">2.2. 镜像命令</a></li>
<li><a href="#23-%e5%ae%b9%e5%99%a8">2.3. 容器</a></li>
<li><a href="#24-%e5%ae%b9%e5%99%a8%e5%91%bd%e4%bb%a4">2.4. 容器命令</a></li>
</ul>
</li>
<li><a href="#3-dockerfile">3. Dockerfile</a><ul>
<li><a href="#31-%e7%9b%b8%e5%85%b3%e6%8c%87%e4%bb%a4">3.1. 相关指令</a></li>
<li><a href="#32-%e6%9e%84%e5%bb%ba">3.2. 构建</a></li>
</ul>
</li>
<li><a href="#4-%e4%bd%bf%e7%94%a8docker%e8%bf%90%e8%a1%8c%e7%a8%8b%e5%ba%8f">4. 使用Docker运行程序</a></li>
<li><a href="#5-%e6%9e%84%e5%bb%ba%e8%87%aa%e5%b7%b1%e7%9a%84%e9%95%9c%e5%83%8f">5. 构建自己的镜像</a><ul>
<li><a href="#51-%e5%88%9b%e5%bb%badockerfile">5.1. 创建Dockerfile</a></li>
<li><a href="#52-%e6%9e%84%e5%bb%ba">5.2. 构建</a></li>
<li><a href="#53-%e8%bf%90%e8%a1%8c">5.3. 运行</a></li>
</ul>
</li>
<li><a href="#6-docker%e5%b8%b8%e7%94%a8%e5%91%bd%e4%bb%a4">6. Docker常用命令</a></li>
<li><a href="#7-docker%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e8%b7%b5">7. Docker深度学习实践</a><ul>
<li><a href="#71-%e7%ab%af%e5%8f%a3%e6%98%a0%e5%b0%84">7.1. 端口映射</a></li>
</ul>
</li>
<li><a href="#8-%e6%88%91%e7%9a%84%e9%95%9c%e5%83%8f">8. 我的镜像</a><ul>
<li><a href="#81-mxnet">8.1. mxnet</a><ul>
<li><a href="#811-%e6%97%a0hdfs">8.1.1. 无hdfs</a></li>
<li><a href="#812-%e6%9c%89hdfs">8.1.2. 有hdfs</a></li>
</ul>
</li>
<li><a href="#82-pytorch">8.2. pytorch</a><ul>
<li><a href="#821-%e6%97%a0hdfs">8.2.1. 无hdfs</a></li>
<li><a href="#822-%e6%9c%89hdfs">8.2.2. 有hdfs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-docker">1. Docker</span></h1><p>参考资料：<br><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&amp;mid=2655824742&amp;idx=1&amp;sn=43dcbd8cd3b3e0dc5f06c83a50983420&amp;chksm=bd74e6b18a036fa7e5fe2229b3fa08f5a4d11c6d201deba29c63e61c47ed37b5aa7f22e441d6&amp;scene=0&amp;xtrack=1&amp;key=1873ed4ed1cb893ec82026059f24db129748acc346da2d85cea356373d0c0fd919da6e704f47695c7743b64ff520c46fb51d245dacff68136d667c05e73d963d768ee11171e66dedea24d39bb3d67ced&amp;ascene=1&amp;uin=MTM1ODU4OTIwOA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=84krGCWgmUm73hPCIwVp8NE9B3dpOiU5R1bRm3jvlvv%2FygbqWRm4O%2BYabIzyFhbf" target="_blank" rel="noopener">资料1</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&amp;mid=2655817429&amp;idx=1&amp;sn=f82daff5e9fad66a0e11cdb92c12715e&amp;chksm=bd74c3028a034a14cadf884d97f2a0fc3372d3baa97fb5ae6724e6d1926520eada090c0ffb16&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">资料2</a></p>
<p>&ensp;&ensp;&ensp;&ensp;Docker就是一个运行在操作系统上的软件。这个软件上运行很多容器，这些容器相互独立，相互隔离。容器中可以安装很多应用程序。<br>&ensp;&ensp;&ensp;&ensp;我们平时要在Windows上安装Linux系统，都需要先安装一个VMWare，然后在上面安装Linux系统。原理就是虚拟出一套硬件资源，然后在上面运行一个完整的操作系统，再在操作系统上运行所需要的应用程序。在安装虚拟机时，需要先提前给虚拟机分配硬盘，内存等资源。一旦分配，这些资源就被虚拟机全部占用。Docker也可以实现虚拟化。但是Docker没有自己的内核，Docker容器内的应用程序是直接运行在宿主的内核，Docker比传统的虚拟机更轻便。Docker就是一个软件。如果以后在Windows上安装Linux系统，可以先在本地电脑上安装一个Windows版本的Docker。</p>
<h1><span id="2-镜像amp容器">2. 镜像&amp;容器</span></h1><h2><span id="21-镜像">2.1. 镜像</span></h2><p>&ensp;&ensp;&ensp;&ensp;官方定义：Docker镜像是一个只读模板，可以用来创建Docker容器。镜像是一种轻量级的，可执行的独立软件包，软件和依赖的环境可以打包成一个镜像。这个镜像包含某个软件需要的所有内容，包括代码、库、环境变量、配置文件等。<br>&ensp;&ensp;&ensp;&ensp;比如我们开发的Web应用需要JDK，Tomcat，环境变量等。那我们就可以把这些都打包成一个镜像，包括代码+JDK+Tomcat+CentOS系统+各种配置文件等。打包后的镜像如果可以运行，那么这个镜像就可以在任何安装有Docker的电脑上运行。<br>&ensp;&ensp;&ensp;&ensp;任何镜像的创建会基于其他的父镜像，也就是说镜像是一层套一层的。比如一个Tomcat镜像需要运行在CentOS上面，那我们的Tomcat镜像就会基于CentOS镜像创建。</p>
<h2><span id="22-镜像命令">2.2. 镜像命令</span></h2><ol>
<li><p>docker images：查看本地主机上所有的镜像。注意是本地主机的！这里能看到镜像的名称、版本、id、大小等基本信息，注意这里的 imageID 是镜像的唯一标识！</p>
</li>
<li><p>docker rmi：删除本地的镜像，如下图所示，可以加上 -f 参数进行强制删除。<br>这里的 rmi 命令跟 Linux 中的删除命令就很像啦，只是这里加了一个 i 代表 image！</p>
</li>
<li><p>docker search：根据镜像名称搜索远程仓库中的镜像！</p>
</li>
<li><p>docker pull：搜索到某个镜像之后就可以从远程拉取镜像啦，有点类似咱们 Git 中的 Pull 命令，当然对应的还有个 dockerpush 的命令。</p>
</li>
</ol>
<h2><span id="23-容器">2.3. 容器</span></h2><p>&ensp;&ensp;&ensp;&ensp;Docker的容器是<strong>用镜像创建的运行实例</strong>，Docker可以利用容器独立运行一个或一组应用。我们可以使用客户端或API控制容器的启动、开始、停止、删除。每个容器都是相互独立的。上一步创建的镜像是一个静态的文件，这个文件想要运行的话，就要先变成容器。我们可以把容器看做是一个简易版的Linux系统和运行在上面的程序。<br><strong>镜像和容器的关系</strong><br>类似于Java中的类和对象的关系。镜像可以看做一个类，容器是镜像的一个实例。可以根据一个类new很多个实例，new出来的实例就相当于一个个容器。镜像是静态的文件，容器是有生命的个体。</p>
<h2><span id="24-容器命令">2.4. 容器命令</span></h2><ul>
<li>docker pull<br>从远程仓库中拉取镜像</li>
<li>通过镜像创建容器<br><code>docker run [OPTIONS] IMAGE</code>可以基于某个镜像运行一个容器，如果本地有指定的镜像则使用本地的镜像，如果没有则远程拉取然后启动<br><code>docker run -v $PWD:/root -d -ti --runtime=nvidia --rm --name wbbmxnet -u 1042 mxnet/python:1.4.1_gpu_cu90_mkl_py3</code></li>
</ul>
<h1><span id="3-dockerfile">3. Dockerfile</span></h1><h2><span id="31-相关指令">3.1. 相关指令</span></h2><p>镜像可以从远程仓库中拉取，也可以自己创建一个镜像。Dockerfile是一个包含用户能够构建镜像的所有命令的文本文档，它有自己的语法和命令。Docker能够从Dockerfile中读取指令并自动构建镜像。<br>如果想构建自己的镜像，就要自己写Dockerfile。  </p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> nvidia/cuda:<span class="number">9.0</span>-cudnn7-devel</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> SongChao chaosong@bjtu.edu.cn</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN apt-get update</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN apt-get install -y gcc make build-essential libssl-dev wget curl vim --allow-unauthenticated</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN mkdir /root/python3.6</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">COPY Python-3.6.8 /root/python3.6/</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">WORKDIR /root/python3.6</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN ./configure &amp;&amp; make &amp;&amp; make install</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN wget https://bootstrap.pypa.io/get-pip.py</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN python3 get-pip.py</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">ENV PYTHONIOENCODING=utf-8</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN pip3 install numpy scipy pandas tensorflow-gpu==1.9.0 -i https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">RUN mkdir /workdir</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">WORKDIR /workdir</span></span><br></pre></td></tr></table></figure>
<p>ENV设置环境变量，或者定义变量<br>在制作dockerfile时，有一些缓存可以删除，可以在RUN里面写相关的操作，这个就只是一层。<br><code>FROM</code>指定基础镜像，当前镜像是基于哪个镜像创建的，有点类似Java中类继承，FROM指令必须是Dockerfile必须是Dockerfile文件的首条命令。<br><code>LABEL</code>给镜像添加元数据，指定作者，邮箱等信息。  </p>
<h2><span id="32-构建">3.2. 构建</span></h2><p>Dockerfile的执行顺序从上到下顺序执行，编写好Dockerfile文件后，就需要使用docker build命令对镜像进行构建了。<br><code>docker build [OPTIONS] PATH | URL | -</code><br><code>docker build -t chaosong/cuda-10.1-cudnn7-devel:with_cuda_samples .</code><br>-f：指定要使用的 Dockerfile 路径，如果不指定，则在当前工作目录寻找 Dockerfile 文件！<br>-t：镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 注意后面的 . , 用于指定镜像构建过程中的上下文环境的目录，.表示当前目录。</p>
<h1><span id="4-使用docker运行程序">4. 使用Docker运行程序</span></h1><ol>
<li>登录上网<br><code>links 10.1.61.1/a30.htm</code><br>登录之后按<code>Ctrl+C</code>退出  </li>
<li>在dockerhub上查询镜像<br>浏览器进入<a href="https://hub.docker.com/" target="_blank" rel="noopener">docker hub</a>，找到自己需要的镜像，gpu28号服务器Cuda的版本是9.0，下载的镜像需要和服务器上Cuda版本一致。  </li>
<li>拉取镜像<br>例如镜像的全称为：<br><code>mxnet/python:1.4.1_gpu_cu90_mkl_py3</code><br><code>docker pull mxnet/python:1.4.1_gpu_cu90_mkl_py3</code><br>这条命令会把镜像下载到服务器上，如果本地已经存在该镜像，docker会在佛那个使用本地的镜像，不再下载。</li>
<li><p>启动镜像<br><code>docker run -v $PWD:/root -d -ti --runtime=nvidia --rm --name wbbmxnet -u 1042 &lt;镜像名:标签&gt;</code><br>例如<br><code>docker run -v $PWD:/root -d -ti --runtime=nvidia --rm --name wbbmxnet -u 1042 mxnet/python:1.4.1_gpu_cu90_mkl_py3</code><br>使用上面的命令可以启动镜像，这里的参数-v通过将宿主机的目录映射到容器内，\$PWD是当前目录，映射目标是容器内的/root目录，这时\$PWD中的目录会全部映射到容器中的/root目录下。<br>启动容器后执行<br><code>cd /root</code><br><code>ls</code><br>可以看到服务器中的目录都映射到容器内，当在容器内创建一个目录时，然后创建一个文件，exit退出容器，可以看到本地服务器也有刚才那个文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir docker_test</span><br><span class="line">touch docker_test.txt</span><br><span class="line">exit</span><br><span class="line">ls</span><br><span class="line">ls docker_test</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行GPU程序<br>先不启动容器，在宿主机上使用<br><code>nvidia-smi</code>查看当前空闲的GPU，使用空闲的GPU运行程序。<br>把py程序上传到服务器上，然后启动容器。  </p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">docker run -v $PWD:/root -d -ti --<span class="keyword">runtime</span>=nvidia --rm --name wbbmxnet -<span class="keyword">u</span> <span class="number">1042</span> mxnet/<span class="keyword">python</span>:<span class="number">1.4</span>.<span class="number">1</span>_gpu_cu90_mkl_py3  </span><br><span class="line"><span class="keyword">cd</span> 进入<span class="keyword">py</span>程序所在的目录  </span><br><span class="line"><span class="keyword">python3</span> xxx.<span class="keyword">py</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1><span id="5-构建自己的镜像">5. 构建自己的镜像</span></h1><h2><span id="51-创建dockerfile">5.1. 创建Dockerfile</span></h2><p>通过Dockerfile构建自己的镜像。镜像构建时，会一层层的构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是该文件一直跟随镜像。因此在创建镜像的时候，需要小心，每一层尽量只添加该层需要的东西，任何额外的东西应该在该层构建结束前清理掉。<br>Docker能够从Dockerfile中读取指令自动的构建镜像。</p>
<p>在服务器上创建一个目录<br><code>mkdir wbb_docker_gcn</code><br><code>cd wbb_docker_gcn</code><br>可以在这个目录下下载一些创建镜像需要的文件<br><code>git clone https://github.com/NVIDIA/cuda-samples.git</code><br>然后编写用于构建镜像的Dockerfile<br><code>vi Dockerfile</code><br>内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#指定镜像要构建在哪个镜像之上  </span></span><br><span class="line"><span class="comment">#如果程序需要用到GPU，那就一定要构建在nvidia/cuda这个镜像上  </span></span><br><span class="line">FROM nvidia/cuda:<span class="number">9.0</span>-cudnn7-devel</span><br><span class="line"></span><br><span class="line"><span class="comment">#给镜像添加元数据，指定作者邮箱等信息</span></span><br><span class="line">LABEL WangBeibei <span class="number">18120408</span>@bjtu.edu.cn</span><br><span class="line"></span><br><span class="line"><span class="comment">#RUN会在当前镜像的最上面创建一个新层，并且能执行任何的命令，</span></span><br><span class="line"><span class="comment">#然后对执行的结果进行提交，提交后的结果镜像在Dockerfile的后续步骤中使用 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#更新Ubuntu的索引</span></span><br><span class="line">RUN apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装gcc工具</span></span><br><span class="line">RUN apt-get install -y wget python3-dev gcc git vim &amp;&amp; \</span><br><span class="line">    wget https://bootstrap.pypa.io/get-pip.py &amp;&amp; \</span><br><span class="line">    python3 get-pip.py   </span><br><span class="line"></span><br><span class="line"><span class="comment">#在容器上创建一个目录，镜像就安装在该目录下</span></span><br><span class="line">RUN mkdir /root/docker_gcn</span><br><span class="line"></span><br><span class="line"><span class="comment">#关于COPY命令，如果要复制目录的话，COPY命令会把目录里面所有文件赋值到另一个目录下，  </span></span><br><span class="line"><span class="comment">#而不是把这个目录直接复制过去，所以上面先在容器中创建了一个docker_gcn的目录，然后COPY命令将服务器本地的wbb_docker_gcn里面的文件都复制到了/root/docker_gcn/里面</span></span><br><span class="line"><span class="comment">#将服务器本地的文件拷贝到容器中的目录上</span></span><br><span class="line">COPY wbb_docker_gcn /root/docker_gcn/</span><br><span class="line"></span><br><span class="line"><span class="comment">#切换到那个目录，如果该目不存在，则创建。WORKDIR是切换当前工作路径，下面的RUN命令都会在这个WORKDIR下面执行</span></span><br><span class="line">WORKDIR /root/docker_gcn</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载</span></span><br></pre></td></tr></table></figure>
<h2><span id="52-构建">5.2. 构建</span></h2><p>Dockerfile是执行是从上到下顺序执行的，每条执行都会创建一个新的镜像层，并对镜像进行提交。编写好Dockerfile文件后，就需要使用dockerbuild命令对镜像进行构建了。<br><code>docker build [OPTIONS] PATH | URL | -</code><br><code>docker build -t chaosong/cuda-10.1-cudnn7-devel:with_cuda_samples .</code><br>-f：指定要使用的 Dockerfile 路径，如果不指定，则在当前工作目录寻找 Dockerfile 文件！<br>-t：镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 注意后面的 . , 用于指定镜像构建过程中的上下文环境的目录。</p>
<h2><span id="53-运行">5.3. 运行</span></h2><p>构建完镜像就可以启动这个容器了，启动完之后就可以运行py脚本<br><code>docker run -itd --rm --runtime=nvidia -v $PWD:/workdir/ --name wbb -u 1042 songchao/tensorflow:1.9.0_py36_cu90_cudnn7 /bin/bash</code><br>由于这个命令非常重要，所以下面列出几个比较重要的参数：</p>
<p>-d：启动容器，并且后台运行（Docker 容器后台运行，就必须要有一个前台进程，容器运行的命令如果不是一直挂起的命令，容器启动后就会自动退出）。使用-d不会进入docker的交互界面，只会返回一个长id。<br>使用docker ps可以看到对应的短id，使用docker attach 短id，进入到docker的交互界面。</p>
<p>-i：以交互模式运行容器，通常与 -t 同时使用。</p>
<p>-t：为容器重新分配一个伪输入终端，通常与 -i 同时使用（容器启动后进入到容器内部的命令窗口）。</p>
<p>-P：随机端口映射，容器内部端口随机映射到主机的高端口。</p>
<p>-p：指定端口映射，格式为：主机(宿主)端口：容器端口。</p>
<p>-v：建立宿主机与容器目录的同步。</p>
<p>—name=”myTomcat”：为容器指定一个名称（如果不指定，则有个随机的名字）。<br>其中—rm表示程序运行完，这个容器就删掉了。<br>-bash表示打开一个命令行</p>
<p><strong>Ctrl+P+Q把容器挂在后台</strong></p>
<h1><span id="6-docker常用命令">6. Docker常用命令</span></h1><ol>
<li><p>列出机器上的镜像<br><code>docker images</code></p>
<p><img src="/2019/06/12/Docker/images.png" alt=""></p>
</li>
<li><p>拉取镜像<br><code>docker pull mxnet/python:1.4.1_gpu_cu90_mkl_py3</code></p>
</li>
<li>将镜像push到个人仓库<br>在上一步从公共仓库拉取了一个镜像<br><code>docker pull mxnet/python:1.1_gpu_cu100_mkl_py3</code><br>将镜像push到27号服务器上面的仓库，先tag再push<br><code>docker tag source des</code><br>例如<br><code>docker tag mxnet/python:1.1_gpu_cu100_mkl_py3 lin-ai-27:5000/mxnet:1.41_cu100_py3</code><br>推送镜像<br><code>docker push lin-ai-27:5000/mxnet:1.41_cu100_py3</code><br>这样镜像就会上传到27号服务器上面。  </li>
<li>在别的服务器上将刚才的镜像pull下来<br><code>docker pull lin-ai-27:5000/mxnet:1.41_cu100_py3</code></li>
<li>镜像重命名<br><code>docker tag 镜像id wangbeibei/mxnetx:latest</code></li>
<li><p>查看容器信息<br>（1）docker ps：显示当前正在运行的容器，在 PORTS 一列，如果暴露的端口是连续的，还会被合并在一起，例如一个容器暴露了3个 TCP 端口：100，101，102，则会显示为 100-102/tcp。</p>
<p>（2）docker ps -a：显示所有的容器，<br>容器的状态共有 7 种：created|restarting|running|removing|paused|exited|dead。<br>（3）docker ps -n 3：显示最后被创建的n个容器<br>（4）docker ps -q：只显示正在运行容器的id，在清理容器时非常好用。<br>（5）docker ps -s：显示容器文件大小，该命令很实用，可以获得 2 个数值：一个是容器真实增加的大小，一个是整个容器的虚拟大小。</p>
</li>
<li><p>以交互式方式启动容器</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">[<span class="symbol">root@</span>localhost ~]# docker run -it --name centos-test centos:<span class="number">7.4</span><span class="number">.1708</span></span><br><span class="line">[<span class="symbol">root@</span>ebd974405f42 /]#</span><br></pre></td></tr></table></figure>
<p>以交互式方式启动容器后，docker会随机分配一个容器名，并为容器分配一个短id。</p>
</li>
<li><p>后台运行容器</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">[<span class="symbol">root@</span>localhost ~]# docker run --name centos-deamon -d centos:<span class="number">7.4</span><span class="number">.1708</span>  </span><br><span class="line"><span class="number">63e5555</span>c9ed35deb7e32c2a16896e0f806dbcf471d0484bdd904724a3cce542d</span><br></pre></td></tr></table></figure>
<p>其中d表示容器在后台运行，使用run命令之后，会把容器挂到后台运行，并且会输出一个长的container id，通过docker ps查看容器的信息，这里输出的是容器id的前12位。</p>
</li>
<li>进入到后台运行的容器<br>通过docker attach ae60c4b64205连接到正在运行的终端，此时使用exit命令退出容器。  </li>
<li><p>容器的状态<br>docker容器有几种状态，分别是created、up、exited、paused。<br>（1）我们通过run命令运行容器时，其实是将容器从created到up状态的过程。<br>（2）当通过stop命令停止容器时，容器进入exited状态。容器退出后，系统仍然保存该容器实例，即退出的容器仍然会占用系统的硬盘资源，需要使用rm删除该容器才能完全清楚容器的资源占用。容器stop或Ctrl+D时，会保存当前容器的状态之后退出，下次start时会保存上次的关闭时更改，而且每次attach进去的界面是一样的，和第一次run启动一样。</p>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">[root<span class="symbol">@localhost</span> ~]<span class="meta"># docker stop e83cf32fbc22</span></span><br><span class="line">e83cf32fbc22</span><br></pre></td></tr></table></figure>
<p>（3）重新启动退出的容器<br>处于exited状态的容器，可以通过start命令重新启动。</p>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">[root<span class="symbol">@localhost</span> ~]<span class="meta"># docker start e83cf32fbc22</span></span><br><span class="line">e83cf32fbc22</span><br></pre></td></tr></table></figure>
<p>（4）重启容器  </p>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">[root<span class="symbol">@localhost</span> ~]<span class="meta"># docker restart e83cf32fbc22</span></span><br><span class="line">e83cf32fbc22</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除容器<br>从上面可以看出，通过stop命令停止容器，容器的相关文件仍然存储在宿主主机中，为了释放这部分空间，需要删除这些容器。<br>（1）先将容器停止<code>docker stop e83cf32fbc22</code><br>（2）<code>docker rm id</code>可以删除已经停止的对应id的容器<br>（3）批量删除除了运行以外的程序<br><code>docker rm $(docker ps -a -q)</code><br>（4）如果要批量删除指定状态的容器<br><code>docker rm $(docker ps -a -q  status=exited)</code></p>
</li>
<li><p><strong>运行jupyter notebook</strong><br>（1）在启动容器之前先查看自己在这台服务器上的id，在服务上直接输入<figure class="highlight plain"><figcaption><span>1042```表示哪个这个容器是属于哪个用户的。  </span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![](Docker/uid.png)</span><br><span class="line"></span><br><span class="line">如果不指定用户，使用```gpustat```查看就会看到当前镜像是属于root用户。容器所挂载的目录所属用户也是root用户。那么你在服务器中去操作操作这个目录就会提示permission denied。例如在使用dicker run命令创建一个容器时，将服务器的deepst这个目录挂在在容器的/root目录下，那么容器中/root目录下面的内容就是服务器中/deepst这个目录下面的内容，并且容器中/root中下面的文件和目录使用-ls -l查看，所属的用户是root用户，那么在服务器中的/deepst目录下面，就没有权限操作当前目录，比如mkdir dira，就会出现permission denied。所以在启动容器的时候，一定要指定```-u 1042</span><br></pre></td></tr></table></figure></p>
<p><img src="/2019/06/12/Docker/gpustat.png" alt=""></p>
<p>（2）先启动容器<br>前面的7000是服务器的端口，后面的7000是容器的端口。这个服务器是共用的，需要找一个没有被占用的端口。容器的端口因为只有自己一个人用，所有没有被占用。<br><code>docker run -it -p 7000:7000 -v $PWD:/root --runtime=nvidia --rm -u 1042 --name wbbJupyter ufoym/deepo:all-jupyter-py36</code><br>后面的—rm表示这个退出这个容器时，容器就自动删除了。这个参数可以不指定。<br>（2）然后会进入到容器中，<br><code>cd /root</code><br>会看到服务器本地的目录<br>（3）在容器中运行jupyter notebook<br><code>jupyter notebook --no-browser --ip 0.0.0.0 --port=7000 --allow-root</code><br>（4）在putty中添加7000的端口号<br>然后再浏览器中输入<a href="http://localhost:7000/tree" target="_blank" rel="noopener">http://localhost:7000/tree</a><br>或者不用在putty中添加端口映射，直接在浏览器中输入<a href="http://gpu28:7000，这里的7000指的是服务器的端口。" target="_blank" rel="noopener">http://gpu28:7000，这里的7000指的是服务器的端口。</a><br>（5）如果想要jupyter notebook在后台运行，按Ctrl+P+Q，会退回到服务器<br>（6）在网页中查看tensorboard。在上面那个容器中，因为使用了jupyter notebook，不能运行命令。所以需要使用上面那个镜像再开一个容器。tensorboard的端口默认是：6006，使用下面的命令</p>
<figure class="highlight docker"><figcaption><span>run -it -p 6688:6006 -v $PWD:/root --runtime</span></figcaption><table><tr><td class="code"><pre><span class="line">进入到容器之后，然后使用下面的命令启动tensorboard</span><br><span class="line">```tensorboard --logdir=.</span><br></pre></td></tr></table></figure>
<p>然后在浏览器上<a href="http://gpu28:6688，就可以看到在网页上看到" target="_blank" rel="noopener">http://gpu28:6688，就可以看到在网页上看到</a><br>（6）使用docker ps会看到你的容器正在后台运行<br>（7）使用docker attach 容器的名字/id再次进入到容器中，按Ctrl+C会看到当前正在有jupyter notebook运行<br>（8）发现这个容器中有一个库没有安装，可以在容器中使用pip install在这个容器中安装使用，但是这个容器删除了之后<br>里面安装的东西也没有了。<br>有2中方法：一是使用commit从容器生成镜像，但是这种方式不推荐，二是写Dockerfile重新build一个镜像<br>（9）在容器中使用exit退出该容器，再次使用docker ps该容器也消失了</p>
</li>
<li><p>docker commit制作镜像（强烈不建议操作）<br>启动一个容器—》在容器中安装环境—》退出容器—》docker commit制作镜像<br><strong>在容器中install GPU版本的第三方库时，一定要先看一下镜像cuda的版本。第三方库的cuda版本要和镜像的版本一致</strong></p>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta">#先使用root用户启动一个容器，因为在构造镜像时，</span></span><br><span class="line"><span class="meta">#Anaconda是使用root用户操作的，即只有root用户</span></span><br><span class="line"><span class="meta">#才可以install和uninstall第三方库。使用root用户install第三方库后，然后使用commit命令导出为</span></span><br><span class="line"><span class="meta">#镜像，别的用户在用这个镜像启动容器。  </span></span><br><span class="line">docker run -it -p <span class="number">6688</span>:<span class="number">6006</span> -v $PWD:/root --runtime=nvidia --rm  --name wbbtensorboard ufoym/deepo:all-jupyter-py36v</span><br><span class="line"><span class="meta">#在容器中安装需要的第三方库</span></span><br><span class="line">pip install nni</span><br><span class="line"><span class="meta">#退出容器</span></span><br><span class="line">Ctrl+P+Q  </span><br><span class="line"><span class="meta">#查看刚刚启动容器的id</span></span><br><span class="line">docker ps</span><br><span class="line"><span class="meta">#从容器中commit镜像  </span></span><br><span class="line">docker commit 容器id wangbeibei/mxnet_nni:latest  </span><br><span class="line"><span class="meta">#删除原来的镜像  </span></span><br><span class="line">docker rmi 镜像id  </span><br><span class="line">镜像id可以通过docker images查看</span><br><span class="line"><span class="meta">#将刚才的镜像重命名  </span></span><br><span class="line">docker tag 镜像id wangbeibei/mxnet:latest</span><br><span class="line"><span class="meta">#将新的镜像push到别的服务器上</span></span><br><span class="line">docker tag wangbeibei/mxnet:latest lin-ai<span class="number">-27</span>:<span class="number">5000</span>/wangbeibei/mxnet:latest</span><br><span class="line">docker push lin-ai<span class="number">-27</span>:<span class="number">5000</span>/wangbeibei/mxnet:latest</span><br><span class="line"><span class="meta">#从其他服务器上pull镜像</span></span><br><span class="line">docker pull lin-ai<span class="number">-27</span>:<span class="number">5000</span>/wangbeibei/mxnet:latest</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Docker的cuda和服务器的cuda是不冲突的。</p>
<h1><span id="7-docker深度学习实践">7. Docker深度学习实践</span></h1><p>Nvidia-Docker让容器可以使用GPU，使用nvidia docker run来运行程序就可以使用GPU。或者—runtime nvidia</p>
<h2><span id="71-端口映射">7.1. 端口映射</span></h2><ul>
<li><p>在服务器上开一个jupyter notebook，把服务器的8888映射到本地8888，这样在本地访问8888就可以了。<br>把容器的8888映射到服务器，然后服务器映射到本地，这样在本地8888就可以打开容器中的jupyter。<br>在容器任何非正常关闭的时候，docker会自启，容器也会自己启动。<br>-p 9988:8888，把容器的8888映射到服务器的9988。  </p>
</li>
<li><p>使用命令<code>id</code>来查看自己的UID。<br>只要UID相同，权限一定是一样的。不能在镜像中创建user<br>在容器中指定user。在启动容器的时候一定要指定-u<br>-u username或者uid：指定使用某用户启动容器</p>
</li>
</ul>
<h1><span id="8-我的镜像">8. 我的镜像</span></h1><h2><span id="81-mxnet">8.1. mxnet</span></h2><h3><span id="811-无hdfs">8.1.1. 无hdfs</span></h3><p><code>lin-ai-27:5000/wangbeibei/mxnet:cu_100</code></p>
<h3><span id="812-有hdfs">8.1.2. 有hdfs</span></h3><p>在OpenPai平台上运行程序，使用该镜像<br><code>lin-ai-27:5000/wangbeibei/mxnet:cu100_hdfs</code></p>
<h2><span id="82-pytorch">8.2. pytorch</span></h2><h3><span id="821-无hdfs">8.2.1. 无hdfs</span></h3><p><code>lin-ai-27:5000/wangbeibei/pytorch_nni:cu_100</code></p>
<h3><span id="822-有hdfs">8.2.2. 有hdfs</span></h3><p>在OpenPai平台上运行程序，使用该镜像<br><code>172.31.246.45:5000/dlspree:hdfs_pyg</code><br><code>lin-ai-27:5000/wangbeibei/pytorch:cu100_hdfs</code></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOs系统matplotlib画图中文乱码</title>
    <url>/2019/05/10/CentOs%E7%B3%BB%E7%BB%9Fmatplotlib%E7%94%BB%E5%9B%BE%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;在集群上使用Python中的matplotlib库画图出现中文乱码，记录一下解决方案。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">1. 解决方案</a><ul>
<li><a href="#11-%e6%ad%a5%e9%aa%a4%e4%b8%80">1.1. 步骤一</a></li>
<li><a href="#12-%e6%ad%a5%e9%aa%a4%e4%ba%8c">1.2. 步骤二</a></li>
<li><a href="#13-%e6%ad%a5%e9%aa%a4%e4%b8%89">1.3. 步骤三</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-解决方案">1. 解决方案</span></h1><h2><span id="11-步骤一">1.1. 步骤一</span></h2><p>&ensp;&ensp;&ensp;&ensp;获取matplotlibrc文件所在的路径，使用jupyter notebook写代码获取路径。我的文件路径在<br>/data/WangBeibei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.matplotlib_fname()</span><br></pre></td></tr></table></figure>
<h2><span id="12-步骤二">1.2. 步骤二</span></h2><p>&ensp;&ensp;&ensp;&ensp;</p>
<ul>
<li>到 anaconda 的 matplotlib 中查看是否有 simhei.ttf 字体   </li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /data/WangBeibei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf</span><br><span class="line">ls -al | grep simhei</span><br></pre></td></tr></table></figure>
<ul>
<li>如果没有输出任何内容，说明没有simhei字体，下载simhei.ttf文件，并上传到/data/WangBeibei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf目录下。</li>
<li>修改/data/WangBeibei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc文件，找到以下3行，改为：  </li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">font.family: sans-serif   </span><br><span class="line">font.sans-serif: simhei,DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif</span><br><span class="line">axes.unicode_minus: False#解决负号'-'显示为方块的问题</span><br></pre></td></tr></table></figure>
<ul>
<li>删除/data/WangBeibei/.cache/matplotlib   </li>
</ul>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">rm -r <span class="regexp">/data/</span>WangBeibei<span class="regexp">/.cache/m</span>atplotlib</span><br></pre></td></tr></table></figure>
<h2><span id="13-步骤三">1.3. 步骤三</span></h2><p>经过以上步骤，再次运行jupyter notebook程序，中文就不会出现乱码。如果还是出现乱码，添加以下两行代码  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'simhei'</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span>  <span class="comment"># 用来正常显示负号#显示所有列</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>中文乱码</tag>
      </tags>
  </entry>
  <entry>
    <title>虚拟环境安装</title>
    <url>/2019/04/27/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;在服务器上安装虚拟环境。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83">1. 虚拟环境</a></li>
<li><a href="#2-%e5%b8%b8%e7%94%a8%e5%91%bd%e4%bb%a4">2. 常用命令</a></li>
<li><a href="#3-%e9%97%ae%e9%a2%98">3. 问题</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-虚拟环境">1. 虚拟环境</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器上可以安装不同的虚拟环境，这些虚拟环境之间互不影响，不同的虚拟环境可以安装不同的python版本，不同的框架。</p>
<ol>
<li>给服务器连网<br>在windows系统中，按<code>Windows+R</code>，输入<code>mstsc</code>，输入服务器的ip登陆，然后在浏览器里登陆 </li>
<li>安装Anaconda<br><code>wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh</code></li>
<li>运行安装向导<br><code>bash Anaconda3-5.2.0-Linux-x86_64.sh</code><br>为了激活安装， 你应该源~/.bashrc文件：<br><code>source ~/.bashrc</code></li>
<li>确认安装成功<br><code>conda --version</code><br>然后使用<code>which python</code>查看你当前使用的是哪个python，如果输出的目录是<code>data/anaconda/python</code>说明你当前使用的还是服务器自带的python，需要重新练连接一下服务器。如果输出是<code>data/WangBeibei/anaconda/python</code>，说明当前使用是自己安装的anaconda</li>
<li><p>配置清华镜像<br>使用conda创建虚拟（运行）环境。conda和pip默认使用国外站点来下载软件，我们可以配置国内镜像来加速下载（国外用户无须此操作）。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改Anaconda的环境变量<br><a href="https://blog.csdn.net/m0_37041325/article/details/77169972" target="_blank" rel="noopener">参考资料</a>  </p>
</li>
<li><p>创建虚拟环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n mxnet python=3.6#创建虚拟环境</span><br><span class="line">conda activate mxnet#激活虚拟环境</span><br></pre></td></tr></table></figure>
</li>
<li><p>在虚拟环境中安装第三方库<br><strong>注：虚拟环境中第三方库的cuda版本一定要和服务器的cuda版本一致</strong>。</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line">cat <span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/cuda/</span>version.txt<span class="meta">#查看服务器cuda版本  </span></span><br><span class="line"><span class="meta">#安装对应版本的mxnet，安装mxnet-cu100是GPU版本的mxnet，</span></span><br><span class="line"><span class="meta">#如果仅使用pip install mxnet安装的CPU版本。</span></span><br><span class="line">pip install -i https:<span class="comment">//pypi.tuna.tsinghua.edu.cn/simple mxnet-cu100  </span></span><br><span class="line"><span class="meta">#安装pytorch   </span></span><br><span class="line">conda config --add channels https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ </span></span><br><span class="line">conda install pytorch torchvision cudatoolkit=<span class="number">10.1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>删除虚拟环境<br> <code>conda remove -n gluon--all</code><br> 为了确定这个名为flowers的环境已经被移除，输入以下命令<code>conda info -e</code> ,会看到已经没有gluon这个环境</p>
</li>
</ol>
<h1><span id="2-常用命令">2. 常用命令</span></h1><ol>
<li><p><code>conda info -e</code><br>查看当前服务器上都有哪些虚拟环境,下面截图中显示，当前存在2个虚拟环境，其中带*的是当前正在使用的虚拟环境。<br><img src="/2019/04/27/虚拟环境安装/conda-info.png" alt=""></p>
</li>
<li><p><code>screen -S WBB</code>（超级有用！！！）<br>因为是外网服务器，所以网络连接经常断开，连接一断开，运行在上面的程序就不能运行了，所以创建虚拟窗口，在这个虚拟窗口内运行程序，就算网络断开了，程序依然会继续运行  </p>
</li>
<li><code>screen -D  -r ＜session-id&gt;</code><br>先踢掉一个用户，再登录</li>
<li><code>screen -S id -X quit</code><br>删除一个screen</li>
<li><p><code>source activate gluon</code><br>激活虚拟环境gluon，若要在gluon这个虚拟环境下安装一些库，需要先切换到这个环境下，然后使用conda install xxx或者pip install xxx，优先选择使用conda install xxx。安装完之后可以通过conda list查看当前已经安装的包。</p>
</li>
<li><p><code>jupyter notebook</code><br>第一次使用jupyter notebook，需要映射端口号，默认jupyter notebook的端口号是8888，但是在这个集群上，如果别人已经把8888端口占用了，集群会自动给你分配一个端口号，然后在putty中映射一下这个端口，具体操作如下：<br>在菜单栏选中change setting，找到Tunnels<br><img src="/2019/04/27/虚拟环境安装/putty.png" alt=""><br><img src="/2019/04/27/虚拟环境安装/port.png" alt="">  </p>
</li>
<li><p><code>ctrl+A+D</code>退出虚拟窗口<br>当把程序运行之后，使用以上按钮退出虚拟窗口，这样程序就在后台运行，就算把电脑换机了，程序还是会运行</p>
</li>
<li>使用清华源下载<br><code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pykafka</code> </li>
<li><p>和conda有关<br> <code>conda --version</code><br> 通过使用如下update命令来升级conda：<br> <code>conda update conda</code>  </p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install 库名</span><br><span class="line">pip install 库名 --upgrade</span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者</span></span><br><span class="line">conda install 库名</span><br><span class="line">conda update 库名</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新所有库</span></span><br><span class="line">conda update --all</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新 conda 自身</span></span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新 anaconda 自身</span></span><br><span class="line">conda update anaconda</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看已安装的包</span></span><br><span class="line">conda list</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用conda install xxx或者pip install xxx，优先选择使用conda install xxx。安装完之后可以通过conda list查看当前已经安装的包   </p>
</li>
<li>删除一个虚拟环境<br><code>conda remove -n gluon--all</code>  为了确定这个名为flowers的环境已经被移除，输入以下命令<code>conda info -e</code> ,会看到已经没有gluon这个环境  </li>
<li>查看GPU的占用情况<br><code>nvidia-smi</code></li>
<li>在screen运行jupyter程序<br><code>screen -S jupyter</code><br><code>source activate gluon</code><br><code>source deactivate</code><br><code>jupyter notebook</code><br><code>screen -r jupyter</code><br><code>screen -X -S 122128 quit</code>  </li>
<li>查看当前的进程是谁的<br><code>ps -ef | grep 35230</code></li>
<li>在windows在激活虚拟环境<br><code>activate gluon</code><br><code>deactivate gluon</code></li>
</ol>
<h1><span id="3-问题">3. 问题</span></h1><ol>
<li>虚拟环境配置出错<br>我创建了一个gluon的虚拟环境，使用<code>source avtivate gluon</code>激活这个环境时，在这里面装了mxnet框架。但是在screen中启动jupyter notebook时，import mxnet时却报错说no module names mxnet，然后我退出jupyter notebook（仍在screen中），使用which python查看当前使用的python，仍然是base的python，不是env/gluon中的python，因为base中的python没有装mxnet，所以import会出错。那怎么把gluon虚拟环境中的python换成env/gluon中的python呢？<br>参考这个网址：<a href="http://www.pianshen.com/article/2276285026/" target="_blank" rel="noopener">http://www.pianshen.com/article/2276285026/</a><br>在虚拟环境下运行以下命令：<br><code>ipython kernelspec list</code><br>查看jupyter notebook内核指定的python运行环境位置，然后cd到这个目录中，会看到有一个kernel.json文件，使用vi命令编辑这个文件，将python解释器的位置换成<code>/data/WangBeibei/anaconda3/envs/gluon/bin/python</code><br><img src="/2019/04/27/虚拟环境安装/python.png" alt=""><br>然后使用<code>source deavtivate</code>断开gluon虚拟环境，在重新激活<code>source avtivate gluon</code>，然后再启动<code>jupyter noteboook</code>就可以了。<br>如果还是出错，使用<code>conda install ipykernel</code>命令，然后再重新激活虚拟环境，再次启动<code>jupyter notebook</code>.</li>
<li><p>cuda出错<br>我创建了一个虚拟环境，名为mxnet，在这个虚拟环境中使用<code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mxnet-cu100</code>安装GPU版本的mxnet。然后再虚拟环境mxnet中使用python运行以下命令可以成功运行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd   </span><br><span class="line">z = nd.ones(shape=(<span class="number">3</span>, <span class="number">3</span>), ctx=mx.gpu(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p> 然后我创建了一个screen，在这个screen中<code>source activate mxnet</code>启动虚拟环境，然后使用python运行以上相同的代码，却出错。</p>
 <figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">OSError: libcudart.so<span class="number">.10</span><span class="number">.0</span>: cannot <span class="built_in">open</span> shared  object <span class="built_in">file</span>: No such <span class="built_in">file</span> <span class="keyword">or</span> <span class="built_in">directory</span></span><br></pre></td></tr></table></figure>
<p> 查了资料解决方案如下：<br> 按Ctrl+A+D退出screen。然后查看.bashrc的内容，原先是</p>
<p> <img src="/2019/04/27/虚拟环境安装/bashrc1.png" alt=""></p>
<p> 将其修改如下：修改完之后使用<code>source .bashrc</code>激活<br> 当有多个PATH时，中间使用冒号拼接。<br> <img src="/2019/04/27/虚拟环境安装/bashrc2.png" alt=""><br> 然后再screen，再次运行上述的python代码成功。</p>
</li>
<li><p>cuda版本出错<br>查看系统cuda的版本，安装对应版本的深度学习框架<br><code>cat /usr/local/cuda/version.txt</code>如果安装错了，先使用<code>pip uninstall mxnet-cu100</code>，删除原先的mxnet，然后再使用<code>pip install mxnet-cu101</code>安装对应的版本.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>虚拟环境</tag>
      </tags>
  </entry>
  <entry>
    <title>数据预处理及网格搜索</title>
    <url>/2019/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;最近在做一个分类任务，根据电池的充放电数据，预测电池绝缘报警是否为虚报，就是一个二分类任务。这里使用逻辑回归进行分类。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86">1. 数据预处理</a><ul>
<li><a href="#11-%e7%89%b9%e5%be%81%e5%80%bc%e8%bf%9e%e7%bb%ad">1.1. 特征值连续</a><ul>
<li><a href="#111-%e5%bd%92%e4%b8%80%e5%8c%96normalization">1.1.1. 归一化(normalization)</a></li>
<li><a href="#112-%e6%a0%87%e5%87%86%e5%8c%96standardization">1.1.2. 标准化(standardization)</a></li>
<li><a href="#113-robustscaler">1.1.3. RobustScaler</a></li>
</ul>
</li>
<li><a href="#12-%e7%89%b9%e5%be%81%e5%80%bc%e7%a6%bb%e6%95%a3">1.2. 特征值离散</a></li>
<li><a href="#13-%e9%a2%84%e5%a4%84%e7%90%86%e6%ad%a5%e9%aa%a4">1.3. 预处理步骤</a></li>
<li><a href="#14-%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81">1.4. 交叉验证</a></li>
<li><a href="#15-%e7%89%b9%e5%be%81%e5%b7%a5%e7%a8%8b">1.5. 特征工程</a></li>
</ul>
</li>
<li><a href="#2-%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87">2. 模型评价指标</a></li>
<li><a href="#3-gridsearchcv">3. GridSearchCV</a></li>
<li><a href="#4-%e6%a0%b7%e6%9c%ac%e4%b8%8d%e5%9d%87%e8%a1%a1%e9%97%ae%e9%a2%98">4. 样本不均衡问题</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-数据预处理">1. 数据预处理</span></h1><p>&ensp;&ensp;&ensp;&ensp;在进行模型训练之前，需要对数据进行预处理。因为多个特征之间的量纲不同，在训练的时候收敛会很慢，所以需要将不同特征值转换为同一量纲。这里将离散特征和连续特征分别处理。</p>
<h2><span id="11-特征值连续">1.1. 特征值连续</span></h2><p>&ensp;&ensp;&ensp;&ensp;对于连续值的预处理主要分为2个：归一化和标准化。这2个操作主要是为了使得不同的特征在同一个量纲，对目标的影响是同级的。归一化和标准化都是先对数据先缩小一定的比例，然后再平移。这2者本质上都是对数据进行线性变换，线性变换不会改变原始数据的数值大小排序。即一个数在原始数据最大，经过归一化和标准化这个数还是最大。<a href="https://blog.csdn.net/dujiahei/article/details/86061924" target="_blank" rel="noopener">这篇博客</a>。<br>&ensp;&ensp;&ensp;&ensp;将特征值缩放到相同的区间可以获得性能更好的模型。就梯度下降而言，一个特征值的范围在1-10之间，另一个特征值范围在1-10000之间，训练的目标是最小化平方误差，所以在使用梯度下降算法的过程中，算法会明显偏向第二个特征，因为它的取值范围更大。在K近邻算法中，使用的欧式距离，也会导致偏向第二个特征。<strong>对于决策树和随机森林以及xgboost算法而言，特征缩放对它们没有什么影响，像逻辑回归和支持向量机算法和K近邻，需要对数据进行特征缩放</strong>。  <strong>在分类，聚类算法中，需要使用距离来度量相似性的时候，standardization表现更好</strong>。<br><img src="/2019/04/24/机器学习/表格.png" alt="归一化"></p>
<h3><span id="111-归一化normalization">1.1.1. 归一化(normalization)</span></h3><p>&ensp;&ensp;&ensp;&ensp;归一化将每一个属性值映射到[0,1]之间。需要计算训练集的最大值和最小值，当有新样本加入时，需要重新计算最值。 </p>
<ul>
<li>特点：多使用于分布有明显边界的情况，如考试成绩，身高，颜色的分布等，都有明显的范围边界，不适用没有范围约定，或者返回非常大的数据。</li>
<li>缺点：受异常值影响较大。归一化的缩放就是将数据拍扁统一到一个区间中，仅有极值决定，而标准化的缩放更加弹性和动态，和整体的分布有关。归一化只用到了最大值和最小值，而标准化和每一个值有关。</li>
</ul>
<p><img src="/2019/04/24/机器学习/归一化.png" alt="归一化"></p>
<h3><span id="112-标准化standardization">1.1.2. 标准化(standardization)</span></h3><p>&ensp;&ensp;&ensp;&ensp; 标准化又叫做Z-score。将所有的数据映射到均值为0，方差为1的正态分布中。  要求原始数据的分布可以近似为正态分布，否则标准化的结果会很差。 标准化表示的是原始值与均值之间差几个标准差，是一个相对值，也有去除量纲的作用，同时还有2个附加好处：均值为0，标准差为1。均值为0的好处是使得数据以0为中心左右分布。</p>
<ul>
<li>适用范围：在分类和聚类算法中，需要使用距离来度量相似性时，例如支持向量机，逻辑回归，或者使用PCA进行降维时，Z-score表现更好。</li>
<li>推荐先使用标准化。</li>
</ul>
<p><img src="/2019/04/24/机器学习/标准化.png" alt="归一化"></p>
<h3><span id="113-robustscaler">1.1.3. RobustScaler</span></h3><p>&ensp;&ensp;&ensp;&ensp;在某些情况下，加入数据中有离群点，可以使用standardization进行标准化，但是标准化后的数据并不理想，因为异常点的特征往往在标准化后容易失去离群特征，此时就要使用RobustScaler针对离群点进行标准化处理   。此方法对数据中心化和缩放健壮性有更强的参数控制能力。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#RobustScaler标准化</span></span><br><span class="line">robustscaler = preprocessing.RobustScaler()</span><br><span class="line">df_r = robustscaler.fit_transform(df)</span><br><span class="line">df_r = pd.DataFrame(df_r,columns=[<span class="string">'value1_r'</span>,<span class="string">'value2_r'</span>])</span><br><span class="line">df_r.head()</span><br></pre></td></tr></table></figure>
<h2><span id="12-特征值离散">1.2. 特征值离散</span></h2><p>&ensp;&ensp;&ensp;&ensp;离散值就是特征值是离散的，不是连续的，例如性别是离散值，只有female和male，颜色是离散的。机器学习算法不能直接处理离散值，需要对其进行一些转换。离散值可以是文本(red，black)或者数值（1，2）。<br>&ensp;&ensp;&ensp;&ensp; 离散数据有2大类：定序(Ordinal)和定类(Nominal)。定序的数据存在一定的顺序意义，例如衣服的尺寸按大小分类(xs,s,m,l),在定类的数据中，属性值之间没有顺序的要求。<br>&ensp;&ensp;&ensp;&ensp;对于定序的数据，没有统一的模块将这些顺序自动转换成映射，可以自定义一些映射规则，比如xs对应1，s对应2，自定义的规则。<br>&ensp;&ensp;&ensp;&ensp;对于文本的定类数据，可以先把文本分类至转换为数字，比如red转换为1，black转换为2，然后对这些数据使用one-hot编码。<br>主要是使用LabelEncoder和OneHotEncoder这2个模块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">gle = LabelEncoder()</span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"><span class="comment">#将每个风格属性映射到一个数值(0,1,2,3…)。</span></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment"># OneHotEncoder的transform方法默认返回系数矩阵，调用toarray()方法将系数矩阵转为一般矩阵</span></span><br><span class="line">dis_feature_data = enc.fit_transform(dis_feature_data).toarray()</span><br><span class="line">print(dis_feature_data)</span><br><span class="line">print(dis_feature_data.shape)</span><br></pre></td></tr></table></figure>
<p>除了sklearn中的OneHotEncoder，还可以使用pandas中的get_dummies对离散值进行one-hot编码，比OneHotEncoder好的一点是:转换之后可以直观的看出当前列对应哪个属性。<br>参考博客：<br><a href="https://blog.csdn.net/wotui1842/article/details/80697444" target="_blank" rel="noopener">https://blog.csdn.net/wotui1842/article/details/80697444</a><br><a href="https://blog.csdn.net/cymy001/article/details/79154135" target="_blank" rel="noopener">https://blog.csdn.net/cymy001/article/details/79154135</a><br><a href="https://blog.csdn.net/m0_37324740/article/details/77169771" target="_blank" rel="noopener">https://blog.csdn.net/m0_37324740/article/details/77169771</a><br><a href="https://blog.csdn.net/wxyangid/article/details/80209156" target="_blank" rel="noopener">https://blog.csdn.net/wxyangid/article/details/80209156</a></p>
<h2><span id="13-预处理步骤">1.3. 预处理步骤</span></h2><ul>
<li>首先使用pandas从csv中读取数据，从数据中取出特征值和目标值，分别存储在X和Y中。</li>
<li>从X中取出离散特征值dis_feature，剩下的是连续特征值con_feature。</li>
<li>对离散特征值dis_feature进行one-hot编码，形成新的特征值new_dis_feature。然后将新的特征值new_dis_feature和原先的连续特征值con_feature进行拼接形成新的特征值new_X</li>
<li>然后对new_X和Y划分为训练集和测试集，然后对训练集进行标准化，使用训练集的均值和标准差再对测试集进行标准化。</li>
<li>使用训练集对模型进行训练，对测试集进行验证。</li>
</ul>
<h2><span id="14-交叉验证">1.4. 交叉验证</span></h2><p>&ensp;&ensp;&ensp;&ensp;sklearn中有2中交叉验证方法，KFold，StratifiedKFold</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold,StratifiedKFold</span><br><span class="line">X=np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],</span><br><span class="line">    [<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>],</span><br><span class="line">    [<span class="number">31</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>],</span><br><span class="line">    [<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>],</span><br><span class="line">    [<span class="number">51</span>,<span class="number">52</span>,<span class="number">53</span>,<span class="number">54</span>],</span><br><span class="line">    [<span class="number">61</span>,<span class="number">62</span>,<span class="number">63</span>,<span class="number">64</span>],</span><br><span class="line">    [<span class="number">71</span>,<span class="number">72</span>,<span class="number">73</span>,<span class="number">74</span>]</span><br><span class="line">])</span><br><span class="line">y=np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">floder = KFold(n_splits=<span class="number">4</span>,random_state=<span class="number">0</span>,shuffle=<span class="keyword">False</span>)</span><br><span class="line">sfolder = StratifiedKFold(n_splits=<span class="number">4</span>,random_state=<span class="number">0</span>,shuffle=<span class="keyword">False</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sfolder.split(X,y):</span><br><span class="line">    print(<span class="string">'Train: %s | test: %s'</span> % (train, test))</span><br><span class="line">    print(<span class="string">" "</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> floder.split(X,y):</span><br><span class="line">    print(<span class="string">'Train: %s | test: %s'</span> % (train, test))</span><br><span class="line">    print(<span class="string">" "</span>)</span><br></pre></td></tr></table></figure>
<p>StratifiedKFold和KFold类似，但是StratifiedKFold是分层采样，确保训练集、测试集各类样本的比例与原始数据集中相同。比如原始数据集中正例:负例=2:1,则训练集和测试集中正例:负例=2:1。<br>KFold和enumerate联合使用<br>enumerate()函数用于将一个可遍历的数据对象(如列表，元组或str)组合成一个序列索引，同时列出数据和数据下标。一般在for循环中使用。<br>语法：<code>enumerate(sequence,[start=0])</code><br>其中<code>sequence</code>表示一个序列，迭代器或可遍历对象，<code>start</code>表示下标起始位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> fold_, (train_, test_) <span class="keyword">in</span> enumerate(kfold.split(X_array, y_array):</span><br><span class="line"><span class="comment">#其中train和test是数据的下标</span></span><br></pre></td></tr></table></figure>
<h2><span id="15-特征工程">1.5. 特征工程</span></h2><ol>
<li>特征缩放<br>使用归一化或标准化对特征进行缩放，使得不同特征值在同一量纲</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用 sklearn中的 scale 函数</span></span><br><span class="line">minmax_scaler = preprocessing.MinMaxScaler()   <span class="comment">#创建 MinMaxScaler对象</span></span><br><span class="line">df_m1 = minmax_scaler.fit_transform(df)    <span class="comment"># 标准化处理</span></span><br><span class="line">df_m1 = pd.DataFrame(df_m1,columns=[<span class="string">'value1_m'</span>,<span class="string">'value2_m'</span>])</span><br><span class="line">df_m1.head()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#RobustScaler is robust to outliers.</span></span><br><span class="line">credit_df[<span class="string">'amount_after_scaling'</span>] = RobustScaler().fit_transform(credit_df[<span class="string">'Amount'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">credit_df[<span class="string">'time_after_scaling'</span>] = RobustScaler().fit_transform(credit_df[<span class="string">'Time'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">credit_df.drop([<span class="string">'Time'</span>,<span class="string">'Amount'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Place the class in the begining of the dataframe</span></span><br><span class="line">Class = credit_df[<span class="string">'Class'</span>]</span><br><span class="line">credit_df.drop([<span class="string">'Class'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">credit_df.insert(<span class="number">0</span>, <span class="string">'Class'</span>, Class)</span><br></pre></td></tr></table></figure>
<ol>
<li>解决样本不均衡问题<br>欠采样或过采样<br><img src="/2019/04/24/机器学习/resample.png" alt="样本不平衡">    </li>
<li><p>检测和删除异常点   </p>
</li>
<li><p>划分数据集<br>划分数据集：训练集，验证集，测试集  </p>
</li>
</ol>
<h1><span id="2-模型评价指标">2. 模型评价指标</span></h1><ul>
<li>拟合模型<br><code>model.fit(X_train, y_train)</code>    </li>
<li>模型预测，对于分类任务，输出最大可能的类别<br><code>model.predict(X_train)</code><br><code>model.predict(X_test)</code></li>
<li>对于分类任务，输出所属每个类别的概率，返回的是一个二维数组，每一行加起来为1<br><code>prob = model.predict_proba(X_train)</code><br><code>model.predict_proba(X_test)</code><br>获取样本属于正例的概率prob[:,1]   </li>
<li>获得这个模型的参数<br><code>model.get_params()</code></li>
<li>为模型进行打分<br>线性回归问题返回预测的确定系数R2<br>逻辑回归（分类）根据给定数据与标签返回分类准确率的均值<br><code>model.score(X_train, y_train)</code><br><code>model.score(X_test, y_test)</code>   </li>
<li>计算分类准确率,和score返回值一样<br><code>train_predicted = model.predict(X_train)</code><br><code>model.accuracy_score(y_train.flatten(),train_predicted)</code>  </li>
<li>返回分类准确率，和上面的结果一样<br><code>np.mean(train_predicted == y_train)</code><br><code>np.mean(test_predicted == y_test)</code>   </li>
<li><p>召回率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">precision, recall, F1, _ = precision_recall_fscore_support(y_test, pred_test, average=<span class="string">"binary"</span>)    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"精准率: &#123;0:.2f&#125;. 召回率: &#123;1:.2f&#125;, F1分数: &#123;2:.2f&#125;"</span>.format(precision, recall, F1))</span><br></pre></td></tr></table></figure>
</li>
<li><p>AUC&amp;&amp;ROC<br>只针对二分类。通过<code>model.predict_proba(X_test)[:,1]</code>可以获取测试集属于正例的概率，将预测概率从大到小排序，然后以每个预测概率作为阈值，即可得到属于2类的样本数。对应计算每个阈值下的”False Positive Rate”(FPR)和”True Positive Rate”(TPR)，以”False Positive Rate”为横轴，以”True Positive Rate”为纵轴，画出ROC曲线，ROC曲线下的面积就是AUC值.<br>“False Positive Rate”(FPR)=负例被划分为正例个数/真正负例个数（负例被分错的个数/真正负例）<br>“True Positive Rate”(TPR)=正例被划分为正例/真正正例个数（正例被分对的个数/真正正例）<br>当阈值取最大时，所有的样本被分为负样本，对应（0,0），当阈值取最小时，所有的样本被分为正样本，对应于（1,1），随着阈值从最大到最小变化，横坐标和纵坐标都在变大，表示被划分为正例的个数越来越多。<br>AUC用来衡量ROC曲线的好坏。如果分类器能完美的将样本分对，那么AUC=1，如果模型是随机猜测的，那么AUC=0.5，对应着y=x直线。分类器越好，则AUC越大。<br>sklearn给了画ROC曲线的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fpr, tpr, thresholds=sklearn.metrics.roc_curve(y_true_label,y_prob,pos_label=<span class="keyword">None</span>,sample_weight=<span class="keyword">None</span>,drop_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#其中test_true_label表示数据集真实的标签，&#123;0,1&#125;或&#123;-1,1&#125;</span></span><br><span class="line"><span class="comment">#y_prob表示数据集被分为正例的概率</span></span><br><span class="line"><span class="comment"># 返回值</span></span><br><span class="line"><span class="comment">#thresholds: array, shape = [n_thresholds]所选取的不同的阈值，按照从大到小的排序，阈值越大，横纵坐标越小。</span></span><br><span class="line"><span class="comment">#fpr,tpt：根据 thresholds算出来的横坐标和纵坐标。在此基础上可以画ROC曲线，</span></span><br><span class="line"><span class="comment">#通过auc(fpr,tpr)可以求出AUC的值</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1><span id="3-gridsearchcv">3. GridSearchCV</span></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">model_selection</span>.<span class="title">GridSearchCV</span><span class="params">(estimator, param_grid, scoring=None, fit_params=None, n_jobs=<span class="number">1</span>, iid=True, refit=True, cv=None, verbose=<span class="number">0</span>, pre_dispatch=‘<span class="number">2</span>*n_jobs’, error_score=’raise’, return_train_score=’warn’)</span></span></span><br></pre></td></tr></table></figure>
<p>GridSearchCV参数介绍：</p>
<ul>
<li>estimator：使用的分类器，并且传入除需要确定最佳的参数之外的其他参数</li>
<li>param_grid：值为字典或者列表，即需要最优化的参数的取值，param_grid = {‘n_estimators’:list(range(10,71,10))}</li>
<li>scoring :准确度评价标准，默认None,表示“GridSearchCV”与“cross_val_score”都会去调用“estimator”自己的“score”；或者如scoring=’roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。scoring参数选择如下：</li>
<li><p>cv :交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，传入的参数可以是int型，也可以是yield训练/测试数据的生成器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kflod = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle = <span class="keyword">True</span>,random_state=<span class="number">7</span>)<span class="comment">#将训练/测试数据集划分10个互斥子集，</span></span><br><span class="line">grid_search = GridSearchCV(model,param_grid,scoring = <span class="string">'neg_log_loss'</span>,n_jobs = <span class="number">-1</span>,cv = kflod)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>refit :默认为True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。</li>
<li>iid:默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。</li>
<li>verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出</li>
<li>n_jobs: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。  </li>
</ul>
<p><strong>常用的方法</strong></p>
<ul>
<li>grid.fit(X, y=None, groups=None, **fit_params)：运行网格搜索，与所有参数组合运行。</li>
<li><p>cv_results_：旧版本是“grid_scores_”，cv_results_是详尽、升级版。内容较好理解，包含了’mean_test_score’(验证集平均得分)，’rank_test_score’(验证集得分排名)，’params’(dict形式存储所有待选params的组合)，甚至还有在每次划分的交叉验证中的得分（’split0_test_score’、 ‘split1_test_score’等），就是输出的内容稍显臃肿。内容以dict形式输出，我们可以转成DataFrame形式，看起来稍微养眼一点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_result = pd.DataFrame.from_dict(clf.cv_results_)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">means = grid_result.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">'params'</span>]</span><br><span class="line"><span class="keyword">for</span> mean,param <span class="keyword">in</span> zip(means,params):</span><br><span class="line">    print(<span class="string">"%f  with:   %r"</span> % (mean,param))</span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line">display(pd.DataFrame(grid.cv_results_).T)</span><br></pre></td></tr></table></figure>
<p>  参考资料：<a href="https://blog.csdn.net/sinat_32547403/article/details/73008127" target="_blank" rel="noopener">https://blog.csdn.net/sinat_32547403/article/details/73008127</a></p>
</li>
<li>best_estimator_ : estimator或dict；由搜索选择的估算器，即在左侧数据上给出最高分数（或者如果指定最小损失）的估算器。 如果refit = False，则不可用。</li>
<li>best_params_ : dict；在保持数据上给出最佳结果的参数设置。对于多度量评估，只有在指定了重新指定的情况下才会出现。</li>
<li>best_score_ : float；best_estimator的平均交叉验证分数，对于多度量评估，只有在指定了重新指定的情况下才会出现。</li>
<li>get_params（[deep]）：这个和‘best_estimator_ ’这个属性相似，但可以得到这个模型更多的参数</li>
<li>inverse_transform（Xt）使用找到的最佳参数在分类器上调用inverse_transform。</li>
<li>predict（X）调用使用最佳找到的参数对估计量进行预测，X：可索引，长度为n_samples；</li>
<li>score（X, y=None）返回给定数据上的分数，X： [n_samples，n_features]输入数据，其中n_samples是样本的数量，n_features是要素的数量。y： [n_samples]或[n_samples，n_output]，可选，相对于X进行分类或回归; 无无监督学习。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">100</span>, <span class="number">125</span>, <span class="number">150</span>, <span class="number">175</span>, <span class="number">200</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = XGBClassifier(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">5</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">evalute_result = optimized_GBM.cv_results_</span><br><span class="line"></span><br><span class="line">print(<span class="string">'参数的最佳取值：&#123;0&#125;'</span>.format(optimized_GBM.best_params_))</span><br><span class="line">print(<span class="string">'最佳模型得分:&#123;0&#125;'</span>.format(optimized_GBM.best_score_))</span><br></pre></td></tr></table></figure>
<p>网格搜索建立在交叉验证的基础上。交叉验证将训练集分成N份，其中N-1份做训练，1份做测试。先选定一个待验证的参数，然后做N次训练和测试，得到平均值，然后再选定下一个参数，做N次训练和测试。</p>
<h1><span id="4-样本不均衡问题">4. 样本不均衡问题</span></h1><p>&ensp;&ensp;&ensp;&ensp;分类问题时，样本不均衡，正例和负例的样本数不均衡，为了实现样本均衡，需要对样本比较少的那类数据进行过采样。<br><a href="https://mp.weixin.qq.com/s/Xxhf_LH0fjLAQHD5cHGcYA" target="_blank" rel="noopener">一文教你如何处理不平衡数据集</a></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>pyspark</title>
    <url>/2019/04/20/pyspark/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;使用Python编写Spark程序，一些常用的操作<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e9%87%8d%e8%a6%81%e6%a6%82%e5%bf%b5%e5%92%8c%e6%9c%af%e8%af%ad">1. 重要概念和术语</a></li>
<li><a href="#2-%e6%89%a7%e8%a1%8c%e6%a8%a1%e5%bc%8f">2. 执行模式</a><ul>
<li><a href="#21-standalone%e6%a8%a1%e5%bc%8f">2.1. standalone模式</a></li>
<li><a href="#22-yarn%e6%a8%a1%e5%bc%8f">2.2. Yarn模式</a></li>
<li><a href="#23-%e5%8f%82%e6%95%b0%e8%b0%83%e4%bc%98">2.3. 参数调优</a></li>
<li><a href="#24-executor">2.4. Executor</a></li>
</ul>
</li>
<li><a href="#3-%e5%88%9b%e5%bb%basc">3. 创建sc</a></li>
<li><a href="#4-rdd%e8%bd%ac%e6%8d%a2">4. RDD转换</a><ul>
<li><a href="#41-%e8%bd%ac%e6%8d%a2%e6%93%8d%e4%bd%9c">4.1. 转换操作</a></li>
<li><a href="#42-%e8%a1%8c%e5%8a%a8%e6%93%8d%e4%bd%9c">4.2. 行动操作</a></li>
<li><a href="#43-%e6%8c%81%e4%b9%85%e5%8c%96">4.3. 持久化</a></li>
</ul>
</li>
<li><a href="#5-%e5%88%86%e5%8c%ba">5. 分区</a></li>
<li><a href="#6-%e5%88%9b%e5%bb%bardd">6. 创建RDD</a><ul>
<li><a href="#61-%e9%80%9a%e8%bf%87paralize%e5%88%9b%e5%bb%bardd">6.1. 通过paralize创建RDD</a></li>
<li><a href="#62-%e8%af%bb%e6%96%87%e6%9c%ac%e6%96%87%e4%bb%b6%e5%88%9b%e5%bb%bardd">6.2. 读文本文件创建RDD</a></li>
</ul>
</li>
<li><a href="#7-map%e5%92%8cflatmap">7. map和flatMap</a></li>
<li><a href="#8-flatmap%e5%92%8cflatmapvalues">8. flatMap和flatMapValues</a></li>
<li><a href="#9-reducebykey%e5%92%8cgroupbykey">9. reduceByKey和groupByKey</a></li>
<li><a href="#10-sortby%e5%92%8csortbykey">10. sortBy和SortByKey</a></li>
<li><a href="#11-%e5%b0%86spark%e8%ae%a1%e7%ae%97%e7%9a%84%e7%bb%93%e6%9e%9c%e5%ad%98%e5%82%a8%e5%9c%a8%e6%96%87%e4%bb%b6%e4%b8%ad">11. 将Spark计算的结果存储在文件中</a><ul>
<li><a href="#111-%e5%86%99%e5%85%a5%e5%88%b0%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%9c%ac%e5%9c%b0%e6%96%87%e4%bb%b6%e4%b8%ad">11.1. 写入到服务器本地文件中</a></li>
<li><a href="#112-%e5%86%99%e5%85%a5%e5%88%b0hdfs%e6%96%87%e4%bb%b6%e4%b8%ad">11.2. 写入到HDFS文件中</a></li>
<li><a href="#113-%e6%89%93%e5%8d%b0rdd%e5%85%83%e7%b4%a0">11.3. 打印RDD元素</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-重要概念和术语">1. 重要概念和术语</span></h1><ul>
<li>Master和Worker是物理节点，Driver和Executor是进程。<br>搭建Spark集群的时候我们就已经设置好了Mater节点和Worker节点。一个集群有多个Master节点和多个Worker节点。<br>Master节点常驻Mater守护进程，负责管理worker节点，我们从master节点提交应用。<br>Worker节点常驻Worker守护进程，与Master节点通信，并且管理Executor进程。<br>PS：一台机器可以同时作为master和worker节点（举个例子：你有四台机器，你可以选择一台设置为master节点，然后剩下三台设为worker节点，也可以把四台都设为worker节点，这种情况下，有一个机器既是master节点又是worker节点）</li>
<li><p>Driver / Driver Program<br>运行main函数并且创建SparkContext的程序。客户端的应用程序，Driver Program类似于wordcount程序中的mian函数。<br>当我们提交应用程序后，便会启动一个对应的Driver进程。Driver会根据我们设置的参数占用一定的资源（主要是CPU核数、内存）。<br>程序启动时，Driver进程首先会向集群资源管理者（Standalone，Mesos，Yarn）申请Spark应用所需的资源，也就是Executor，然后集群管理者会根据Spark应用所设置的参数在各个Worker上分配一定数量的Executor，每个Executor都占用一定数量的CPU和Memory。在申请到应用所需的资源后，Driver就开始调度和执行我们的程序了。Driver进程会把我们编写的Spark程序拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor中执行。<br>Executor进程在Worker节点上，一个Worker可以有多个Executor，每个Executor都有一个进程池，每个进程执行一个task。Executor执行完task之后将结果返回给Driver，每个Executor执行的task属于一个spark程序。此外Executor还有一个功能是为应用程序中的RDD提供内存，RDD是直接缓存在Executor进程内的。<br><a href="https://blog.csdn.net/hongmofang10/article/details/84587262" target="_blank" rel="noopener">这篇博客讲的很好</a></p>
<p><a href="https://blog.csdn.net/qq_21383435/article/details/78653427" target="_blank" rel="noopener">通俗易懂</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit --master yarn --num-executors 32 --executor-memory 8G --executor-cores 8 --jars ../jars/spark-examples_2.10_my_converters_test-1.6.0.jar spark_streaming_all.py</span><br></pre></td></tr></table></figure>
<p>其中参数的含义：</p>
<ul>
<li>num-executors：创建多少个 executor</li>
<li>executor-memory：各个 executor 使用的最大内存，不可超过单机的最大可使用内存</li>
<li>executor-cores：各个 executor 使用的并发线程数目，也即每个 executor 最大可并发执行的 Task 数目</li>
</ul>
</li>
<li>Cluster Manager<br>集群的资源管理器，在集群上获取资源的外部服务，例如Standalone，Mesos，Yarn。<br>拿Yarn举例，客户端程序会向Yarn申请运行我这个任务需要多少，多少CPU等，然后Cluster Manager会通过调度告诉客户端可以使用，然后客户端就可以把程序送到每个Worker Node上面执行。</li>
</ul>
<h1><span id="2-执行模式">2. 执行模式</span></h1><p>&ensp;&ensp;&ensp;&ensp;运行spark程序有3种模式，local，standalone，yarn。在使用spark-submit命令提交程序时，需要指定一些参数。</p>
<ul>
<li>—master:如spark://host:7077, mesos://host:port, yarn,  yarn-cluster,yarn-client, local</li>
<li>—calss CLASS_NAME 应用程序的主类</li>
<li>—name NAME 应用程序的名称,这个可以在程序中通过setAppName(“kafka_hbase”)指定  </li>
<li>—jars JARS 逗号分隔的本地jar包，后面添加jar的路径</li>
<li>—driver-memory MEM Driver内存，默认1G</li>
<li>—num-executors NUM，启动的executor的个数，默认为2，在yarn中使用。</li>
<li>—executor-core NUM，每个executor的核数。在yarn或者standalone下使用</li>
<li>—executor-memory MEM 每个executor的内存，默认是1G</li>
<li>—total-executor-cores NUM,所有executor总共的核数，仅仅在mesos或standalone中使用</li>
<li>driver-cores NUM Driver的核数，默认是1，这个参数只在standalone模式下使用</li>
</ul>
<h2><span id="21-standalone模式">2.1. standalone模式</span></h2><p>&ensp;&ensp;&ensp;&ensp;运行一个pyspark程序，使用standalone模式来提交程序，需要使用的参数有：</p>
<ul>
<li>—master spark://hz4:7077</li>
<li>—jars xxx1.jar,xxx2.jar<br>不使用—num-executors,这个在yarn中使用</li>
<li>—executor-memory MEM,每个executor占用的内存，如果一个executor占用4G，有5个executor，那这个程序占用20G </li>
<li>—executor-core NUM，表示每个executor的核数</li>
<li><strong>—total-executor-cores NUM</strong>,所有的executor占用的核数。使用total-executor-cores / executor-core得到executor的个数，假设total-executor-cores设置为30，executor-core为6，则表示运行这个程序一共有5个executor，分别在不同worker上。一个worker可以有多个executor。 假设有5个executor，2个worker，那么一个worker上有多个executor。如果不指定—total-executor-cores，程序会把worker上的核全都占用，这样别人提交程序的时候就没有办法运行。<br>&ensp;&ensp;&ensp;&ensp;运行一个程序的命令：spark-submit —master spark://hz4:7077  —executor-memory 4G —executor-cores 6 —total-executor-cores 30 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar spark_streaming.py</li>
</ul>
<h2><span id="22-yarn模式">2.2. Yarn模式</span></h2><p>&ensp;&ensp;&ensp;&ensp;yarn模式可以用的参数有：</p>
<ul>
<li>—master yarn</li>
<li>—jars xxx1.jar,xxx2.jar</li>
<li><strong>—num-executors NUM</strong>, 启动的executor的个数，默认为2，不要使用默认，会很慢。在yarn中使用。yarn资源管理器会在不同的worker上分配executor给程序。</li>
<li>—executor-memory MEM,每个executor占用的内存，如果一个executor占用4G，有5个executor，那这个程序占用20G </li>
<li>—executor-core NUM，表示每个executor的核数，如果有5个executor，每个executor占用4G，那这个程序运行时占用20G内存。<br>运行一个程序的命令：<code>spark-submit --master yarn --num-executors 20 --executor-memory 4G --executor-cores 4 --jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar spark_streaming.py</code></li>
</ul>
<h2><span id="23-参数调优">2.3. 参数调优</span></h2><ul>
<li><p>num-executors：该参数用于设置Spark作业总共要用多少个Executor进程来执行,Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在<br>集群的各个工作节点上，启动相应数量的Executor进程。<br><strong>参数调优建议</strong>：<br>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；<br>设置的太多的话，大部分队列可能无法给予充分的资源。</p>
</li>
<li><p>executor-memory：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。<br><strong>参数调优建议</strong>：<br>每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列<br>的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，<br>那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</p>
</li>
<li><p>executor-cores：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个<br>task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。<br><strong>参数调优建议</strong>：<br>Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的<br>Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过<br>队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。  </p>
</li>
<li><p>driver-memory：该参数用于设置Driver进程的内存。<br><strong>参数调优建议</strong>：<br>Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，<br>那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</p>
</li>
<li><p>—conf spark.default.parallelism：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<br><strong>参数调优建议</strong>：<br>Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量<br>来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会<br>导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的<br>Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍<br>较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</p>
</li>
</ul>
<h2><span id="24-executor">2.4. Executor</span></h2><p>&ensp;&ensp;&ensp;&ensp;在运行pyspark程序时出错：   Container killed by YARN for exceeding memory limits. 16.9 GB of 16 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead”这个错误总会使你的job夭折。它的意思是：因为超出内存限制，集群停掉了container。<br>Spark的Excutor的Container内存有两大部分组成：Excutor内存和堆外内存</p>
<p><img src="/2019/04/20/pyspark/executor.png" alt=""></p>
<p>Spark底层shuffle的传输方式是使用netty传输，netty在进行网络传输的过程会申请堆外内存（netty是零拷贝），所以使用了堆外内存，即spark.yarn.executor.memoryOverhead。<br><strong>Executor内存</strong><br>又spark.executor.memory参数设置，在spark-shell中由—executor-memory指定，分为2部分，shuffle.memoryFraction和storage.memoryFraction。  </p>
<ul>
<li><p><strong>spark.shuffle.memoryFractio</strong><br>该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。<br><strong>参数调优</strong><br>如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</p>
</li>
<li><p><strong>spark.storage.memoryFractio</strong><br>该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。<br><strong>参数调优</strong><br>如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</p>
</li>
<li><strong>spark.yarn.executor.memoryOverhead</strong><br>executor执行的时候，用的内存可能会超过executor-memoy，所以会为executor额外预留一部分内存。spark.yarn.executor.memoryOverhead代表了这部分内存。这个参数如果没有设置，会有一个自动计算公式(位于ClientArguments.scala中)，代码如下：<br>其中，MEMORY_OVERHEAD_FACTOR默认为0.1，executorMemory为设置的executor-memory, MEMORY_OVERHEAD_MIN默认为384m。参数MEMORY_OVERHEAD_FACTOR和MEMORY_OVERHEAD_MIN一般不能直接修改，是Spark代码中直接写死的。</li>
</ul>
<p>关于Executor 计算的相关公式，见源码org.apache.spark.deploy.yarn.Clent，org.apache.spark.deploy.yarn.ClentArguments<br>主要部分如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">var executorMemory = <span class="number">1024</span> // 默认值，<span class="number">1024</span>MB</span><br><span class="line">val MEMORY_OVERHEAD_FACTOR = <span class="number">0.10</span>  // OverHead 比例参数，默认<span class="number">0.1</span></span><br><span class="line">val MEMORY_OVERHEAD_MIN = <span class="number">384</span></span><br><span class="line"></span><br><span class="line">val executorMemoryOverhead = sparkConf.getInt(<span class="string">"spark.yarn.executor.memoryOverhead"</span>,</span><br><span class="line">math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))</span><br><span class="line">// 假设有设置参数，即获取参数，否则使用executorMemoryOverhead 的默认值</span><br><span class="line">val executorMem = args.executorMemory + executorMemoryOverhead</span><br><span class="line">// 最终分配的executor 内存为 两部分的和</span><br></pre></td></tr></table></figure>
<p><strong>解决方案</strong></p>
<p>在参数中设置<strong>spark.yarn.executor.memoryOverhead=4096</strong>，单位是MB，一般是2的幂,这里使用4G，默认申请的堆外内存是Executor内存的10%，真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit --master yarn --num-executors 20 --executor-memory 4G --executor-cores 4 --conf spark.yarn.executor.memoryOverhead=4096 --jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar feature_extraction.py</span><br></pre></td></tr></table></figure>
<p><strong>executor-memory+spark.yarn.executor.memoryOverhead&lt;MonitorMemory</strong><br>指定的 ExecutorMemory与MemoryOverhead 之和大于 MonitorMemory，则会导致Executor申请失败，程序直接不能运行；若运行过程中，实际使用内存超过上限阈值，Executor进程会被Yarn终止掉（kill）。<br>在运行程序中发现CPU的占用率不高，，增加num-executors的个数，减少executor-cores的个数</p>
<p>参考资料：<br><a href="https://www.cnblogs.com/haozhengfei/p/5fc4a976a864f33587b094f36b72c7d3.html" target="_blank" rel="noopener">https://www.cnblogs.com/haozhengfei/p/5fc4a976a864f33587b094f36b72c7d3.html</a></p>
<p><a href="https://blog.csdn.net/hammertank/article/details/48346285" target="_blank" rel="noopener">https://blog.csdn.net/hammertank/article/details/48346285</a></p>
<p><a href="http://www.raychase.net/3546" target="_blank" rel="noopener">http://www.raychase.net/3546</a></p>
<p><a href="https://www.jianshu.com/p/10e91ace3378" target="_blank" rel="noopener">https://www.jianshu.com/p/10e91ace3378</a></p>
<h1><span id="3-创建sc">3. 创建sc</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器中的命令行中，输出：pyspark，会打开spark-shell交互窗口，这时spark-shell会自动创建一个sc，不用再创建sc，手动创建了也不能用，会出错。如果在py文件中写程序，首先需要手动创建一个sc。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line">conf = SparkConf().set(<span class="string">"spark.executorEnv.PYTHONHASHSEED"</span>, <span class="string">"0"</span>).setAppName(<span class="string">"kafka_hbase"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure>
<p>或者使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line">conf = SparkConf().setMaster(<span class="string">'local'</span>).setAppName(<span class="string">'My_App'</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br></pre></td></tr></table></figure>
<p>首先创建一个SparkConf对象来配置应用，然后基于该SparkConf来创建一个SparkContext对象。<br><code>.setMaster()</code>给定了集群的URL，高速spark如何连接到集群上，这里的<code>local</code>表示让spark运行在单机单变成上。<br>也可以是<code>.setMaster(&#39;spark://192.168.1.11:7077&#39;)</code>表示使用standalone运行spark程序。<br><code>.setAppName()</code>给出应用的名字，当连接到一集群上时，这个值可以帮助你找到你的应用。</p>
<h1><span id="4-rdd转换">4. RDD转换</span></h1><p>&ensp;&ensp;&ensp;&ensp;RDD被创建好之后，在后续使用过程中有2中操作：</p>
<ul>
<li>转换（Transformation）：基于现有的RRD创建一个新的RDD</li>
<li>行动（Action）：在数据集上进行运算，返回计算值</li>
</ul>
<h2><span id="41-转换操作">4.1. 转换操作</span></h2><p>&ensp;&ensp;&ensp;&ensp;对于RDD而言，每一次转换操作都会产生不同的RDD，如果说rdd2 = rdd1.map(lamda x : x+1),rdd1的值不会改变，通过转换操作返回一个新的rdd供下一个转换操作。转换得到的RDD是惰性的，也就是说，整个过程只记录了转换的轨迹，并不会发生真正的计算，只有遇到Action操作时，才会发生真正的计算。开始从血缘关系源头开始，进行物理的转换操作。<br>&ensp;&ensp;&ensp;&ensp;下面列出一些常见的转换（Transformation）操作。</p>
<ul>
<li>filter(func)：筛选出满足函数func的元素，并返回一个新的数据集</li>
<li>map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集</li>
<li>flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果</li>
<li>groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</li>
<li>reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合</li>
</ul>
<h2><span id="42-行动操作">4.2. 行动操作</span></h2><p>&ensp;&ensp;&ensp;&ensp;行动操作是真正触发计算的地方。Spark程序执行到行动操作，才会执行真正的计算，从文件中加载数据，完成一次有一次转换操作，最终，完成行动操作得到结果。<strong>在触发Action操作时，开始真正的计算，这时，Spark会把计算分解成多个任务在不同机器上执行，每台机器上运行位于属于它自己的map和reduce，最后把结果返回给Driver Program</strong>。<br>&ensp;&ensp;&ensp;&ensp;下面给出一些常见的行动（Action）操作</p>
<ul>
<li>count() 返回数据集中的元素个数</li>
<li>collect() 以数组的形式返回数据集中的所有元素</li>
<li>first() 返回数据集中的第一个元素</li>
<li>take(n) 以数组的形式返回数据集中的前n个元素</li>
<li>reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</li>
<li>foreach(func) 将数据集中的每个元素传递到函数func中运行</li>
</ul>
<h2><span id="43-持久化">4.3. 持久化</span></h2><p>&ensp;&ensp;&ensp;&ensp;在Spark中，RDD采用惰性的机制，每次遇到Action操作，都会从头开始执行计算。如果整个Spark程序只有一次Action操作，当然不会又什么问题。但是，在一些情况下，我们需要对一个RDD多次调用不同的Action，这就意味着，每次调用Action操作，都会触发一次从头开始的计算，代价很大，并且这些Action操作都是对一个RDD而言，所以可以把这个RDD持久化。<br>比如下面是多次对一个RDD进行Action操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</span><br><span class="line">rdd = sc.parallelize(list)</span><br><span class="line"><span class="comment">#count()是一个Action操作，触发一次真正从头到尾的计算</span></span><br><span class="line">print(rdd.count())</span><br><span class="line">&gt;&gt;&gt;<span class="number">3</span></span><br><span class="line"><span class="comment">#collect()也是一个Action()操作，触发一个真正从头到尾的计算</span></span><br><span class="line">print(<span class="string">','</span>.join(rdd.collect()))</span><br><span class="line">&gt;&gt;&gt;a,b,c</span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;上面代码执行过程中，前后共触发了2次从头到尾的计算。<br>&ensp;&ensp;&ensp;&ensp;实际上，可以通过持久化(缓存)机制避免这种重复计算的开销。可以使用persist()方法对一个RDD<strong>标记为持久化</strong>，之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算RDD并把它持久化，而是要等到第一个Action操作触发时，才开始计算RDD，并把RDD的内容进行持久化。持久化的RDD将会被保留在计算节点的内存中，以便被后面的Action操作重复使用。<br>&ensp;&ensp;&ensp;&ensp;persist()方法可以传入持久化级别参数</p>
<ul>
<li>persist(MEMOEY_ONLY)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容。</li>
<li>persist(MEMORY_AND_DISK)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存储在硬盘上。</li>
<li>一般使用cache()方法时，会调用persist(MEMORY_ONLY)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</span><br><span class="line">rdd = sc.parallelize(list)</span><br><span class="line"><span class="comment">#会调用persist(MEMORY_ONLY)，但是语句执行到这里，并不会缓存rdd的内容，因为这时rdd还没有被计算生成</span></span><br><span class="line">rdd.cache()</span><br><span class="line"><span class="comment">#count()是一个Action操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache()，把这个rdd的内容放在缓存中。</span></span><br><span class="line">print(rdd.count())</span><br><span class="line">&gt;&gt;&gt;<span class="number">3</span></span><br><span class="line"><span class="comment">#collect()也是一个Action()操作，不需要触发一个真正从头到尾的计算，只需要重复使用上面缓存中的rdd。</span></span><br><span class="line">print(<span class="string">','</span>.join(rdd.collect()))</span><br><span class="line">&gt;&gt;&gt;a,b,c</span><br></pre></td></tr></table></figure>
<p>最后，可以使用unpersist()方法手动地把持久化的RDD从缓存中移除。</p>
<h1><span id="5-分区">5. 分区</span></h1><p>&ensp;&ensp;&ensp;&ensp;RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上。RDD的一个分区原则是使得分区的个数尽量等于集群中CPU核心（core）数目。<br>&ensp;&ensp;&ensp;&ensp;对于不同的Spark部署而言（local，Standalone,yarn，Mesos）,都可以通过设置spark.default.parallelism这个参数的值，来配置默认的分区数据，一般而言：</p>
<ul>
<li>local模式：默认为本地机器的CPU数目，若设置了local[N],则默认为N</li>
<li>Standalone和yarn：max(集群中所有CPU核心数目总和,2)作为默认值</li>
<li>Mesos：默认的分区数为8</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;<span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">scala&gt;<span class="keyword">val</span> rdd = sc.parallelize(array,<span class="number">2</span>) #设置两个分区</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">13</span>] at parallelize at &lt;console&gt;:<span class="number">29</span></span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;对于textFile而言，如果没有在方法中指定分区数，则默认为min(defaultParallelism,2)，其中，defaultParallelism对应的就是spark.default.parallelism。<br>&ensp;&ensp;&ensp;&ensp;如果是从HDFS中读取文件，则分区数为文件分片数(比如，128MB/片)。</p>
<h1><span id="6-创建rdd">6. 创建RDD</span></h1><p>&ensp;&ensp;&ensp;&ensp;创建RDD有2种方式，一种是通过列表创建，一种是通过读取文件创建。<strong>RDD的内部其实是一个Iterator\<string\></string\></strong></p>
<h2><span id="61-通过paralize创建rdd">6.1. 通过paralize创建RDD</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">string=<span class="string">'a\nb\nc\na\nd\ne'</span></span><br><span class="line">b = string.split(<span class="string">'\n'</span>)</span><br><span class="line">sc.parallelize(b)</span><br></pre></td></tr></table></figure>
<p><code>b</code>是一个list列表，通过列表b可以创建一个RDD。</p>
<h2><span id="62-读文本文件创建rdd">6.2. 读文本文件创建RDD</span></h2><p>&ensp;&ensp;&ensp;&ensp; 读取文本文件获取RDD，可以从服务器本地读取(其他节点也可以)，也可以从hdfs上读取文件。文本每行的内容以字符串的形式作为RDD的一个元素。<br>从服务器本地读取文件时，需要加上file://</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.textFile(<span class="string">"file:///file0/input/test.txt"</span>)</span><br></pre></td></tr></table></figure>
<p>从HDFS上读取文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.textFile(<span class="string">'hdfs://master:8020/pc2/data.csv'</span>)</span><br></pre></td></tr></table></figure>
<h1><span id="7-map和flatmap">7. map和flatMap</span></h1><p>&ensp;&ensp;&ensp;&ensp;map是对RDD中的每个元素执行一个函数，每个元素返回一个list，然后把每个元素的list再组成一个大的list，例如[[a,a],[b,b]]，然后flatMap就是先对每个元素执行一个函数，每个元素返回一个list，然后把每个元素的list的内容取出来，组成一个大的list，例如[a,a,b,b]。</p>
<p>&ensp;&ensp;&ensp;&ensp;<a href="https://www.jianshu.com/p/c76ba3091a21" target="_blank" rel="noopener">这篇博客</a>讲解的比较好。说明flatMap中的函数返回类型一定是一个可迭代的类型，先把元素生成一个列表，然后再把每个列表中的元素取出来拼接成一个大的列表。<br><a href="https://www.4spaces.org/spark-map-flatmap/" target="_blank" rel="noopener">这篇也讲的很好</a></p>
<h1><span id="8-flatmap和flatmapvalues">8. flatMap和flatMapValues</span></h1><p>&ensp;&ensp;&ensp;&ensp;flatMap针对的RDD中的每个元素先做map操作，再做flatten操作，最后形成超大的list返回。flatMapValues只针对元素是<k,v>格式的RDD，原RDD中的key保持不变，只对value进行变换，变换之后的value和原来的key组成新的<k,v1>，作为RDD中的一个元素。参考<a href="http://blog.cheyo.net/172.html" target="_blank" rel="noopener">这篇博客</a></k,v1></k,v></p>
<h1><span id="9-reducebykey和groupbykey">9. reduceByKey和groupByKey</span></h1><p>&ensp;&ensp;&ensp;&ensp;推荐使用reduceByKey，<a href="https://blog.csdn.net/zongzhiyuan/article/details/49965021" target="_blank" rel="noopener">这篇博客</a>对于两者的区别进行了解释。<br>groupByKey涉及数据的shuffle操作，shuffle是spark重建数据的机制，将来自不同分区的数据进行分组，开销很大。</p>
<h1><span id="10-sortby和sortbykey">10. sortBy和SortByKey</span></h1><p>&ensp;&ensp;&ensp;&ensp;sortByKey针对(key,value)对中的key进行排序。</p>
<p>&ensp;&ensp;&ensp;&ensp;sortBy可以根据我们需要的值进行排序，不一定是key，比如统计单词出现的次数，然后按照次数进行排序(key,value)，我们就是对value进行排序，可以使用sortBy函数。<br>sortBy()中有3个参数，第一个参数是一个函数，第二个参数是ascending，表示升序还是降序，默认是True(升序)。第三个参数是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等。</p>
<h1><span id="11-将spark计算的结果存储在文件中">11. 将Spark计算的结果存储在文件中</span></h1><h2><span id="111-写入到服务器本地文件中">11.1. 写入到服务器本地文件中</span></h2><p>&ensp;&ensp;&ensp;&ensp;假设pyspark计算的结果存储在results变量中，然后将<code>results</code>的内容存储在文件中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将结果写入到服务器本地的文件中</span></span><br><span class="line">filename = <span class="string">'result.txt'</span></span><br><span class="line"><span class="keyword">with</span> open(filename,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> results:</span><br><span class="line">        f.write(line)</span><br><span class="line">        f.write(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="112-写入到hdfs文件中">11.2. 写入到HDFS文件中</span></h2><p>&ensp;&ensp;&ensp;&ensp;spark将RDD中的每个元素作为一行写入到文本文件中。在写入到HDFS之前，首先把results中的每个元素转成字符串的形式。<br>&ensp;&ensp;&ensp;&ensp;比如<code>rdd1</code>为<code>[(&#39;b&#39;,3),(&#39;a&#39;,2),(&#39;c&#39;,1)]</code>，<code>rdd1</code>中的每个元素是一个元组，需要把每个元素转换成字符串类型。<br><code>rdd2 = rdd1.map(lamda x: x[0]+&quot;,&quot;+str(x[1]))</code> ,然后使用<code>rdd2.saveAsTextFile(&#39;/tmp/word_count_result&#39;)</code>，把结果存储到<code>word_count_result</code>这个文件中，这个文件没有后缀名。</p>
<h2><span id="113-打印rdd元素">11.3. 打印RDD元素</span></h2><p>&ensp;&ensp;&ensp;&ensp;在实际编程中，我们经常需要把RDD中的元素打印输出到屏幕上（标准输出stdout），一般会采用语句rdd.foreach(println)或者rdd.map(println)。当采用本地模式（local）在单机上执行时，这些语句会打印出一个RDD中的所有元素。但是，当采用集群模式执行时，在worker节点上执行打印语句是输出到worker节点的stdout中，而不是输出到任务控制节点Driver Program中，因此，任务控制节点Driver Program中的stdout是不会显示打印语句的这些输出内容的。为了能够把所有worker节点上的打印输出信息也显示到Driver Program中，可以使用collect()方法，比如，rdd.collect().foreach(println)，但是，由于collect()方法会把各个worker节点上的所有RDD元素都抓取到Driver Program中，因此，这可能会导致内存溢出。因此，当你只需要打印RDD的部分元素时，可以采用语句rdd.take(100).foreach(println)。</p>
]]></content>
      <categories>
        <category>分布式平台</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Python学习</title>
    <url>/2019/04/15/Python%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;学习Python！！！！<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e8%a7%84%e8%8c%83">1. 规范</a></li>
<li><a href="#2-%e5%ba%8f%e5%88%97">2. 序列</a></li>
<li><a href="#3-%e8%af%8d%e5%85%b8">3. 词典</a></li>
<li><a href="#4-ndarray%e5%92%8clist">4. ndarray和list</a><ul>
<li><a href="#41-%e5%88%9b%e5%bb%bandarray">4.1. 创建ndarray</a><ul>
<li><a href="#411-%e9%80%9a%e8%bf%87nparray">4.1.1. 通过np.array</a></li>
<li><a href="#412-%e9%80%9a%e8%bf%87nparange">4.1.2. 通过np.arange</a></li>
<li><a href="#413-list%e5%92%8cndarray%e7%9a%84%e7%b4%a2%e5%bc%95">4.1.3. list和ndarray的索引</a></li>
<li><a href="#414-npmax">4.1.4. np.max()</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5-pandas">5. Pandas</a><ul>
<li><a href="#%e8%8e%b7%e5%8f%96dataframe%e4%b8%ad%e7%9a%84%e6%95%b0%e6%8d%ae">获取dataframe中的数据</a></li>
<li><a href="#dateframe%e5%b8%b8%e7%94%a8%e6%96%b9%e6%b3%95">DateFrame常用方法</a></li>
</ul>
</li>
<li><a href="#dict">Dict</a></li>
<li><a href="#%e5%87%bd%e6%95%b0%e5%8f%82%e6%95%b0">函数参数</a><ul>
<li><a href="#%e4%bd%8d%e7%bd%ae%e5%8f%82%e6%95%b0">位置参数</a></li>
<li><a href="#%e5%85%b3%e9%94%ae%e5%ad%97%e5%8f%82%e6%95%b0">关键字参数</a></li>
<li><a href="#%e9%bb%98%e8%ae%a4%e5%8f%82%e6%95%b0">默认参数</a></li>
<li><a href="#%e5%8f%af%e5%8f%98%e5%8f%82%e6%95%b0">可变参数</a><ul>
<li><a href="#args">*args</a></li>
<li><a href="#args-1">**args</a></li>
</ul>
</li>
<li><a href="#%e6%80%bb%e7%bb%93">总结</a></li>
</ul>
</li>
<li><a href="#%e6%96%87%e4%bb%b6%e8%af%bb%e5%8f%96">文件读取</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-规范">1. 规范</span></h1><ul>
<li>运算符的左右加空格，例如a + b</li>
<li>如果有多行赋值，将上下赋值的=对齐<br>num = 1<br>secNum = 2</li>
<li>变量的所有字母小写，单词之间用下划线连接，table_name=’test’</li>
</ul>
<h1><span id="2-序列">2. 序列</span></h1><p>&ensp;&ensp;&ensp;&ensp;序列是一种容器，是有顺序的数据集合。序列有两种：元组（Tuple）和列表（List）。列表是可变的，元组是不可变的。所以经常会创建空的列表，a=[],而不会创建一个空的元组。<br>&ensp;&ensp;&ensp;&ensp;对序列(元组和列表)范围引用，a[起始,结束,步长]，包含起始，不包含结束。循环获取序列的值：for i in list，这里的a就是值，而不是下标</p>
<h1><span id="3-词典">3. 词典</span></h1><p>&ensp;&ensp;&ensp;&ensp;词典中的数据是无序的，不能通过位置下标来获取</p>
<h1><span id="4-ndarray和list">4. ndarray和list</span></h1><p>&ensp;&ensp;&ensp;&ensp;list是Python的内置数据类型，list中的数据类型不必相同。例如：<code>[1,2,&#39;a&#39;,3.9]</code>。<br>&ensp;&ensp;&ensp;&ensp;首先需要明确的一点是array和ndarray是什么。ndarray是一种类型，array不是一种数据类型，可以通过np.array()来创建一个ndarray的对象。ndarray是numpy的一种数据类型，ndarray中的元素类型必须相同，例如：<code>[1,2,3,4]</code></p>
<h2><span id="41-创建ndarray">4.1. 创建ndarray</span></h2><h3><span id="411-通过nparray">4.1.1. 通过np.array</span></h3><p>&ensp;&ensp;&ensp;&ensp;通过np.array()来创建，传入的参数可以是list，也可以是tuple，使用ndarray的shape属性来获取ndarray的形状<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = np.array((<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>))</span><br><span class="line">c = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br></pre></td></tr></table></figure></p>
<p>使用reshape改变ndarray的形状<br><code>c.reshape((3,-1))</code>,reshape传入的形状是可以是<br>reshape((3,-1)),<br>reshape([3,1]),<br>reshape(3,-1)</p>
<h3><span id="412-通过nparange">4.1.2. 通过np.arange</span></h3><p>numpy提供了很多方法直接创建一个ndarray对象.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr1=np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">arr2=np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)  </span><br><span class="line"><span class="keyword">print</span> (arr1,arr1.dtype)  </span><br><span class="line"><span class="keyword">print</span> (arr2,arr2.dtype)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>] <span class="built_in">int</span>32</span><br><span class="line">[ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span>] <span class="built_in">float</span>64</span><br></pre></td></tr></table></figure>
<p>np.arange(a,b,c)表示产生从a~b，不包括b，间隔为c的一个ndarray，数据类型默认是int32。<br>np.linspace(a,b,c)表示把a~b（包括b），平均分成c份。<br>np.arange()和range都可以用来生成序列。注意arange是numpy的函数，range可以直接调用。arange和range不同的是：range只能生成int类型，写<code>rang(1,10,0.1)</code>是错误的，arange可以生成float类型，可以写成<code>np.arange(1,10,0.1)</code></p>
<p><img src="/2019/04/15/Python学习/range.png" alt=""></p>
<p><strong>使用print输出时，list中的元素之间有逗号分开，ndarray元素之间没有逗号</strong>。</p>
<p><img src="/2019/04/15/Python学习/print.png" alt=""></p>
<p><strong>虽然有很多产生ndarray类型的方法，但是大部分情况下我们都是从list进行转换生成ndarray。因为我们从文件中读取数据存储在list中，然后转换成ndarray</strong><br>比如定义一个list,a = [1,2,3,4],然后使用np.array(a)将list转换成ndarray类型</p>
<h3><span id="413-list和ndarray的索引">4.1.3. list和ndarray的索引</span></h3><p>定义一个list<br><code>list1=[[1,2,3],[4,5,6],[7,8,9]]</code><br>定义一个ndarray<br><code>arr1 = np.array(list1)</code></p>
<p><img src="/2019/04/15/Python学习/list.png" alt=""></p>
<p><img src="/2019/04/15/Python学习/arr.png" alt=""></p>
<p>ndarray比list的索引方式更多，这也是两者经常遇到的区别。<br><strong>因为list可以存储任意类型的数据，因为list中存储数据存放的地址，简单说就是指针，并非数据，这样保存一个list就太麻烦了，例如list1=[1,2,3,’a’]就需要4个指针和4个数据，增加了存储和CPU消耗</strong></p>
<h3><span id="414-npmax">4.1.4. np.max()</span></h3><p>&ensp;&ensp;&ensp;&ensp;numpy常用的统计函数如下：</p>
<ul>
<li>np.sum()，返回求和</li>
<li>np.mean()，返回均值</li>
<li>np.max()，返回最大值</li>
<li>np.min()，返回最小值</li>
<li>np.ptp()，数组沿指定轴返回最大值减去最小值，即（max-min）</li>
<li>np.std()，返回标准偏差（standard deviation）</li>
<li>np.var()，返回方差（variance）</li>
<li>np.cumsum()，返回累加值</li>
<li>np.cumprod()，返回累乘积值<br>注意：在使用以上这些函数时，需要指定axis的方向，若不指定，默认统计整个数组。axis=0表示列，axis=1表示行。一般axis=0比较符合实际情况。</li>
</ul>
<p><img src="/2019/04/15/Python学习/max.png" alt=""></p>
<h1><span id="5-pandas">5. Pandas</span></h1><p>&ensp;&ensp;&ensp;&ensp; DataFrame根据某一列排序，其中inplace=True表示修改df的值，默认是false，表示不修改df的值，会返回一个排好序的DataFrame。<br><code>df.sort_values(&quot;数据时间&quot;,inplace=True)</code></p>
<p>排好序的dataframe的index列还是原先dataframe的index。比如下面的图是排序之间的dataframe</p>
<p><img src="/2019/04/15/Python学习/排序前.png" alt=""></p>
<p>使用<code>df.sort_values(&quot;数据时间&quot;,inplace=True)</code>按照时间排序，但是index还是原先的index，我想让排序后的dataframe的index从0开始。</p>
<p><img src="/2019/04/15/Python学习/排序后.png" alt=""></p>
<p><code>df[2:4]</code>表示返回第3和4行的数据，即索引为3731，512这2行的数据，而不是返回索引为2和3的数据。<br>使用<code>sort_df.reset_index(drop=True, inplace=True)</code> 重新定义索引，使其从0开始。</p>
<p><img src="/2019/04/15/Python学习/reindex.png" alt=""></p>
<p>获取dataframe中的索引<br><code>firstIndex = df.index.tolist()</code>  返回一个list，存储的是dataframe中的索引列表。<br><code>firstIndex = df.index.tolist()[0]</code> 返回的是第一行数据的索引<br><code>firstIndex = df.index.tolist()[-1]</code> 返回的是最后一行数据的索引</p>
<h2><span id="获取dataframe中的数据">获取dataframe中的数据</span></h2><p><img src="/2019/04/15/Python学习/1.png" alt=""></p>
<ol>
<li><p><code>df[1:4]</code>表示获取表的第2至4行<br><img src="/2019/04/15/Python学习/2.png" alt=""></p>
</li>
<li><p>df.head()默认返回dataframe中的前5行，如果返回前10行，使用head(10)<br><img src="/2019/04/15/Python学习/3.png" alt=""></p>
</li>
<li><p>使用<code>df.iloc[]</code>和<code>df.loc[]</code>获取数据。<br><img src="/2019/04/15/Python学习/4.png" alt=""><br><img src="/2019/04/15/Python学习/5.png" alt=""></p>
</li>
</ol>
<p>通过 <code>df.iloc[]</code>传入的参数是数据，而<code>df.loc[]</code>传入的参数是字符串索引，除非索引是数字，这时loc[]可以传入数字。<br>比如df1.loc[2]表示获取索引为’2’的那一行，而df.iloc[2]表示获取df1的第3行数据，是一个相对位置。<br>参考资料：<br><a href="https://www.jb51.net/article/141665.htm" target="_blank" rel="noopener">https://www.jb51.net/article/141665.htm</a></p>
<h2><span id="dateframe常用方法">DateFrame常用方法</span></h2><ul>
<li>获取df中某一列特征值的个数<br><code>credit_df[&#39;Class&#39;].value_counts()</code>或<br><code>credit_df[&#39;Class&#39;].value_counts()[0]</code></li>
<li>显示df中的详细信息<br><code>df.info()</code><br><code>df.describe()</code></li>
<li>获取df中的所有列名<br><code>col_names = list(df.columns.values)</code></li>
<li>将df按照某一特征进行分组<br><code>df.groupby([&#39;total_vol&#39;]).size()</code>获取每个组中元素的个数<br><code>df.groupby([&#39;total_vol&#39;,&#39;soc&#39;]).size()</code>按照多个属性分组</li>
<li><p>从df中获取样本的特征和标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取特征</span></span><br><span class="line">X = df.drop(<span class="string">"误报"</span>,axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment">#获取标签</span></span><br><span class="line">Y = df[<span class="string">"误报"</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取df中的一列或多列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">one_col = df[<span class="string">'total_vol'</span>]</span><br><span class="line">multi_cols_name = [<span class="string">'total_vol'</span>,<span class="string">'soc'</span>,<span class="string">'cur'</span>]</span><br><span class="line">multi_cols = df[multi_cols_name]</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看df中为空的个数,输出每一列为nan的个数<br><code>df.isna().sum()</code></p>
</li>
</ul>
<h1><span id="dict">Dict</span></h1><p>python创建一个字典有3中方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">dict</span><span class="params">(**kwarg)</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">dict</span><span class="params">(mapping,**kwarg)</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">dict</span><span class="params">(iterable,**kwarg)</span></span></span><br></pre></td></tr></table></figure>
<p>其中<code>**kwarg</code>是python中可变参数，代表关键字参数，允许你传入0个或任意多个含参数名的参数，这个关键字参数在函数内部自动组装成一个dict。</p>
<ul>
<li><p>class dict(**kwarg)<br>通过关键字参数创建一个字典，例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = dict(name=<span class="string">'Tom'</span>,age=<span class="number">23</span>)</span><br><span class="line">out: &#123;<span class="string">'age'</span>: <span class="number">23</span>, <span class="string">'name'</span>: <span class="string">'Tom'</span>&#125;</span><br><span class="line"></span><br><span class="line">d = dict(a = <span class="number">12</span>, b = <span class="number">13</span>, c = <span class="number">15</span>)  </span><br><span class="line">out: &#123;<span class="string">'a'</span>: <span class="number">12</span>, <span class="string">'b'</span>: <span class="number">13</span>, <span class="string">'c'</span>: <span class="number">15</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>class dict(mapping,<strong>kwarg)<br>通过从一个映射函数对象中构造一个新字典，与dict(</strong>kwarg)函数不一样的地方是参数输入是一个映射类型的函数对象，比如zip函数，map函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#以映射函数方式来构造字典</span></span><br><span class="line">d2 = dict(zip([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))  </span><br><span class="line">out: &#123;<span class="string">'one'</span>: <span class="number">1</span>, <span class="string">'three'</span>: <span class="number">3</span>, <span class="string">'two'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>class dict(iterable,**kwarg)<br>其中iterable表示可迭代对象，可迭代对象可以使用for…in…来遍历，在Pytohn中list，tuple，str，dict，set等都是可迭代对象。创建dict时如果传入的是可迭代对象，则可迭代对象中每一项自身必须是可迭代的，并且每一项只能由有2个对象，第一个对象称为字典的key，第二个对象为key对应的value。如果key有重复，其value为最后重复项的值</p>
<p><img src="/2019/04/15/Python学习/dict.png" alt=""></p>
</li>
</ul>
<h1><span id="函数参数">函数参数</span></h1><p>&ensp;&ensp;&ensp;&ensp;在python中定义一个函数，可以传入4种参数：<br>位置参数，默认参数，关键字参数，可变参数</p>
<h2><span id="位置参数">位置参数</span></h2><p>普通的参数，参数之间是有顺序的， 顺序不能写错</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<h2><span id="关键字参数">关键字参数</span></h2><p>函数调用时使用关键字参数来确定传入的参数值，使用关键字参数允许函数调用时参数的顺序和声明的顺序不一致，因为python解释器会根据参数名来匹配参数值。使用key=value格式来指定参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">power(<span class="number">5</span>,<span class="number">2</span>)会得到<span class="number">25</span></span><br><span class="line">power(<span class="number">2</span>,<span class="number">5</span>)会得到<span class="number">32</span></span><br><span class="line">power(n=<span class="number">2</span>,x=<span class="number">5</span>)会得到<span class="number">25</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">可写函数说明</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printinfo</span><span class="params">( name, age )</span>:</span></span><br><span class="line">   <span class="string">"打印任何传入的字符串"</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"名字: "</span>, name)</span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"年龄: "</span>, age)</span><br><span class="line">   <span class="keyword">return</span></span><br><span class="line"><span class="comment">#调用printinfo函数</span></span><br><span class="line">printinfo( age=<span class="number">50</span>, name=<span class="string">"runoob"</span> )</span><br></pre></td></tr></table></figure>
<p><strong>注意：关键字参数必须写在位置参数之后，否则会报错</strong></p>
<h2><span id="默认参数">默认参数</span></h2><p>在定义函数时，使用赋值运算符=就为参数设置了一个默认值，默认参数是可选的，就是说可以指定，也可以不指定。当不指定时就使用默认值，如果指定，会覆盖默认值。有了一个默认参数，这样即使传入调用<code>power(5)</code>,这样就默认n=2，如果要计算的幂次大于2，就需要明确的指定n的值，<code>power(5,3)</code>,这是n=3</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n=<span class="number">2</span>)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p><strong>设置默认参数时，需要注意以下几点：</strong></p>
<ol>
<li>必选参数在前面，默认参数在后面，否则Python的解释器会报错<br>使用默认参数的好处是：比如学生注册的时候，需要传入的参数为：姓名，性别，年龄。把年龄设置为默认参数19，这样大部分学生注册时不需要提供年龄，只需要提供2个必须的参数，只有与默认参数不符的学生才提供额外的信息。可见，使用默认参数降低了函数调用的难度</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enroll</span><span class="params">(name, gender,age=<span class="number">19</span>)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'name:'</span>, name</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'gender:'</span>, gender</span><br></pre></td></tr></table></figure>
<h2><span id="可变参数">可变参数</span></h2><p>可变参数就是传入的参数个数是可变的，可以是1个、2个到任意个，还可以是0个。  当函数中有位置参数和可变参数时，位置参数始终在可变参数之前。通常情况下，可变参数会出现在形参的最后，因为它们会把传递给函数的所有剩余参数都收集起来。可变参数之后出现的任何参数都是“强制关键字”参数，也就是说，可变参数之后的参数必须是关键字参数，而不能是位置参数。</p>
<h3><span id="args">*args</span></h3><p>我们以数学题为例子，给定一组数字a，b，c……，请计算a2 + b2 + c2 + ……。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> numbers:</span><br><span class="line">        sum = sum + n * n</span><br><span class="line">    <span class="keyword">return</span> sum</span><br></pre></td></tr></table></figure>
<p>在函数内部，参数number接收到的是一个tuple，例如<code>calc(1,2)</code>得出来的结果是5，也可以直接传入一个list或者tuple,在list或tuple前面加上一个*，把list或tuple的元素变成可变参数传进去。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">calc(*nums1)</span><br><span class="line"><span class="number">14</span></span><br><span class="line"></span><br><span class="line">nums2 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">calc(*nums1,*nums2)</span><br><span class="line"><span class="number">28</span></span><br></pre></td></tr></table></figure>
<h3><span id="args">**args</span></h3><p>可变参数允许传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名称的参数，这些关键字参数函数内部自动组装成一个dict</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">person</span><span class="params">(name, age, **kw)</span>:</span></span><br><span class="line">    print( <span class="string">'name:'</span>, name)</span><br><span class="line">    print( <span class="string">'age:'</span>, str(name))</span><br><span class="line">    print( <span class="string">'other:'</span>, kw)</span><br><span class="line">person(<span class="string">'Michael'</span>, <span class="number">30</span>)  </span><br><span class="line">person(<span class="string">'Bob'</span>, <span class="number">35</span>, city=<span class="string">'Beijing'</span>)</span><br><span class="line">kw = &#123;<span class="string">'city'</span>: <span class="string">'Beijing'</span>, <span class="string">'job'</span>: <span class="string">'Engineer'</span>&#125;</span><br><span class="line">person(<span class="string">'Jack'</span>, <span class="number">24</span>, **kw)</span><br></pre></td></tr></table></figure>
<h2><span id="总结">总结</span></h2><p><strong>在函数定义时，参数的顺序为：位置参数，默认参数，<em>args，\</em>*args</strong></p>
<p><strong>在函数调用时，参数的顺序为位置参数、关键字参数/默认参数，<em>args，\</em>*args</strong></p>
<h1><span id="文件读取">文件读取</span></h1><p>当一个文件很大时，不能一次性读取所有的内容加载到内存中，需要使用生成器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    读取文件</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.readline().strip()</span><br><span class="line">        <span class="keyword">while</span> data:</span><br><span class="line">            <span class="keyword">yield</span> data</span><br><span class="line">            data = f.readline().strip()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> read_file(filename):</span><br><span class="line">        value = json.loads(line)[<span class="string">'data'</span>]</span><br><span class="line">        rowkeys.add(list(value.keys())[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH集群</title>
    <url>/2019/04/04/CDH%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;最近去给甲方安装CDH集群，对于集群的搭建和测试在这里总结一下。<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#1-%e7%89%88%e6%9c%ac%e6%8e%a7%e5%88%b6">1. 版本控制</a></li>
<li><a href="#2-linux%e7%9b%ae%e5%bd%95%e4%bb%8b%e7%bb%8d">2. Linux目录介绍</a></li>
<li><a href="#3-%e5%ae%89%e8%a3%85%e5%89%8d%e8%af%b4%e6%98%8e">3. 安装前说明</a></li>
<li><a href="#4-%e5%b0%8f%e5%b8%b8%e8%af%86">4. 小常识</a></li>
<li><a href="#5-%e5%ae%89%e8%a3%85cdh%e9%9b%86%e7%be%a4">5. 安装CDH集群</a><ul>
<li><a href="#51-%e5%85%b3%e9%97%ad%e6%89%80%e6%9c%89%e6%9c%ba%e5%99%a8%e7%9a%84%e9%98%b2%e7%81%ab%e5%a2%99">5.1. 关闭所有机器的防火墙</a></li>
<li><a href="#52-%e4%bf%ae%e6%94%b9%e6%9c%ba%e5%99%a8%e7%9a%84hosts">5.2. 修改机器的hosts</a></li>
<li><a href="#53-%e6%9f%a5%e7%9c%8b%e7%bd%91%e7%bb%9c%e6%98%af%e5%90%a6%e8%bf%9e%e9%80%9a">5.3. 查看网络是否连通</a></li>
<li><a href="#54-%e7%94%9f%e6%88%90%e4%b8%bb%e8%8a%82%e7%82%b9%e7%9a%84ssh%e5%af%86%e9%92%a5%e5%b9%b6%e5%88%86%e5%8f%91">5.4. 生成主节点的ssh密钥并分发</a></li>
<li><a href="#55-%e5%ae%89%e8%a3%85ntp%e6%9c%8d%e5%8a%a1">5.5. 安装ntp服务</a></li>
<li><a href="#56-%e5%8d%b8%e8%bd%bd%e6%9c%ba%e5%99%a8%e8%87%aa%e5%b8%a6%e7%9a%84openjdk">5.6. 卸载机器自带的openjdk</a></li>
<li><a href="#57-%e5%ae%89%e8%a3%85jdk">5.7. 安装JDK</a></li>
<li><a href="#58-%e5%ae%89%e8%a3%85mysql">5.8. 安装MySQL</a></li>
<li><a href="#59-%e5%88%9b%e5%bb%bamysql%e6%95%b0%e6%8d%ae%e5%ba%93">5.9. 创建MySQL数据库</a></li>
<li><a href="#510-%e5%ae%89%e8%a3%85cloudera-manager-server%e5%92%8cagent">5.10. 安装Cloudera Manager Server和Agent</a></li>
<li><a href="#511-%e6%89%93%e5%bc%80%e7%bd%91%e9%a1%b5%e9%85%8d%e7%bd%ae">5.11. 打开网页配置</a></li>
<li><a href="#512-hdfs%e7%9a%84ha%e5%ae%89%e8%a3%85">5.12. HDFS的HA安装</a></li>
<li><a href="#513-%e5%ae%89%e8%a3%85anaconda">5.13. 安装Anaconda</a></li>
<li><a href="#514-%e5%ae%89%e8%a3%85sparkstandalone">5.14. 安装Spark（standalone）</a></li>
<li><a href="#515-%e4%bf%ae%e6%94%b9hdfs%e6%9d%83%e9%99%90">5.15. 修改hdfs权限</a></li>
<li><a href="#516-%e5%ae%89%e8%a3%85python%e4%b8%89%e6%96%b9%e5%ba%93">5.16. 安装python三方库</a></li>
<li><a href="#517-%e5%ae%89%e8%a3%85kafka">5.17. 安装kafka</a></li>
<li><a href="#518-yarn">5.18. yarn</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-版本控制">1. 版本控制</span></h1><div class="table-container">
<table>
<thead>
<tr>
<th>组件</th>
<th>版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>CenOS</td>
<td>CenOS7</td>
</tr>
<tr>
<td>JDK</td>
<td>JDK1.8</td>
</tr>
<tr>
<td>CDH集群</td>
<td>CDH5.7.2</td>
</tr>
<tr>
<td>CDH-kafka</td>
<td>CDH-kafka1.2.0</td>
</tr>
<tr>
<td>Python</td>
<td>Python3.5</td>
</tr>
</tbody>
</table>
</div>
<p>现在实验室的cdh集群版本是CDH5.7.2,其中每个组件的版本是</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>组件</th>
<th>CDH5.7.2</th>
<th>CDH5.16.1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop</td>
<td>2.6.0</td>
<td>2.6.0</td>
</tr>
<tr>
<td>HDFS</td>
<td>2.6.0</td>
<td>2.6.0</td>
</tr>
<tr>
<td>HBase</td>
<td>1.2.0</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Hive</td>
<td>1.1.0</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Spark</td>
<td>1.6.0</td>
<td>1.6.0</td>
</tr>
<tr>
<td>Kafka</td>
<td>0.10.0</td>
<td>0.10.0</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>3.4.5</td>
<td>3.4.5</td>
</tr>
</tbody>
</table>
</div>
<p>以后安装CDH集群，应该安装CDH5.16.1版本</p>
<h1><span id="2-linux目录介绍">2. Linux目录介绍</span></h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">目录</th>
<th style="text-align:center">说明</th>
<th style="text-align:center">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">bin</td>
<td style="text-align:center">存储普通用户可执行的指令</td>
<td style="text-align:center">即使在单用户模式下也能够执行处理</td>
</tr>
<tr>
<td style="text-align:center">dev</td>
<td style="text-align:center">设备目录</td>
<td style="text-align:center">所有的硬件设备及周边均放置在这个设备目录中</td>
</tr>
<tr>
<td style="text-align:center">home</td>
<td style="text-align:center">主要存放用户的个人数据</td>
<td style="text-align:center">每个用户在home中都有一个文件夹(除root外)，存储每个用户的设置文件，用户的桌面文件夹、用户的数据</td>
</tr>
<tr>
<td style="text-align:center">etc</td>
<td style="text-align:center">各种配置文件目录</td>
<td style="text-align:center">大部分配置属性均存放在这里</td>
</tr>
<tr>
<td style="text-align:center">lib/lib64</td>
<td style="text-align:center">开机时常用的动态链接库</td>
<td style="text-align:center">bin及sbin指令也会调用对应的lib库</td>
</tr>
<tr>
<td style="text-align:center">opt</td>
<td style="text-align:center">第三方软件安装目录</td>
<td style="text-align:center">现在习惯放在/usr/local中</td>
</tr>
<tr>
<td style="text-align:center">run</td>
<td style="text-align:center">系统运行所需的文件</td>
<td style="text-align:center">重启后重新生成对一个的目录数据</td>
</tr>
<tr>
<td style="text-align:center">sbin</td>
<td style="text-align:center">只有root用户才能运行的管理指令</td>
<td style="text-align:center">跟bin类似，但只属于root管理员</td>
</tr>
<tr>
<td style="text-align:center">tmp</td>
<td style="text-align:center">存储临时文件目录</td>
<td style="text-align:center">所有用户对该目录均可读写</td>
</tr>
<tr>
<td style="text-align:center">usr</td>
<td style="text-align:center">应用程序放置目录</td>
<td style="text-align:center">/usr/local存储那些手动安装的软件，/usr/bin存储程序，/usr/share存储一些共享数据，例如音乐文件或者图标，/usr/lib存储那些不能直接运行的，但却是很多程序运行所必须的一些函数库文件</td>
</tr>
</tbody>
</table>
</div>
<h1><span id="3-安装前说明">3. 安装前说明</span></h1><ul>
<li>Zookeeper和Kafka不分主节点，要装3台</li>
<li>HBase和HDFS分主从节点，都需要2个主节点，一个active主节点，一个standby主节点，剩下的机器作为从节点</li>
<li>关闭防火墙，安装ntp、jdk、mysql，anaconda可以同时进行。   </li>
<li>本次以3个服务器为例安装CDH集群</li>
<li>192.168.1.201   node1</li>
<li>192.168.1.202   node2</li>
<li>192.168.1.203   node3</li>
</ul>
<h1><span id="4-小常识">4. 小常识</span></h1><ul>
<li>使用vi命令编辑文件</li>
<li>键盘a———输入模式，编辑文件</li>
<li>键盘Esc——修改完之后按Esc退出输入模式</li>
<li>:wq——-保存，并退出</li>
<li>:q———不保存，退出</li>
</ul>
<h1><span id="5-安装cdh集群">5. 安装CDH集群</span></h1><h2><span id="51-关闭所有机器的防火墙">5.1. 关闭所有机器的防火墙</span></h2><p><font color="red">每台机器都要执行</font><br>#关闭防火墙<br>systemctl stop firewalld.service<br>#禁止firewall开机启动<br>systemctl disable firewalld.service<br>#关闭selinux<br>vi /etc/selinux/config<br>将SELINUX设置为disabled<br>如下SELINUX=disabled<br>#重启<br>reboot<br>#重启机器后使用root用户查看Selinux状态<br>getenforce</p>
<h2><span id="52-修改机器的hosts">5.2. 修改机器的hosts</span></h2><p><font color="red">每台机器都要执行</font><br>#使用ip addr查看每台机器的ip地址<br>#修改hosts文件<br>vi /etc/hosts</p>
<p><font color="red">在最下面一行添加以下内容</font><br>192.168.1.201 node1<br>192.168.1.202 node2<br>192.168.1.203 node3</p>
<h2><span id="53-查看网络是否连通">5.3. 查看网络是否连通</span></h2><p><font color="red">每台机器都要执行</font><br>#在node1上执行<br>ping node2<br>ping node3<br>#在其余两个节点分别ping<br>按Ctrl+C中断ping命令</p>
<h2><span id="54-生成主节点的ssh密钥并分发">5.4. 生成主节点的ssh密钥并分发</span></h2><p>生成主节点root账户的ssh密钥，分发至其他机器，要实现免密码登录其他机器的root账户</p>
<p><font color="red">只在node1上执行</font><br>#生成ssh密钥（node1上）<br>ssh-keygen -t rsa<br>然后一路回车<br>接下来分发密钥，请仔细观察显示的内容，会让你输入yes和密码<br>ssh-copy-id node1<br>ssh-copy-id node2<br>ssh-copy-id node3</p>
<h2><span id="55-安装ntp服务">5.5. 安装ntp服务</span></h2><p><font color="red">每台机器都要执行</font><br>yum install ntpdate<br>在执行这条命令时我的电脑出现以下错误：</p>
<p><font color="red">问题：Could not resolve host: mirrorlist.centos.org Centos 7</font><br><strong>解决方案</strong>：<a href="https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7" target="_blank" rel="noopener">https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7</a>   </p>
<p><font color="red">只在node1上执行</font><br><code>yum install ntp</code></p>
<p><font color="red">只在node1上执行</font><br>vi /etc/ntp.conf<br>注释以下4行，在前面加#<br>server 0.centos.pool.ntp.org iburst<br>server 1.centos.pool.ntp.org iburst<br>server 2.centos.pool.ntp.org iburst<br>server 3.centos.pool.ntp.org iburst<br>在最下面加上<br>restrict default ignore<br>restrict <font color="red">192.168.1.0</font>   mask 255.255.255.0<br>nomodify notrap<br>server 127.127.1.0   </p>
<p><font color="red">注意：192.168.1.0是这3台机器ip地址的前3位，最后一位是0</font><br>#重启ntp服务<br>service ntpd restart<br>#设置ntp服务器开机自动启动<br>chkconfig ntpd on       </p>
<p><font color="red">只在node2和node3执行</font><br>#以下为客户端的配置（除node1外的node2和node3），设定每天00:00向服务器(node1)同步时间，并写入日志<br>crontab -e<br>#输入以下内容后保存并退出<br>0 0 <em> </em> <em> /usr/sbin/ntpdate <em>*node1</em></em>&gt;&gt; /root/ntpd.log   </p>
<p><font color="red">只在node2和node3执行</font><br>ntpdate node1</p>
<h2><span id="56-卸载机器自带的openjdk">5.6. 卸载机器自带的openjdk</span></h2><p>！！！！！！！一定要卸载    </p>
<h2><span id="57-安装jdk">5.7. 安装JDK</span></h2><p>安装自己的jdk到/opt/java/下面，如/opt/java/jdk1.8.0_90    </p>
<p><font color="red">只在node1上执行</font><br>首先使用filezilla把cdh_deployment压缩包上传到/usr下面，然后再opt下面创建一个java文件夹。<br>将jdk的安装包拷贝到/opt/java/下面<br>cp /usr/CDH_deployment/jdk-8u11-linux-x64.tar.gz /opt/java/<br>#解压<br>tar -zxvf jdk-8u11-linux-x64.tar.gz<br>#修改环境变量<br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH<br>#刷新配置文件<br>source /etc/profile<br>#复制jdk到其他服务器上<br>scp -r /opt/java/jdk1.8.0_11/ node2:/opt/java/<br>scp -r /opt/java/jdk1.8.0_11/ node3:/opt/java/      </p>
<p><font color="red">在node2执行</font><br>// WangBeibei-DC-2 上<br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH  </p>
<p><font color="red">在node3执行</font><br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH<br>测试java -version<br>看到java的版本说明安装成功  </p>
<p><font color="red">在每台上执行</font><br>mkdir /usr/java<br>ln -s /opt/java/jdk1.8.0_90/  /usr/java/default    </p>
<h2><span id="58-安装mysql">5.8. 安装MySQL</span></h2><p><font color="red">只在node1上执行</font><br>yum remove mysql mysql-server mysql-libs compat-mysql51<br>rm -rf /var/lib/mysql<br>rm -rf /etc/my.cnf<br>将mysql.jar拷贝到/usr/local下面<br>解压<br>tar -zxvf /opt/mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz<br>// 改名为mysql<br>mv mysql-5.6.37-linux-glibc2.12-x86_64 mysql<br>// 删除安装包<br>rm mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz<br>// 修改环境变量<br>vi /etc/profile<br>在最下面添加<br>export MYSQL_HOME=/usr/lcoal/mysql<br>export PATH=\$MYSQL_HOME/bin:\$PATH<br>// 刷新环境变量<br>source /etc/profile<br>将服务文件mysql.server拷贝到init.d下<br>cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql.server<br>MySQL开机自启，赋予可执行权限<br>chmod +x /etc/init.d/mysql.server<br>添加服务<br>chkconfig —add mysql.server<br>显示服务列表<br>chkconfig —list<br>如果看到mysql的服务，并且3、4、5都是on的话则成功。如果mysql.server的 3, 4, 5 不是on，使用下面的命令给他变成on:<br>chkconfig —level 345 mysql.server on<br>// 新建mysql 用户<br>groupadd mysql 在/etc/group 中可以看到<br>useradd -r -g mysql -s /bin/false mysql 在/etc/passwd 中可以看到<br>cd /usr/local/mysql<br>chown -R mysql:mysql .<br>scripts/mysql_install_db —user=mysql<br>// 修改当前目录拥有者为root 用户<br>chown -R root .<br>// 修改当前data 目录拥有者为mysql 用户<br>chown -R mysql data<br>// 新建一个虚拟窗口，叫mysql<br>screen -S mysql<br>bin/mysqld_safe —user=mysql &amp;<br>// 退出虚拟窗口<br>Ctrl+A+D<br>cd /usr/local/mysql<br>// 登陆mysql<br>bin/mysql<br>// 登陆成功后退出即可<br>exit;<br>// 进行root 账户密码的修改等操作<br>bin/mysql_secure_installation<br>首先要求输入root 密码，由于我们没有设置过root 密码，括号里面说了，如果没有root 密码就直接按回车。<br>是否设定root 密码，选y，设定密码为cluster，是否移除匿名用户：y。然后有个是否关闭root 账户的远程<br>登录，选n，删除test 这个数据库？y，更新权限？y，然后ok。<br>cp support-files/mysql.server /etc/init.d/mysql.server<br>// 进入mysql 虚拟窗口<br>screen -r mysql<br>// 查看mysql 的进程号<br>ps -ef | grep mysql<br>// 如果有的话就kill 掉，保证mysql已经中断运行了，一般kill 掉/usr/local/mysql/bin/mysqld 开头的即可<br>kill 进程号<br>// 关闭虚拟窗口<br>exit<br>// 启动mysql<br>/etc/init.d/mysql.server start -user=mysql<br>exit<br>还需要配置一下访问权限，#授权root用户在主节点拥有所有数据库的访问权限：<br>$ mysql -u root -p<br>mysql&gt; GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’%’ IDENTIFIED BY ‘cluster’ WITH GRANT OPTION;<br>mysql&gt; FLUSH PRIVILEGES;    </p>
<h2><span id="59-创建mysql数据库">5.9. 创建MySQL数据库</span></h2><p>MySQL中root账户执行：<br>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</p>
<h2><span id="510-安装cloudera-manager-server和agent">5.10. 安装Cloudera Manager Server和Agent</span></h2><p>（1） <font color="red">在node1执行</font><br>把cloudera-manager-centos7-cm5.7.2_x86_64.tar.gz的压缩包解压到/opt下面<br>（2）<font color="red">在node1执行</font><br>把mysql-connector-java-5.1.43-bin.jar复制到/opt/cm-5.7.2/share/cmf/lib/ 里面<br>（3）<font color="red">在node1执行</font><br>在主节点初始化CM5的数据库：<br>/opt/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh<br>mysql cm -h[mysql数据库的主机名]  -uroot -p[password] —scm-host [cm server的主机名] [cm的数据库] [cm数据库访问用户] [cm数据库访问用户的密码]<br>执行/opt/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -pcluster —scm-host localhost scm scm scm<br>（4）<font color="red">在node1执行</font><br>Agent配置<br>修改 vi /opt/cm-5.7.2/etc/cloudera-scm-agent/config.ini 这里面的server_host，改成自身的机器名node1，也就是指名主节点的机器名<br>（5）<font color="red">在node1执行</font><br>将cm-5.7.2的目录复制到其他机器上，同步Agent到其他节点<br>确保复制到所有的机器上<br>scp -r /opt/cm-5.7.2 root@node2:/opt/<br>scp -r /opt/cm-5.7.2 root@node3:/opt/<br>（6）<font color="red">在所有机器上</font><br>在所有节点创建cloudera-scm用户<br>执行<br>useradd —system —home=/opt/cm-5.7.2/run/cloudera-scm-server/ —no-create-home —shell=/bin/false —comment “Cloudera SCM User” cloudera-scm<br>（7）<font color="red">在node1执行</font><br>执行 mkdir -p /opt/cloudera/parcel-repo/<br>（8）<font color="red">在node1执行</font><br>把<br>CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel，<br>CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha，<br>manifest.json<br>这三个文件，复制到/opt/cloudera/parcel-repo/这里面<br>（9）<font color="red">在node1执行</font><br>ssh node2<br>mkdir /usr/share/java<br>把mysql-connector-java-5.1.43-bin.jar复制到/usr/share/java下，并命名为mysql-connector-java.jar<br>cp /opt/cm-5.7.2/share/cmf/lib/   mysql-connector-java-5.1.43-bin.jar /usr/share/java/   mysql-connector-java.jar   </p>
<p><font color="red">其中machine2是第二台机器，把这个machine2改成其他机器的名字，分别执行一遍</font><br>（10）<font color="red">在node1执行</font><br>执行启动服务端：<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-server start<br>执行启动Agent服务端：<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>启动其他机器的Agent<br>执行：<br>ssh node2<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>ssh node3<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>用这个ssh命令将其他所有机器的agent都启动      </p>
<p><font color="red">问题：在启动时出错<br>/opt/cm-5.7.0/etc/init.d/cloudera-scm-server start<br>/opt/cm-5.7.0/etc/init.d/cloudera-scm-server: line 109: pstree: command not found   </font><br><strong>解决方案</strong>：因为系统是最小化安装，默认没有安装，运行下面的命令。<br>yum -y install psmisc</p>
<h2><span id="511-打开网页配置">5.11. 打开网页配置</span></h2><p>（1）打开浏览器，地址是：主节点的ip:7180，用户名和密码都是admin<br>（2）选第一个免费版！<br>（3）<a href="http://www.cnblogs.com/jasondan/p/4011153.html，" target="_blank" rel="noopener">http://www.cnblogs.com/jasondan/p/4011153.html，</a><br>然后按照那个博客里面的图片安装就行了<br>（4）安装服务的时候，错开机器，别把所有的服务都堆在前几台机器上，zookeeper要3个，安装的时候hive会报错，博客里面写了怎么解决，oozie也会报错，都是一样的解决方法，最好默认不要修改。<br>（5）按照博客可以完成CDH集群的安装<br>（6）问题：<br><img src="/2019/04/04/CDH集群/jdk.png" alt=""><br>解决方案：这里需要强调一下CDH5默认识别的jdk路径为：/usr/java/default<br>没有往/usr/java中添加软链接，而这里默认是去/usr/java/default中找环境变量，才会报找不到java_home。安装jdk的方法:把jdk软连接到/usr/java/default首先查看是否有/usr/java目录，没有的话新建此目录：mkdir /usr/java。然后添加软连接到/usr/java/default，命令如下: ln -s /opt/java/jdk1.8.0_11 /usr/java/default<br>问题：<br><img src="/2019/04/04/CDH集群/hive.png" alt=""><br>解决方案：这里安装Hive的时候可能会报错，因为我们使用了MySql作为hive的元数据存储，hive默认没有带mysql的驱动，通过以下命令拷贝一个就行了：<br>cp /opt/cm-5.7.2/share/cmf/lib/mysql-connector-java-5.1.43-bin.jar /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/hive/lib/</p>
<h2><span id="512-hdfs的ha安装">5.12. HDFS的HA安装</span></h2><p>HDFS HA的安装：<br><a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12_1，" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12_1，</a><br>看里面的Enabling High Availability and Automatic Failover。按操作安装完后，<a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp，" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp，</a><br>完成里面的Upgrading the Hive Metastore to Use HDFS HA和Configuring Hue to Work with HDFS HA<br>问题：Cloudera Manager （重新）部署集群报错：fail to format namenode<br>解决方案：<a href="https://www.jianshu.com/p/1e8b25e63ab9" target="_blank" rel="noopener">https://www.jianshu.com/p/1e8b25e63ab9</a></p>
<h2><span id="513-安装anaconda">5.13. 安装Anaconda</span></h2><p>首先创建一个file0的目录，在这个目录下面运行下面的命令：<br>bash Anaconda3-4.1.1-Linux-x86_64.sh<br>这样Anaconda3就安装在file0下面。<br><a href="https://blog.csdn.net/m0_37548423/article/details/81173678" target="_blank" rel="noopener">https://blog.csdn.net/m0_37548423/article/details/81173678</a>    </p>
<p>在所有机器上安装anaconda4.2.0，结尾最后一步，是否添加至环境变量，选择no<br>使用which python查看使用的是哪个版本的python，运行程序的时候要用<br>/file0/anaconda3/bin/python user.py</p>
<h2><span id="514-安装sparkstandalone">5.14. 安装Spark（standalone）</span></h2><p>（1）选择这个添加服务，安装Spark (standalone)<br> <img src="/2019/04/04/CDH集群/spark1.png" alt=""><br>（2）点击spark<br><img src="/2019/04/04/CDH集群/spark2.png" alt=""><br>（3）点击配置<br> <img src="/2019/04/04/CDH集群/spark3.png" alt=""></p>
<p>（4）搜索栏输入spark-env.sh<br>export PYSPARK_PYTHON=/file0/anaconda3/bin/python<br>export PYSPARK_DRIVER_PYTHON=/file0/anaconda3/bin/ipython<br>export PYTHONHASHSEED=0<br><img src="/2019/04/04/CDH集群/spark4.png" alt=""></p>
<p>找到这个服务高级配置的代码段，改成这个样子，把python的路径指名为anaconda的python路径<br>（5）按照上一步把 Spark (standalone) 的spark-env也改成上面的样子</p>
<h2><span id="515-修改hdfs权限">5.15. 修改hdfs权限</span></h2><p>主节点执行：<br>sudo -u hdfs hdfs dfs -mkdir /user/root<br>sudo -u hdfs hdfs dfs -chown root /user/root<br>sudo -u hdfs hdfs dfs -chmod -R 777 /user</p>
<h2><span id="516-安装python三方库">5.16. 安装python三方库</span></h2><p>主节点执行：<br>/file0/anaconda3/bin/pip install<br>—index-url=file:///file0/CDH_deployment/pypi/simple pymysql happybase pykafka</p>
<h2><span id="517-安装kafka">5.17. 安装kafka</span></h2><p>Kafka的安装过程参照：<br><a href="https://jingyan.baidu.com/article/e9fb46e139dead7521f7662e.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/e9fb46e139dead7521f7662e.html</a><br>里面要求下载的四个文件我已经下载好了，其中一个叫manifest.json的文件，我重命名为了manifest_kafka.json，这个重命名是因为防止和之前的那个manifest冲突，直接按照第18步，将两个kafka的parcel文件和这个manifest拷到那个目录里面就行，按照博客里面的要求安装即可。  </p>
<p><font color="red">注意：只设置kafka broker，不设置Kafka MirrorMaker<br>安装的时候会配置kafka，配置完后会启动kafka，kafka一定启动不了，右下角有个重试按钮，这时候需要再开一个管理界面，像上面配置spark一样，配置kafka，如下图 </font><br><img src="/2019/04/04/CDH集群/kafka1.png" alt=""><br>分别点进三个超链接，选择左上角的配置，针对每个broker进行配置。<br>需要配置的是两项：</p>
<ol>
<li>Broker ID：可以机器顺序分别改成1，2，3</li>
<li>Java Heap Size of Broker：改成1G<br>然后回到刚才安装的那个界面，点击重试。</li>
</ol>
<h2><span id="518-yarn">5.18. yarn</span></h2><p>CDH 5.9 以前的版本，如果使用 python3 且使用 yarn 作为master，需手动修复CDH 集群的bug。CDH 5.9 以前的版本在使用 yarn 作为 spark master 的时候，如果使用 python3，会出现 yarn 內部 topology.py 这个文件引发的 bug。这个文件是 python2 的语法，我们使用 python3 运行任务的时候，python3 的解释器在处理这个文件时会出错。<br>解决方案是：将这个文件重写为 python3 的版本，每次在重启 yarn 之后，将这个文件复制到所有机器<br>的 /etc/hadoop/conf.cloudera.yarn/ 目录下。<br><strong>以下操作在所有机器上都要操作，并使用root用户，不可以使用普通用户。如果/etc/hadoop/conf.cloudera.yarn目录不存在，先创建一个同名目录，然后将topology.py复制到该目录下</strong>。</p>
]]></content>
      <categories>
        <category>分布式平台</category>
      </categories>
      <tags>
        <tag>CDH集群</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式组件介绍</title>
    <url>/2019/03/26/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>介绍Spark，HDFS，HBase，Hive的内容<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-spark-rdd">1. Spark-RDD</a></li>
<li><a href="#2-hdfs">2. HDFS</a></li>
<li><a href="#3-hive和hbase">3. Hive和HBase</a><ul>
<li><a href="#31-应用场景">3.1. 应用场景</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-spark-rdd">1. Spark-RDD</span></h1><p>&ensp;&ensp;&ensp;&ensp;RDD从表现形式上类似数据库的视图。可以理解为Java中的一个lis或者数据库中的一张表（或者视图）。Spark对RDD的操作，类似于对SQL中的一张表进行操作。但是RDD类似于数据库中的视图，而不是表，因为RDD是弹性的，就是一个RDD的数据并不一定是物理真实存在的。把一个超大的数据集，切分成N个小堆，找M个执行器(M&lt;N)，每一个执行器各自拿一块或者多块数据慢慢计算，等到计算出结果再收集在一起，这就算执行完了。那么Spark做了一项工作就是：凡是能够被我计算的，都要符合我的要求，所以spark无论处理什么数据，都要先整理成一个拥有多个分块的数据集，这个数据集就是RDD。<br>&ensp;&ensp;&ensp;&ensp;当你写一个spark程序时，比如下面的程序，获取了一个RDD，叫做<code>lines</code>，但是这个RDD不包含任何待处理的数据，真正的数据在执行时才会被加载，加载时数据要么来自spark外部，例如hdfs，要么来自spark内部，前提是你已经对它做了cache。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"wordcount"</span>).setMaster(<span class="string">"local"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">"/Users/jianhong1/testfile"</span>);</span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;Spark的计算执行可以认为是这样一个过程：在代码中创建一个RDD，但是这个RDD并不包含数据，只有等到Action算子的时候，开始计算，RDD才会加载数据。从一个RDD中读取数据，然后处理数据</p>
<h1><span id="2-hdfs">2. HDFS</span></h1><p>&ensp;&ensp;&ensp;&ensp;HDFS分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成了一个集群。这些节点分为主节点和从节点。HDFS以Master-Slave模式运行。主节点：名称节点(NameNode)，从节点：数据节点(DataNode)。其中有一个NameNode和多个DataNode。<br>&ensp;&ensp;&ensp;&ensp;NameNode有2个职责：（1）负责客户端请求的响应。（2）用于保存HDFS的元数据信息，比如文件系统的命名空间，块信息。维护着文件系统树以及整棵树所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件(Namespace image)和编辑日志文件(edit log)。FsImage用于维护文件系统的目录结构以及元数据信息，文件与数据块(block)列表的对应关系。操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作。元数据存放在fsimage中，在运行时加载到内存中（读写比较快）。操作日志写到edits中。<br>&ensp;&ensp;&ensp;&ensp;DataNode的职责是：HDFS有多个DataNode。（1）存储管理用户的文件块数据。（2）定期（默认1h）向NameNode汇报自身所持有的block信息(通过心跳信息上报)。当NameNode长时间没有收到DataNode-n的心跳信息，则认为DataNode-n不可用。DataNode提供真实文件数据的存储服务。</p>
<ul>
<li>文件块(block)：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的0偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称为一个Block。</li>
<li>Hdfs块大小如何设定？在hdfs-default.xml中设置。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span>   #block块存储的配置信息</span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   #这里的块的容量最大是128M，请注意</span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The default block size for new files, in bytes.</span><br><span class="line">      You can use the following suffix (case insensitive):</span><br><span class="line">      k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),</span><br><span class="line">      Or provide complete size in bytes (such as 134217728 for 128 MB).</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>HDFS默认Block大小是128MB，以一个256MB的文件为例，一共需要2个Block。不同于普通文件系统的是，HDFS中，如果一个文件小于数据块的大小，并不占用整个数据块存储空间。</li>
<li>Replication，默认是3，在hdfs-site.xml的dfs.replication属性。<br>使用<code>vi hdfs-site.xml</code>命令可以修改，配置文件对全局生效。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1><span id="3-hive和hbase">3. Hive和HBase</span></h1><ul>
<li>Hive：通俗地讲，Hive是构建在Hadoop之上的数据仓库。是因为（1）数据存储在HDFS上，并不是存储在Hive中，（2）数据计算时使用MapReduce。Hive是一种构建在Hadoop文件系统上的数据仓库框架，并对存储在HDFS中的数据进行分析和管理。它可以将结构化的数据文件映射成一张数据库表，并提供完整的SQL查询功能，就是把写好的hql转换为map-reduce程序操作用来查询存放在HDFS上的数据。HQL是一种类SQL语言。</li>
<li>HBase：通俗地讲，HBase可以认为是HDFS的一个包装，HBase的本质是数据存储，是一个Nosql数据库。HBase部署在HDFS之上，并且克服了HDFS在随机读写方面的缺点。HBase是一种Key/Value系统，它运行在HDFS之上。和Hive不一样，HBase能够在它的数据库上实时运行，而不是运行MapReduce任务。</li>
</ul>
<h2><span id="31-应用场景">3.1. 应用场景</span></h2><p>&ensp;&ensp;&ensp;&ensp;Hive用于对一段时间内的数据进行分析查询。例如，用于计算趋势或者网站的日志。<strong>Hive不适合用来进行实时查询，因为它需要很长时间才可以返回结果</strong>。Hive本身不存储和计算数据，它完全依赖HDFS和MapReduce，Hive中的表只是逻辑的表。Hive很适合数据仓库的统计分析，Hive的最佳使用场合是大数据集的批处理作业。例如网络日志分析，一般按照天、周、月、年来进行数据统计。不适合实时查询。例如Hive在几百兆的数据集上执行查询一般都有分钟级别的时间延迟。因此Hive不适合低延迟的应用，例如：联机事务处理。<br>&ensp;&ensp;&ensp;&ensp;<strong>HBase非常适合用来进行大数据的实时查询</strong>。Facebook用HBase进行消息和实时的分析。HBase中的表是物理表，不是逻辑表，提供一个超大的内存Hash表，搜索引擎通过它来存储索引，方便查询操作。<br>&ensp;&ensp;&ensp;&ensp;HDFS作为底层存储，是存储文件的系统，而HBase负责组织文件。Hive需要用到HDFS存储文件，需要用到MapReduce计算框架。</p>
]]></content>
      <categories>
        <category>分布式平台</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Spark</tag>
        <tag>HDFS</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>各类工具的使用</title>
    <url>/2019/03/14/%E5%90%84%E7%B1%BB%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;在做项目的过程中，学习到很多的东西，在这里记录下来。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-git">1. Git</a></li>
<li><a href="#2-使用gpu服务器">2. 使用GPU服务器</a><ul>
<li><a href="#21-查看当前gpu的占用情况">2.1. 查看当前GPU的占用情况</a></li>
<li><a href="#22-查看当前的进程是谁的">2.2. 查看当前的进程是谁的</a></li>
<li><a href="#23-查看某个用户的进程">2.3. 查看某个用户的进程</a></li>
</ul>
</li>
<li><a href="#3-screen相关命令">3. Screen相关命令</a><ul>
<li><a href="#31-新建窗口">3.1. 新建窗口</a></li>
<li><a href="#32-会话分离">3.2. 会话分离</a></li>
<li><a href="#33-恢复会话窗口">3.3. 恢复会话窗口</a></li>
<li><a href="#34-kill会话窗口">3.4. kill会话窗口</a></li>
<li><a href="#35-清除死去的窗口">3.5. 清除死去的窗口</a></li>
<li><a href="#36-解除窗口占用">3.6. 解除窗口占用</a></li>
</ul>
</li>
<li><a href="#4-使用jupyter运行程序">4. 使用jupyter运行程序</a></li>
<li><a href="#5-部署代码到服务器">5. 部署代码到服务器</a></li>
<li><a href="#6-tomcat端口占用">6. Tomcat端口占用</a></li>
<li><a href="#7-修复集群">7. 修复集群</a></li>
<li><a href="#8-运行spark">8. 运行Spark</a></li>
<li><a href="#9-使用vscode">9. 使用vsCode</a></li>
<li><a href="#10-字节">10. 字节</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-git">1. Git</span></h1><p>&ensp;&ensp;&ensp;&ensp;Git是一个用来版本控制的软件，在用的过程中涉及到以下内容</p>
<ol>
<li>在服务器上创建仓库。首先在服务器上创建一个文件夹，例如test。cd到这个文件夹下，使用<code>git init</code>命令，将该文件夹初始化为一个git仓库，会看到这个文件夹下多了一个.git的文件，然后我们把项目代码拷贝到test这个文件夹下。</li>
<li>在自己的电脑上下载并安装Git软件</li>
<li>从服务器git仓库上把代码clone到本地电脑。首先在自己电脑上，创建一个保存代码的目录，在这个目录下右键选择<code>Git Bash</code>，输入下面的代码<code>git clone username@172.11.11.111:/file0/repo/EQ/</code>，这样服务器git仓库中的东西就完封不动的clone到本地了。</li>
<li>在自己电脑上打开eclipse或者myeclipse，在file中导入这个项目，就可以进行修改代码了。</li>
<li>pull和push。右键选中这个工程，点击<code>team</code>，就可以对项目进行pull和push。当在本地修改完代码，本地的git会记录下来这些改动，所以本地文件夹中的git也有原先的历史记录，当push的时候会读取本地的git，找到本地的代码在服务器上的源，然后把改动同步到服务器上的git中。 </li>
<li>除了使用eclipse中的team来pull和push代码，还可以使用命令来进行pull或push。每次改完代码提交的时候，打开EQ这个目录（一定要在这个目录下，因为只有这个目录有git这个文件夹），右键<code>Git Bash Here</code>输入以下命令:<br><code>git add –all</code>,<br><code>git commit –m &quot;修改说明&quot;</code>,<br><code>git push</code></li>
<li>每次写代码之前，先进行pull。也是打开EQ这个目录，右键<code>Git Bash Here</code>输入以下命令:<code>git pull</code>这样就会把别人改动过的代码更新到EQ这个文件夹下。</li>
</ol>
<p><strong>问题1</strong>：在eclipse中右键点击team—&gt;show in History，会出现所有的改动。点击改动的一个文件，应该会出现2个窗口，一个窗口显示现在的代码，一个窗口显示原先的代码，可以明显的看出哪里进行了改动，但是我的电脑确没有出现历史窗口，只有现在代码的窗口。<br><strong>解决：</strong> 在点击show in history之后右键一个文件，点击“compare with previous version”就会出现历史窗口。</p>
<p><strong>问题2</strong>：在每次写代码之前都要先pull一下，把别人push的代码拉取到本地。但有时候你在写代码之前忘了pull别人的代码，直接在本地电脑上修改，然后你在pull别人的代码时会出现conflict，那是因为你和服务器上的代码修改了同一个地方，git不知道该采用谁的修改。<br><strong>解决:</strong> 首先在eclipse中，右键点击team，把你自己本地的代码先commit，不要push。然后打开vscode，导入和eclispe中一样的代码（和eclipse导入的代码在同一个文件下，带有.git），然后在vscode中点击左侧的第三个图标(git的图标)，然后在<code>...</code>中右键从服务器上pull到本地。然后再点击左侧的第一个图标，在这里面如果文件的右侧出现M，说明这个文件被修改过，然后你在这里面进行解决冲突。是采用本地的修改还是采用服务上的修改。等到冲突都解决时保存。然后再回到eclispe，会发现项目已经没有报错了。这时在eclispe中右键team，把代码commit and push到服务器中。</p>
<h1><span id="2-使用gpu服务器">2. 使用GPU服务器</span></h1><p>&ensp;&ensp;&ensp;&ensp;涉及到2个软件：Putty是一个ssh工具。FileZilla用户和服务器进行文件传输。</p>
<h2><span id="21-查看当前gpu的占用情况">2.1. 查看当前GPU的占用情况</span></h2><p><code>nvidia-smi</code><br><code>gpustat -i 2</code>:其中<code>-i 2</code>是可选的，表示每2秒刷新一次。</p>
<h2><span id="22-查看当前的进程是谁的">2.2. 查看当前的进程是谁的</span></h2><p><code>ps -ef | grep 进程号</code><br><code>kill -9 进程号</code></p>
<h2><span id="23-查看某个用户的进程">2.3. 查看某个用户的进程</span></h2><p><code>ps -ef|grep WangBeibei</code></p>
<h1><span id="3-screen相关命令">3. Screen相关命令</span></h1><h2><span id="31-新建窗口">3.1. 新建窗口</span></h2><p>新建一个窗口有3种方法：</p>
<ul>
<li>screen #这样就可以新建窗口，进入到一个窗口中，但是这样窗口就没有名字，无法区分他们</li>
<li>screen -S name #这样新建一个名字为name的窗口，并入到该窗口中<br>例如：screen -S count 新建了一个叫count的窗口并进入</li>
<li>screen command #这样新建一个窗口并在窗口中执行command，同样没有名字<br>例如：screen python ./a.py 新建并执行a.py程序</li>
</ul>
<h2><span id="32-会话分离">3.2. 会话分离</span></h2><p>我们在一个窗口运行某个程序之后，想退出登录关闭terminal干点别的事，让程序在后台运行。这时就需要和窗口会话分离,有2种方式：</p>
<ul>
<li>在当前会话窗口中按Ctrl+a +d快捷键可以实现分离，这时窗口会弹出[detached]的提示，并回到主窗口。</li>
<li>screen -d name #远程detach某个session,前提是已经跳出了name窗口  </li>
</ul>
<h2><span id="33-恢复会话窗口">3.3. 恢复会话窗口</span></h2><p>首先查看有哪些窗口正在运行</p>
<blockquote>
<p>screen -ls #列出窗口列表</p>
<p>There is a screen on:<br>2637.count (12/17/2015/10:00:32 AM) (Detached)</p>
<p>screen -r 2637 #进入2637线程，恢复count会话窗口</p>
</blockquote>
<p>这样就能回到count窗口了  </p>
<h2><span id="34-kill会话窗口">3.4. kill会话窗口</span></h2><p>如果想关掉一个多余的窗口，有3种方法：</p>
<ul>
<li><code>kill -9 threadnum</code> 例如在上面的2637，kill -9 2637 即可杀死线程，当然就杀死了窗口</li>
<li>使用<code>Ctrl a +k</code> 杀死当前窗口和窗口中运行的程序</li>
<li><code>screen -S 进程号 -X quit</code>  </li>
</ul>
<h2><span id="35-清除死去的窗口">3.5. 清除死去的窗口</span></h2><p>当窗口被杀死后，再用<code>screen -ls</code> 可以看到该窗口后面的(???dead)字样，说明窗口死了，但是仍在占用空间。这时需要清除窗口<br><code>screen -wipe #自动清除死去的窗口</code>  </p>
<h2><span id="36-解除窗口占用">3.6. 解除窗口占用</span></h2><p>有时你以为已经退出窗口了，但是<code>screen -ls</code>命令仍显示该窗口是<code>attach</code>状态，说明该窗口xxx时被占用的，可以用<code>screen -d xxxx</code>来解除占用后再进入。</p>
<h1><span id="4-使用jupyter运行程序">4. 使用jupyter运行程序</span></h1><ol>
<li>创建一个jupyter窗口<br><code>screen -S jupyter</code>   </li>
<li>激活虚拟环境并启动jupyter<br><code>source activate insis_template_3.6</code>激活环境<br><code>jupyter notebook</code>打开jupyter  </li>
<li><p>端口映射<br>第一次使用jupyter notebook，需要映射端口号，默认jupyter notebook的端口号是8888，但是在这个集群上，如果别人已经把8888端口占用了，集群会自动给你分配一个端口号，然后在putty中映射一下这个端口，具体操作如下：<br>在菜单栏选中change setting，找到Tunnels</p>
<p><img src="/2019/03/14/各类工具的使用/putty.png" alt=""><br><img src="/2019/03/14/各类工具的使用/port.png" alt=""></p>
</li>
<li><p>浏览器中打开</p>
</li>
</ol>
<p>使用screen创建一个窗口运行jupyter notebook程序的好处：就算ssh和28号服务器的连接断开，jupyter notebook的程序依然可以在后台运行。如果你打开的jupyter notebook的程序运行完了，有3种关闭程序的方法：<br>1、在jupyter notebook菜单栏，有一个close and hot的按钮。<br>2、在jupyter notebook中running中shutdown掉程序<br>3、screen -r 23560切入到虚拟窗口，然后在这个窗口ctrl+c<br>关闭jupyter notebook进程。不用使用exit，因为使用exit是关闭<br>虚拟窗口，直接按shift+a+d从虚拟窗口中切出，这样这个窗口</p>
<h1><span id="5-部署代码到服务器">5. 部署代码到服务器</span></h1><ol>
<li>首先在服务器上安装Tomcat</li>
<li>使用FileZilla进入到服务器，使用putty进入到服务器。</li>
<li>关闭Tomcat：使用putty cd到到file0/apache-tomcat下，输入bin/shutdown.sh关掉Tomcat服务器。然后把webapps下的项目删除，进入到logs删除里面的日志。</li>
<li>使用eclipse把项目导出成war压缩包</li>
<li>使用FileZilla把war上传到webapps下。</li>
<li>使用bin/startup.sh启动Tomcat服务器</li>
<li>查看日志主要在catalina.out这个文件中</li>
<li>浏览器查看，<a href="http://hz1:8080/EQ/jsp/login.jsp" target="_blank" rel="noopener">http://hz1:8080/EQ/jsp/login.jsp</a></li>
</ol>
<h1><span id="6-tomcat端口占用">6. Tomcat端口占用</span></h1><ol>
<li>win + R,输入cmd回车进入dos界面 </li>
<li>输入netstat -ano|findstr 8080 查看占用8080端口的进程 </li>
<li>输入taskkill /pid 10148 /f 将显示的进程号（我的是10148）结束掉 </li>
<li>重启tomcat</li>
</ol>
<h1><span id="7-修复集群">7. 修复集群</span></h1><p>当集群中有个节点不能启动时，可以在虚拟机管理界面把虚拟机启动。<br>在浏览器中输入<a href="https://172.31.43.150" target="_blank" rel="noopener">https://172.31.43.150</a><br>点击“登录到vsphere web client”<br>用户名：xxxx，<br>密码：<strong>*</strong>，<br>点击右侧的“vCenter清单列表”，然后点击左侧的“虚拟机”，在左侧找到需要重启的虚拟机，单击选中，在右侧“关闭虚拟机电源”，关闭后再点击“打开虚拟机电源”。这样就把虚拟机重启了。还需要启动Cloudera manager agent。使用putty，例如输入hz5，<br>进入到ssh，用户名xxx，密码<em>**</em>。<br>先和server同步时间ntpdate hz1，<br>然后启动agent：/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start</p>
<p>平时就可以使用/vsphere-client/?csp#extensionId%3Dvsphere.core.viVms.domainView</p>
<h1><span id="8-运行spark">8. 运行Spark</span></h1><p>不论是用什么语言写的spark程序在运行的时候都要用spark-submit来运行<br>如果是java程序需要指定—class 主程序的报名.类名<br>例如—class test.Test<br>python不需要指定—class这个参数</p>
<ol>
<li>local模式：spark-submit —class test.Test —master local[2] mySpark.py</li>
<li>standalone模型： spark-submit —class test.Test —master spark://hz4:7077 mySpark.py   </li>
<li>yarn模式： spark-submit —class test.Test —master yarn mySpark.py</li>
</ol>
<h1><span id="9-使用vscode">9. 使用vsCode</span></h1><p>&ensp;&ensp;&ensp;&ensp;使用vscode编写python程序时，如果出现空格和Tab共用时导致程序出错。需要把所有的Tab换成空格，在vscode最下面有一个蓝色的条纹，在条纹的右边有”空格4”，点击这个字，然后在vscode的最上面会出现几个选项：点击“将缩进转换成空格”这个选项，再运行就没错了。  </p>
<h1><span id="10-字节">10. 字节</span></h1><p>&ensp;&ensp;&ensp;&ensp;int是4个字节，无论数字多长都是4个字节，如果用string定义数字时，字节的长度就是字符的个数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">11111111</span>;</span><br><span class="line">System.out.println(a+<span class="string">"(int)的字节数："</span>+Bytes.toBytes(a).length);</span><br><span class="line"><span class="keyword">int</span> b=<span class="number">22</span>;</span><br><span class="line">System.out.println(b+<span class="string">"(int)的字节数："</span>+Bytes.toBytes(b).length);</span><br><span class="line">String aString = <span class="string">"11111111"</span>;</span><br><span class="line">System.out.println(aString+<span class="string">"(string)的字节数："</span>+Bytes.toBytes(aString).length);</span><br><span class="line">String bString = <span class="string">"22"</span>;</span><br><span class="line">System.out.println(bString+<span class="string">"(string)的字节数："</span>+Bytes.toBytes(bString).length);</span><br></pre></td></tr></table></figure>
<p>输出如下所示：<br><img src="/2019/03/14/各类工具的使用/bytes.png" alt=""></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Git</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention</title>
    <url>/2019/03/12/Attention/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;主要介绍Attention的机制，以及在NLP中的应用。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-attention">1. Attention</a></li>
<li><a href="#2-attention%e7%9a%84%e8%ae%a1%e7%ae%97%e5%85%ac%e5%bc%8f">2. Attention的计算公式</a></li>
<li><a href="#3-attention%e5%9c%a8nlp%e7%9a%84%e5%ba%94%e7%94%a8">3. Attention在NLP的应用</a></li>
<li><a href="#4-attention%e5%88%86%e7%b1%bb">4. Attention分类</a><ul>
<li><a href="#41-soft-attenion">4.1. Soft Attenion</a></li>
<li><a href="#42-hard-attention">4.2. Hard Attention</a></li>
<li><a href="#43-global-attention">4.3. Global Attention</a></li>
<li><a href="#44-local-attention">4.4. Local Attention</a></li>
<li><a href="#45-self-attention">4.5. Self Attention</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-attention">1. Attention</span></h1><p><a href="https://blog.csdn.net/BVL10101111/article/details/78470716" target="_blank" rel="noopener">Attentions详细讲解</a><br>Attention一般有2种，（1）：Location-based Attention，这里的attention没有其他额外需要关注的对象，即多个$h_i$内部做attention。（2）Concatenation-based Attention：有额外需要关注的对象，即多个$h_i$对$h_t$的attention。我们平时用第2种比较多一些。</p>
<h1><span id="2-attention的计算公式">2. Attention的计算公式</span></h1><p>&ensp;&ensp;&ensp;&ensp;在计算Attention分数时，有很多中计算方式，包括addictive，concat，dot product。其中涉及点积运算的评分函数的思路是度量2个向量间的相似度。  以下展示了6种计算Attention分数的方法：</p>
<p><img src="/2019/03/12/Attention/function.png" alt=""></p>
<ol>
<li>Additive/Concat:将2个向量拼接在一起，然后输入到全连接中，得到attention分数，$score=W[h_i:h_j]$  </li>
<li>Dot product:2个向量点积计算attention score，$score = h_i . h_j^T$</li>
<li>Scaled dot product:缩放点积，$score = \tfrac{h_i . h_j^T}{\sqrt{n}}$</li>
<li>Location-based Attention:这里Attention涉及的对象只有1个，自己和自己Attention计算得到分数。</li>
<li>Cos：2个向量Cos运算</li>
<li>General<br><img src="/2019/03/12/Attention/function_score.png" alt=""></li>
</ol>
<h1><span id="3-attention在nlp的应用">3. Attention在NLP的应用</span></h1><p>&ensp;&ensp;&ensp;&ensp;在上面的编码器-解码器中，从编码器传到解码器的背景变量$c$是不变的。就是说解码器在翻译第一个词和第二个词是c是不变的。但是实际情况中，比如英语they are watching。翻译成法语是：IIs regardent。比如在翻译IIS时，和they are更相关，在翻译regardent和watching更相关，所以希望把背景变量$c$设置成一个变化的值。当翻译IIS时，对编码器的隐藏变量$h_1,h_2$更看重，当翻译regardent对隐藏变量$h_3$更看重，所以就需要在解码器中，在不同时间步时，对编码器的隐藏变量$h_1,h_2,h_3$分配不同的权重，加权平均得到背景变量$c$。</p>
<p>&ensp;&ensp;&ensp;&ensp;原先解码器隐藏层变量的计算是上一时刻的输出$y_{t’-1}$，上一时刻的隐藏状态$s_{t’-1}$以及背景变量$c$，在加入attention机制后，这里的背景变量变成了$c_{t’}$，每一步的背景变量都不一样。  </p>
<p><img src="/2019/03/12/Attention/st.png" alt=""><br><img src="/2019/03/12/Attention/attention.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;下面看一下$c_{t’}$是怎么设计的。 就是编码器的不同时刻的隐藏状态的加权平均。只是这里的权重$\alpha_{t’t}$在每一个时刻是一个变化的值。  注意这里的$t’$是输出(解码器)的时间戳，$t$是输入(编码器)的时间戳。首先我们先固定$t’$，下面的式子中$t’$是不变的。在计算$c_{t’}$时，变化$t从1到T$，遍历所有的$h_t$，然后给定一个$h_t$，怎么求$h_t$对应的权重$\alpha_{t’t}$。</p>
<p><img src="/2019/03/12/Attention/ct.png" alt=""></p>
<p>例如计算解码器在第$t’$个时间步的背景向量$t’$，使用解码器$t’-1$个时间步的隐藏状态$S_{t’-1}$作为query，将编码器所有时间步的隐藏状态$h_0,…h_t$作为key和value，先将$S_{t’-1}$和$h_0,…h_t$做Attention，然后将Attention score使用$softmax$归一化，再和$h_0,…h_t$加权求和，得到解码器中第$t’$个时间步的背景向量$c_{t’}$</p>
<p><img src="/2019/03/12/Attention/nlp.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;下面我们看一下$\alpha_{t’t}$是怎么来表示。加权平均就要使所有的权值加起来为1，所以用到softmax运算。softmax中的每一个值是<br>$e$,这个是怎么计算的.$e_{t’t}$通过解码器上一时刻的隐藏变量$s_{t’-1}$和编码器所有时间步的隐藏变量$h_t$计算得到。</p>
<p><img src="/2019/03/12/Attention/alpha.png" alt=""></p>
<p><img src="/2019/03/12/Attention/e.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;注意力机制对函数$a$设计有很多。下面是一种设计方法。首先一定要有$s_{t’-1}$和编码器所有时间步的隐藏状态$h_t$。然后引入了3个模型参数$v^T,W_s,W_h$,这3个参数通过训练得到。对$s_{t’-1}$做一个projection，对$h_t$做一个projection，使得projection之后的向量长度相等，这样就可以加在一起，然后使用tanh激活函数，这时的向量长度还是projection之后的长度，但是我们希望$e_{t’t}$是一个标量，那就再引入向量$v^T$，和右边的向量做一个点乘得到一个标量。其实注意力可以通过多层感知机（全连接层）得到。 </p>
<p><img src="/2019/03/12/Attention/a.png" alt=""></p>
<p>下面介绍计算解码器的隐藏变量时的函数$g$是什么？g可以看到是一个GRU单元。</p>
<p><img src="/2019/03/12/Attention/g.png" alt=""></p>
<p>其中$y_{t’-1}$是解码器上一时间步的输出，可以看做当前时间步的输入，$s_{t’-1}$是解码器上一时间步的隐藏状态，$c_{t’}$是当前时间步的背景向量。<br>这里总结一下模型的参数都有哪些：编码器中的W和b，上式中解码器中的W和b，还有计算attention中的$v^T,W_s,W_h$。  </p>
<h1><span id="4-attention分类">4. Attention分类</span></h1><h2><span id="41-soft-attenion">4.1. Soft Attenion</span></h2><p>加权求和，可以求导，可以放到模型中训练，用的比较多</p>
<h2><span id="42-hard-attention">4.2. Hard Attention</span></h2><p>Hard attention 是一个随机的过程，Hard attention不会选择整个encoder的隐层输出作为其输入，Hard Attention会依赖概率$𝑆_𝑖$来采样输入端的隐状态的一部分来进行计算，而不是整个encoder 的隐状态。由于其是一个随机过程，所以不能直接求导，为了实现梯度的反向传播，需要采用蒙特卡洛采样等方法来估算模块的梯度。  </p>
<h2><span id="43-global-attention">4.3. Global Attention</span></h2><p>Global attention和传统的attention model是一样的，所有的hidden state都被用于计算Context vector的权重，其长度等于Encoder输入的长度。<br>缺点：encoder很长时计算量大</p>
<h2><span id="44-local-attention">4.4. Local Attention</span></h2><p>并不是使用Encoder中所有时间步的隐藏状态做Attention，只使用一部分和decoder做Attention。  </p>
<h2><span id="45-self-attention">4.5. Self Attention</span></h2><p>Q,K,V都是自己本身，即自己对自己的注意力机制。有以下优点：</p>
<ol>
<li>可以捕获同一个句子中单词之间的句法特征和语义特征。</li>
<li>更容易捕获句子中长距离的相互依赖特征，而RNN则需要依次按顺序计算，捕获的可能性降低。</li>
<li>self attention 计算中将句子中任意两个单词通过一个计算步骤连接，远距离依赖之间的距离被极大缩短。</li>
<li>对于增加计算的并行性有直接的帮助作用。</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</title>
    <url>/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;今天实验室分享了一篇2019年AAAI的论文：<a href="http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf" target="_blank" rel="noopener">Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand</a>。讲的是通过时空多图卷积来进行打车需求的预测。<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#1-%e4%bb%bb%e5%8a%a1">1. 任务</a></li>
<li><a href="#2-%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86">2. 数据处理</a><ul>
<li><a href="#21-neighborhood">2.1. Neighborhood</a></li>
<li><a href="#22-functional-similarity">2.2. Functional similarity</a></li>
<li><a href="#23-transportation-connectivity">2.3. Transportation connectivity</a></li>
</ul>
</li>
<li><a href="#3-st-mgcn%e6%a8%a1%e5%9e%8b">3. ST-MGCN模型</a><ul>
<li><a href="#31-temporal-correlation-modeling">3.1. Temporal correlation modeling</a></li>
<li><a href="#32-multi-graph-convolution">3.2. Multi-Graph Convolution</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p>&ensp;&ensp;&ensp;&ensp;上学期实验室也分享论文，但是分享之后就忘了。所以决定从这学期开始，对一些我觉得对我有帮助的论文记录下来</p>
<p>&ensp;&ensp;&ensp;&ensp;这篇论文和以前读到的图卷积的论文的不同是：这篇论文构建了多个图。<br>&ensp;&ensp;&ensp;&ensp;在介绍这篇论文之前，先写出自己以前一直没有弄清楚的一个点。<strong>一个图有2个矩阵，一个是邻接矩阵A，一个是图信号矩阵X。邻接矩阵A表示的是这个图的结构，节点与节点之间的连接关系。图信号矩阵表示这个图中每个节点的特征。假设一个图有100个节点，每个节点有3个特征，那么邻接矩阵A的维度是100x100，图信号矩阵X的维度是100x3。在构造下面的3个不同的图时，只是邻接矩阵A不同，图信号矩阵都是一样的。在计算图卷积的时候会同时用到图的邻接矩阵和图信号矩阵。</strong></p>
<h1><span id="1-任务">1. 任务</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文的任务就是一个区域$T$个时间段的特征(可能有1个特征即打车订单数，也可有多个特征)预测第$T+1$时刻的特征值。</p>
<h1><span id="2-数据处理">2. 数据处理</span></h1><p>&ensp;&ensp;&ensp;&ensp;首先这篇论文将一个城市进行网格划分，一个网格表示一个区域region。然后对这些网格构建图。<strong>多图就体现在构建图上，原先的论文是构建一个图，这篇论文构建了3个图</strong>。论文的任务是打车需求量预测。论文中说一个region的打车需求可能和以下3个方面有关系：邻近区域neighborhood，和该区域功能相近的区域，和该区域道路可达的区域。所以论文根据以上3种关系分别构建了3个图。</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/道路图.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;例如上图所示，和region1相邻的区域是region2，和region1功能相似的是region3，和region1交通可达的是region4。</p>
<h2><span id="21-neighborhood">2.1. Neighborhood</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的邻居图用$\mathcal{G}_N=(V,A_N)$表示，其中V表示所有的区域，$A_N$表示区域与区域之间的邻居关系。构建的图一个节点表示一个region，2个区域有边表示这2个区域是邻居，没有边相连表示不是邻居。这样就可以从原始的图中根据邻近关系构建出一张图</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/邻居图.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;怎么判断两个区域是否相邻？首先把一个city划分成grid，一个区域周围的8个区域就是这个区域相邻，那么$A_{N,ij}=1$,否则为0。这样就构建了一个大小为$\mathbb{R}^{|V|\times|V|}$的矩阵$A$。</p>
<h2><span id="22-functional-similarity">2.2. Functional similarity</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的功能相似图用$\mathcal{G}_N=(V,A_S)$表示，其中V表示所有的区域，$A_S$表示区域与区域之间的功能相似关系。构建的图一个节点表示一个region，2个区域有边表示这2个区域之间的相似关系，是一个[0,1]之间的数。计算两个区域之间的相似关系：首先需要知道每个区域的POI向量。POI向量是这个区域在某个类别的POI的个数，例如POI有5类，分别是：学校、公园、医院、商场、居民区，这种区域有1个学校，2个公园、0个医院、3个商场、1个居民区，则POI向量为[1,2,0,3,1]，计算两个region的功能相似性即计算两个POI向量的相似性。</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/功能图.png" alt=""></p>
<h2><span id="23-transportation-connectivity">2.3. Transportation connectivity</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的交通可达图用$\mathcal{G}_N=(V,A_C)$表示，其中V表示所有的区域，$A_C$表示区域与区域之间的可达关系。构建的图一个节点表示一个region，2个区域可达判断的依据是通过OpenStreetMap，看两个区域之间是否有highway，subway，motorway，如果有的话就表示这两个区域交通可达。如果两个<br>region交通可达，则conn($v_i,v_j$)=1,否则为0。然后需要注意的是这里减去了邻居关系。因为考虑了交通可达，不想再次考虑邻居关系。取max为了保证没有负值。</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/可达图.png" alt=""></p>
<h1><span id="3-st-mgcn模型">3. ST-MGCN模型</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文提出的模型是$spatiotemporal \quad   multi-graph \quad convolution \quad network\quad(ST-MGCN)$。首先输入是原始数据，一个城市所有区域的特征，不同的层表示不同的时间，那么输入$X\in\mathbb{R}^{T\times|V|\times{P}}$，每一个时刻有一个图信号矩阵，维度为$|V|\times{P}$，一共有$T$个时刻。然后从原始数据中提取出3张图，分别表示region之间的邻居图、功能相似图、交通可达图。这三张图的维度也是$\mathbb{R}^{T\times|V|\times{P}}$，只是这3张图的邻接矩阵不同。然后经过$contextual \quad   gated \quad recurrent \quad neural\quad nwtwork\quad (CGRNN)$对时间进行建模，然后分别输出一张图，一共输出3张图。然后再用multi-graph convolution对空间进行建模，最终输出一张图，表示下一时刻各个节点的特征</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/模型图.png" alt=""></p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/模型图解释.png" alt=""></p>
<h2><span id="31-temporal-correlation-modeling">3.1. Temporal correlation modeling</span></h2><p>&ensp;&ensp;&ensp;&ensp;提出$Contextual \quad Gated \quad Recurrent \quad Neural \quad Network \quad (CGRNN)$来对不同时刻的图信号进行建模。在一个城市的打车需求，在某一个时刻，可以记录每个节点(region)的特征，因为是时间序列数据，就有了时间的维度，这样就可以形成T个图信号。CGRNN在对时间建模时引入了上下文信息contextual information。其中contextual information指的是邻近区域的信息，通过图卷积GCN来获取。模型框架图如下所示：</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/temporal.png" alt="时间模型"></p>
<p>&ensp;&ensp;&ensp;&ensp;有T个时刻，第$t$个时刻的观察值是$X^{(t)}\in\mathbb{R}^{|V|\times{P}}$，其中$P$表示每个节点的特征维度，如果$P=1$表示只有一个特征，仅包含region的打车订单数。  </p>
<p>&ensp;&ensp;&ensp;&ensp;按照上面的模型图，执行的顺序是：<br>（1）先把原始的图$[X^{(t)},X^{(t+1)},…]\in\mathbb{R}^{T\times|V|\times{P}}$经过一个池化层，把一个特征在所有区域的特征值取平均，这样就变成维度为$\mathbb{R}^{T\times1\times{P}}$，与此同时对原始的图信号$[X^{(t)},X^{(t+1)},…]\in\mathbb{R}^{T\times|V|\times{P}}$中的每个时刻的图信号$\mathbb{R}^{|V|\times{P}}$进行卷积操作，K=K‘，表示得到一个结合K`阶邻居信息图$\mathbb{R}^{|V|\times{P}}$，T个时刻共得到T个$\mathbb{R}^{|V|\times{P}}$,即$\mathbb{R}^{T\times|V|\times{P}}$，再经过一个池化操作，得到一个$\mathbb{R}^{T\times1\times{P}}$，然后把2个$\mathbb{R}^{T\times1\times{P}}$进行拼接concatenate，得到一个$\mathbb{R}^{T\times1\times{2P}}$，每个region的特征维度变成2P，前一个P表示的是自己的特征，后一个P表示的整合邻居后的特征。<br>上述的操作在论文中的式子如下：</p>
<script type="math/tex; mode=display">\hat{X}^{(t)}=[X^{(t)},F_\mathcal{G}^{K`}(X^{(t)})] \quad for \quad t=1,2,...T \tag{6}</script><script type="math/tex; mode=display">\mathcal{z}^{(t)}=F_{pool}(\hat{X}^{(t)})=\frac{1}{|V|}\sum_{i=1}^{|V|}\hat{X}_{i,;}^{(t)} \quad for \quad t=1,2...T \tag{7}</script><p>论文中的式子是先拼接成$\mathbb{R}^{T\times|V|\times{2P}}$，然后再池化成$\mathbb{R}^{T\times1\times{2P}}$，但是模型图上是先分别池化成$\mathbb{R}^{T\times1\times{P}}$，再拼接成$\mathbb{R}^{T\times1\times{2P}}$<br>（2）然后经过2个全连接层，得到关于时间的attention,s的维度是$s\in\mathbb{R}^T$</p>
<script type="math/tex; mode=display">s=\sigma(W_2\delta(W_1\mathcal{z}))\tag{8}</script><p>（3）将s和原始图信号进行内积，得到缩放后的图信号，维度为$\mathbb{R}^{T\times|V|\times{P}}$。</p>
<script type="math/tex; mode=display">\widetilde{X}^{(t)}=X^{(t)}\circ s^{(t)} \quad for\quad t=1,2...T\tag{9}</script><p>（4）最后经过一个共享的RNN神经网络。对于每一个节点，在每一个时刻会有一个P个特征值，经过T个时刻，会形成一个$\mathbb{R}^{T\times{P}}$的矩阵，一共有V个节点，对每一个节点的时间序列都应用这个RNN，最后每个节点都会输出一个隐藏变量$H_i$,V个节点会形成V个隐藏变量，最终会输出一个$\mathbb{R}^{|V|\times{P}}$的矩阵。</p>
<script type="math/tex; mode=display">H_{i,:}=RNN(\widetilde{X}_{i,:}^{(1)},...,\widetilde{X}_{i,:}^{(T)};W_3) \quad for \quad i=1,2...|V|\tag{10}</script><p>&ensp;&ensp;&ensp;&ensp;一共有三个图，所以经过CGRNN会输出3个$\mathbb{R}^{|V|\times{P}}$的矩阵,这时的邻接矩阵还是输入图的A。</p>
<h2><span id="32-multi-graph-convolution">3.2. Multi-Graph Convolution</span></h2><p>&ensp;&ensp;&ensp;&ensp;对时间建模完成之后，接下来使用Multi-graph Convolution对空间进行建模。</p>
<script type="math/tex; mode=display">X_{l+1}=\sigma(\bigsqcup_{A\in\mathbb{A}}f(A;\theta_i)X_lW_l)</script><p>其中$\bigsqcup$是聚合函数，可以是sum，avg，max等，其中$f(A;\theta_i)$是ChebNet卷积核，是$\sum_{k=0}^K\theta_kL^k$,卷积运算可以写成$g_{\theta}*x=(\sum_{k=0}^K\theta_kL^k)x$，其中$L_{ij}^k=0$表示节点$i$到节点$j$可以在k跳到达，$k$定义了图卷积的感受野。以road connectivity为例，$\mathcal{G}_C=(V,A_C)$中，拉普拉斯矩阵L由邻接矩阵A计算得到，在道路连通图中的邻接矩阵$A_{C,1,4}=1;A_{C,1,6}=0;A_{C,4,6}=1$表示region1和4道路连通，region1和6道路不连通，region4和6道路连通。拉普拉斯矩阵L中，对角线表示这个节点的度，其余值不为0表示这个节点的一阶邻居，道路连通图的拉普拉斯矩阵$L_{C,1,4}^1\ne0;L_{C,1,6}^1=0;L_{C,4,6}^1\ne0$。如果在ChebNet中K=1，则对于region1这个节点来说，经过一次卷积运算后，下一层的输出region1这个节点新的特征中只包含了region1的一阶邻居的信息，不包含region6的信息，因为$L_{C,1,4}^1\ne0;L_{C,1,6}^1=0$，如果ChebNet中的K=2，则region1经过一次图卷积运算，下一层region1的特征包含一阶邻居region4和二阶邻居region6的信息，因为$L_{C,1,4}^2\ne0;L_{C,1,6}^2\ne0$。</p>
<p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/chebnet.png" alt="时间模型"></p>
<p>&ensp;&ensp;&ensp;&ensp;这个图中，假设ChebNet的$K=k$,根据卷积的运算公式$g_{\theta}*x=(\sum_{k=0}^K\theta_kL^k)x$，对于上图中，把黑色的点当做中心结点，黄色的点是一阶邻居，红色的点是二阶邻居。蓝色的点是三阶邻居。首先计算$k=1$时，则对于黑色的中心结点，整合了一阶邻居的信息得到一个图信息矩阵。然后计算$k=2$时，则对于黑色的中心结点，结合一阶邻居和二阶邻居的信息得到一个新的图信号矩阵，一直到$K=k$，得到了$k$个图信号矩阵，然后把这$k$个图信号矩阵相加得到下一层黑色的中心结点的表示。如果对$X_l$中的每个节点都执行这样的操作，则经过图卷积，下一层的输出$X_{l+1}$则表示每个节点整合了邻居信息得到的一个新的图信号矩阵<br>&ensp;&ensp;&ensp;&ensp; 这样每一个图经过一个图卷积得到一个新的图信号矩阵，3个图得到3个新的图信号矩阵，然后再执行$\bigsqcup$聚合操作，在这篇论文中，$\bigsqcup$选择的是sum操作，就是说得到的3个新的图信号矩阵，直接相加就得到一个图，表示最终的预测结果。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>时空领域</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>图卷积</title>
    <url>/2019/03/03/%E5%9B%BE%E5%8D%B7%E7%A7%AF/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;看了很久关于图卷积的内容，但总觉得自己理解不深刻，在这里把自己的一些想法写出来，也算把图卷积的内容梳理一下。</p>
<p><a href="https://mp.weixin.qq.com/s/X4kWloqPb2j4AuS1Q9N4PA" target="_blank" rel="noopener">参考资料</a><br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c">1. 卷积神经网络</a></li>
<li><a href="#2-%e5%9b%be%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c">2. 图卷积神经网络</a><ul>
<li><a href="#21-%e9%a1%b6%e7%82%b9%e5%9f%9fvertex-domain">2.1. 顶点域(Vertex Domain)</a><ul>
<li><a href="#211-%e5%9b%be%e4%b8%ad%e9%a1%b6%e7%82%b9%e7%9a%84%e9%80%89%e6%8b%a9node-sequence-selection">2.1.1. 图中顶点的选择Node Sequence Selection</a></li>
<li><a href="#212-%e6%89%be%e5%88%b0%e4%b8%ad%e5%bf%83%e7%bb%93%e7%82%b9%e7%9a%84%e9%82%bb%e5%9f%9fneighborhood-assembly">2.1.2. 找到中心结点的邻域Neighborhood Assembly</a></li>
<li><a href="#213-%e5%9b%be%e8%a7%84%e8%8c%83%e5%8c%96%e8%bf%87%e7%a8%8bgraph-normalization">2.1.3. 图规范化过程Graph Normalization</a></li>
<li><a href="#214-%e5%8d%b7%e7%a7%af%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84convolutional-architecture">2.1.4. 卷积网络结构Convolutional Architecture</a></li>
<li><a href="#215-%e4%bc%aa%e4%bb%a3%e7%a0%81">2.1.5. 伪代码</a></li>
<li><a href="#216-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b">2.1.6. 算法流程</a></li>
</ul>
</li>
<li><a href="#22-%e8%b0%b1%e5%9f%9fspectral-domain">2.2. 谱域Spectral Domain</a><ul>
<li><a href="#221-%e5%9b%be%e5%8d%b7%e7%a7%af%e8%bf%90%e7%ae%97">2.2.1. 图卷积运算</a></li>
<li><a href="#222-%e7%ac%ac%e4%b8%80%e4%bb%a3%e5%9b%be%e5%8d%b7%e7%a7%afscnn">2.2.2. 第一代图卷积SCNN</a></li>
<li><a href="#223-%e7%ac%ac%e4%ba%8c%e4%bb%a3%e5%9b%be%e5%8d%b7%e7%a7%afchebnet">2.2.3. 第二代图卷积ChebNet</a></li>
<li><a href="#224-%e7%ac%ac%e4%b8%89%e4%bb%a3%e5%9b%be%e5%8d%b7%e7%a7%af">2.2.4. 第三代图卷积</a></li>
<li><a href="#225-%e5%9b%be%e5%8d%b7%e7%a7%af%e5%a4%9a%e7%a7%8d%e5%bd%a2%e5%bc%8f">2.2.5. 图卷积多种形式</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p>&ensp;&ensp;&ensp;&ensp;在介绍图卷积之前，先介绍一下卷积神经网络。</p>
<h1><span id="1-卷积神经网络">1. 卷积神经网络</span></h1><p>&ensp;&ensp;&ensp;&ensp;卷积神经网络在图像上用的比较多。因为图像的一些重要的性质是：(1)局部性；(2)稳定性；即平移不变性和有大量相似的碎片；(3)多尺度性，简单的结构组合形成复杂的抽象结构。<br>&ensp;&ensp;&ensp;&ensp;图像本来就是一个规范的形状，可以形式化成一个规则的矩阵，再定义一个卷积核，卷积核在图像矩阵上滑动，把周围的几个像素值整合成一个值，获取了图像的局部性。可能会有多个卷积核，用来识别图像中不同的特征，比如下面例子所示，第一个卷积核用来识别左右的边缘，第二个卷积核用来识别上下的边缘。</p>
<p><img src="/2019/03/03/图卷积/多个卷积核.png" alt=""></p>
<ul>
<li>在卷积神经网络中，需要训练的参数是卷积核。</li>
<li>在卷积神经网络中，卷积层后面通常跟一个池化层，防止参数越来越多。</li>
<li>卷积核的大小通常是3x3和5x5</li>
<li>池化层如果是3x3，步长为2，那么图像大小会变成原来的一半，变成原先图像的多少和步长有关。</li>
<li>在图卷积层的最后一层是全连接层，可以使用1x1的卷积核来代表全连接，比如最后池化层输出是5x5x16，表示一层是5x5，一共有16层(通道)，经过全连接变成一个400x1的向量.</li>
</ul>
<p><img src="/2019/03/03/图卷积/CNN实例.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;使用卷积神经网络对一张图片进行分类时，首先给定这张图片的特征，比如是32x32x3和这样图片的实际类别，如果是10个类，这张图片是第2个类，那么就是一个[0,1,0,…]的向量。将图片输入卷积层中，卷积核刚开始是随机初始化的，输入X和卷积核做卷积操作，经过池化层，再经过一个卷积层和池化层，然后是一个全连接层，最后一个全连接层的输出结点个数是10，表示10个类别，如果输出的结果和实际的类别有偏差，然后通过误差反向传播，更新卷积核，直到误差最小，得到模型参数：卷积核。卷积核的参数通过优化求出才能实现特征提取的作用</p>
<p><img src="/2019/03/03/图卷积/CNN训练.png" alt=""></p>
<h1><span id="2-图卷积神经网络">2. 图卷积神经网络</span></h1><p>&ensp;&ensp;&ensp;&ensp;有了卷积神经网络，为什么还要引入图卷积神经网络？<br>&ensp;&ensp;&ensp;&ensp;因为卷积神经网络处理的是规则的矩阵，像图像和视频中的像素点都是排列整齐的矩阵，也是论文中提到的欧式结构(Euclidean Structure)。  </p>
<p><img src="/2019/03/03/图卷积/欧式结构.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;但在科学研究中，还有很多非欧式结构(Non Euclidean Structure)的数据,例如社交网络、信息网络。</p>
<p><img src="/2019/03/03/图卷积/非欧式结构.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;实际上，这样的网络结构就是图论中抽象意义上的拓扑图。<br>&ensp;&ensp;&ensp;&ensp;为什么要研究GCN，原因如下三个：</p>
<ol>
<li>卷积神经网络CNN无法处理非欧式结构的数据，在非欧式结构数据中，图中每个顶点的相邻顶点个数可能不同，无法用一个同样尺寸的卷积核进行卷积运算。</li>
<li>由于卷积神经网络CNN无法处理非欧式结构的数据，但是又希望在这样的数据结构上有效地提取空间特征来进行机器学习，所以GCN成为了研究的重点。</li>
<li>拓扑结构的数据在生活中很常见，社交网络、交通领域都涉及到非欧式结构的数据。</li>
</ol>
<p>&ensp;&ensp;&ensp;&ensp;图卷积网络GCN的本质目的是提取拓扑图的空间特征。图卷积神经网络中有2种：顶点域和谱域。这是提取拓扑图空间特征的两种方式，就是给定非欧式结构数据，从中构建图的两种方法。</p>
<h2><span id="21-顶点域vertex-domain">2.1. 顶点域(Vertex Domain)</span></h2><p>&ensp;&ensp;&ensp;&ensp;提取拓扑图上的空间特征，就是把每个顶点的邻居找出来。这里的问题是:(1)按照什么条件去找中心顶点的邻居，也就是如何确定感受野？(2)确定了邻居，按照什么方式处理包含不同数据邻居的特征？<br>&ensp;&ensp;&ensp;&ensp;<a href="https://arxiv.org/abs/1605.05273" target="_blank" rel="noopener">Learning Convolutional Neural Networks for Graphs</a>是2016年在ICML中发表的一篇论文，这是<a href="http://www.matlog.net/icml2016_slides.pdf" target="_blank" rel="noopener">PPT讲解</a>。由于CNN并不能有效的处理非欧式结构数据，这篇paper的motivation就是想将CNN在图像上的应用generalize到一般的graph上面。<br>&ensp;&ensp;&ensp;&ensp;本文提到的算法思想是：将一个图结构的数据转化为CNN能够高效处理的结构。处理的过程主要分为三个步骤：(1)从图结构中选出一个固定长度具有代表性的结点序列；(2)对于选出的每一个结点，收集固定大小的邻居集合。(3)对由当前节点及其对应的邻居构成的子图进行规范化，作为卷积结构的输入。算法具体分为4个步骤。</p>
<h3><span id="211-图中顶点的选择node-sequence-selection">2.1.1. 图中顶点的选择Node Sequence Selection</span></h3><p>&ensp;&ensp;&ensp;&ensp;首先对于输入的一个Graph，指定中心顶点的个数，然后确定确定图中的中心结点，确定中心结点主要采取的方法是：centrality，也就是中心化的方法，就是越处于中心位置的点越重要。这里的中心位置不是空间上的概念，应该是度量一个点的关系中的重要性的概念，简单的举例说明。如图5当中的两个图实际上表示的是同一个图，对其中红色标明的两个不同的nodes我们来比较他们的中心位置关系。比较的过程当中，我们计算该node和其余所有nodes的距离关系。我们假设相邻的两个node之间的距离都是1。    </p>
<p><img src="/2019/03/03/图卷积/2个node.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;那么对于左图的红色node，和它直接相连的node有4个，因此距离+4；再稍微远一点的也就是和它相邻点相邻的有3个，距离+6;依次再相邻的有3个+9；最后还剩下一个最远的+4；因此我们知道该node的总的距离为23。同理我们得到右边的node的距离为3+8+6+8=25。那么很明显node的选择的时候左边的node会被先选出来。</p>
<p>&ensp;&ensp;&ensp;&ensp;当然，这只是一种node的排序和选择的方法，其存在的问题也是非常明显的。Paper并没有在这次的工作当中做详细的说明。</p>
<h3><span id="212-找到中心结点的邻域neighborhood-assembly">2.1.2. 找到中心结点的邻域Neighborhood Assembly</span></h3><p>&ensp;&ensp;&ensp;&ensp;选出目标node之后，我们之后就要为目标node确定感受野大小。但是在确定之前，我们先构建一个candidate set，然后在从这个candidate set中选择感受野的node。这些感受野的candidate set，称为目标node的neighborhood。如下图所示，为每个目标node选择至少4个node（包括自己，k即为感受野node的个数）。接下来对选出来的每一个中心结点确定一个感受野receptive filed，以便进行卷积操作。但是在这之前，首先找到每个结点的邻域区域（neighborhood filed），然后再从当中确定感受野当中的结点。假设感受野的大小为k，那么对于每个结点会有2种情况：邻域结点的个数不够k个，或者邻域结点个数大于k个。</p>
<p><img src="/2019/03/03/图卷积/选邻居.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;如上图所示，选出6个中心结点，对于每个中心结点，首先找到与其直接相邻的结点(被称为1阶邻居)，如果还不够再增加2阶邻居，那么对于1阶邻居已经足够的情况下，先全部放在候选 的区域中，在下一步中通过规范化做最终的选择。</p>
<h3><span id="213-图规范化过程graph-normalization">2.1.3. 图规范化过程Graph Normalization</span></h3><p>&ensp;&ensp;&ensp;&ensp;这一步的目的在于将从candidate中选择感受野的node，并确定感受野中node的顺序。最终的结果如下图所示：假设上一步选择邻域过程中一个中心结点的邻居(一阶或者二阶)有N个，那么N可能和感受野大小K不相等。因此，normalize的过程就是要对N个邻居打上排序标签并进行选择，并且按照该顺序映射到向量中。    </p>
<p><img src="/2019/03/03/图卷积/normalize.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;如果这个中心结点的邻居个数不够k个的话，直接把所有的邻居全部选上，不够补上哑结点(dummy nodes),但还是需要排序的。如果中心结点的邻居个数N大于感受野k，则需要按照排序截断后面的结点。如上图所示，表示从中心结点到选邻居的整个过程。Normalize进行排序之后就能够映射到一个vector中，因此这一步最重要的是对结点进行排序。</p>
<p>&ensp;&ensp;&ensp;&ensp;对于任意一个中心结点求解它的感受野的过程。这里的卷积核的大小为4(2x2的卷积核)，因此最终要选出4个邻居，包括中心结点本身。因此，需要给这些结点打标签(排序)。怎样打标签才是最好的？如上图要在7个结点中选出4个结点组成一个含有4个结点的图集合。作者认为，在一种标签下，就是已经给图中的节点排好序了，随机从集合中选出2个图，计算它们在向量空间的图距离和在图空间的图距离的差异的期望，如果这个期望越小那么就表示标签越好。得到最好的标签之后，就能够按着顺序将结点映射到一个有序的向量中，也就得到了感受野。</p>
<h3><span id="214-卷积网络结构convolutional-architecture">2.1.4. 卷积网络结构Convolutional Architecture</span></h3><p>&ensp;&ensp;&ensp;&ensp;文章使用的是一个2层的卷积神经网络，将输入转化为一个向量vector之后便可以用来进行卷积操作了。具体的操作所示。</p>
<p><img src="/2019/03/03/图卷积/卷积操作.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;首先最底层的灰色块为网络的输入，每一个块表示的是一个node的感知野（receptive field）区域，也是前面求解得到的4个nodes。其中an表示的是每一个node的数据中的一个维度（node如果是彩色图像那就是3维；如果是文字，可能是一个词向量……这里表明数据的维度为n）。粉色的表示卷积核，核的大小为4，但是宽度要和数据维度一样。因此，和每一个node卷季后得到一个值。卷积的步长（stride）为4，表明每一次卷积1个node，stride=4下一次刚好跨到下一个node。（备注：paper 中Figure1 当中，（a）当中的stride=1，但是转化为（b）当中的结构后stride=9）。卷积核的个数为M，表明卷积后得到的特征图的通道数为M，因此最终得到的结果为V1……VM，也就是图的特征表示。有了它便可以进行分类或者是回归的任务了。</p>
<h3><span id="215-伪代码">2.1.5. 伪代码</span></h3><p><img src="/2019/03/03/图卷积/算法1.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;这个算法用来选择要进行卷积操作的node，其中w为要选择的node的个数。s为stride的大小。其中一个关键在于graph labeling procedure l。labeling算法用来确定一个graph中node的次序。这个算法可以根据node degree来确定，或者根据其他确定centrality的测量方式，比如：between centrality， WL algorithm等。或者其他你认为可行的算法。<br><img src="/2019/03/03/图卷积/算法2.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;这个算法非常简单，就是一个BFS算法，对每个目标node，寻找离node最近的至少k个node。</p>
<p>&ensp;&ensp;&ensp;&ensp;这是整个论文的重点，这个步骤的目的在于将目标node无序的neighbors映射为一个有序的vector。这个label要实现一个目的：assign nodes of two different graphs to a similar relative position in the respective adjacency matrices if and only if their structural roles within the graph are similar. 也就是说，对于两个不同的graphs， 来自这两个graph的子结构g1和g2，它们在各自的graph中有相似的结构，那么他们label应该相似。为了解决这个问题，论文中定义了一个optimal graph normalization问题，定义如下：</p>
<p><img src="/2019/03/03/图卷积/算法3-1.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;这个等式的解在于寻找一个一个labeling L， 使得从图的集合中任意选取两个图G1和G2，它们在vector space距离差距和它们在graph space的距离差距最小化。但是这个问题是NP-hard的问题，所以作者选择找一个近似解。即它比较了各种labeling方法，并从其中找出最优解。具体如下：<br><img src="/2019/03/03/图卷积/算法3-2.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;在特征选择阶段，只有第一层和传统的CNN有区别，之后的卷积层和传统的一样。下面来举例来说明PATCHY-SAN如何提取顶点特征和边特征。我们假设a_v为顶点特征的个数，a_e为边特征的个数。w为目标node的个数，k为感受野中node的个数。对于每个输入图结构，运用上面的一系列normalization算法，我们可以得到两个tensor (w,k,a_v)和(w,k,k,a_e),分别对应于顶点特征和边特征。这两个tensor可以被reshape成(wk, a_v)和(wk^2, a_e)，其中a_v和a_e可以分别看成是CNN中channel的个数。现在我们可以对它们做一维度的卷积操作，其中第一个的感受野大小为k，第二个感受野大小为k^2。而之后的卷积层的构造和传统的CNN一样了。</p>
<h3><span id="216-算法流程">2.1.6. 算法流程</span></h3><p>输入：任意一张图<br>输出：每个channel输出w个receptive field</p>
<p>Step1： graph labeling（对图的节点做标记，比如可以用节点的度做标记，做图的划分，也 可以叫做color refinement or vertex classification）<br>文中采用The Weisfeiler-Lehman algorithm做图的划分。由此可以得到每个节点的rank 值（为了不同的图能够有一个规范化的组织方式）</p>
<p>Step2：对labeling好的节点排序，取前w个节点，作为处理的节点序列。（这样就可以把不 同size的graph，变成同一个size）若不足w个节点，则，在输出中加全零的receptive field，相当于padding</p>
<p>Step3：采用stride=s来遍历这w个节点。文中s=1（若s）1，为了输出有w个receptive field， 也用step2的方式补全）</p>
<p>Step4：对遍历到的每个节点v（称作root），采用bfs的方式获得此节点的k个1-neighborhood， 如果不k个，再遍历1-neighborhood的1-neighborhood。直到满足k个，或者所有的 邻居节点都遍历完。此节点和他的k个邻居节点就生成了neighborhood graph。</p>
<p>Step5： step4就生成了w个（s=1）neighborhood graph。需要对着w个graph 进行labeling， 根据离root节点v的远近来计算每个节点的rank，根据算法4是离v越近，r越小。 如果每个neighborhood graph不足k个节点，用0节点补充</p>
<p>Step6：规范化step5得到了已经label好的graph，因为需要把它变成injective，使每个节点 的标签唯一，采用nauty的算法通过这w个receptive field就能得到一个w(k+1)维的向量。</p>
<h2><span id="22-谱域spectral-domain">2.2. 谱域Spectral Domain</span></h2><p>&ensp;&ensp;&ensp;&ensp;谱域就是GCN的理论基础了。这种思路就是借助图谱的理论来实现卷积操作。<br>&ensp;&ensp;&ensp;&ensp;谱图理论就是借助图的拉普拉斯矩阵的特征值和特征向量来研究图的性质。<br>&ensp;&ensp;&ensp;&ensp;<strong>在这里需要明确一点：谱图和顶点域的本质完全不一样。顶点域其实还是先对图进行处理，然后像类似卷积核在图上滑动来计算。这里谱域就没有卷积核在图上滑动的这个概念了。里面的卷积运算就理解成矩阵之间的运算，不要再去想卷积核滑动的思想了。</strong></p>
<h3><span id="221-图卷积运算">2.2.1. 图卷积运算</span></h3><p>&ensp;&ensp;&ensp;&ensp;由传统的傅里叶变换得到图上的傅里叶变换，我们不需要知道怎么由传统的傅里叶变换得到图上的傅里叶变换，只需要知道图上的傅里叶变换是$F(f)=U^Tf$，其中f是待变换的函数，$F(f)$是$f$傅里叶变换之后的函数。傅里叶的逆变换为$F^{-1}(f)=Uf$。<br>&ensp;&ensp;&ensp;&ensp;由传统的卷积定理得到图上的卷积，因为在计算传统的卷积时需要用到傅里叶变换，所以在计算图上的卷积时也需要用到图上的傅里叶变换。下图中的$F$表示的是傅里叶变换，$g*f$表示卷积运算，可以看到$g和f$的卷积等于$g$的傅里叶变换和$f$的傅里叶变换乘积的逆傅里叶变换。其中$f$是待卷积的函数,就是一个待卷积的图，$g$就是卷积核。 把图上的傅里叶变换代入到下面中，就可以得到图上的卷积运算。<br>&ensp;&ensp;&ensp;&ensp;首先计算$f$的傅里叶变换为$U^Tf$，卷积核的傅里叶变换写成对角矩阵的形式为:是L的特征值的函数</p>
<script type="math/tex; mode=display">\left(
\begin{matrix}
  \hat{g}(\lambda_1) & &  \\
  &  \ddots &  \\
  & &  \hat{g}(\lambda_n)\\
\end{matrix}
\right)</script><p>两者的傅里叶变换乘积即为</p>
<script type="math/tex; mode=display">\left(
\begin{matrix}
  \hat{g}(\lambda_1) & &  \\
  &  \ddots &  \\
  & &  \hat{g}(\lambda_n)\\
\end{matrix}
\right)U^Tf</script><p>再乘上$U$求两者傅里叶变换乘积的逆变换，则求出$f和g的卷积$</p>
<script type="math/tex; mode=display">(f*g)_G=U\left(
\begin{matrix}
  \hat{g}(\lambda_1) & &  \\
  &  \ddots &  \\
  & &  \hat{g}(\lambda_n)\\
\end{matrix}
\right)U^Tf</script><p><strong>很多论文中会把上式写成$(f*g)_G=U((U^Tg)\odot(U^Tf))$</strong></p>
<p>可以看出$U$为特征向量组成的特征矩阵，$f$为待卷积函数，在图中就是图信号矩阵，重点在于设计可训练、共享参数的卷积核$g$。</p>
<p><img src="/2019/03/03/图卷积/谱图1.png" alt="">   </p>
<p><img src="/2019/03/03/图卷积/谱图2.png" alt="">  </p>
<p><img src="/2019/03/03/图卷积/谱图3.png" alt="">  </p>
<p><img src="/2019/03/03/图卷积/谱图4.png" alt="">  </p>
<p><img src="/2019/03/03/图卷积/谱图5.png" alt="">  </p>
<p><img src="/2019/03/03/图卷积/谱图6.png" alt="">      </p>
<p>&ensp;&ensp;&ensp;&ensp;给出一个无向图，可以写出这个图的邻接矩阵$A$,无向图的邻接矩阵是一个对称矩阵，其中对角线上全是0。度矩阵$D$是一个对角矩阵，只有对角线上有值，其余全是0，$D_{i,i}=\sum_jA_{i,j}$，把$A$中的每一行加起来就是这个点的度。拉普拉斯矩阵$L=D-A$，拉普拉斯矩阵是一个对称矩阵，只有中心节点和一阶相连的顶点非0，其余位置全为0。对角线上表示这个节点有几个一阶邻居(不包括自己)，这一行中值为-1的表示是该节点的一阶邻居。  </p>
<p><img src="/2019/03/03/图卷积/图.png" alt=""></p>
<p><img src="/2019/03/03/图卷积/矩阵.png" alt=""></p>
<h3><span id="222-第一代图卷积scnn">2.2.2. 第一代图卷积SCNN</span></h3><p>&ensp;&ensp;&ensp;&ensp;第一代图卷积模型SCNN是在2014年发表在NIPS中的<a href="https://arxiv.org/abs/1312.6203" target="_blank" rel="noopener">Spectral Networks and Deep Locally Connected Networks On Graph</a>提出来的。<br><img src="/2019/03/03/图卷积/SCNN.png" alt="">  </p>
<p><img src="/2019/03/03/图卷积/第一代.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;第一代卷积。谱图卷积就是给定一个图信号，和一个卷积核。图信号就是假设有一个图，如果有200个节点，每个节点有3个特征，x就是一个200<em>3的矩阵，这个矩阵就是图信号矩阵。卷积核就是一个参数𝜃，谱图卷积的定义就是对归一化的拉普拉斯矩阵特征分解，得到特征值组成的矩阵$\Lambda$和特征向量组成的矩阵U，由于L是对称矩阵，所以U-1等于UT。谱图卷积的定义：在欧式空间内卷积的定义是傅里叶变换乘积的逆变换，研究图上的卷积是怎么来的，将图上的信号做一个图傅里叶变换，将卷积核做一个图傅里叶变换，将这两个做一个内积，然后再做一个图上的逆傅里叶变换。所以一个卷积核在图信号的谱图卷积就定义出来了。这个式子直接把卷积核g的傅里叶变换的对角矩阵当做参数，<strong>此时的卷积核不需要训练，因为右式中的所有值都是已知的。U已知，中间的对角矩阵的值</strong> $\theta$ <em>*就是傅里叶算子，也是已知的，图信号矩阵x也是已知的</em></em>。这种方式太简单，这个卷积核不具有局部性，不能捕获局部关系。并且这个式子的时间复杂度很高，是n的立方，第二个是参数过多。所以后人对其进行改进。</p>
<h3><span id="223-第二代图卷积chebnet">2.2.3. 第二代图卷积ChebNet</span></h3><p><img src="/2019/03/03/图卷积/ChebNet.png" alt="">  </p>
<p><img src="/2019/03/03/图卷积/改进1.png" alt=""> </p>
<p>&ensp;&ensp;&ensp;&ensp;第二代图卷积。$g_\theta$是关于特征值的一个函数，以前是直接把特征值变成卷积核，现在把每一个特征值上都乘上一个特征值矩阵的k次幂，然后把0到k-1次幂加到一起，构造成一个新的卷积核。把这个卷积核代入，得到右侧这个式子，右边这个式子不需要做特征值分解，直接把L连乘k次就可以了。<strong>这个卷积核是需要训练的，其中的参数是</strong>$\theta_k$<strong>通过初始化赋值再通过反向传播进行调整</strong>。所以这个一个改进，这个式子比前面那个式子的时间复杂度低。但是右边的这个时间复杂度也不低，因为涉及到L的k次幂，现在就想办法把右边这个式子的时间复杂度降低。   </p>
<h3><span id="224-第三代图卷积">2.2.4. 第三代图卷积</span></h3><p><img src="/2019/03/03/图卷积/图卷积3.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;ICLR2017<a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">Semi-supervised Classification with Graph Convolutional Networks</a>这篇论文就对上面那个式子进行改进，让K=1，最大特征值=2，这个式子就变成了右边这个式子，继续化简，让$\theta_0$和$\theta_1$互为相反数，称为一个权重，为什么能这么变呢，因为他相信训练的时候卷积核就可以学出互为相反数的参数。这样只考虑一阶邻居，如果堆叠2个卷积层的话，就可以考虑2阶邻居了。    </p>
<p><img src="/2019/03/03/图卷积/图卷积4.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;然后这篇论文又对刚刚的那个式子作一个归一化的trick，就是把邻接矩阵A加上一个单位矩阵，原先A是一个邻接矩阵，对角线上全为0，现在加上一个1，就是说节点自己到自己是没有边的，现在是自己有一条边又到达了自己，也就是增加一个自连接，然后重新计算一个度矩阵，然后就得到重新归一化的拉普拉斯矩阵，然后代入化成矩阵相乘的形式就是右边这个式子。这样卷积就定义完了，其中输入就是图信号X，卷积核就是Θ，前面的东西就是一个常量，只要拥有这三部分就可以得到GCN的卷积。卷积定义完了，然后加一个relu激活函数，这样一层图卷积就定义完了，如果要叠加一层，就是把上一层图卷积输出后的表示作为图卷积的输入，再做一次卷积。因为这篇论文做的是分类任务，所以要输出每个节点属于某个类的概率，所以用一个softmax就可以输出概率。<br>&ensp;&ensp;&ensp;&ensp;形象的解释：每个节点拿到邻居节点信息然后聚合到自身Embedding上。在上面的公式中，$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$可以看做是归一化后的邻接矩阵。$X\Theta$相当于对原始节点的Embedding做一次线性变换，左乘邻接矩阵表示对每个节点来说，该节点的特征变为邻居节点特征相加后的结果。<br>&ensp;&ensp;&ensp;&ensp;这就是现在谱图卷积计算的式子，实际</p>
<hr>
<p>2020.2.17更新</p>
<h3><span id="225-图卷积多种形式">2.2.5. 图卷积多种形式</span></h3><ol>
<li>$f(H^{(l)},A) = \sigma(AH^{(l)}W^{(l)})$<br>直接将$AH$做矩阵相乘，然后再通过一个权重矩阵$W^{(l)}$做线性变换，之后再经过非线性激活函数$\sigma(·)$,例如$ReLU$,最后得到下一层的输入$H^{(l+1)}$。</li>
<li>$f(H^{(l)},A) = \sigma(\hat{A}H^{(l)}W^{(l)})$<br>方式1这里直接将权重矩阵$A$和图信号矩阵$H$相乘，表示对于每个节点，将其邻居的特征加起来作为其新的表示。<br>有以下问题：<ul>
<li>虽然获取了周围节点的信息，但是自身的信息却没了，除非邻接矩阵中有自连接。解决方案：对每个节点手动增加一条<code>self-loop</code>到每一个节点，即$\hat{A}=A+I$,其中$I$是单位矩阵</li>
</ul>
</li>
<li>$f(H^{(l)},A) = \sigma(D^{-1}AH^{(l)}W^{(l)})$<br>方式1这里直接将权重矩阵$A$和图信号矩阵$H$相乘，表示对于每个节点，将其邻居的特征加起来作为其新的表示。<br>有以下问题：<ul>
<li>经过一层变换，每个节点特征都会变成邻接特征之和，这样得到的输出会越来越大，即特征向量$X$的scale会改变，在经过多层的变化之后，将和输入的scale差距越来越大。解决方案：原先邻接矩阵$A$中的值非0即1，是否可以将邻接矩阵$A$做归一化，使得每一行之和为1，使得$AH$获得的是<code>weighted sum</code>，可以将$A$的每一行除以行的和，这就可以得到<code>normalized</code>的$A$,而每一行的和就是每个节点的<code>degree</code>,用矩阵表示为$A=D^{-1}A$，对于邻接矩阵中的每个值变成$A_{ij}=\frac{A_{ij}}{d_i}$</li>
</ul>
</li>
<li>$f(H^{(l)},A) = \sigma(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}H^{(l)}W^{(l)})$<ul>
<li>但是在实际运用中，我们采用的对称的<code>normalization</code>,$A=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$,对于$A_{ij}=\frac{A_{ij}}{\sqrt{d_i}\sqrt{d_j}}$</li>
</ul>
</li>
<li>$f(H^{(l)},A) = \sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$<br>结合公式2和公式4，得到公式5</li>
</ol>
<p>【<strong>参考资料</strong>】<br><a href="https://zhuanlan.zhihu.com/p/107162772" target="_blank" rel="noopener">图卷积网络（GCN）入门详解</a></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>拦截器</title>
    <url>/2019/03/01/%E6%8B%A6%E6%88%AA%E5%99%A8/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;拦截器是SpringMVC中的一个核心应用组件,主要用于处理多个Controller的共性问题.当我们的请求由DispatcherServlet<strong>派发到具体Controller之前</strong>首先要执行拦截器中一些相关方法,在这些方法中可以对请求进行相应预处理(例如权限检测,参数验证),这些方法可以决定对这个请求进行拦截还是放行。   <a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8e%e8%af%b7%e6%b1%82">1. 服务器与请求</a><ul>
<li><a href="#11-%e5%b8%b8%e8%a7%81%e7%9a%84web%e6%9c%8d%e5%8a%a1%e5%99%a8">1.1. 常见的WEB服务器</a></li>
<li><a href="#12-%e5%8f%91%e9%80%81%e8%af%b7%e6%b1%82">1.2. 发送请求</a></li>
<li><a href="#13-%e9%80%9a%e8%bf%87%e6%b5%8f%e8%a7%88%e5%99%a8%e5%8f%91%e9%80%81url%e8%af%b7%e6%b1%82">1.3. 通过浏览器发送URL请求</a></li>
<li><a href="#14-js%e6%96%87%e4%bb%b6%e5%8f%91%e9%80%81%e8%af%b7%e6%b1%82">1.4. js文件发送请求</a></li>
</ul>
</li>
<li><a href="#2-%e5%ae%9e%e7%8e%b0%e6%8b%a6%e6%88%aa%e5%99%a8">2. 实现拦截器</a><ul>
<li><a href="#21-controller">2.1. Controller</a></li>
<li><a href="#22-interceptor">2.2. Interceptor</a></li>
<li><a href="#23-mvc%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6">2.3. mvc配置文件</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-服务器与请求">1. 服务器与请求</span></h1><h2><span id="11-常见的web服务器">1.1. 常见的WEB服务器</span></h2><ol>
<li>Toncat服务器：我最常用的服务器，开放源代码的，运行servlet和JSP Web应用软件基于Java，比绝大多数的商业用的软件服务器要好。</li>
<li>Apache服务器：使用广泛，开源代码，支持多个平台，相对其他服务器占的内存较大，是重量级产品。</li>
<li>Microsoft IIS服务器：微软的，包括Web服务器，FTP服务器，NNTP服务器和SMTP服务器。需要购买。</li>
<li>Nginx服务器：俄罗斯的一个站点开发的，相比于Apache服务器，Nginx占用内存小且较稳定。</li>
</ol>
<h2><span id="12-发送请求">1.2. 发送请求</span></h2><p>&ensp;&ensp;&ensp;&ensp;前端向服务器发送请求有2种，(1)通过浏览器发送请求，(2)进入到系统后，通过js发送请求。</p>
<h2><span id="13-通过浏览器发送url请求">1.3. 通过浏览器发送URL请求</span></h2><p>(1)用户在浏览器上输入网址，包含协议和域名.<br>(2)浏览器获得IP地址，浏览器先找自身缓存是否有记录，没有的话再找操作系统缓存，再没有就请求本地DNS服务器帮忙，本地DNS再找不到再一层层往上，最终浏览器获得对应的IP地址。<br>(3)浏览器发送请求，浏览器根据HTTP协议，给对应IP地址的主机发送请求报文，默认端口为80，报文包括请求内容，浏览器信息，本地缓存，cookie等信息。<br>(4)web服务器接收请求，寻找文件,Tomcat服务器接收到请求，找对应的html文件<br>(5)返回数据，web服务器向浏览器反馈html文件，浏览器进行渲染，页面加载。</p>
<h2><span id="14-js文件发送请求">1.4. js文件发送请求</span></h2><p>&ensp;&ensp;&ensp;&ensp;在项目中，使用ajax向服务器发送请求，例如xxx.do。</p>
<h1><span id="2-实现拦截器">2. 实现拦截器</span></h1><p>拦截器需要实现   HandleInterceptor接口,或者继承HandlerInterceptorAdaptor抽象类;<br>HandlerInterceptor接口的三个方法:</p>
<ol>
<li>preHandle()</li>
<li>postHandle()</li>
<li>afterCompletion()</li>
</ol>
<p>&ensp;&ensp;&ensp;&ensp;inceptor的作用是，每次在前端向后台发送一个请求时do,后台都会先经过inceptor中的preHandle这个函数，判断这个请求是否满足要求（是否已经登录，是否是管理员），如果满足要求就返回true，系统会自动把这个do请求提交给controller对应的函数进行处理，controller中的函数调用完之后，再次进入Inception中的postHandle()和afterCompletion()方法中。否则preHandle返回false，不会提交这个请求，不会执行Controller中的函数，也不会执行之后的Inception中的postHandle()和afterCompletion()方法。<br>&ensp;&ensp;&ensp;&ensp;服务器一启动,就会创建拦截器对象;拦截器是单例的,整个过程,拦截器只有一个实例对象。<br>&ensp;&ensp;&ensp;&ensp;项目中需要实现一个登录系统，当用户没有登录时，不能访问系统的主页和其他页面，但是可以访问系统的登录界面，所以需要在mvc.xml中设置一下，不拦截登录的请求。</p>
<h2><span id="21-controller">2.1. Controller</span></h2><p>下面是用户登录的Controller实现,当前端访问login.jsp时，这时登录的请求不会被拦截器拦截，会执行login()方法，验证前端用户输入的用户名和密码是否正确，如果正确的话，将userName放入到session中，并返回给前端index，那么界面将会跳转到index.jsp，如果用户名或密码错误，那么返回给前端login，前端界面还是login.jsp。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line">	<span class="meta">@Autowired</span></span><br><span class="line">	<span class="keyword">private</span> UserService userService;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">UserController</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="keyword">this</span>.getClass().getName() + <span class="string">" 初始化"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@RequestMapping</span>(value = <span class="string">"/login"</span>, method = RequestMethod.POST)</span><br><span class="line">	<span class="meta">@ResponseBody</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">login</span><span class="params">(HttpServletRequest request, HttpSession session)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> SQLException, IOException, NoSuchAlgorithmException, InvalidKeySpecException </span>&#123;</span><br><span class="line">		<span class="comment">// userList存储了所有的用户</span></span><br><span class="line">		<span class="comment">// 每个用户以HashMap的形式存储</span></span><br><span class="line">		<span class="comment">// key分别是："userName"，"password"，"salt"</span></span><br><span class="line">		ArrayList&lt;HashMap&lt;String, String&gt;&gt; userList = userService.getUserInfo();</span><br><span class="line"></span><br><span class="line">		String input_userName = request.getParameter(<span class="string">"userName"</span>);</span><br><span class="line">		String input_password = request.getParameter(<span class="string">"password"</span>);</span><br><span class="line"></span><br><span class="line">		PBKDF2Util pbkdf2Util = <span class="keyword">new</span> PBKDF2Util();</span><br><span class="line">		<span class="comment">// 判断当前用户输入的用户名和密码是否正确</span></span><br><span class="line">		<span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; userList.size(); i++) &#123;</span><br><span class="line">			HashMap&lt;String, String&gt; oneUser = userList.get(i);</span><br><span class="line">			String actual_userName = oneUser.get(<span class="string">"userName"</span>);</span><br><span class="line">			String actual_password = oneUser.get(<span class="string">"password"</span>);</span><br><span class="line">			String salt = oneUser.get(<span class="string">"salt"</span>);</span><br><span class="line">			<span class="keyword">boolean</span> password_match = pbkdf2Util.authenticate(input_password, actual_password, salt);</span><br><span class="line">			<span class="keyword">if</span> (input_userName.equals(actual_userName) &amp;&amp; password_match) &#123;</span><br><span class="line">				flag = <span class="keyword">true</span>;</span><br><span class="line">				<span class="keyword">break</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="comment">// for</span></span><br><span class="line">		<span class="keyword">if</span> (flag) &#123;</span><br><span class="line">			session.setAttribute(<span class="string">"user"</span>, input_userName);</span><br><span class="line">			<span class="keyword">return</span> <span class="string">"&#123;\"status\": 0, \"url\": \"index\"&#125;"</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			request.setAttribute(<span class="string">"msg"</span>, <span class="string">"用户名或密码错误"</span>);</span><br><span class="line">			<span class="keyword">return</span> <span class="string">"&#123;\"status\": 1, \"url\": \"login\", \"msg\": \"用户名或密码错误\"&#125;"</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@RequestMapping</span>(<span class="string">"/logout"</span>)</span><br><span class="line">	<span class="meta">@ResponseBody</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">logout</span><span class="params">(HttpSession session)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="comment">// System.out.println("进入到logout()方法中");</span></span><br><span class="line">		<span class="comment">// 清除session的数据</span></span><br><span class="line">		session.invalidate();</span><br><span class="line">		<span class="keyword">return</span> <span class="string">"&#123;\"status\": 0, \"url\": \"login\"&#125;"</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2><span id="22-interceptor">2.2. Interceptor</span></h2><p>在interceptor中会拦截URL请求，如果session中的用户名为空会重定向到login.jsp。但在在做项目时遇见一个问题，拦截器只能拦截js中的ajax发来的URL请求，不能拦截浏览器发送的URL请求。也就是说如果用户在浏览器中输入index.jsp，不会经过拦截器，如果是js中的ajax发送的请求，会经过拦截器。如果用户没有登录直接在浏览器中输入index.jsp，这时页面依然可以进入到index.jsp，这说明拦截器没有起作用。为了应对这一情况，有三种解决方案：<br>（1）把判断用户是否登录的代码写到了jsp中，在jsp中写java代码需要加上&lt;%%&gt;，在这里判断session中的用户名，如果为空的话，直接重定向到login.jsp，这样用户在未登录的情况下，在浏览器上输入index.jsp，页面不会跳转到index.jsp中，还是在login.jsp中。<br>（2）把所有的jsp文件放在WEB-INF文件里,这样用户是直接不能访问WEB-INF文件下的jsp文件的。spring mvc的理念也是通过controller里的@RequestMapping来请求相关jsp页面，而非用户直接访问jsp页面。也就是说，jsp页面的访问需要通过controller来进行一次请求，因为会拦截对controller的请求，所以也就相当于拦截了jsp页面。如果要做登陆拦截，只需要把登陆页面不拦截，其余页面拦截进行是否登陆的验证即可。<br>（3）jsp如果不放在WEB-INF文件下，spring mvc是无法拦截的，这种情况下需要用最原始的servlet的Filter接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&lt;%</span><br><span class="line">	<span class="comment">//实现登录检查，如果用户没有登录，重定向到登录界面</span></span><br><span class="line">	<span class="comment">//这段代码要加载所有需要验证页面里,使用</span></span><br><span class="line">	<span class="comment">//&lt;%@include file="/jsp/navigation.jsp"把登录验证加载其余jsp中</span></span><br><span class="line">	Object userName = <span class="string">""</span>;</span><br><span class="line">	<span class="keyword">if</span> (session == <span class="keyword">null</span>) &#123;</span><br><span class="line">		response.sendRedirect(request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>);</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		userName = session.getAttribute(<span class="string">"user"</span>);</span><br><span class="line">		<span class="keyword">if</span> (userName == <span class="keyword">null</span>) &#123;</span><br><span class="line">			response.sendRedirect(request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			userName = userName.toString();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">%&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginInterceptor</span> <span class="keyword">implements</span> <span class="title">HandlerInterceptor</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 步骤1</span></span><br><span class="line">	<span class="comment">// 在前端发出一个url(xxx.do)请求时，先执行这个方法，判断当前用户是否为空</span></span><br><span class="line">	<span class="comment">// 如果用户已经登录，则返回true,否则返回false</span></span><br><span class="line">	<span class="comment">// 只有当该函数返回true时，才会调用controller中对应的函数，</span></span><br><span class="line">	<span class="comment">// 返回false不用调用controller中的函数</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object arg2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		String path = request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>;</span><br><span class="line">		HttpSession session = request.getSession(<span class="keyword">false</span>);</span><br><span class="line">		<span class="keyword">if</span> (session == <span class="keyword">null</span> || !request.isRequestedSessionIdValid()) &#123;</span><br><span class="line">			response.sendRedirect(path);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 获取登录用户信息</span></span><br><span class="line">		String user = session.getAttribute(<span class="string">"user"</span>).toString();</span><br><span class="line">		<span class="keyword">if</span> (user == <span class="keyword">null</span>) &#123;</span><br><span class="line">			response.sendRedirect(path);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 步骤2</span></span><br><span class="line">	<span class="comment">// 当preHandle返回true，调用controller中的函数之后，会执行该函数</span></span><br><span class="line">	<span class="comment">// 当preHandle返回false，不会执行该函数</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postHandle</span><span class="params">(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, ModelAndView arg3)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		<span class="comment">// System.out.println("拦截后...");</span></span><br><span class="line">		<span class="comment">// System.out.println("进入到LoginInterceptor的postHandle()方法中");</span></span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 步骤3</span></span><br><span class="line">	<span class="comment">// 当preHandle返回true，调用controller中的函数之后，执行完postHandle，会调用该函数</span></span><br><span class="line">	<span class="comment">// 当preHandle返回false，不会执行该函数</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		<span class="comment">// System.out.println("页面渲染后...");</span></span><br><span class="line">		<span class="comment">// System.out.println("进入到LoginInterceptor的afterCompletion()方法中");</span></span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2><span id="23-mvc配置文件">2.3. mvc配置文件</span></h2><p>在mvc.xml配置文件中，需要对拦截器进行配置，因为login请求不需要拦截，所以把这个请求排除，这样当前端访问login.jsp页面时，就会显示出登录的界面。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mvc:interceptors</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line">			<span class="comment">&lt;!-- 拦截以任意字符结尾的路径 ，匹配所有的路径 --&gt;</span></span><br><span class="line">			<span class="comment">&lt;!--/**表示拦截所有的url及其子路径  --&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">mvc:mapping</span> <span class="attr">path</span>=<span class="string">"/**"</span> /&gt;</span></span><br><span class="line">			<span class="comment">&lt;!-- 登录不进行拦截 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mvc:exclude-mapping</span> <span class="attr">path</span>=<span class="string">"/**/*login*"</span> /&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"com.hz.EQbigdata.interceptor.LoginInterceptor"</span>&gt;</span><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mvc:interceptors</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>页面加载的顺序：<br>前端输入一个网址，相当于发出一个url，比如querywda.jsp。首先拦截器拦截这个url，判断是否合法，如果合法，会交给controller处理，处理完之后才会显示querywda的界面，调用相应的querywda.js。如果不合法，就应该在inception就把这个请求拦截下来，重定向到login，这样querywda的界面也不会加载出来</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase</title>
    <url>/2019/02/28/HBase/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;在做项目的过程中用到了HBase，遇到了一些问题，当数据过大的时候，向HBase中会出现热点问题。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0">1. 问题描述</a></li>
<li><a href="#2-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88">2. 解决方案</a><ul>
<li><a href="#21-%e9%94%99%e8%af%af%e7%9a%84%e9%a2%84%e5%88%86%e5%8c%ba">2.1. 错误的预分区</a></li>
<li><a href="#22-%e6%ad%a3%e7%a1%ae%e7%9a%84%e9%a2%84%e5%88%86%e5%8c%ba">2.2. 正确的预分区</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1><span id="1-问题描述">1. 问题描述</span></h1><p>&ensp;&ensp;&ensp;&ensp;HBase默认建表时有一个分区（region），这个region的rowkey是没有边界的，即没有startkey和endkey，<strong>hbase的中的数据是按照字典序排序的</strong>，在数据写入时，所有数据都会写入这个默认的region，随着数据量的不断增加，此region已经不能承受不断增长的数据量，当一个region过大（达到hbase.hregion.max.filesize属性中定义的阈值，默认10GB）时，会进行split，分成2个region。在此过程中，会产生两个问题：</p>
<ul>
<li>数据往一个region上写,会有写热点问题。</li>
<li>region split会消耗宝贵的集群I/O资源。</li>
</ul>
<h1><span id="2-解决方案">2. 解决方案</span></h1><p>&ensp;&ensp;&ensp;&ensp;基于此我们可以控制在建表的时候，创建多个空region，并确定每个region的起始和终止rowky，这样只要我们的rowkey设计能均匀的命中各个region，就不会存在写热点问题。自然split的几率也会大大降低。当然随着数据量的不断增长，该split的还是要进行split。像这样预先创建hbase表分区的方式，称之为预分区，下面给出一种预分区的实现方式:<br>&ensp;&ensp;&ensp;&ensp;解决这个问题，关键是要设计出可以让数据分布均匀的rowkey，与关系型数据库一样,rowkey是用来检索记录的主键。访问hbase table中的行，rowkey 可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，rowkey保存为字节数组，存储时，数据按照rowkey的字典序排序存储。<br>预分区的时候首先需要指定按什么来划分rowkey，<br>&ensp;&ensp;&ensp;&ensp;设计的rowkey应该由regionNo+messageId组成。设计rowkey方式：随机数+messageId，如果想让最近的数据快速get到，可以将时间戳加上，原先我们设计的行键是数据产生的时间，格式为2018-01-21 12:23:06,没有设置预分区，这样数据就会出现热点问题。</p>
<h2><span id="21-错误的预分区">2.1. 错误的预分区</span></h2><p>&ensp;&ensp;&ensp;&ensp;后来采用预分区的方式，按照秒进行预分区，splitKeys={“01|”,”02|”,…”59|”},在设计行键的时候在原先的时间上再添加当前的秒数，例如原先的行键是2018-01-21 12:23:06，现在的行键是062018-01-21 12:23:06，这样在存储的时候行键的前2个字符06，我这里的region是01|到59|开头的，因为hbase的数据是字典序排序的,行键开头为06，06大于05，并且06后面字符的ASCII码小于|，则当前这条数据就会保存到05|~06|这个region里。rowkey组成：秒数+messageId，因为我的messageId都是字母+数字，“|”的ASCII值大于字母、数字。<br>&ensp;&ensp;&ensp;&ensp;下图展示了HBase的分区情况。第一个分区没有startKey，endKey为01|，表示比01|小的行键都存储在这里，那就是以00和01开头的行键都存储在这里，最后一个分区的startKey是59|，表示比59大的存储在这个分区里，但是一分钟内的秒数没有比59大的，所以request一直是0。这份分区是不对的，<strong>正确的分区是splitKeys={“00|”,”01|”,…”58|”}</strong></p>
<p><img src="/2019/02/28/HBase/table.png" alt=""></p>
<h2><span id="22-正确的预分区">2.2. 正确的预分区</span></h2><p>&ensp;&ensp;&ensp;&ensp;需要注意的是，行键分配值按照rowkey的前几个字符进行匹配的，并不是按照数的大小。例如分区是 -10,10-20,20-30,30-40,40-50,50-60,60-70,70-80,80-90,90-，如果插入的数据rowkey是80 60 22这种两位数，肯定会落到某个分区，如果rowkey是100 333 9955 555544 66910 这种大于两位值，都会落在最后一个分区，还是只取rowkey的前两位与startkey/endkey对应？答案是：是按前两位匹配rowkey的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">byte</span>[][] getSplitKeys() &#123;</span><br><span class="line">  String[] keys = <span class="keyword">new</span> String[] &#123; <span class="string">"10|"</span>, <span class="string">"20|"</span>, <span class="string">"30|"</span>, <span class="string">"40|"</span>, <span class="string">"50|"</span>,</span><br><span class="line">    <span class="string">"60|"</span>, <span class="string">"70|"</span>, <span class="string">"80|"</span>, <span class="string">"90|"</span> &#125;;</span><br><span class="line">  <span class="keyword">byte</span>[][] splitKeys = <span class="keyword">new</span> <span class="keyword">byte</span>[keys.length][];</span><br><span class="line">  TreeSet&lt;<span class="keyword">byte</span>[]&gt; rows = <span class="keyword">new</span> TreeSet&lt;<span class="keyword">byte</span>[]&gt;(Bytes.BYTES_COMPARATOR);<span class="comment">//升序排序</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; keys.length; i++) &#123;</span><br><span class="line">   rows.add(Bytes.toBytes(keys[i]));</span><br><span class="line">  &#125;</span><br><span class="line">  Iterator&lt;<span class="keyword">byte</span>[]&gt; rowKeyIter = rows.iterator();</span><br><span class="line">  <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (rowKeyIter.hasNext()) &#123;</span><br><span class="line">   <span class="keyword">byte</span>[] tempRow = rowKeyIter.next();</span><br><span class="line">   rowKeyIter.remove();</span><br><span class="line">   splitKeys[i] = tempRow;</span><br><span class="line">   i++;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> splitKeys;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是，在上面的代码中用treeset对rowkey进行排序，必须要对rowkey排序，否则在调用admin.createTable(tableDescriptor,splitKeys)的时候会出错。创建表的代码如下:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建预分区hbase表</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> columnFamily 列簇</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@SuppressWarnings</span>(<span class="string">"resource"</span>)</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">createTableBySplitKeys</span><span class="params">(String tableName, List&lt;String&gt; columnFamily)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> (StringUtils.isBlank(tableName) || columnFamily == <span class="keyword">null</span></span><br><span class="line">					|| columnFamily.size() &lt; <span class="number">0</span>) &#123;</span><br><span class="line">				log.error(<span class="string">"===Parameters tableName|columnFamily should not be null,Please check!==="</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line">			<span class="keyword">if</span> (admin.tableExists(tableName)) &#123;</span><br><span class="line">				<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				HTableDescriptor tableDescriptor = <span class="keyword">new</span> HTableDescriptor(</span><br><span class="line">						TableName.valueOf(tableName));</span><br><span class="line">				<span class="keyword">for</span> (String cf : columnFamily) &#123;</span><br><span class="line">					tableDescriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(cf));</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">byte</span>[][] splitKeys = getSplitKeys();</span><br><span class="line">				admin.createTable(tableDescriptor,splitKeys);<span class="comment">//指定splitkeys</span></span><br><span class="line">				log.info(<span class="string">"===Create Table "</span> + tableName</span><br><span class="line">						+ <span class="string">" Success!columnFamily:"</span> + columnFamily.toString()</span><br><span class="line">						+ <span class="string">"==="</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">catch</span> (MasterNotRunningException e) &#123;</span><br><span class="line">			<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">			log.error(e);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125; <span class="keyword">catch</span> (ZooKeeperConnectionException e) &#123;</span><br><span class="line">			<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">			log.error(e);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">			<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">			log.error(e);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;HBase中出现热点问题带来的影响是：（1）在我们的项目中，原先使用一个分区，等到这个分区容量达到阈值时，这个分区开始split，然后数据来的时候就会向第二个分区写数据，不会向第一个region中写数据，所以在某一时候只能向一个region中写数据，这样写的速度会变慢。（2）在读数据的时候，因为行键设置的时间，连续的时间一般存储在一个region中，所以读数据的时候也是从一个region中读取数据，读取的速度也会变慢。项目原先应对取数据慢的问题解决方案使用HBase的scan函数，设置起始和终止的行键，使用scan查询数据。<br>&ensp;&ensp;&ensp;&ensp;按秒对表进行预分区时，就相当于把数据均匀分布在60个region中，存储一段时间的数据时，会同时向60个region中写入数据，取数据的时候也会同时从60个region中取数据。这样取数据的时候就不能使用起止行键用scan来查询数据了，只能使用getRow来查询数据，但是这样查询的性能也不会很差，因为是从60个region中同时查询数据，使用scan的时候是从1个region中查询数据。</p>
]]></content>
      <categories>
        <category>分布式平台</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP和Attention</title>
    <url>/2019/02/15/NLP%E5%92%8CAttention/</url>
    <content><![CDATA[<h1><span id="1-词嵌入">1. 词嵌入</span></h1><p>&ensp;&ensp;&ensp;&ensp;<strong>词向量</strong>就是用来表示词的向量，也可以是词的特征向量或表征。把词映射成向量的技术叫<strong>词嵌入</strong>。</p>
<p>前几天看到了一篇<a href="https://mp.weixin.qq.com/s/thUjPlkqpu6H_t92SUDtLg" target="_blank" rel="noopener">讲解Word2Vec的推送</a>，讲的超级好，通俗易懂。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e8%af%8d%e5%b5%8c%e5%85%a5">1. 词嵌入</a><ul>
<li><a href="#11-%e8%b4%9f%e9%87%87%e6%a0%b7">1.1. 负采样</a></li>
<li><a href="#12-%e5%b1%82%e5%ba%8fsoftmax">1.2. 层序Softmax</a></li>
<li><a href="#13-%e5%ae%9e%e7%8e%b0%e6%ad%a5%e9%aa%a4">1.3. 实现步骤</a></li>
<li><a href="#%e5%8f%91%e5%b1%95">发展</a></li>
</ul>
</li>
<li><a href="#2-seq2seq">2. Seq2Seq</a><ul>
<li><a href="#21-%e7%bc%96%e7%a0%81%e5%99%a8">2.1. 编码器</a></li>
<li><a href="#22-%e8%a7%a3%e7%a0%81%e5%99%a8">2.2. 解码器</a></li>
<li><a href="#23-%e4%bc%98%e7%bc%ba%e7%82%b9">2.3. 优缺点</a></li>
</ul>
</li>
<li><a href="#3-attention">3. Attention</a></li>
</ul>
<!-- /TOC -->
<p>&ensp;&ensp;&ensp;&ensp;一种最简单的词嵌入是one-hot向量，每个词使用非0即1的向量表示。这样构造虽然简单，但不是一个好的选择。因为one-hot词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度，因为任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度很难通过one-hot向量准确表示出来。<br>&ensp;&ensp;&ensp;&ensp;word2vec工具的提出正是为了解决上面的问题。2013年Google团队发表了word2vec工具。它将每个词表示成一个<strong>定长的向量，并使得这些向量可以较好地表达不同词之间的相似和类比关系</strong>。word2vev包含了2个模型：跳字模型(skip-gram)和连续词袋模型(continuous bag of words CBOW)。以及2种高效训练的方法：负采样(negative sampling)和层序softmax(hierarchical softmax)。<br>&ensp;&ensp;&ensp;&ensp;skip-gram是给定一个中心词，计算周围词出现的概率。在skip-gram中每个词被表示成2个d维向量。当它为中心词时向量被表示成$v_i\in\mathbb{R}^d$,当它为背景词时向量被表示成$u_i\in\mathbb{R}^d$。给定中心词生成背景词的条件概率可以通过softmax运算得到：</p>
<p><img src="/2019/02/15/NLP和Attention/式1.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;假设中心词是loves，判断根据loves生成背景词son的概率，分子是loves的中心词向量和son的背景词向量做内积，分母是除去loves的所有词的背景词向量分别和loves的中心词向量做内积，再相加。然后分子/分母得到根据loves生成son的概率。<br>&ensp;&ensp;&ensp;&ensp;假设给定一个长度为$T$的文本序列$w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为$m$时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率。</p>
<p><img src="/2019/02/15/NLP和Attention/式2.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;文本序列是预先给定的，已经有一句话了。首先时间$t=1$，确定一个中心词，然后根据这个中心词计算背景词的概率，把2m个概率相乘。然后t+1，再找下一个中心词，再计算这个中心词生成背景词的概率，再把2m个概率相乘，直到中心词到最后一个。</p>
<p>&ensp;&ensp;&ensp;&ensp;训练skip-gram模型就是为了得到每个词的中心词向量和背景词向量，每个词所对应的的中心词向量和背景词向量是skip-gram的模型参数。训练中通过最大化似然函数来学习模型参数，即最大化似然估计。训练结束后，我们可以得到字典中所有词的中心词向量和背景词向量。在自然语言处理应用中，一般使用skip-gram的中心词向量作为词的表征向量。<br>&ensp;&ensp;&ensp;&ensp;连续词袋模型基于背景词来生成中心词。比如一个句子the man - his son，根据前后4个词预测中间的词。也是通过最大化似然函数来训练得到字典中每个词的中心词向量和背景词向量。和跳字模型不一样的是，我们一般使用连续词袋模型的背景词向量作为词的表征向量。<br>&ensp;&ensp;&ensp;&ensp;但是skip-gram和CBOW的计算开销都比较大，下面介绍2个近似训练法：负采样和层序softmax.通过这2种方法可以减小训练开销。</p>
<h2><span id="11-负采样">1.1. 负采样</span></h2><p>&ensp;&ensp;&ensp;&ensp;跳字模型的核心在于使用softmax运算得到给定中心词$w_c$来生成背景词的概率。<br><img src="/2019/02/15/NLP和Attention/式1.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;由于softmax运算考虑了背景词可能是词典$\mathcal{V}$中的任一词，在计算损失函数时计算了所有背景词的损失。不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练方法，即负采样（negative sampling）或层序softmax（hierarchical softmax）。</p>
<p>&ensp;&ensp;&ensp;&ensp;在CBOW模型中，已知词$w$的上下文$Context(w)$,需要预测$w$，因此对于$Context(w)$，词$w$就是一个正样本，其他词就是一个负样本。从所有的负样本中选择一个负样本子集。训练的目标就是增大当上下文为$Context(w)$时，中心词$w$出现的概率，并且同时降低负样本的概率。<br>&ensp;&ensp;&ensp;&ensp;对于一个给定的词$w$，怎么生成这个词的负采样子集$NEG(W)$?<br>词典中的词出现的次数有高有低，对于那些高频词，被选为负样本的概率就比较大，对于那些低频词，被选中负样本的概率就小，本质上就是一个<strong>带权采样问题</strong>。对于一对中心词和背景词，随机采样K个负样本，论文中的建议K=5，负样本采样的概率$P(w)$设为$w$词频与总词频之比的$3/4$次方。</p>
<p><img src="/2019/02/15/NLP和Attention/负采样.png" alt="负采样"></p>
<p>&ensp;&ensp;&ensp;&ensp;在训练中，首先给出所有的句子。对于skip-gram模型负采样，给定一个中心词预测周围的词。对于一个句子，首先把语料分割成(context(w),w)样本，对于每一个中心词，都可以在这个句子中找出这个中心词的背景词（周围词）,并在词典中找出这个中心词的负样本（非背景词），一个中心词的负样本论文建议个数为5，就是对于一个中心词找出5个负样本。对语料进行预处理形成以下数据集：一个样本包括一个中心词，它所对应的n个背景词，m个噪声词（负样本）。每个样本的背景词窗口大小可能不一样，即每个中心词的背景词和噪声词的个数可能不一样。<br>&ensp;&ensp;&ensp;&ensp;<strong>负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关</strong>。</p>
<h2><span id="12-层序softmax">1.2. 层序Softmax</span></h2><p>&ensp;&ensp;&ensp;&ensp;使用哈夫曼二叉树来存储词典，叶子节点就是字典$\mathcal{V}$中的每个词，非叶子节点就是一些隐藏向量。<br>&ensp;&ensp;&ensp;&ensp;<strong>层序Softmax使用了二叉树，并根据根节点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关</strong>。</p>
<h2><span id="13-实现步骤">1.3. 实现步骤</span></h2><p>&ensp;&ensp;&ensp;&ensp;给定一个训练集，首先对数据进行处理，给定一个背景词窗口大小，对于每一个中心词，找到中心词在句子中的背景词、噪声词。这就是训练集，其中一个样本是(第i个中心词，n个背景词，m个噪声词)，使用这些样本作为训练。</p>
<p>&ensp;&ensp;&ensp;&ensp;嵌入层不需要自己写，直接使用Embedding来定义，根据嵌入层可以获取一个词的词向量。嵌入层有一个嵌入矩阵，输入是词典的大小（词的个数），输出是词向量的纬度。所以嵌入矩阵的维度为(词典大小，词向量维度)。嵌入层的输入是语料库中的词的索引[0,1,2,3…]，输入一个词的索引i,嵌入层返回权重矩阵的第i行作为它的词向量。<br>&ensp;&ensp;&ensp;&ensp;skip-gram的输入是包含中心词索引向量，背景词和噪声词索引向量。这2个向量先通过嵌入层得到词向量，然后输出中心词向量与背景词向量噪声词向量的内积作为中心词的词向量。</p>
<h2><span id="发展">发展</span></h2><p>&ensp;&ensp;&ensp;&ensp;在word2vec提出来之后，之后又有了新的发展。在2014年Stanford团队提出了GloVe，在2017年Facebook提出了fastText。其中GloVe提出两个词共现的概率，用词向量表达共现词频的对数。fastText提出每个词都是由子词提出来的，把中心词向量表示成所有子词的词向量的和。</p>
<h1><span id="2-seq2seq">2. Seq2Seq</span></h1><p>&ensp;&ensp;&ensp;&ensp;原先都是给定一个不定长的序列，输出一个定长的序列。比如给定一个不定长的序列预测下一个词。在自然语言处理中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出也可以是一段不定长的法语文本序列。当输入和输出都是不定长序列时，我们可以使用编码器-解码器（encoder-decoder）或者seq2seq模型。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。<br>Seq2Seq就是RNN Encoder-Decoder，其中的RNN通常是LSTM。 序列到序列模型就像一个翻译模型，输入是一个序列，输出也是一个序列。这种结构最重要的是输入和输出序列的长度是可变的。  </p>
<p>&ensp;&ensp;&ensp;&ensp;图10.8描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，我们可以在每个句子后附上特殊符号“\<eos>”（end of sequence）以表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“\<eos>”。图10.8中使用了<strong>编码器在最终时间步的隐藏状态作为输入句子的表征或编码信息</strong>。解码器在各个时间步中使用<strong>输入句子的编码信息c</strong>和<strong>上个时间步的输出</strong>以及<strong>隐藏状态</strong>作为输入。 我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号“\<eos>”。 需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号“\<bos>”（beginning of sequence）。其中每个词用$x_t$向量来表示，$x_t$可以通过预训练好的词向量来获取。</bos></eos></eos></eos></p>
<p><img src="/2019/02/15/NLP和Attention/seq2seq.png" alt=""></p>
<h2><span id="21-编码器">2.1. 编码器</span></h2><p>&ensp;&ensp;&ensp;&ensp;编码器的作用是把一个不定长的输入序列变成一个定长的背景变量$c$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。背景变量$c=q(h_1,…h_T)$,编码器通过自定义函数$q$将各个时间步的隐藏状态变换成背景变量</p>
<h2><span id="22-解码器">2.2. 解码器</span></h2><p>&ensp;&ensp;&ensp;&ensp;编码器输出的背景变量$c$编码了整个输入序列$x_1,…x_T$的信息。给定训练样板中的输出序列$y_1,…y_{T’}$，对每个时间步$t’$，解码器输出$y_{t’}$的条件概率将基于之前的输出序列$y_1,…y_{t’-1}$和背景变量$c$，即$P(y_t’|y_1,…y_{t’-1},c)$。为此，我们可以使用另一个循环神经网络作为解码器。在时间步$t’$，解码器根据背景变量$c$，上一时间步的输出$y_{t’-1}$和上一时间步的隐藏变量$s_{t’-1}$来生成当前时间步的隐藏变量$s_{t’}$。有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算生成当前时间步的输出$y_{t’}$。即计算的顺序是：先根据背景变量$c$，上一时间步的输出$y_{t’-1}$和上一时间步的隐藏变量$s_{t’-1}$来生成当前时间步的隐藏变量$s_{t’}$，然后再根据当前时间步的隐藏状态$s_{t’}$生成当前时间步的输出$y_{t’}$。<br>3个小trick：</p>
<ul>
<li>解码器什么时候停止？当预测的下一个词是\<eos>时停止。</eos></li>
<li>对于解码器，当生成第一个隐藏状态$s_1$时，需要给定$s_0,y_0和c$,其中$y_0=<bos>$对应的词向量</bos></li>
<li>对于编码器，生成第一个隐藏状态$h_1$时，需要给定当前的输入$x_1和上一时刻的隐藏状态h_0$，$h_0$可以初始化为全零的向量。对于编码器，$s_0$也可以初始化为全零向量。也可以初始化为$s_0=tanh(W\overleftarrow{h_1})$，编码器从右向左输入，最后得到第一个词的隐藏向量，然后用来初始化解码器的$s_0$</li>
</ul>
<h2><span id="23-优缺点">2.3. 优缺点</span></h2><p>&ensp;&ensp;&ensp;&ensp;encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量C。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，还有就是先输入的内容携带的信息会被后输入的信息稀释掉，或者说，被覆盖了。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息， 那么解码的准确度自然也就要打个折扣了</p>
<h1><span id="3-attention">3. Attention</span></h1><p><a href="https://blog.csdn.net/BVL10101111/article/details/78470716" target="_blank" rel="noopener">Attentions详细讲解</a><br>Attention一般有2种，（1）：Location-based Attention，这里的attention没有其他额外需要关注的对象，即多个$h_i$内部做attention。（2）Concatenation-based Attention：有额外需要关注的对象，即多个$h_i$对$h_t$的attention。我们平时用第2种比较多一些。</p>
<p>&ensp;&ensp;&ensp;&ensp;在上面的编码器-解码器中，从编码器传到解码器的背景变量$c$是不变的。就是说解码器在翻译第一个词和第二个词是c是不变的。但是实际情况中，比如英语they are watching。翻译成法语是：IIs regardent。比如在翻译IIS时，和they are更相关，在翻译regardent和watching更相关，所以希望把背景变量$c$设置成一个变化的值。当翻译IIS时，对编码器的隐藏变量$h_1,h_2$更看重，当翻译regardent对隐藏变量$h_3$更看重，所以就需要在解码器中，在不同时间步时，对编码器的隐藏变量$h_1,h_2,h_3$分配不同的权重，加权平均得到背景变量$c$。<br>&ensp;&ensp;&ensp;&ensp;原先解码器隐藏层变量的计算是上一时刻的输出$y_{t’-1}$，上一时刻的隐藏状态$s_{t’-1}$以及背景变量$c$，在加入attention机制后，这里的背景变量变成了$c_{t’}$，每一步的背景变量都不一样。</p>
<p><img src="/2019/02/15/NLP和Attention/st.png" alt=""></p>
<p><img src="/2019/02/15/NLP和Attention/attention.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;下面看一下$c_{t’}$是怎么设计的。 就是编码器的不同时刻的隐藏状态的加权平均。只是这里的权重$\alpha_{t’t}$在每一个时刻是一个变化的值。  注意这里的$t’$是输出(解码器)的时间戳，$t$是输入(编码器)的时间戳。首先我们先固定$t’$，下面的式子中$t’$是不变的。在计算$c_{t’}$时，变化$t从1到T$，遍历所有的$h_t$，然后给定一个$h_t$，怎么求$h_t$对应的权重$\alpha_{t’t}$。</p>
<p><img src="/2019/02/15/NLP和Attention/ct.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;下面我们看一下$\alpha_{t’t}$是怎么来表示。加权平均就要使所有的权值加起来为1，所以用到softmax运算。softmax中的每一个值是<br>$e$,这个是怎么计算的.$e_{t’t}$通过解码器上一时刻的隐藏变量$s_{t’-1}$和当前编码器的隐藏变量$h_t$计算得到。</p>
<p><img src="/2019/02/15/NLP和Attention/alpha.png" alt=""></p>
<p><img src="/2019/02/15/NLP和Attention/e.png" alt=""></p>
<p>&ensp;&ensp;&ensp;&ensp;注意力机制对函数$a$设计有很多。下面是一种设计方法。首先一定要有$s_{t’-1}$和$h_t$。然后引入了3个模型参数$v^T,W_s,W_h$,这3个参数通过训练得到。对$s_{t’-1}$做一个projection，对$h_t$做一个projection，使得projection之后的向量长度相等，这样就可以加在一起，然后使用tanh激活函数，这时的向量长度还是projection之后的长度，但是我们希望$e_{t’t}$是一个标量，那就再引入向量$v^T$，和右边的向量做一个点乘得到一个标量。其实注意力可以通过多层感知机（全连接层）得到。  </p>
<p><img src="/2019/02/15/NLP和Attention/a.png" alt=""></p>
<p>下面介绍计算解码器的隐藏变量时的函数$g$是什么？g可以看到是一个GRU单元</p>
<p><img src="/2019/02/15/NLP和Attention/g.png" alt=""></p>
<p>这里总结一下模型的参数都有哪些：编码器中的W和b，上式中解码器中的W和b，还有计算attention中的$v^T,W_s,W_h$。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>Seq2Seq</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN</title>
    <url>/2019/02/12/RNN/</url>
    <content><![CDATA[<h1><span id="1-门控循环单元gru">1. 门控循环单元GRU</span></h1><p>&ensp;&ensp;&ensp;&ensp;梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#1-%e9%97%a8%e6%8e%a7%e5%be%aa%e7%8e%af%e5%8d%95%e5%85%83gru">1. 门控循环单元GRU</a><ul>
<li><a href="#11-%e5%80%99%e9%80%89%e9%9a%90%e8%97%8f%e7%8a%b6%e6%80%81">1.1. 候选隐藏状态</a></li>
<li><a href="#12-%e9%9a%90%e8%97%8f%e7%8a%b6%e6%80%81">1.2. 隐藏状态</a></li>
<li><a href="#13-%e6%80%bb%e7%bb%93">1.3. 总结</a></li>
</ul>
</li>
<li><a href="#2-%e9%95%bf%e7%9f%ad%e6%9c%9f%e8%ae%b0%e5%bf%86lstm">2. 长短期记忆LSTM</a></li>
<li><a href="#rnn">RNN</a></li>
</ul>
<!-- /TOC -->
<p>&ensp;&ensp;&ensp;&ensp;门控循环神经网络的提出(2014年提出)正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。其中，门控循环单元是一种常用的门控循环神经网络。门控循环神经单元引入了重置门和更新门的概念，从而修改了循环神经网络中隐藏状态的计算方式。重置门和更新门的激活函数是sigmoid函数，可以将元素的值变换到0到1之间，因为重置门$R_t$和更新们$Z_t$中每个元素的值域都是[0,1]。</p>
<h2><span id="11-候选隐藏状态">1.1. 候选隐藏状态</span></h2><p>&ensp;&ensp;&ensp;&ensp;候选隐藏状态用来辅助后面的隐藏状态计算。重置门为0，意味着重置对应隐藏状态元素为0，即丢弃上一时间步的隐藏状态。如果重置门为1，表示保留上一时间步的隐藏状态。重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃和预测无关的历史信息。</p>
<h2><span id="12-隐藏状态">1.2. 隐藏状态</span></h2><p>&ensp;&ensp;&ensp;&ensp;时间步$t$的隐藏状态$H_t$的计算使用当前时间步的更新们$Z_t$来对上一时间步的隐藏状态$H_t-1$和当前时间步的候选隐藏状态$\tilde{H}_{t}$做组合。<br>&ensp;&ensp;&ensp;&ensp;更新门控制了包含当前时间步信息的候选隐藏状态如何流入隐藏状态。</p>
<p><img src="/2019/02/12/RNN/gru.png" alt=""></p>
<script type="math/tex; mode=display">X_t,H_{t-1},R_t(控制H_{t-1})----->\tilde{H}_t,H_{t-1},Z_t(控制\tilde{H}_t,H_{t-1})----->H_t</script><p>&ensp;&ensp;&ensp;&ensp;假设更新门$Z_t$在t时刻为1，那么时间步$t$的输入信息没有流入当前时间步的隐藏状态$H_t$，实际上，上一时间步的隐藏状态$H_{t-1}$保存并传递到当前时间步$t$。<strong>这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</strong></p>
<h2><span id="13-总结">1.3. 总结</span></h2><ul>
<li>重置门有助于捕捉时间序列里短期的依赖关系。重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃和预测无关的历史信息。</li>
<li>更新门有助于捕捉时间序列里长期的依赖关系。更新门$Z_t$在t时刻为1，那么时间步$t$的输入信息没有流入当前时间步的隐藏状态$H_t$，实际上，上一时间步的隐藏状态$H_{t-1}$保存并传递到当前时间步$t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。    </li>
<li>为什么叫GRU也叫做循环神经网络，因为门控循环单元中上一时间步的隐藏状态会传到当前时间步，体现了循环的性质。</li>
</ul>
<h1><span id="2-长短期记忆lstm">2. 长短期记忆LSTM</span></h1><p>&ensp;&ensp;&ensp;&ensp;另外一种常用的门控循环神经网络是LSTM(1997年提出)，比门控循环单元的结构稍微复杂一些。<br>&ensp;&ensp;&ensp;&ensp;GRU中的术语是：重置门，更新门，候选隐藏状态，隐藏状态。<br>&ensp;&ensp;&ensp;&ensp;LSTM的术语是：输入门，遗忘门，输出门，候选记忆细胞(与候选隐藏状态形状相同)，记忆细胞(与隐藏状态形状相同)，隐藏状态。<br>&ensp;&ensp;&ensp;&ensp;输入门$I_t$,遗忘门$F_t$,输出门$O_t$,候选记忆细胞$\tilde{C}_t$,记忆细胞$C_t$。<br>&ensp;&ensp;&ensp;&ensp;其中输入门$I_t$,遗忘门$F_t$,输出门$O_t$,候选记忆细胞$\tilde{C}_t$取决于$X_t和H_{t-1}$,记忆细胞$C_t$取决于$遗忘门F_t,C_{t-1},输入门I_t,\tilde{C}_t$</p>
<h1><span id="rnn">RNN</span></h1><p>循环神经网络(RNN,LSTM,GRU)的时间步长都是固定的。句子中词的个数表示时间步个数，如果句子长度不一样，需要补0进行padding。<br>构造样本时，一个样本是2维的，表示(时间步*特征数)</p>
<p><img src="/2019/02/12/RNN/lstm.jpg" alt=""></p>
<p>参考资料：<a href="https://www.zhihu.com/question/41949741" target="_blank" rel="noopener">https://www.zhihu.com/question/41949741</a></p>
<p><img src="/2019/02/12/RNN/公式.png" alt=""></p>
<p>LSTM实例化的时候只用指定隐藏层的单元个数，不用指定输出层的单元个数。即<br><code>lstm = LSTM(hidden_size=64,num_layers=1)</code></p>
<ol>
<li>LSTM在运算的时候需要传入的参数有2个，data和states<br>data的输入的数据，LSTM的输入维度默认是TNC，维度是(time_step,batch_size,input_size)。<br>states：list类型，里面有2个元素，用来初始化LSTM中的H和C。每个元素都是张量，维度是(num_layers,batch_size,num_hidden)，这个参数是可选的，可以不填。</li>
<li>输出有2个，分别是<code>out,out_states = lstm(input)</code>.<br>out的维度是：(time_step,batch_size,num_hidden)表示最后一层所有时间步的隐藏状态，该输出并不涉及输出层的设计，它们们通常作为后续输出层的输入。<br>out_states的维度是：输出每个LSTM层最后一个时间步的(H,C)，类型是list，里面有2个元素，每个元素的维度和输入的states一样，都是(num_layers,batch_size,num_hidden)。如果输入的states不填，那么out_states将不会被返回。</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>lovely dog</title>
    <url>/2019/02/12/lovely-dog/</url>
    <content><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;记得第一次养小狗狗，还是我上小学的时候，是一个白色的小狗，超级好看，我用香皂给它洗澡，后来家里种小麦的时候我爸用农药拌小麦，小狗吃了一些，就死了(((m-__-)m))。从那之后家里也没养过狗。<br><a id="more"></a></p>
<p> &ensp;&ensp;&ensp;&ensp;前几天去我奶奶家，她有一个小卖部，年级大了，不敢开车，让我弟弟去帮她进货，进货的时候要先把需要进什么货都写下来，我奶奶不识字，我就去帮她写。到她家发现她家的狗不见了，奶奶说前几天小狗和她去曹庄的时候丢了，我问小狗自己找不到家吗？因为小狗一直拴着，没有出去过，所以不记得家。当时觉得挺可惜的，那么好的一个小狗。今天我和我妈去伯党乡洗澡，路上看见一个小花狗，在大路上一直看来往的人，我妈就说这是不是你奶奶家的狗？我当时想，我奶奶家的狗是在曹庄丢的，应该不会在这，说应该不是吧，就走了。走了之后越来越感觉好像就是这只小狗，但当时我妈开车已经走了好远了，也不好意思回去找。突然我妈说：你看，这只小狗追着我们呢。一看，哇！真的是我奶奶家的狗，竟然认识我们，因为我和我妈都经常不在家，小狗也没见过我们几次，竟然记得我们，就跟着我们的车跑。我们停下车，想把它抱到车上，但是它老动，抱不上去，就让它在后面跟着我们的车跑。我们到洗澡的地方，把车停在院子里，考虑要不要把小狗狗锁在车里，怕再次跑丢，当时想的是我们的车就在这，它应该不会走吧。然后我们就去洗澡了，小狗狗在院子里。等我们洗完的时候，叫了好几声都没发现那个小狗，当时还挺自责的，为啥不把它锁在车里呢，小狗又丢了。我姥姥就在洗澡的附近，洗完澡和我妈又开车去我姥姥家了，路上看见小狗就觉得是我家的狗，到我姥姥家也没有找到。中午在我姥姥家吃了饭，待了一会就回家。路上我妈说你看看路上有没有小狗。又走到上午发现小狗的那条路上，不知道小狗从那出来的，看见我们又跟在我们的车后，看见小狗当时好高兴，心想，这只小狗好聪明，找不到我们又回到原来的地方，刚走一会就看见我奶奶开个车在找她家的小狗，我就叫我奶奶，说狗找到啦。小狗看见我奶奶一个劲的往她身上蹭，我奶奶看见小狗，眼睛都红了，对着小狗说：这几天去哪了，也不知道回家。我奶奶把小狗抱上车回家了。<br> &ensp;&ensp;&ensp;&ensp;看到小狗又找到了，真的好高兴。以前看过《忠犬八公的故事》，电影中的小狗的主人因病去世了，但小狗狗每次到下班的点都去地铁站等着主人，但是主人再也不会从出站口出来了。小狗真的好有灵性，我们养只小狗可能觉得好玩，看家，小狗只是我们生活的一部分，但是我们却是它生活的全部。真的不敢想我奶奶家的小狗如果没有找到，又找不到回家的路该怎么办，在外面吃啥睡哪，万幸小狗狗找到了，以后不要再乱跑了，在家陪着奶奶吧.</p>
<p> <img src="/2019/02/12/lovely-dog/dog3.jpg" alt=""></p>
<p> <img src="/2019/02/12/lovely-dog/dog1.jpg" alt=""></p>
<p> <img src="/2019/02/12/lovely-dog/dog2.jpg" alt=""></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN和残差</title>
    <url>/2019/02/05/CNN+%E6%AE%8B%E5%B7%AE/</url>
    <content><![CDATA[<h1><span id="1-卷积">1. 卷积</span></h1><p>卷积操作需要有1一个数组和一个卷积核，假设卷积核的形状为pxq，代表卷积核的高和宽。二维卷积层的输入输出用4维表示，格式为(样本，通道，高，宽)<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e5%8d%b7%e7%a7%af">1. 卷积</a><ul>
<li><a href="#11-%e5%a1%ab%e5%85%85">1.1. 填充</a></li>
<li><a href="#12-%e6%ad%a5%e5%b9%85">1.2. 步幅</a></li>
<li><a href="#13-%e5%b0%8f%e7%bb%93">1.3. 小结</a></li>
<li><a href="#14-%e9%80%9a%e9%81%93channel">1.4. 通道channel</a></li>
<li><a href="#15-%e6%b1%a0%e5%8c%96%e5%b1%82">1.5. 池化层</a></li>
<li><a href="#16-cnn%e5%ba%94%e7%94%a8">1.6. CNN应用</a></li>
</ul>
</li>
<li><a href="#2-cnn%e4%ba%94%e5%a4%a7%e7%bb%8f%e5%85%b8%e6%a8%a1%e5%9e%8b">2. CNN五大经典模型</a><ul>
<li><a href="#21-lenet">2.1. LeNet</a></li>
<li><a href="#22-alexnet">2.2. AlexNet</a></li>
<li><a href="#23-googlenet">2.3. GoogleNet</a></li>
<li><a href="#24-vgg">2.4. VGG</a></li>
<li><a href="#25-nin">2.5. NiN</a></li>
<li><a href="#26-%e6%89%b9%e9%87%8f%e5%bd%92%e4%b8%80%e5%8c%96%e5%b1%82">2.6. 批量归一化层</a></li>
<li><a href="#27-%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9cresnet">2.7. 残差网络ResNet</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p>假设输入的形状为$n_h,n_w$,卷积核的形状为$k_hxk_w$，那么输出的形状为  </p>
<script type="math/tex; mode=display">(n_h-k_h+1)*(n_w-k_w+1)</script><p>卷积层的输出形状由输入形状和卷积核窗口形状决定，下面介绍卷积层的两个超参数，填充和步幅。  </p>
<h2><span id="11-填充">1.1. 填充</span></h2><p>填充通常在输入的高和宽填充0元素，如果在高的<strong>两侧一共</strong>填充$p_h$行，在宽的<strong>两侧一共</strong>填充$p_w$列，那么输出形状为</p>
<script type="math/tex; mode=display">(n_h-k_h+p_h+1)*(n_w-k_w+p_w+1)</script><h2><span id="12-步幅">1.2. 步幅</span></h2><p>步幅表示卷积核一次移动的个数，当高的步幅为$s_h$,宽的步幅为$s_w$，输出形状为</p>
<script type="math/tex; mode=display">\left\lfloor(n_h-k_h+p_h+s_h)/s_h\right\rfloor*\left\lfloor(n_w-k_w+p_w+s_w)/s_w\right\rfloor</script><h2><span id="13-小结">1.3. 小结</span></h2><ul>
<li><strong>填充可以增加输出的高和宽，常用来使输出与输入具有相同的高和宽</strong></li>
<li><strong>步幅可以减小输出的高和宽，使得输出的高和宽为输入的$1/n$</strong>  </li>
</ul>
<h2><span id="14-通道channel">1.4. 通道channel</span></h2><p>通道(channel)：每个卷积层中卷积核的数量。<a href="https://blog.csdn.net/sscc_learning/article/details/79814146" target="_blank" rel="noopener">这篇文章</a>关于channel讲的很好<br>下面X(x_in,h,w)<br>K(k_out,k_in,h,w)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = nd.random.uniform(shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = nd.random.uniform(shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2><span id="15-池化层">1.5. 池化层</span></h2><p>pooling层(池化层)的输入一般是上一个卷积层，主要有以下2个作用：  </p>
<ol>
<li>保留主要的特征，同时减少下一层的参数和计算量，防止过拟合</li>
<li>保持某种不变性，包括平移，旋转，常用的平均池化和最大池化<br><strong>池化层的输出通道数和输入通道数相同</strong></li>
</ol>
<h2><span id="16-cnn应用">1.6. CNN应用</span></h2><p>（1）2D卷积，输入和输出形状一样：一般kernel_size=(3,3),padding=1,stride=1，输入和输出的形状一样<br>（2）2D卷积，输入和输出高和宽减半：kernel_size=(3,3),padding=1,stride=2，输出的形状是输入一半<br>（3）3D卷积，一般kernel_size=(3,3,3),padding=1,stride=1，输入和输出的形状一样<br>（4）3D卷积，一般kernel_size=(1,1,1),padding=0,stride=1，输入和输出的形状一样</p>
<h1><span id="2-cnn五大经典模型">2. CNN五大经典模型</span></h1><ol>
<li>Lenet：1986年</li>
<li>Alexnet：2012年</li>
<li>GoogleNet：2014年</li>
<li>VGG：2014年</li>
<li>Deep Residual Learning：2015年</li>
</ol>
<h2><span id="21-lenet">2.1. LeNet</span></h2><p>LeNet交替使用卷积层和最大池化层后接全连接层进行图像分类。网络结构如下所示</p>
<p><img src="/2019/02/05/CNN+残差/LeNet.png" alt="LeNet"></p>
<h2><span id="22-alexnet">2.2. AlexNet</span></h2><p>2012年，ImageNet比赛冠军的model—AlexNet，以第一作者alex命名。这个model的意义比后面的那些model都大很多。首先它证明了CNN在复杂模型下的有效性，然后GPU实现使得训练在可接受的时间范围内得到结果，让CNN和GPU都火了一把。<br>AlexNet包含8层变换，其中5层卷积和2层全连接层隐藏层，1个全连接输出层。<br>AlexNet将sigmoid激活函数改成了简单的ReLu激活函数。一方面，ReLu激活函数更简单，例如它没有sigmoid激活函数中的求幂运算。另一方面，ReLu激活函数在不同的参数初始化方法下使得模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度为0，从而造成反向传播无法继续更新部分模型参数；而ReLu激活函数在正区间的梯度恒为1.因为，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p>
<h2><span id="23-googlenet">2.3. GoogleNet</span></h2><p>2014年的ImageNet图像s识别挑战赛的冠军。<br>GoogleNet中的基础卷积块叫做Inception块。</p>
<p><img src="/2019/02/05/CNN+残差/Inception.png" alt=""></p>
<h2><span id="24-vgg">2.4. VGG</span></h2><p><strong>VGG卷积块</strong>的组成规律是：连续使用数个相同的填充为1，窗口形状为3x3的卷积层后接一个步幅为2，窗口形状为2x2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。<br>VGG网络=VGG卷积块+n个全连接层<br>VGG卷积块=n个相同的卷积层+1个最大池化层</p>
<h2><span id="25-nin">2.5. NiN</span></h2><p>前面介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以卷积层构成的模块充分抽取空间特征，再以全连接层构成的模块来输出分类结果。其中AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节介绍网络中的网络（NiN），即串联多个由卷积层和全连接层构成的 小网络来构建一个深层网络。</p>
<p><img src="/2019/02/05/CNN+残差/NiN.png" alt=""></p>
<p>解决深度为<br><strong>全连接层可以由1x1卷积层充当</strong><br>NiN块是NiN中的基本块。它由一个卷积层加两个充当全连接层的1x1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二个和第三个卷积层的超参数一般是固定的。</p>
<h2><span id="26-批量归一化层">2.6. 批量归一化层</span></h2><p>标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为0，标准差为1.标准化处理输入数据使各个特征的分布相近：这样往往更容易训练处有效的模型。<br>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化但对深层神经网络来说，即使输入数据已经做了标准化，训练中模型参数的更新依然很容易造成靠近输出层的输出剧烈变化。这种计算数值的不稳定性通常令我们难以训练处有效的深度模型。<br>标准归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，<strong>批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出</strong>，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>BatchNorm主要是让训练收敛更快。</strong><br>对全连接层和卷积层做批量归一化的方法不同。</p>
<ul>
<li>对全连接层做批量归一化<br>权重参数和偏差参数分别为$W和b$，激活函数为$\phi$,批量归一化运算符为$BN$,使用批量归一化的全连接层的输出为</li>
</ul>
<script type="math/tex; mode=display">\phi(BN(Wx+b))</script><ul>
<li>对卷积层做批量归一化<br>对卷积层来说，批量归一化发生在卷积计算之后，应用激活函数之前.<br>对于前面的模型，我们可以在卷积层或全连接层之后、激活层之前加入批量归一化层，以LeNet为例：</li>
<li>未加入批量归一化层</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(channels=<span class="number">6</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.MaxPool2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">       nn.Conv2D(channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.MaxPool2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">       nn.Dense(<span class="number">120</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.Dense(<span class="number">84</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>加入批量归一化层<br>在卷积层或全连接层之后，激活层之前加入批量归一化层</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2D(<span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Dense(<span class="number">120</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.Dense(<span class="number">84</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h2><span id="27-残差网络resnet">2.7. 残差网络ResNet</span></h2><p>2015年ImageNet冠军model。<br>深度网络的好处：特征的等级随着网络深度的加深二变高，及其深的深度使得该网络拥有强大的表达能力。<br>但不是网络层数越多，效果就越好。随着网络深度的加深，(1)会出现梯度衰减的问题，在反向传播时，使梯度不断下降直至消失，对于权重的更新会越来越慢，直至不更新。(2)并且较深层网络比较浅的网络有更高的训练误差，称为退化问题。<br>深度残差网络主要思想很简单，就是在标准的前馈卷积网络上，加一个跳跃绕过一些层的连接。每绕过一层就产生一个残差块(residual block)，卷积层预测加输入张量的残差。普通的深度前馈网络难以优化。除了深度，所加层也使得training和validation的错误率增加，即使用上了batch normalization也是如此。残差神经网络由于存在shorcut connections，网络间的数据流通更为顺畅。残差网络结构的解决方案是，增加卷积层输出求和的捷径连接。<br>实验表明，残差网络更容易优化，并且能够通过增加相当的深度来提高准确率。核心是解决了增加深度带来的副作用（退化问题），这样能够通过单纯地增加网络深度，来提高网络性能。</p>
<ul>
<li>网络的深度为什么重要？<br>因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。</li>
<li><p>为什么不能简单地增加网络层数？<br>对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。<br>对于该问题的解决方法是正则化初始化和中间的正则化层（Batch Normalization），这样的话可以训练几十层的网络。虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。 作者通过实验：通过浅层网络+ y=x 等同映射构造深层模型，结果深层模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。</p>
</li>
<li><p>怎么解决退化问题？<br>深度残差网络。如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。但是，如果把网络设计为H(x) = F(x) + x。我们可以转换为学习一个残差函数F(x) = H(x) - x. 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。</p>
</li>
</ul>
<p><img src="/2019/02/05/CNN+残差/ResNet1.png" alt=""></p>
<p>二层平原网络我们根据输入$x$,去拟合$H(x)$,$H(x)$是任意一种理想的映射，希望第2层权重输出能够与理想$H(x)$拟合。</p>
<p><img src="/2019/02/05/CNN+残差/ResNet2.png" alt=""></p>
<p>为了解决深度神经网络的2个问题，提出残差网络ResNet。</p>
<p><img src="/2019/02/05/CNN+残差/ResNet3.png" alt=""></p>
<p>残差是$F(X)$，让$F(x)=0$，这样$H(X)就趋近于x，是一个恒等映射$，输出和输入相等，这样计算增加网络深度，也不会造成训练误差上升（退化问题）。</p>
<p><img src="/2019/02/05/CNN+残差/ResNet4.png" alt=""></p>
<p><img src="/2019/02/05/CNN+残差/ResNet5.png" alt=""></p>
<p><img src="/2019/02/05/CNN+残差/ResNet6.png" alt=""></p>
<p>残差网络的基础块是残差块，在残差块中，输入可通过跨层的数据线路更快地向前传播。<br><img src="/2019/02/05/CNN+残差/ResNet7.png" alt=""></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Gluon编程学习</title>
    <url>/2019/01/31/Gluon%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<p>以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-softmax%e5%9b%9e%e5%bd%92">1. Softmax回归</a></li>
<li><a href="#2-sigmoid%e5%92%8csoftmax">2. Sigmoid和Softmax</a></li>
<li><a href="#3-%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0">3. 交叉熵损失函数</a><ul>
<li><a href="#31-softmax%e8%bf%90%e7%ae%97%e6%ad%a5%e9%aa%a4">3.1. softmax运算步骤</a></li>
</ul>
</li>
<li><a href="#4-%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95">4. 优化算法</a><ul>
<li><a href="#41-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgd">4.1. 梯度下降GD</a></li>
<li><a href="#42-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dsgd">4.2. 随机梯度下降SGD</a></li>
<li><a href="#43-%e5%b0%8f%e6%89%b9%e9%87%8f%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d">4.3. 小批量随机梯度下降</a></li>
<li><a href="#44-%e5%ad%a6%e4%b9%a0%e7%8e%87">4.4. 学习率</a></li>
</ul>
</li>
<li><a href="#5-batch-size">5. batch size</a><ul>
<li><a href="#51-%e6%80%bb%e7%bb%93">5.1. 总结</a></li>
</ul>
</li>
<li><a href="#6-%e4%bd%bf%e7%94%a8gluon%e5%ae%9a%e4%b9%89%e6%a8%a1%e5%9e%8b">6. 使用gluon定义模型</a><ul>
<li><a href="#61-%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92">6.1. 线性回归</a></li>
<li><a href="#62-softmax%e5%9b%9e%e5%bd%92">6.2. softmax回归</a></li>
<li><a href="#63-%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba">6.3. 多层感知机</a></li>
</ul>
</li>
<li><a href="#7-%e8%bf%87%e6%8b%9f%e5%90%88%e5%92%8c%e6%ac%a0%e6%8b%9f%e5%90%88">7. 过拟合和欠拟合</a><ul>
<li><a href="#71-%e9%aa%8c%e8%af%81%e6%95%b0%e6%8d%ae%e9%9b%86">7.1. 验证数据集</a></li>
<li><a href="#72-%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f">7.2. 权重衰减</a></li>
<li><a href="#73-%e4%b8%a2%e5%bc%83%e6%b3%95">7.3. 丢弃法</a></li>
</ul>
</li>
<li><a href="#8-%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e5%88%9d%e5%a7%8b%e5%8c%96">8. 模型参数初始化</a><ul>
<li><a href="#81-%e9%9a%8f%e6%9c%ba%e5%88%9d%e5%a7%8b%e5%8c%96">8.1. 随机初始化</a></li>
<li><a href="#82-xavier%e9%9a%8f%e6%9c%ba%e5%88%9d%e5%a7%8b%e5%8c%96">8.2. Xavier随机初始化</a></li>
<li><a href="#83-%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e7%9a%84%e5%bb%b6%e5%90%8e%e5%88%9d%e5%a7%8b%e5%8c%96">8.3. 模型参数的延后初始化</a></li>
</ul>
</li>
<li><a href="#9-gpu%e8%ae%a1%e7%ae%97">9. GPU计算</a></li>
<li><a href="#10-block">10. Block</a><ul>
<li><a href="#101-sequential%e5%92%8cblock%e7%9a%84%e5%85%b3%e7%b3%bb">10.1. Sequential和Block的关系</a></li>
<li><a href="#102-%e4%bd%bf%e7%94%a8block%e8%87%aa%e5%ae%9a%e4%b9%89%e5%b1%82">10.2. 使用Block自定义层</a></li>
<li><a href="#103-%e4%b8%8d%e5%90%ab%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e7%9a%84%e8%87%aa%e5%ae%9a%e4%b9%89%e5%b1%82">10.3. 不含模型参数的自定义层</a></li>
<li><a href="#104-%e5%90%ab%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e7%9a%84%e8%87%aa%e5%ae%9a%e4%b9%89%e5%b1%82">10.4. 含模型参数的自定义层</a></li>
</ul>
</li>
<li><a href="#11-%e6%b7%b7%e5%90%88%e7%bc%96%e7%a8%8b">11. 混合编程</a><ul>
<li><a href="#111-%e4%bd%bf%e7%94%a8hybridsequential%e7%b1%bb%e6%9e%84%e9%80%a0%e6%a8%a1%e5%9e%8b">11.1. 使用HybridSequential类构造模型</a></li>
<li><a href="#112-%e4%bd%bf%e7%94%a8hybridblock%e8%87%aa%e5%ae%9a%e4%b9%89%e6%a8%a1%e5%9e%8b">11.2. 使用HybridBlock自定义模型</a></li>
</ul>
</li>
<li><a href="#12-%e7%96%91%e6%83%91">12. 疑惑</a><ul>
<li><a href="#121-bn">12.1. BN</a></li>
</ul>
</li>
<li><a href="#13-%e5%81%8f%e7%bd%ae">13. 偏置</a></li>
<li><a href="#14-%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0">14. 模型参数</a></li>
<li><a href="#15-%e8%87%aa%e5%ae%9a%e4%b9%89%e5%b1%82">15. 自定义层</a></li>
</ul>
<!-- /TOC -->
<h1><span id="1-softmax回归">1. Softmax回归</span></h1><ol>
<li>Softmax回归是用来分类的，输入的个数表示特征，输出的个数表示类别。</li>
<li>Softmax运算</li>
</ol>
<script type="math/tex; mode=display">\hat{y_1},\hat{y_2},\hat{y_3}=softmax(o_1,o_2,o_3)</script><p>&ensp;&ensp;其中</p>
<script type="math/tex; mode=display">\hat{y}_1=\frac{\exp(o_1)}{\sum_{i=1}^3\exp{(o_i)}},\qquad \hat{y}_2=\frac{\exp(o_2)}{\sum_{i=1}^3\exp{(o_i)}},\qquad\hat{y}_3=\frac{\exp(o_3)}{\sum_{i=1}^3\exp{(o_i)}}</script><p>&ensp;&ensp;&ensp;&ensp;softmax运算是把数据归一化到(0,1)之间。Softmax回归中有Softmax运算才可以使得输出的结果相加为1。如果没有softmax运算，输出结果也是可以用来分类的，例如$y_1=0.1$,$y_2=10$,$y_3=0.1$，最终属于的类别是2。但是如果$y_1=100$,$y_2=10$,$y_3=0.1$，最终属于的类别是1，没有经过softmax运算，会使得输出层的输出值的范围不确定，难以直观判断这些值的意义。</p>
<h1><span id="2-sigmoid和softmax">2. Sigmoid和Softmax</span></h1><p>&ensp;&ensp;&ensp;&ensp;<strong>sigmoid通常用于二分类，不用于多分类。现在的深度学习模型多分类最后一层都是softmax</strong>。Softmax是把一个向量映射成另一个向量，这个向量中每个值在0~1之间，且元素之和为1。sigmoid作为最后一层输出一个值，这个值在(0,1)之间，表示属于正例（一类）的概率。softmax会输出n个值，这n个值在(0,1)之间，表示属于n个类的概率。</p>
<h1><span id="3-交叉熵损失函数">3. 交叉熵损失函数</span></h1><p>&ensp;&ensp;&ensp;&ensp;对于2分类来说，模型最后通常会经过一个sigmoid函数，真实的标签是[0,1]，sigmoid函数会输出一个概率值，这个概率值反映了这个样本属于正类1的概率，在(0,1)之间。分类问题的准确性通过交叉熵损失函数来判定。单个样本的交叉熵损失函数的公式:</p>
<script type="math/tex; mode=display">L = -[ylog\hat{y}+(1-y)log(1-\hat{y})]</script><p>如果是计算N个样本总的损失函数，只要将N个loss叠加就可以了：</p>
<script type="math/tex; mode=display">L = -\sum_{i=1}^{N}y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})</script><p>交叉熵损失函数可以表示真实样本标签和预测标签之间的差值。先看一个样本的交叉熵损失函数，当真实标签$y=1$时，$L = -log\hat{y}$ ，这时，损失L与预测输出的关系如下：</p>
<p><img src="/2019/01/31/Gluon编程/cross1.png" alt="softmax运算例子"></p>
<p>&ensp;&ensp;&ensp;&ensp;横坐标是预测输出，纵坐标是误差。当预测输出越接近1，损失函数L越小，预测输出越接近0，L越大。因此函数的变化趋势完全符合实际需要的情况。L表示预测输出与真实y的差距。模型输出的$\hat{y}$反映了这个样本属于正例的概率，当输出为1时，说明这个样本属于正例的概率为1，即预测为正例，L=0。预测出样本属于正例的概率越大，L越小。<br>&ensp;&ensp;&ensp;&ensp;在多分类中，使用softmax作为最后的输出层，输出的是一个向量，经过softmax运算会得到这个样本属于n个类的概率，在计算交叉熵时，需要用到这个概率向量。一个样本真实的分类结果可以用一个向量来表示，其中只有一个是1，其余全为0。第i个样本真实的向量为$\boldsymbol{y}^{(i)}$,预测的向量为$\boldsymbol{\hat{y}^{(i)}}$<br>交叉熵用来评估预测值和真实值之间的差异</p>
<script type="math/tex; mode=display">H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\sum_{j=1}^q y_j^{(i)}\log\hat{y}_j^{(i)}</script><p>&ensp;&ensp;&ensp;&ensp;向量$\boldsymbol{}y^{(i)}$中共有q个元素，其中只有一个元素为1，其余全部为0，于是$H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\log\hat{y}_j^{(i)}$,<strong>交叉熵只关心正确类别的预测概率</strong>，比如样本$i$的真实类别为5，那么只关心预测向量$\hat{y}^{(i)}$中的第5个元素，即样本$i$属于第5个类别的概率。<br>假设训练数据集的样本个数为$n$，交叉熵损失函数定义为</p>
<script type="math/tex; mode=display">\ell(\boldsymbol{\Theta})=\frac{1}{n}\sum_{i=1}^nH\left(\boldsymbol{y^{(i)}},\hat{y}^{(i)}\right)</script><p>&ensp;&ensp;&ensp;&ensp;其中$\Theta$代表模型参数，对于这$n$个样本，每一个样本都求出这个样本的交叉熵。如果是一个样本只属于一个类，那么向量$\boldsymbol{}y^{(i)}$中只有1个为1，其余全为0。交叉熵损失函数可以简写成$\ell(\boldsymbol{\Theta})=-\frac{1}{n}\sum_{i=1}^n\log\hat{y}_j^{(i)}$,若要交叉熵损失函数$\ell(\boldsymbol{\Theta})$最小，就要使$\sum_{i=1}^n\log\hat{y}_j^{(i)}$最大，即最大化$\prod_{i=1}^n\hat{y}_j^{(i)}$，即每个样本属于自己正确类别的联合概率。</p>
<h2><span id="31-softmax运算步骤">3.1. softmax运算步骤</span></h2><p>&ensp;&ensp;&ensp;&ensp;对于分类问题，输出的结果是$O$，$O$是一个矩阵，其中行数表示样本的个数，列数表示类别的个数，假设有100个样本，5类，$O$是一个100*5的矩阵。通过softmax运算使得一行的和为1，可以直观的看出每个样本属于每个样本的概率大小。</p>
<ul>
<li>首先对矩阵的中每个元素做exp()运算</li>
<li>计算出每一行的sum()</li>
<li><p>然后用一行中的每个元素/该行的sum()<br> <img src="/2019/01/31/Gluon编程/softmax运算.png" alt="softmax运算例子"></p>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">  X_exp = X.exp()</span><br><span class="line">  partition = X_exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1><span id="4-优化算法">4. 优化算法</span></h1><p>&ensp;&ensp;&ensp;&ensp;优化算法就是用来更新模型参数的一种算法，模型在训练的时候通过反向传播计算梯度，然后更新模型参数，使得模型的损失越来越小，当模型参数不再变化时，训练结束。</p>
<h2><span id="41-梯度下降gd">4.1. 梯度下降GD</span></h2><p>&ensp;&ensp;&ensp;&ensp;一次迭代中更新一次模型参数，梯度下降在每一次迭代中，使用整个训练数据集来计算梯度，更新一次参数。一个epoch只有一次迭代，下一次epoch再次使用所有的训练数据集更新模型参数。  </p>
<h2><span id="42-随机梯度下降sgd">4.2. 随机梯度下降SGD</span></h2><p>&ensp;&ensp;&ensp;&ensp;梯度下降每次更新模型参数时都需要遍历所有的data，当数据量太大或者一次无法获取全部数据时，这种方法并不可行。这个问题基本思路是：每次迭代只通过一个随机选取的数据$(x_n,y_n)$来获取梯度，以此对w进行更新，这种方法叫做随机梯度下降。一次迭代使用一个样本更新模型参数，这样一个epoch就需要很多次迭代，每次迭代随机采样一个样本更新模型参数。<br>&ensp;&ensp;&ensp;&ensp;小批量随机梯度下降中，当批量大小为1时是随机梯度下降；当批量大小为训练数据样本数时是梯度下降。当batch size较小时，每次迭代中使用的样本少，导致并行处理和内存使用效率变低。这使得在计算相同数据样本的情况下比使用更大batch size时所花的时间更多，即相同的训练数据，batch size越小，训练时间越长。当批量较大时，每个批量梯度里可能含有更多的冗余信息，为了得到较好的模型参数，批量较大时比批量较小时需要计算的样本数目可能更多，即迭代周期数多。</p>
<ul>
<li>相同的训练数据，batch size较小比batch size大时需要的训练时间长。</li>
<li>相同的训练数据，batch size大时，为了达到和batch size小时一样的训练效果，需要的epoch多。</li>
</ul>
<h2><span id="43-小批量随机梯度下降">4.3. 小批量随机梯度下降</span></h2><p>小批量随机梯度下降：在每次迭代中，随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，更新模型参数。<br><strong>小批量随机梯度下降的学习率可以在迭代中自我衰减</strong></p>
<h2><span id="44-学习率">4.4. 学习率</span></h2><p>&ensp;&ensp;&ensp;&ensp;当学习率很小时，模型参数更新非常慢，训练时间会很长。当学习率很大时，模型可能会越过最优解，导致模型不收敛，训练误差会越来越大，出现nan。当loss出现nan的时候，可以减少学习率。</p>
<h1><span id="5-batch-size">5. batch size</span></h1><p>&ensp;&ensp;&ensp;&ensp;梯度下降是用来寻找模型最佳的模型参数w和b的迭代优化<strong>算法</strong>，通过最小化损失函数(线性回归的平方差误差、softmax的交叉熵损失函数),来寻找w和b。<br>&ensp;&ensp;&ensp;&ensp;只有在数据量比较大的时候，才会用到epoch和batch size和迭代，但这3个词代表什么意思呢？一直不太清楚</p>
<ul>
<li>epoch：当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个epoch。</li>
<li>batch size：当一个完整数据集太大时，不能一次将全部数据输入到神经网络中进行训练，所以需要将完整数据集进行分块，每块样本的个数就是batch size。batch size是为了在内存效率和内存容量之间寻找最佳平衡。</li>
<li>迭代：就是以batch size向神经网络中输入样本，将完整数据集输入到神经网络中所需的次数，即完成一次epoch的次数。迭代数=batch的个数。比如完整数据集2000个样本，每个batch有200个样本，那么共有10个batch，完成一个epoch需要10次迭代。<br>在读取数据的时候传入一个参数batch_size,这个函数返回的X和y分别是含有batch_size个样本的特征和标签。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, y)</span><br></pre></td></tr></table></figure>
<p> &ensp;&ensp;&ensp;&ensp;在进行小批量随机梯度算法中，一个batch size更新一次梯度，如果完整训练集中有2000个样本，一个batch有200个样本，那么一次epoch中更新10次模型参数。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">   <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">       <span class="comment"># 这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</span></span><br><span class="line">       param[:] = param - lr * param.grad / batch_size</span><br></pre></td></tr></table></figure>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期epoch中，会使用所有的训练样本一次</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="comment">#每次读取batch_size个样本的特征和标签，用来训练，一个batch更新一次模型参数</span></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用batch size个样本来更新模型参数w和b</span></span><br><span class="line">    <span class="comment">#一个epoch之后，使用更新后的w和b来计算误差。传入的参数是一个 </span></span><br><span class="line">    <span class="comment"># list，里面有所有样本的预测值和真实值，返回的train_l也是一个list，</span></span><br><span class="line">    <span class="comment"># 包含每个样本的真实值和预测值的误差。print中输出的是</span></span><br><span class="line">    <span class="comment"># 一个标量：train_l.mean().asnumpy()，对于所有的样本的误差求一个平均值输出</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>
<h2><span id="51-总结">5.1. 总结</span></h2><p> CIFAR10 数据集有 50000 张训练图片，10000 张测试图片。现在选择 Batch Size = 500 对模型进行训练.  </p>
<ul>
<li>每个epoch要训练的图片数量：50000</li>
<li>训练集中具有的batch个数：50000/500=100</li>
<li>每次epoch需要的batch个数：100</li>
<li>每次epoch需要的迭代(iteration)个数：100 </li>
<li>每次epoch中更新模型参数的次数：100</li>
<li>如果有10个epoch，模型参数更新的次数为：100*10=1000</li>
<li>一次epoch使用的是全部的训练集50000中图片，下一次epoch中使用的还是这50000张图片，但是对模型参数的权重更新值却是不一样的，因为不同epoch的的模型参数不一样，模型训练的次数越多，损失函数越小，越接近谷底。</li>
<li>适当增加batch size的优点：<br>（1）提高内存利用率<br>（2）一次epoch的迭代次数减少，相同数据量的处理速度更快，但是达到相同精度所需的epoch越多<br>（3）梯度下降方向准确度增加，训练震荡越小</li>
<li>减少batch size的缺点<br>（1）小的batch size引入的随机性越大，难以达到收敛</li>
</ul>
<h1><span id="6-使用gluon定义模型">6. 使用gluon定义模型</span></h1><p><strong>在gluon中无须指定每一层输入的形状，例如线性回归的输入个数，当模型得到数据时，例如执行后面的net(X)时，模型将自动推断出每一层的输入个数</strong></p>
<h2><span id="61-线性回归">6.1. 线性回归</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先导入nn模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#导入初始化模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="comment">#导入损失函数模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line"><span class="comment">#先定义一个模型变量net,sequential可以看做是串联各个层的容器，在构造模型时，向该容器依次添加层</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="comment">#全连接层是Dense(),定义全连接层的输出层个数为1</span></span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数：w和b,w初始化为均值为0，标准差为0.01的正太分布，b默认初始化为0</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#平方差损失</span></span><br><span class="line">loss = gloss.L2Loss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD,该优化算法将用来更新通过add添加的层所包含的全部参数</span></span><br><span class="line">tariner = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.03</span>&#125;)</span><br><span class="line"><span class="comment">#在训练模型时，调用Trainer实例的step()函数来更新模型参数w和b</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,num_epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autogard.record():</span><br><span class="line">            <span class="comment">#计算预测值和真实值的误差，l是一个长度为batch_size的数组</span></span><br><span class="line">            l=loss(net(X),y)</span><br><span class="line">        <span class="comment">#因为上面l是一个数组，实际是执行l.sum().backward()，把l变成标量</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#更新w和b，损失函数对w和b求梯度，w=w-r*Δ(l/w)/batch_size,</span></span><br><span class="line">        tariner.step(batch_size)</span><br><span class="line">    <span class="comment">#一个epoch结束，输入所有的训练数据集，更新完w和b，使用更新后的w和b，对所有的数据进行预测，计算预测值和真实值的误差l，这个l是一个len(所有样本)的数组，每个元素表示一个样本的预测值和真实值的误差，print对l求均值l.mean()变成标量</span></span><br><span class="line">    l = loss(net(features),label)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>
<h2><span id="62-softmax回归">6.2. softmax回归</span></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(Dense(<span class="number">10</span>))<span class="comment">#输出层有10个神经元，10个类别</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#定义交叉熵损失函数</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD</span></span><br><span class="line">trainer=gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.1</span>&#125;)</span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat,y).sum</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        y=y.astype(<span class="string">'float32'</span>)</span><br><span class="line">        train_l_sum+=l.asscalar()</span><br><span class="line">        train_acc_sum+=(y_hat.argmax(axis=<span class="number">1</span>)=y).sum().acscalar()</span><br><span class="line">        n+=y.size</span><br><span class="line">    test_acc=evaluate_accuracy(test_iter,net)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br></pre></td></tr></table></figure>
<h2><span id="63-多层感知机">6.3. 多层感知机</span></h2><p>输入层、隐藏层256个节点，输出层10个节点,relu激活函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line">loss = gluon.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.01</span>&#125;)</span><br><span class="line">batch_size=<span class="number">256</span></span><br></pre></td></tr></table></figure>
<h1><span id="7-过拟合和欠拟合">7. 过拟合和欠拟合</span></h1><h2><span id="71-验证数据集">7.1. 验证数据集</span></h2><p>测试数据集只能在所有超参数和模型参数都选定后使用一次。不可以使用测试数据集选择模型参数。所以需要验证集用来选择模型，验证集不参会模型训练。</p>
<h2><span id="72-权重衰减">7.2. 权重衰减</span></h2><p>权重衰减等于L2范数正则化，用来减少过拟合</p>
<h2><span id="73-丢弃法">7.3. 丢弃法</span></h2><p>深度学习模型常常使用丢弃法(dropout)来应对过拟合。在训练过程中，对<strong>隐藏层</strong>使用丢弃法，这样隐藏层中的某些神经元将会为0，即被丢弃。下图是一个多层感知机，隐藏层有5个神经元。  </p>
<p><img src="/2019/01/31/Gluon编程/多层感知机.png" alt="多层感知机"></p>
<p>其中</p>
<script type="math/tex; mode=display">h_i=\phi(x_1w_{1i}+x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i)</script><p>隐藏层计算的结果$h_i$将以$p$的概率被丢弃，即$h_i=0$,丢弃概率$0&lt;=p&lt;=1$。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1$…$h_5$中的任一个都有可能被清零，输出层的计算无法过度依赖隐藏层$h_1$…$h_5$中的任一个，从而在训练模型时起到正则化的作用，用来应对过拟合。<strong>在测试模型时，为了拿到更加确定的结果，一般不使用丢弃法。</strong><br>假设$h_2=0,h_5=0$,使用丢弃法之后的模型为</p>
<p><img src="/2019/01/31/Gluon编程/丢弃感知机.png" alt="丢弃后的多层感知机"></p>
<p><strong>代码实现</strong><br>在Gluon中，只需要在全连接层后面添加Dropout层并指定丢弃概率。在训练模型时，Dropout将以指定的丢失概率随机丢弃上一层的输出元素；在测试模型时，Dropout不起作用。<br><strong>一般在靠近输入层的丢弃率较小</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">        nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure>
<h1><span id="8-模型参数初始化">8. 模型参数初始化</span></h1><h2><span id="81-随机初始化">8.1. 随机初始化</span></h2><p>在Mxnet中，随机初始化通过net.initialize(init.Normal(sigma=0.01))对模型的权重参数w采用正太分布的随机初始化。如果不指定初始化方法，如net.initialize()，默认的初始化方法：权重参数w每个元素随机采样于-0.07到0.07之间的均匀分布，偏差b为0。</p>
<h2><span id="82-xavier随机初始化">8.2. Xavier随机初始化</span></h2><p>假设某全连接层的输入个数为$a$，输出个数为$b$,Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p>
<script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a+b}},\sqrt{\frac{6}{a+b}})</script><h2><span id="83-模型参数的延后初始化">8.3. 模型参数的延后初始化</span></h2><p>模型net在调用初始化函数 initialize之后，在做前向计算net(X)之前，权重参数的形状出现了0.</p>
<p><img src="/2019/01/31/Gluon编程/params.png" alt=""></p>
<p>在之前使用gluon创建的全连接层都没有指定输入个数，例如使用感知机net里，创建的隐藏层仅仅指定输出大小为256，当调用initialize函数时，由于隐藏层输入个数依然未知，系统无法知道隐藏层权重参数的形状，只有在当我们将形状为(2,20)的输入X传进网络进行前向计算net(X)时，系统才推断该层的权重参数形状为(256,20)，因此，这时候才真正开始初始化参数.</p>
<h1><span id="9-gpu计算">9. GPU计算</span></h1><p>使用GPU进行计算，通过ctx指定，NDArray存在内存上，在创建NDArray时可以通过指定ctx在指定的gpu上创建数组</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],ctx=mx.gpu())</span><br><span class="line">b=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>),ctx=mx.gpu(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>同NDArray类似，Gluon的模型也可以在初始化时通过ctx参数指定设备，下面的代码将模型参数初始化在显存上。当输入x是显存上的NDArray时，gluon会在同一块显卡的显存上计算结果。</p>
<p><strong>mxnet要求计算的所有输入数据都在内存或同一块显卡的显存上</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line">net.initialize(ctx=mx.gpu())</span><br><span class="line">x=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">8</span>))</span><br><span class="line">y=net(x)</span><br></pre></td></tr></table></figure>
<h1><span id="10-block">10. Block</span></h1><p>&ensp;&ensp;&ensp;&ensp;上面使用<code>Sequential</code>来构造模型，下面介绍另一种方法：使用<code>Block</code>类来构造模型，它让模型的构造更加灵活。<br>Bolck类是nn模块里提供的一个模型构造类，我们可以继承它来构造我们想要的模型。下面继承Block类构造MLP，需要重载Block类的<strong>init</strong>函数和forward函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        super(MLP, self).__init__(**kwargs)</span><br><span class="line">        self.hidden = nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.output = nn.Dense(<span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.output(self.hidden(x))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = nd.random.uniform(shape=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">net = MLP()</span><br><span class="line">net.initialize()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p>首先需要实例化MLP，得到一个对象net，初始化net并传入X做一次前向计算。net(X)会调用MLP继承自Block类的<strong>call</strong>函数，这个函数会自动调用forward函数完成前向计算。</p>
<h2><span id="101-sequential和block的关系">10.1. Sequential和Block的关系</span></h2><p>&ensp;&ensp;&ensp;&ensp;Block类是一个通用的部件，Sequential类也继承了Block。Sequential类可以定义一些简单的模型，且不需要定义forward函数，但是直接继承Block类可以极大的拓展模型构造的灵活性。  </p>
<h2><span id="102-使用block自定义层">10.2. 使用Block自定义层</span></h2><p>虽然Gluon提供了大量常用的层，但是有时候还需要自定义的层。下面介绍如何使用NDArray来自定义一个Gluon层，从而可以重复调用。</p>
<h2><span id="103-不含模型参数的自定义层">10.3. 不含模型参数的自定义层</span></h2><p>下面的CenteredLayer类通过继承Block类自顶一个将输入减掉均值后输出的层，并将层的计算定义在forward函数中。这个层只做了减法，所以不需要模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="comment">#因为模型不需要参数，所以__init__也没有传入参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,**kwargs)</span>:</span></span><br><span class="line">        super(CenteredLayer,self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br></pre></td></tr></table></figure>
<h2><span id="104-含模型参数的自定义层">10.4. 含模型参数的自定义层</span></h2><p>我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学到。<strong>在使用自定义层时，定义了模型参数，这时只是定义了参数的形状，并不会给参数初始化，参数初始化还是使用net.initialize()函数</strong>在自定义含模型参数的层时，我们可以利用Block类自带的ParameterDict类型的成员变量params。它是一个由<strong>字符串类型</strong>的参数名字映射到<strong>Parameter类型</strong>的模型参数的字典。我们可以<strong>通过get函数从ParameterDict创建Parameter实例</strong>。下面实现一个含参数权重和偏差参数的全连接层，使用ReLU函数作为激活函数，其中in_units表示输入的个数，units表示输出的个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,units,in_units,**kwargs)</span>:</span></span><br><span class="line">        super(MyDense,self).__init__(**kwargs)</span><br><span class="line">        self.weight = self.params.get(<span class="string">'weight'</span>, shape=(in_units, units))</span><br><span class="line">        self.bias = self.params.get(<span class="string">'bias'</span>, shape=(units,))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        linear = nd.dot(x,slef.weight.data())+self.bias().data()</span><br><span class="line">        <span class="keyword">return</span> nd.relu(linear)</span><br><span class="line"></span><br><span class="line">dense = MyDense(units=<span class="number">3</span>,in_units = <span class="number">5</span>)</span><br><span class="line">dense.initialize()</span><br></pre></td></tr></table></figure>
<p><strong>总结：使用nn.Block自定义层，在自定层中的<strong>init</strong>()方法中，自定义模型参数，使用self.params.get(‘参数名称’,shape=(a,b))。还可以自定义层，这个层中没有参数，我们在层中添加gluon.nn中的层，比如self.conv1 = nn.Conv2D()。初始化完之后，定义forward函数。在这个层定义好之后，需要创建一个网络对象，使用net=Sequential(),然后再net.add()添加需要的层，</strong></p>
<h1><span id="11-混合编程">11. 混合编程</span></h1><p>&ensp;&ensp;&ensp;&ensp;使用HybridBlock类和HybridSequential类构建模型。默认情况下，它们和Block类或者Sequential类一样依据命令式编程的方式执行。当我们调用hybridize函数后，Gluon会转换成依据符号式编程的方式执行。事实上，绝大多数模型都可以接受这样的混合式编程的执行方式。<br>通过调用hybridize函数来编译和优化HybridSequential实例中串联的层的计算。模型的计算结果不变。</p>
<p><strong>只有继承HybridBlock类的层才会被优化计算。例如，HybridSequential类和Gluon提供的Dense类都是HybridBlock类的子类，它们都会被优化计算</strong>。如果一个层只是继承自Block类而不是HybridBlock类，那么它将不会被优化。</p>
<h2><span id="111-使用hybridsequential类构造模型">11.1. 使用HybridSequential类构造模型</span></h2><p>我们之前学习使用Sequential来串联多个层，为了使用混合式编程，将Sequential换成HybridSequential类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd,sym  </span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span><span class="params">()</span>:</span></span><br><span class="line">    net = nn.HybridSequential()<span class="comment">#创建HybridSequential实例  </span></span><br><span class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            nn.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            nn.Dense(<span class="number">2</span>))</span><br><span class="line">    net.initialize()</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line">x = nd.random.normal(shape=(<span class="number">1</span>, <span class="number">512</span>))</span><br><span class="line">net = get_net()</span><br><span class="line">net.hybridize()<span class="comment">#调用hybridize来编译优化HybridSequential实例中串联的层的计算，模型的计算结果不变。</span></span><br><span class="line">net(x)</span><br></pre></td></tr></table></figure>
<h2><span id="112-使用hybridblock自定义模型">11.2. 使用HybridBlock自定义模型</span></h2><p>和Sequential类与Block类之间的关系一样，HybridSequential类是HybridBlock类的子类。与Block实例需要实现forward函数不太一样的是，对于HybridBlock实例，我们需要实现hybrid_forward函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HybridNet</span><span class="params">(nn.HybridBlock)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(HybridNet, self).__init__(**kwargs)</span><br><span class="line">        self.hidden = nn.Dense(<span class="number">10</span>)</span><br><span class="line">        self.output = nn.Dense(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span><span class="params">(self, F, x)</span>:</span></span><br><span class="line">        print(<span class="string">'F: '</span>, F)</span><br><span class="line">        print(<span class="string">'x: '</span>, x)</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        print(<span class="string">'hidden: '</span>, x)</span><br><span class="line">        <span class="keyword">return</span> self.output(x)</span><br></pre></td></tr></table></figure>
<p>在继承HybridBlock类时，我们需要在hybrid_forward函数中添加额外的输入F。我们知道，MXNet既有基于命令式编程的NDArray类，又有基于符号式编程的Symbol类。由于这两个类的函数基本一致，MXNet会根据输入来决定F使用NDArray或Symbol</p>
<h1><span id="12-疑惑">12. 疑惑</span></h1><p>Dense(5)，其中的5表示这一层输出的个数，Gluon中不需要指定每一层输入的形状，net(X)做的操作就是wx+b，只要把输入传进去，内部会自动进行前向计算，不需要传入w和b。</p>
<h2><span id="121-bn">12.1. BN</span></h2><p>参考资料<br><a href="https://www.jiqizhixin.com/articles/2018-08-29-7" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-08-29-7</a></p>
<p><a href="https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf" target="_blank" rel="noopener">https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf</a></p>
<p><strong>批量归一化层，不断调整神经网络的中间输出，从而使得整个神经网络在各层输出的中间值更稳定，主要是让收敛变快，加速训练，，对准确率影响不大</strong><br>BN层的作用：</p>
<ol>
<li>加快模型训练速度，更快收敛</li>
<li>允许使用更大的学习率，提高训练速度<br>可以选择比较大的初始学习率，</li>
<li>减少对初始化的依赖，对参数初始化不敏感<br>提升训练稳定性</li>
<li><p>BN引入的噪声能够起到对模型参数进行正则化的作用，有利于增强模型泛化能力<br>BN的局限：</p>
</li>
<li><p>Batchsize太小效果不佳<br>BN是严重依赖Mini-Batch中的训练实例的，如果Batch Size比较小，则效果有明显的下降。之所以这样，是因为小的BatchSize意味着数据样本少，因为得不到有效统计量，也就是说噪声太大。</p>
</li>
<li>BN在MLP和CNN上效果很好，但是在<br>RNN等动态网络上效果不明显.<br>对于RNN来说，尽管其结构看上去是个静态网络，但在实际运行展开时是个动态网络结构，因为输入的Sequence序列是不定长的，也就是说一个Mini-Batch中的训练实例又长又短。对于类似RNN这种动态网络结构，BN使用起来不方便</li>
</ol>
<h1><span id="13-偏置">13. 偏置</span></h1><p>原先一直以为卷积操作计算时不涉及偏置，后来发现卷积也涉及到偏置。 二维卷积层将输入和卷积核做卷积运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差，在训练的过程中，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size, **kwargs)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__(**kwargs)</span><br><span class="line">        self.weight = self.params.get(<span class="string">'weight'</span>, shape=kernel_size)</span><br><span class="line">        self.bias = self.params.get(<span class="string">'bias'</span>, shape=(<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight.data()) + self.bias.data()</span><br></pre></td></tr></table></figure>
<h1><span id="14-模型参数">14. 模型参数</span></h1><p>在定义好模型之后，使用initialize()对模型所有的参数进行初始化。在自定义层的时候，如果自定义层含有参数，这时候只需要对模型的参数指定名称（为了以后容易区分）和形状（不是必须的，如果是用参数延后初始化，可以不指定），一般不对参数指定初始化方式。使用initialize()对模型中的所有参数进行初始化，使用同一种初始化方式。</p>
<h1><span id="15-自定义层">15. 自定义层</span></h1><p>在自定义含有参数的层时，需要重写2个函数：init(),forward()。这2个函数中如果需要参数可以传入相应的参数。在<strong>init</strong>中主要是定义权重和偏差等，在forward中定义前向计算。实例化一个自定义层时传入的参数是<strong>init</strong>中的参数，比如gcn1 = gcn_layer(256),其中256是<strong>init</strong>的参数，然后前向计算时，传入的参数是forward的参数，gcn1(x)</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Mxnet</tag>
      </tags>
  </entry>
  <entry>
    <title>学习Markdown</title>
    <url>/2019/01/21/Markdown/</url>
    <content><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>Markdown是一种轻量级标记语言，它用简洁的语法代替排版。它的目的是实现易读易写，成为一种适用于网络的书写语言。同时，Markdown支持嵌入html标签。<br><a id="more"></a><br><!-- TOC --></p>
<ul>
<li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a><ul>
<li><a href="#11-%e6%a0%87%e9%a2%98">1.1. 标题</a></li>
<li><a href="#12-%e5%88%97%e8%a1%a8">1.2. 列表</a></li>
<li><a href="#13-%e5%bc%95%e7%94%a8">1.3. 引用</a></li>
<li><a href="#14-%e5%bc%ba%e8%b0%83">1.4. 强调</a></li>
<li><a href="#15-%e5%9b%be%e5%83%8f%e4%b8%8e%e9%93%be%e6%8e%a5">1.5. 图像与链接</a></li>
<li><a href="#16-%e4%bb%a3%e7%a0%81">1.6. 代码</a></li>
</ul>
</li>
<li><a href="#2-%e8%a1%a8%e6%a0%bc">2. 表格</a><ul>
<li><a href="#21-%e5%88%86%e5%89%b2%e7%ba%bf">2.1. 分割线</a></li>
<li><a href="#22-%e6%8d%a2%e8%a1%8c">2.2. 换行</a></li>
</ul>
</li>
<li><a href="#3-%e5%b8%b8%e7%94%a8%e5%bc%a5%e8%a1%a5markdown%e7%9a%84html%e6%a0%87%e7%ad%be">3. 常用弥补Markdown的html标签</a><ul>
<li><a href="#31-%e5%ad%97%e4%bd%93">3.1. 字体</a></li>
<li><a href="#32-%e6%8d%a2%e8%a1%8c">3.2. 换行</a></li>
<li><a href="#33-%e6%96%87%e6%9c%ac%e5%af%b9%e5%85%b6%e6%96%b9%e5%bc%8f">3.3. 文本对其方式</a></li>
<li><a href="#34-%e4%b8%8b%e5%88%92%e7%ba%bf">3.4. 下划线</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p>注意：Markdown使用#、+、*等符号来标记，符号后面必须跟上至少一个空格才有效！  </p>
<h2><span id="11-标题">1.1. 标题</span></h2><p>在标题前面加上1~6个#，依次表示一级标题，二级标题…六级标题</p>
<blockquote>
<h1><span id="一级标题">一级标题</span></h1><h2><span id="二级标题">二级标题</span></h2><h3><span id="三级标题">三级标题</span></h3><h6><span id="六级标题">六级标题</span></h6></blockquote>
<h2><span id="12-列表">1.2. 列表</span></h2><p>Markdown支持有序列表和无序列表<br>无序列表使用-、+、和*作为列表标记<br>使用-作为列表标记</p>
<blockquote>
<ul>
<li>Red</li>
<li>Green</li>
<li>Blue<br>使用+作为列表标记</li>
</ul>
<ul>
<li>Red</li>
<li>Green</li>
<li>Blue<br>使用*作为列表标记</li>
</ul>
<ul>
<li>Red</li>
<li>Green</li>
<li>Blue<br>有序列表使用数组加英文句点.来表示</li>
</ul>
<ol>
<li>Red</li>
<li>Green</li>
<li>Blue  </li>
</ol>
</blockquote>
<h2><span id="13-引用">1.3. 引用</span></h2><p>引用用&gt;来表示，引用支持多级引用，标题，列表，代码块，分割线等常规语法。<br>常见的引用写法：</p>
<blockquote>
<p>这是一段应用    //在&gt;后面有1个空格</p>
<pre><code>这是引用的代码块形式  // 在&gt;后面有5个空格
</code></pre><p>代码例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">super</span>.onCreate(savedInstanceState);  </span><br><span class="line">        setContentView(R.layout.activity_main);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>一级引用</p>
<blockquote>
<p>二级引用</p>
<blockquote>
<p>三级引用</p>
</blockquote>
</blockquote>
</blockquote>
<ol>
<li>这是第一行列表项</li>
<li>这是第二行列表项</li>
</ol>
<h2><span id="14-强调">1.4. 强调</span></h2><p>两个<em>或_代表加粗，一个 </em>或者_代表斜体，<del>代表删除<br><strong>加粗文本</strong> 或者<br><strong>加粗文本</strong><br><em>斜体文本</em> 或者<br>_斜体文本_<br>~~删除文本</del></p>
<h2><span id="15-图像与链接">1.5. 图像与链接</span></h2><p>图片与链接的语法很像，区别在于一个!,二者格式：</p>
<p> 图片：<img src="/2019/01/21/Markdown/" alt=""> <img src="/2019/01/21/Markdown/图片地址" alt="图像文本(可忽略)"></p>
<p> <img src="http://pic6.huitu.com/res/20130116/84481_20130116142820494200_1.jpg" alt=""></p>
<p><img src="https://images0.cnblogs.com/blog/404392/201501/122257231047591.jpg" alt="Markdown"></p>
<p><strong>在博客中插入本地图片</strong></p>
<p>1.修改配置文件_config.yml 里的post_asset_folder:这个选项设置为true<br>2.在你的hexo目录下执行这样一句话npm install hexo-asset-image —save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git（未验证有什么用）<br>3.等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹<br>4.最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：</p>
<p>输入![你想输入的替代文字]和(xxxx/图片名.jpg)</p>
<p> <strong>注意</strong></p>
<ul>
<li>导入的图片路径可以使用绝对路径也可以使用相对路径，建议使用相对路径。  </li>
<li>通常的做法是Markdown文档的同级目录下建立一个pictures文件夹，里面放置所有所需的图片，如果图片多的话，你也可以在pictures文件夹里建立子文件夹归类。</li>
<li>如果你的markdown在一个文件目录下，需要添加另一个目录下的图片，绝对路径是不可行的。需要 “迂回”<br>所谓 迂回，即需要先用../../命令返回上一文件目录，直至可以顺利找到要添加图片的目录。<br>举个栗子:<br>比如你的markdown在~/Document/mymarkdown/test下，需要添加~/Downloads/Pic/background目录下的sunlight.jpg<br>你需要做的是:先写![]，再加上(../../../Downloads/Pic/background/sunlight.jpg)</li>
</ul>
<blockquote>
<p>链接：<a href=""></a>  <a href="链接地址">链接文本</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener">百度</a> </p>
</blockquote>
<h2><span id="16-代码">1.6. 代码</span></h2><p>代码分为行内代码和代码块</p>
<ul>
<li>行内代码使用<code>代码</code>标识，可嵌入文本中</li>
<li>代码块使用4个空格，或者<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<p>这里是代码<br><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">+ 代码语法高亮在```后面加上空格和语言名称即可</span><br><span class="line"></span><br><span class="line">``` java</span><br><span class="line">protected void on<span class="constructor">Create(Bundle <span class="params">savedInstanceState</span>)</span> &#123;  </span><br><span class="line">        super.on<span class="constructor">Create(<span class="params">savedInstanceState</span>)</span>;  </span><br><span class="line">        set<span class="constructor">ContentView(R.<span class="params">layout</span>.<span class="params">activity_main</span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1><span id="2-表格">2. 表格</span></h1><p>表格对齐方式</p>
<ul>
<li>居左：:——</li>
<li>居中：:——:或——-</li>
<li>居右：——:<br>例子<blockquote>
<p>|标题1|标题2|标题3|<br>|:—-|:—-:|—-:|<br>|居左文本1|居中文本1|居右文本1|<br>|居左文本2|居中文本2|居右文本2|<br>|居左文本3|居中文本3|居右文本3|<br>表头1   | 表头2<br>————|———<br>内容1    | 内容2<br>内容3    | 内容3</p>
</blockquote>
</li>
</ul>
<h2><span id="21-分割线">2.1. 分割线</span></h2><p>在一行中用三个以上的*、-、_、来建立一个分割线，行内不能有东西，也可以在行内插入空格</p>
<blockquote>
<h2><span id=""><em>*</em></span></h2><hr>
<hr>
</blockquote>
<h2><span id="22-换行">2.2. 换行</span></h2><p>在行尾添加两个空格加回车表示换行</p>
<h1><span id="3-常用弥补markdown的html标签">3. 常用弥补Markdown的html标签</span></h1><h2><span id="31-字体">3.1. 字体</span></h2><font face="微软雅黑" color="red" size="3">字体及字体颜色和大小</font>
<font color="#0000ff">字体颜色</font>

<h2><span id="32-换行">3.2. 换行</span></h2><p>使用html标签<code>&lt;br/&gt;</code><br>换行</p>
<h2><span id="33-文本对其方式">3.3. 文本对其方式</span></h2><blockquote>
<p align="left">居左文本</p>
<p align="center">居中文本</p>
<p align="right">居右文本</p>

</blockquote>
<h2><span id="34-下划线">3.4. 下划线</span></h2><p><u>下划线文本</u></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
