<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Echo&#39;s blog</title>
  
  <subtitle>远方到底有多远</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-18T03:43:32.346Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Echo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/10/18/metrics/"/>
    <id>http://yoursite.com/2019/10/18/metrics/</id>
    <published>2019-10-18T02:48:03.890Z</published>
    <updated>2019-10-18T03:43:32.346Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: metrics<br>date: 2019-10-18T10:48:03.000Z<br>tags:</p><h2><span id="-评价指标-mse-mae-mape">  - 评价指标、MSE、MAE、MAPE</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>介绍在回归问题中的主要评价指标，以及各自的特点</p><a id="more"></a>      <!-- TOC --><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-%e9%97%ae%e9%a2%98">2. 问题</a></li><li><a href="#3-%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87">3. 评价指标</a><ul><li><a href="#31-mae%e5%b9%b3%e5%9d%87%e7%bb%9d%e5%af%b9%e8%af%af%e5%b7%ae">3.1. MAE(平均绝对误差)</a></li><li><a href="#32-mse%e8%af%af%e5%b7%ae%e5%b9%b3%e6%96%b9%e5%b9%b3%e5%9d%87%e5%80%bc">3.2. MSE(误差平方平均值)</a></li><li><a href="#33-rmse%e5%9d%87%e6%96%b9%e6%a0%b9%e8%af%af%e5%b7%ae">3.3. RMSE(均方根误差)</a></li><li><a href="#34-mape%e5%b9%b3%e5%9d%87%e7%bb%9d%e5%af%b9%e7%99%be%e5%88%86%e6%af%94%e8%af%af%e5%b7%ae">3.4. MAPE(平均绝对百分比误差)</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-问题">2. 问题</span></h1><p>在回归问题中，标签y的分布不均衡，范围在[1,88]，其中70%的值都在1左右。解决的方法：从损失函数入手，设计不同的评价指标，让其更关注一些大的y值。   </p><h1><span id="3-评价指标">3. 评价指标</span></h1><p><a href="https://zhuanlan.zhihu.com/p/74627482" target="_blank" rel="noopener">Metircs参考资料</a>  </p><h2><span id="31-mae平均绝对误差">3.1. MAE(平均绝对误差)</span></h2><p><img src="/2019/10/18/metrics/mae.png" alt=""><br>绝对误差的平均值。</p><ol><li>范围在$[0,\infin]$</li><li>单看MAE并不能看出这个模型的好坏，因为不知道y的平均值。比如MAE=10,y的平均值为1000，则这个模型还不错，但是如果y的平均值为1，那这个模型就非常不好。</li><li>改进：$MAE/y_{mean}$ </li></ol><h2><span id="32-mse误差平方平均值">3.2. MSE(误差平方平均值)</span></h2><p><img src="/2019/10/18/metrics/mse.png" alt="">   </p><ol><li>范围在$[0,\infin]$,  </li><li>很多算法的loss函数都是基于MSE的，因为MSE计算速度快，比RMSE更容易操作。但是我们很少把MAE作为最终的评价指标。</li><li>更关注一些y比较大的值，但是它的代价是对异常点过于敏感。如果预测出的y很不合理，则它的误差比较大，从而对RMSE的值有很大的影响。 </li></ol><h2><span id="33-rmse均方根误差">3.3. RMSE(均方根误差)</span></h2><p><img src="/2019/10/18/metrics/rmse.png" alt=""><br>在MSE上加了根号，误差的结果和数据是一个级别，在数量级上更直观，如果RMSE=10，可以认为回归问题效果与真实结果平均相差10。</p><ol><li>范围在$[0,\infin]$   </li><li>RMSE把更大的注意力放在y更大的值上，只有更大的y值预测准确了，模型的效果才会好。</li></ol><h2><span id="34-mape平均绝对百分比误差">3.4. MAPE(平均绝对百分比误差)</span></h2><p><img src="/2019/10/18/metrics/mape.png" alt="">  </p><ol><li>范围$[0,\infin]$，当MAPE=0%表示完美模型，MAPE大于100%表示劣质模型。</li><li>当真实值有数据等于0时，存在分母为0的情况，该公式不可用</li><li>比如将y1预测为1.5，和100预测为100.5，差值是一样的，。即1的MAPE比100的MAPE大很多。以MAPE作为loss函数时，更加关注y比较小的值。</li><li>单看MAPE的大小是没有意义的，因为MAPE是个相对值，而不是绝对值。MAPE只能用来对不同模型同一组数据的评估。比如对于同一组数据，模型A的MAPE比模型B小，可以说明模型A比模型B好。但是如果说MAPE=10%，并不能判断这个模型好还是不好。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: metrics&lt;br&gt;date: 2019-10-18T10:48:03.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;评价指标、MSE、MAE、MAPE&quot;&gt;&lt;a href=&quot;#评价指标、MSE、MAE、MAPE&quot; class=&quot;headerlink&quot; title=&quot;  - 评价指标、MSE、MAE、MAPE&quot;&gt;&lt;/a&gt;  - 评价指标、MSE、MAE、MAPE&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;介绍在回归问题中的主要评价指标，以及各自的特点&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/09/07/keras%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2019/09/07/keras基础知识/</id>
    <published>2019-09-07T08:21:04.092Z</published>
    <updated>2019-09-18T03:23:18.841Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: keras基础知识<br>date: 2019-09-07T16:21:04.000Z<br>tags:</p><h2><span id="-keras-深度学习">  - keras、深度学习</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>最近看到keras容易上手，封装比较好，学习一下<br><a id="more"></a>    </p><h2><span id="11-基础层的介绍">1.1. 基础层的介绍</span></h2><h3><span id="111-flatten">1.1.1. Flatten</span></h3><p>&ensp;&ensp;&ensp;&ensp;Flatten用来将输入“压平”，把多维的输入变成一维。常用在从卷积层到全连接层的过渡，特征全都变成1维就可以输入到全连接层中。Flatten不影响batch的大小。<br>通过<code>keras.layers.Flatten</code>来引入。<br>比如通过卷积层的输出形状为(batch_size,output_channel,width,height),例如(16,64,32,32),通过<code>Flatten</code>层之后变换成(16,64<em>32\</em>32)=(16,65536)，不改变batch_size的大小。</p><h3><span id="112-dense">1.1.2. Dense</span></h3><p>Dense全连接层。<br>如果Dense是在第一层的话，需要指定output_dim和input_dim，即<code>Dense(output_dim=64,input_dim=44)</code>。在Dense中的API中，看到其实没有input_dim或者input_shape这个参数，查看源码看到，input_dim和input_shape是在<strong>kwargs中，因为并不是所有的Dense层都需要传入输入的形状，只有第一层需要。可以输入<code>input_dim=44</code>,也可以是<code>input_shape=(44,)</code>。一般都是用input_shape     </strong>注意：在input_shape中不包含batch的大小**<br><img src="/2019/09/07/keras基础知识/dense.png" alt=""><br>如果Dense不是在第一层，则只需要指定output_dim，而input_dim则默认为上一层的输出维度。即<code>Dense(output_dim=64)</code><br>如果需要激活函数，则需要再指定激活函数，例如<br><code>Dense(64,avtivation=&#39;relu&#39;)</code></p><h3><span id="113-embedding">1.1.3. Embedding</span></h3><p>Embedding只能作为模型的第一层。基本上用户自然语言处理方面，用来做词嵌入。</p><h3><span id="114-lstm">1.1.4. LSTM</span></h3><p>LSTM传入的参数<br><code>LSTM(batch_input_dim=(batch_size,time_steps,input_size),output_dim=cell_size,return_sequences=True,stateful=True)</code><br>其中LSTM的input_dim是(batch_size，时间步的个数，每个时间步的特征个数)<br>output_dim是LSTM中隐藏层单元的个数<br>return_sequences默认是False，表示一个时间步长为T的序列，只在最后一个时间步输出一个结果，True表示每个时间步都输出一个结果，保留起来。<br>stateful默认是False，表示这个batch与batch之间是不是有联系的，表示这个batch和下一个batch的状态是不是要连起来。 </p><h3><span id="115-merge层">1.1.5. Merge层</span></h3><p>Merge层提供了用于融合<strong>两个层或两个张量</strong>的方法，如果方法以大写字母开头，例如<code>Add()</code>表示融合两个层，如果以小写字母开头，例如<code>add()</code>表示融合两个张量。<br>通过以下来调用<br><code>keras.layers.Add()或keras.layers.add()</code><br>例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">input1 = keras.layers.Input(shape=(<span class="number">16</span>,))</span><br><span class="line">x1 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input1)</span><br><span class="line">input2 = keras.layers.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">x2 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input2)  </span><br><span class="line"><span class="comment"># 相当于added = keras.layers.add([x1, x2])</span></span><br><span class="line">added = keras.layers.Add()([x1, x2])  </span><br><span class="line"></span><br><span class="line">out = keras.layers.Dense(<span class="number">4</span>)(added)</span><br><span class="line">model = keras.models.Model(inputs=[input1, input2], outputs=out)</span><br></pre></td></tr></table></figure></p><h2><span id="12-训练">1.2. 训练</span></h2><pre><code class="lang-python">model = Sequential()model.add()....model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metric=[&#39;mse&#39;])model.fit(train_x,train_y,epochs=1000,batch_size=32)#对测试集进行验证loss或metricmodel.evaluate(test_x,test_y,batch_size=16)#对测试集输出预测的结果。model.predict(test_x)</code></pre><p>在训练的时候，有以下3种方法：fit，train_on_batch,fit_gen   </p><ol><li>fit()<br>当使用fit()函数时，首先要保证2个条件：（1）训练数据可以完成的放在内存中，（2）数据已经不需要再做任何处理了，可以直接训练。</li><li>train_on_batch()<br>train_on_batch()函数接收一个batch的输入和标签，然后反向传播，更新参数。大部分情况下都不需要用到train_on_batch()，例如<code>cost = train_on_batch(train_x,train_y)</code>,返回值是误差  </li></ol><h2><span id="13-模型构建">1.3. 模型构建</span></h2><p>在keras中，一些简单的组件可以直接使用def来实现，在这个输入是input，直接使用某些层，得到输出,不使用model。<br>在实现整个模型的框架时，使用<code>model = Model(input=input,output=output)</code>,然后<code>return model</code><br>对于有些层，这个层在<code>keras.layers</code>中没有，这时候就需要自己定义一个层，这个层中的参数需要自己定义。自定义层中的参数也是需要学习的。</p><h2><span id="14-callbacks">1.4. Callbacks</span></h2><p>传入fit()函数中的callbacks必须是一个list，里面是一个或多个callback实例。</p><h3><span id="141-modelcheckpoint">1.4.1. ModelCheckpoint</span></h3><p>回调函数<code>Callbacks</code>是一组在<strong>训练阶段</strong>被调用的函数集，使用回调函数来查看训练过程中网络内部的状态和统计信息。在模型上调用<code>fit()</code>函数时，可以将<code>ModelCheckpoint</code>传递给训练过程。<br>训练深度学习模型时，<code>Checkpoint</code>是模型的权重。<code>ModelCheckpoint</code>回调类运行定义检查模型权重的位置，文件应如何命名，  </p><h3><span id="142-earlystopping">1.4.2. EarlyStopping</span></h3><p>EarlyStopping是Callbacks的一种，用于提前停止训练。停止训练的标准是当val_loss或者val_root_mean_square_error不再减少，或者val_acc不在增加。我们在训练模型时，主要目的是获得最好的泛化性能。模型的泛化能力通常使用验证集来评估。模型在训练的时候，模型在训练集上loss一直在变小，但是在验证集上的loss却是先变小后变大。说明出现了过拟合。解决过拟合的方法有2种方法：权重衰减和早停法。早停法就是模型在验证集上的表现开始下降时，停止训练。这样就可以避免过拟合的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: keras基础知识&lt;br&gt;date: 2019-09-07T16:21:04.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;keras、深度学习&quot;&gt;&lt;a href=&quot;#keras、深度学习&quot; class=&quot;headerlink&quot; title=&quot;  - keras、深度学习&quot;&gt;&lt;/a&gt;  - keras、深度学习&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;最近看到keras容易上手，封装比较好，学习一下&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/08/21/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://yoursite.com/2019/08/21/正则表达式/</id>
    <published>2019-08-21T11:35:25.359Z</published>
    <updated>2019-08-21T14:14:57.660Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 正则表达式<br>date: 2019-08-21T19:35:25.000Z<br>tags:</p><h2><span id="-正则表达式">  - 正则表达式</span></h2><p>&ensp;&ensp;&ensp;&ensp;最近项目中需要用到正则表达式对文件名进行匹配。在上传文件时，如果文件名中含有连续的相同字母或数字，则不允许上传。并且文件名中必须含有设备型号。<br><a id="more"></a><br>正则表达式描述了字符串的匹配模型。</p><ul><li>+：表示前面的字符必须出现至少一次（1次或多次）<br>eg：runoo+b可以匹配runoob、runooob、runoooob</li><li><em>：表示前面的字符可以不出现，也可以出现一次或多次（0次，1次或多次）<br>eg：runoo\</em>b可以匹配runob、runoob、runooob</li><li>？：表示前面的字符出现0次或1次。<br>eg：do(es)?可以匹配”do”或”does”<br>eg：colou?r 可以匹配 color 或者 colour</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 正则表达式&lt;br&gt;date: 2019-08-21T19:35:25.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;正则表达式&quot;&gt;&lt;a href=&quot;#正则表达式&quot; class=&quot;headerlink&quot; title=&quot;  - 正则表达式&quot;&gt;&lt;/a&gt;  - 正则表达式&lt;/h2&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;最近项目中需要用到正则表达式对文件名进行匹配。在上传文件时，如果文件名中含有连续的相同字母或数字，则不允许上传。并且文件名中必须含有设备型号。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/08/02/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/08/02/论文总结/</id>
    <published>2019-08-02T02:52:51.522Z</published>
    <updated>2019-10-28T14:19:09.562Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 论文总结<br>date: 2019-08-02T10:52:51.000Z<br>tags:</p><h2><span id="-spatial-temporal-图卷积-flow-predicting">  - Spatial-Temporal、图卷积、flow predicting</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;这学期看了很多论文，下面把看过的论文总结一下。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-%e6%80%bb%e7%bb%93">2. 总结</a></li><li><a href="#3-poi%e6%8e%a8%e8%8d%90">3. POI推荐</a><ul><li><a href="#31-point-of-interest-recommendation-exploiting-self-attentive-autoencoders-with-neighbor-aware-influence-cikm2018">3.1. Point-of-Interest Recommendation Exploiting Self-Attentive Autoencoders with Neighbor-Aware Influence (CIKM2018)</a></li><li><a href="#32-hst-lstm-a-hierarchical-spatial-temporal-long-short-term-memory-network-for-location-prediction2018ijcai">3.2. HST-LSTM: A Hierarchical Spatial-Temporal Long-Short Term Memory Network for Location Prediction（2018IJCAI）</a></li></ul></li><li><a href="#4-%e6%97%b6%e7%a9%ba%e6%95%b0%e6%8d%ae%e9%a2%84%e6%b5%8b">4. 时空数据预测</a><ul><li><a href="#41-hyperst-net-hypernetworks-for-spatio-temporal-forecasting2019aaai">4.1. HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting（2019AAAI）</a></li><li><a href="#42-restful-resolution-aware-forecasting-of-behavioral-time-series-data2018cikm">4.2. RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data（2018CIKM）</a></li><li><a href="#43-spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand2019aaai">4.3. Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand（2019AAAI）</a></li><li><a href="#44-revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-predictionaaai2019">4.4. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction（AAAI2019）</a></li><li><a href="#45-deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-predictionaaai2017">4.5. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction（AAAI2017）</a></li><li><a href="#46-attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting2019aaai">4.6. Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting（2019AAAI）</a></li><li><a href="#47-urbanfm-inferring-fine-grained-urban-flows2019kdd">4.7. UrbanFM: Inferring Fine-Grained Urban Flows（2019KDD）</a></li><li><a href="#48-deep-multi-view-spatial-temporal-network-for-taxi-demand-prediction2018aaai">4.8. Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction（2018AAAI）</a></li><li><a href="#49-urban-traffic-prediction-from-spatio-temporal-data-using-deep-meta-learning2019kdd%e9%83%91%e5%ae%87">4.9. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning（2019KDD郑宇）</a></li></ul></li><li><a href="#5-%e5%9b%be%e5%8d%b7%e7%a7%af">5. 图卷积</a><ul><li><a href="#51-semi-supervised-classification-with-graph-convolutional-networks2017iclr">5.1. Semi-Supervised Classification with Graph Convolutional Networks（2017ICLR）</a></li><li><a href="#52-diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting2018iclr">5.2. Diffusion Convolutional Recurrent Neural Network Data-Driven Traffic Forecasting（2018ICLR）</a></li><li><a href="#53-graph-attention-networks2018iclr">5.3. Graph Attention Networks（2018ICLR）</a></li><li><a href="#54-deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning2018aaai">5.4. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning（2018AAAI）</a></li></ul></li><li><a href="#6-traffic-accident%e9%a2%84%e6%b5%8b">6. traffic accident预测</a><ul><li><a href="#61-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">6.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference（2016AAAI）</a></li><li><a href="#62-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">6.2. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction（2018IEEE-ITSC）</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-总结">2. 总结</span></h1><ul><li>在POI推荐上，考虑考虑用户对不同POI的喜爱程度，对每个POI分配不同的权重，同时对一个POI也要考虑这个POI在不同方面的权重（比如饭店在食物和环境的权重）</li><li>在时空建模上，时间上考虑不同的方面，比如recent，periodic等方面，在空间上考虑neighbor，function similarity等方面，其中功能相似一般通过POI来度量</li><li>考虑不同区域之间的相似性，可以使用2个区域之间的traffic flow，2个区域之间的traffic flow越大，说明这2个区域关联性越强。还可以使用2个区域的POI，2个区域之间的POI越相似，这2个区域的function越相似。</li><li>不管time interval是多少，一般只说预测将来多少个时间段的数据，而不是说预测将来多长时间的数据。比如说预测将来5个时间段的数据。如果time interval=30min，则预测的是将来1.5h的数据，如果time interval=1h，则预测的是将来5h的数据。</li><li>如果是将城市划分成网格，每个网格都有空间特征，比如这个网格的flow或者speed。使用CNN获取这个区域的空间特征，所有区域的形成的矩阵形状为(F,I,J)，其中F表示每个区域的空间特征个数，I和J表示网格的高和宽。如果将不同时间段的网格图拼接起来，比如将t个时间段的网格按照时间拼接起来，则形成的形状为(F*t,I,J),然后通过卷积来捕获空间特征。但是这样有一个问题，将时间拼接起来形成通道，会损失通道信息。如果预测是所有区域下一个时间段的inflow和outflow，则输出为(2,I,J)。如果预测的是所有区域接下来p个时间段的速度，则输出为(p,I,J)。</li><li>比如说网格数据有I*J个网格，每个网格有F个特征，这F个特征都是关于这个网格的特征。一般像外部因素不会放在网格中。因为外部因素，像温度，天气等一般不放在网格中。但也有放在网格中的，比较少。</li><li>比如预测第t天的flow，用到该预测天前hour，day，week数据，同时还要考虑外部因素，这里使用的外部因素只考虑被预测当天的外部因素。</li><li>对于预测flow问题，如果只预测一个区域的flow，在构造样本的时候对于每一个区域都构造一个以该区域为目标区域的样本。训练模型是用所有区域的样本来训练，预测的时候输出一个区域的flow。并不是预测一个区域，只用这个区域的历史数据来训练，而是用所有的区域来训练。</li><li>《2018[AAAI] When will you arrive estimating travel time based on deep neural networks》用户的轨迹数据本来是一个时间序列数据，每个轨迹点使用&lt;经度，纬度&gt;表示，可以使用嵌入将用户的轨迹转换成一个矩阵，使用1D卷积捕获空间关系，然后再送入LSTM中，捕获时间关系。</li></ul><h1><span id="3-poi推荐">3. POI推荐</span></h1><h2><span id="31-point-of-interest-recommendation-exploiting-self-attentive-autoencoders-with-neighbor-aware-influence-cikm2018">3.1. Point-of-Interest Recommendation Exploiting Self-Attentive Autoencoders with Neighbor-Aware Influence   (CIKM2018)</span></h2><p><a href="http://delivery.acm.org/10.1145/3280000/3271733/p697-ma.pdf?ip=218.247.253.241&amp;id=3271733&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1564715146_1e8dd5a9fd9356658c3d093a628ac7c5" target="_blank" rel="noopener">论文地址</a><br>论文题目：邻居感知的自注意自编码器的POI推荐  </p><ul><li>挑战<br>（1）建模用户POI之间的非线性关系，原先都是所有的POI权重一样；<br>（2）结合上下文信息，例如POI地理坐标。<br>（3）用户去过的POI是一小部分，而所有的POI非常多，使得POI矩阵变得非常稀疏</li><li>模型<br><strong>self-attentive encoder and a neighbor-aware decoder（SAE-NAD）</strong><br>&ensp;&ensp;&ensp;&ensp;通过self-attentive encoder区分用户对访问过的POI的喜好程度，用户的访问POI向量中每个POI的权重不同，这样可以学到更好的user hidden representation。<br>&ensp;&ensp;&ensp;&ensp;通过neighbor-aware decoder结合地理上下文信息，使得用户之前到达区域的附近或相似的区域可达性变大。将访问的POI嵌入和未访问的POI嵌入做内积，基于RBF（2个POI的点对点距离） kernel，来计算访问过的POI对未访问POI的影响。<br>&ensp;&ensp;&ensp;&ensp;为了建模稀疏矩阵，我们给未访问的POI分配相同的小权重，给访问过的POI通过访问频率分配不同的大权重。这样对于每个用户就可以区分未访问，少访问，经常访问的POI   </li><li>贡献<br><strong>第一篇使用基于attention的自编码器在POI推荐上</strong></li><li><strong>目标</strong><br><strong>根据用户check-in的记录，向用户推荐一系列从未去过的POIS</strong></li><li><p>Definitoin<br>POI(类型，经纬度)<br>Check-in(用户id，POI ID，时间)     </p><p><img src="/2019/08/02/论文总结/1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;使用stack autoencoder（堆叠自编码器）学习用户隐藏表示。输出是一个用户去过和没去过的POI，一个n维的0/1向量，其中的$l_2,l_4,l_6$表示去过的POI下标，根据去过POI的下标在POI嵌入矩阵$W^{(1)}$中取出对应的POI向量组成矩阵$W^{(1)_{[L_u]}}$，得到用户已经去过区域的POI嵌入，这n个POI有些POI更能表示用户的喜好，用到self-attentive机制，为$W^{(1)_{[L_u]}}$中的每个POI嵌入学习不同的权重，来构成user hidden representation。一般的attention给每个POI嵌入学习一个分数，这个分数只能反映POI在一方面的重要性。例如饭店，在味道方面这个用户喜欢这家饭店，但是在环境方面用户不喜欢，为了从不同方面捕获用户的喜好，使用multiple-dimension attention，分别对不同方面进行打分。</p></li></ul><p><img src="/2019/08/02/论文总结/2.png" alt=""><br><img src="/2019/08/02/论文总结/3.png" alt=""><br>这n个POI嵌入向量，从$d_a$个方面进行打分，得到的$A_u \in R^{d_a \times n}$ ,然后把n个POI嵌入乘上分数再相加，得到第u个用户隐藏表示$Z^{(1)}_u \in R^{d_a  times H_1}$，从$d_a$个方面来表示这个用户，为了让这个矩阵输入到encoder中，通过全连接将$d_a$个方面整合成一个方面，从一个矩阵变成一个向量。 然后经过2个encoder得到$z^{(3)}_u$<br><img src="/2019/08/02/论文总结/4.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;用户访问过的POI会对没访问过的POI有影响，影响程度有着2个POI的相似性和距离决定。和已访问过的POI相似或邻近的POI用户访问的概率比较大。使用内积的方式求2个POI的相似性，但是这没有考虑到2个POI之间的距离，我们采用RBF kernel根据2个POI之间的距离计算2个POI之间的相关性，得到一个N*N的矩阵。然后把相似性和距离相关性相乘，得到2个POI的最终区域相关性，<br><img src="/2019/08/02/论文总结/6.png" alt=""><br>解码器阶段：将用户隐藏进行解码。其中$z^{(3)}_u$表示根据用户去过的POI得到的用户表示，$p_u$表示去过的POI对未去过的POI的影响。<br><img src="/2019/08/02/论文总结/7.png" alt="">   </p><ul><li>总结<br>根据一个用户之前去过的POI对这个用户进行表示，不同的POI有不同的权重，同一个POI在不同的方面也有不同的权重，得到一个user hidden representation，将用户表示经过2层encoder，然后在解码的时候，用到邻居信息，去过的POI对未去过的POI有影响，影响大小根据这2个POI之间的相似性和距离决定。    </li></ul><h2><span id="32-hst-lstm-a-hierarchical-spatial-temporal-long-short-term-memory-network-for-location-prediction2018ijcai">3.2. HST-LSTM: A Hierarchical Spatial-Temporal Long-Short Term Memory Network for Location Prediction（2018IJCAI）</span></h2><p>这篇论文没怎么看懂<br>弱实时预测，向用户推荐下一分钟或小时要去的地点。 在LSTM中使用时空信息。<br><img src="/2019/08/02/论文总结/8.png" alt="">   </p><ul><li>贡献<br>提出HST-LSTM结合时空影响到LSTM中，来解决位置预测中数据稀疏的问题。<br>HST-LSTM建模用户的历史访问序列，使用encoder-decoder的方式来提高预测性能。</li><li>模型<br><strong>HST-LSTM model</strong><br>AOI:具有一种功能的区域，例如购物中心，工作区<br>Visit Record：用户在一段时间内（几周或几个月）访问的所有AOIS<br>Visit Session：一个用户在在一个时间段（一天）访问的AOI序列，在一个session中的AOI有强烈的相关性，揭示了用户的运动模式。<br>Visit Session Sequence：一个用户连续的visit sessions，可以作为上下文信息预测下一个AOI。<br>多个AOI组成一个visit session，多个visit session组成visit record，</li><li>目标<br>用户在一段时间内访问了N个AOI，这N个AOI按照时间排序（AOI可能有重复），给定前j个用户去过的AOI，预测接下来用户要去的N-j个AOI，是一个多对多的预测。</li></ul><p><img src="/2019/08/02/论文总结/9.png" alt=""><br>在LSTM中3个门控机制中，输入门，遗忘门，输出门加入时空因素。其中s和q都是d维的向量，分别表示空间和时间的影响因素。<br><img src="/2019/08/02/论文总结/10.png" alt=""><br>基于提出的ST-LSTM，对每个visit session建模。使用STLSTM，每一个时间步输入的信息是一个visit session中的AOI嵌入，使用一个STLSTM对一个session进行建模，输出最后一个时间步的隐藏状态$h^i_e$作为第$i$个session的表示。对$n-1$个session进行建模得到n-1个隐藏状态，将这n-1个隐藏状态使用Contextual LSTM建模长期的visit sequence，在global context encoding阶段，每个时间步输入的是上一个STLSTM中session的表示。<br><img src="/2019/08/02/论文总结/11.png" alt=""><br><img src="/2019/08/02/论文总结/12.png" alt=""><br>在Decoding阶段，使用前i-1个session的推断接下来要去的AOI。</p><ul><li>总结<br>在LSTM阶段加入时空信息，提出STLSTM。<br>使用encoder-decoder来实现POI推荐，encoder和decoder都是LSTM</li></ul><h1><span id="4-时空数据预测">4. 时空数据预测</span></h1><h2><span id="41-hyperst-net-hypernetworks-for-spatio-temporal-forecasting2019aaai">4.1. HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting（2019AAAI）</span></h2><p>题目：超时空网络预测<br>以前的方法分别对时间和空间分别建模，没有考虑到时间和空间内在的因果关系。空间的属性（POI或路网）影响空间的特征（工作区或居民区），从而影响时间特征（inflow trend）<br><img src="/2019/08/02/论文总结/13.png" alt="">  </p><ul><li>目标<br>预测一个区域。根据这个区域的空间和时间特征，预测ST数据，例如空气质量预测，交通流量预测。<br>本篇论文提出一个框架，包含3部分：空间模块，时间模块，演绎模块（deduction module）。<br><strong>这是第一篇考虑空间和时间特征内在因果关系的框架。</strong><br>使用spatial module从spatial attribute建模spatial characteristic，然后使用deduction module从spatial characteristic建模temporal characteristic。<br><img src="/2019/08/02/论文总结/14.png" alt=""><br>Spatial module：两阶段模块，在第一阶段，将spatial attribute建模成spatial characteristic。在第二阶段，生成多个独立的因素，deduction module使用它们来建模时间模块中对应神经网络的参数。例如a—》A，b—》B，c—》C。空间模块像一个hypernetwork。<br>Temporal module：应用不同的HyperST层，HyperST层的参数由deduction module计算得到，可以被看做object的时间characteristic。<br>Deduction module：连接空间和时间模块，空间和时间的内在因果关系被考虑进去。<br><img src="/2019/08/02/论文总结/15.png" alt=""> </li></ul><h2><span id="42-restful-resolution-aware-forecasting-of-behavioral-time-series-data2018cikm">4.2. RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data（2018CIKM）</span></h2><p>基于不同粒度的行为时间序列数据预测<br>行为数据，例如购买行为，邮件行为。<br>现在的预测方法经常仅仅使用一种时间粒度（天或周），然而现在的行为时间数据经常有多重时间粒度模式，每种时间模式之间相互依赖。本篇论文提出RESolution-aware Time series Forecasting（RESTFul），使用循环神经网络来编码不同粒度的时间模式成一个低维表示。不同时间粒度的表示在融合阶段，使用卷积融合框架。最终学到的conclusive embedding向量输入到MLP中用来预测行为时间序列数据。<br><strong>这是第一篇使用多时间粒度来预测时间序列数据。</strong><br>不同粒度的时间序列长度是一样的，比如为5，就表示最近3天，5周。  </p><ul><li>定义<ol><li>Behavioral Time Series：表示一段时间段内的行为数据，$X=[x_1,x_2…x_t,…x_T]，其中t \in [1,2,…T],x_t是一个标量，数值或离散值，表示第t个时间段的行为数据$</li><li>Behavioral Time Series Forecasting：给定历史行为时间序列数据，给定前k个时间段的历史数据$[x_T-k,…x_T],预测x_{T’},其中T’ &gt;= T$  </li><li>Interval Resolution $\alpha$:$s_t和s_{t+1}之间的时间差距$，如果为1天，表示1天测量一次，如果为1周，表示1周测量一次。</li><li>Remporal Resolution $\beta$:如果$\beta=week$表示一次测量1周，如果为1day，表示一次测量1天。</li><li>限制$\alpha &gt;= \beta$  如果$\alpha=1week,\beta=1day,表示1周测量1次，1次测1天。 \alpha=1week,\beta=1week,表示1周测量1次，1次测1周。其中x_t = g(x_t,x_{t+1}…x_{x+\beta})的聚合值，例如一周的平均值或最大值$ </li></ol></li><li>模型<br>RESTFul有2个阶段，第一个阶段，使用循环神经网络来编码不同时间粒度的时间模式。第2个阶段，使用卷积融合模块来融合不同时间粒度的表示。<br><img src="/2019/08/02/论文总结/16.png" alt=""><br>$\alpha,\beta \in {day:1,week:7},这样&lt;\alpha,\beta&gt;就有3种组合，分别是<1,1>,<7,1>,<7,7>$，对于每种组合都有一组行为时间序列数据，对于每组时间序列数据，都使用GRU来这个序列进行时间建模，得到最后一个时间步的隐藏状态向量。将每种组合得到的隐藏状态拼接起来，最终得到一个张量，维度是$R^{|\alpha|\times|\beta|\times d_s}$。然后使用卷积操作，先使用2次2*2的卷积，同时使用padding保证得到的结果大小不变，只改变通道的大小，最终得到的结果是$R^{|\alpha|\times|\beta|\times d_s/4}$,然后展开得到一个$|\alpha|\times|\beta|\times d_s/4$的向量，输入到MLP中，最终可以用来预测回归任务和分类任务。回归任务的损失函数是均方差，分类任务的损失函数是交叉熵。</7,7></7,1></1,1></li><li>总结：<br>考虑不同时间粒度，对不同时间粒度的序列使用GRU建模，将最后一个时间步的隐藏状态拼接起来使用CNN。使用2维卷积对时间数据进行建模，不太合适，可以考虑时间3维卷积。   <h2><span id="43-spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand2019aaai">4.3. Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand（2019AAAI）</span></h2><a href="https://echohhhhhh.github.io/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/" target="_blank" rel="noopener">论文总结</a><br>预测出租车流量，对一个区域在空间上考虑neighbor，function similarity，road connectivity。   <h2><span id="44-revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-predictionaaai2019">4.4. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction（AAAI2019）</span></h2>&ensp;&ensp;&ensp;&ensp;</li><li>挑战：空间依赖性时动态的，随着时间变化，比如早上居住区和工作区的联系强烈，晚上联系较弱。时间上不是严格的周期性，存在dynamic temporal shifting。比如早高峰在7点值9点，每天可能不一样。</li><li>模型：Spatial-Temporal Dynamic Network（STDN），流量门控机制学习location之间动态相似性，periodically shifting attention机制捕获长期周期时间shifting。</li><li>将一个城市划分成a*b=n个网格，将一个时间段（eg.一个月）划分成m个长度相等的时间段。<br>traffic volume：区域$i$的start流量$y^s_{i,t}$：在第$t$个时间段离开这个区域的trip个数，区域$i$的end流量$y^e_{i,t}$：在第$t$个时间段到达这个区域的trip个数。<br>traffic flow：从在第$t$个时间段从区域$i$出发，在第$\tau$个时间段到达$j$区域的traffic flow使用$f^{j,\tau}_{i,t}$表示。</li><li>目标：给定时间段t及其之前的traffic volume和traffic flow，预测第$t+1$个时间段的start and end traffic volume。</li><li>模型：使用Local CNN和LSTM捕获时间和空间关系。<br><img src="/2019/08/02/论文总结/STDN.png" alt=""><br>在提出本文的组件之前，先介绍一下2个base model。</li><li>空间 Local CNN<br>使用traffic volume来获取空间相关性，使用local CNN得到区域表示。<br><img src="/2019/08/02/论文总结/volume.png" alt=""></li><li>Short-term 时间依赖，短期比如说预测今天9:00~9:30的traffic volume，输入是今天7:00~8:30。<br>使用LSTM来获取短期时间依赖。<br><img src="/2019/08/02/论文总结/short-term.png" alt="">  </li><li>下面提取本文的改进，<br><strong>Local CNN—》Flow Gate Mechanism<br>Short-term temporal—》Periodically Shifted Attention Mechanism</strong></li></ul><p><strong>Spatial Dynamic Similarity: Flow Gating Mechanism(空间动态相似性)</strong><br>在local CNN中，local spatial dependency主要是traffic volume。Y表示traffic volume。  但是traffic volume是静态的，不能完全反映目标区域和周围邻居的关系，traffic flow可以更加直接的反应区域之间的联系。两个区域之间的flow越多表示2个区域联系越强（eg.这2个区域越相似）。设计Flow Gating Mechanism(FGM)捕获区域间的dynamic spatial dependency。<br>traffic flow分为2种：inflow和outflow。<br>给定一个目标区域$i$，获取该区域历史$l$个时间段的traffic flow(从$t-l+1到t$),将历史$l$个时间段的inflow和outflow拼接在一起，形成一个三维张量$F_{i,t} \in \mathbb{R}^{S \times S \times 2l}$，其中$S$表示邻近区域，使用CNN建模区域之间的空间相关性。其中$F_{i,j}$作为第一层的输入。  </p><p><img src="/2019/08/02/论文总结/cnn.png" alt=""><br>在每一个卷积层，<strong>使用traffic flow信息来捕获区域之间的动态相似性</strong>，通过一个流量门来限制空间信息。每一层的输出是空间表示$Y^{i,k}_t$，受流量门调整。<br>即对上式的traffic volume，通过traffic flow来控制。 $\sigma$的取值是[0,1]，对traffic volume起到门控机制。  </p><p><img src="/2019/08/02/论文总结/gate.png" alt=""><br><strong>Temporal Dynamic Similarity：Periodically Shifted Attention Mechanism(时间动态相似性)</strong><br>以前的LSTM没有考虑长期依赖(例如：周期)，比如预测第$t$天9点的volume，考虑昨天或前天这个时间段的数据。但是traffic volume并不是严格周期的，在时间上会有平移，图a显示了在天之间的时间平移，图b显示了在周之间的时间平移。<br><img src="/2019/08/02/论文总结/shifting.png" alt=""><br>因为时间具有shifting，因此设计了Periodically Shifted Attention Mechanism（PSAM），这里只考虑天周期性，不考虑周周期。从前P天对应时间段的数据来预测，为了解决time shifting，获取每一天的$Q$个时间段,假设预测的时间段是9:00~9:30，$Q=5$,则获取该时间段前后1个小时的数据8:00~10:30。<br>即获取前P天的数据，并且从每天中获取Q个时间段。如模型图所示，对于每一天都有Q个时间段，可以获取每个时间段的traffic volume和traffic flow，然后使用图卷积，即可以得到一个区域每个时间段的表示。每一天都有一个自己的LSTM，每个LSTM都有Q个时间步，每个时间步都会得到一个隐藏状态向量，即会得到Q个隐藏状态，使用Attention，将Q个隐藏状态整合成一个隐藏状态，用$h^p_{i,t}$表示。其中attention中的$\alpha^{p,q}_{i,t}$表示在第$p$天，第$q$个时间段的重要性。$\alpha^{p,q}_{i,t}$根据长期的隐藏状态和被预测天的短期隐藏状态$h_{i,t}$计算得到。 </p><p><img src="/2019/08/02/论文总结/lstm.png" alt=""><br><img src="/2019/08/02/论文总结/attention.png" alt=""><br>经过attention之后，P天会得到P个隐藏状态，然后再经过一个LSTM来保存周期的序列信息，最终得到长期依赖表示$\hat{h}^p_{i,t}$。  </p><p><img src="/2019/08/02/论文总结/periodic.png" alt=""><br><strong>Joint Traning</strong><br>将短期表示$h_{i,t}$和长期依赖$\hat{h}^p_{i,t}$拼接得到$h^c_{i,t}$，送到一个全连接神经网络中，得到最终的输出，表示为$y^i_{s,t+1}$$y^i_{e,t+1}$作为start volume和end volume。<br><img src="/2019/08/02/论文总结/output.png" alt=""></p><h2><span id="45-deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-predictionaaai2017">4.5. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction（AAAI2017）</span></h2><p>预测flow of crowd，提出ST-ResNet，使用残差网络来建模traffic crowd的时间邻近，周期，区域属性，对每一种属性，设计一个残差卷积单元，建模traffic crowd的空间属性。ST-ResNet对3个残差神经网络的输出分配不同的权重，动态地结合3个输出，在整合3个输出的时候同时考虑外部因素，例如天气，day of week。在这篇论文中，预测2种crowd flow：inflow和outflow。inflow是在一个时间段内从其他区域进行到目标区域的crowds。outflow是在一个时间段内离开目标区域的corwds。inflow和outflow是行人数量、车的数据、公共交通系统上的人数量或者3个的总和。</p><ul><li>gloal：给定历史t个时间段所有区域的inflow和outflow，预测第t+1个时间段所有区域的inflow和outflow。<br>将一个city网格划分成$I<em>J$，下面定义inflow和outflow<br><img src="/2019/08/02/论文总结/inflow.png" alt=""><br>inflow：从其他区域进入到(i,j)<br>outflow：从(i,j)出发到其他区域<br>其中$Tr:g1—&gt;g2…—&gt;g_{|Tr|}$<br><img src="/2019/08/02/论文总结/X.png" alt=""><br>其中X是所有区域的inflow和outflow矩阵。<br><img src="/2019/08/02/论文总结/problem.png" alt=""><br><img src="/2019/08/02/论文总结/ST-ResNet.png" alt=""><br>这个模型由4个组件构成：temporal closeness，period，trend和external。<br>每个时间段内都有一个网格图，2通道，表示所有区域的inflow和outflow。多个时间段按照时间排列会有多个图。在时间段上划分为3部分：recnet、near、distant，分别送到3个模块中：closeness，period，trend，然后对三个模块的输出分配不同的权重融合，再和external信息融合送到Tanh中。<br><strong>Conv-ResNet</strong><br>前3个模块内部是相同的结构，由2部分组成：卷积和残差单元<br><img src="/2019/08/02/论文总结/conv-resnet.png" alt=""><br>拿closeness模块举例，首先使用Conv来捕获near和distant区域的关系。  将closeness的图拼接在一起，closeness一共有$l_c$个时间段，每个时间段有2个通道，将这$l_c$时间段的图拼接在一起，变成$2</em>l_c <em> I </em> J$的数据送入到第一层卷积层。<br><img src="/2019/08/02/论文总结/closeness.png" alt=""><br><img src="/2019/08/02/论文总结/resnet.png" alt=""><br>根据上述结构分别对period和trend进行编码<br><strong>External Component</strong><br>主要考虑以下的外部因素，使用2层全连接提取外部因素。第一层是嵌入层，第二层是转换低维到高维，和$X_t$的维度一样。<br><img src="/2019/08/02/论文总结/external.png" alt=""><br><strong>Fusion</strong><br>所有的区域都被closeness，period，trend影响，但是不同的区域影响程度不同，<br><img src="/2019/08/02/论文总结/fusion.png" alt=""><br><strong>Fusion the external component</strong><br>将3个closeness，period，trend的输出融合，然后再和被预测时间段t的外部因素融合。<br><img src="/2019/08/02/论文总结/fusion-external.png" alt=""> </li></ul><h2><span id="46-attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecasting2019aaai">4.6. Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting（2019AAAI）</span></h2><p>&ensp;&ensp;&ensp;&ensp;使用时空图卷积预测所有节点在未来n个时间步的traffic flow。将traffic flow分为3个时间粒度级别：recent，daily，weekly，3个时间粒度的数据使用3个相同的module来建模，每个module有2个submodule：时空Attention和时空GCN    </p><h2><span id="47-urbanfm-inferring-fine-grained-urban-flows2019kdd">4.7. UrbanFM: Inferring Fine-Grained Urban Flows（2019KDD）</span></h2><p>从粗粒度级的flow推断细粒度级的flow。比如给出的是3<em>3区域的flow，需要推断6\</em>6区域的flow.大的区域称为superregion，划分的小区域称为subregion，同时考虑superregion和subregion的flow约束关系，加起来和superregion的flow相等。<br><img src="/2019/08/02/论文总结/fine-grain.png" alt=""><br>模型的总体框架如下：<br>主要分为2个部分：inference network和external factor subnet。其中推断网络由2个模块组成，特征提取模块和分布上采样模块。<br>在推断网络中，输入是I*J的flow，先经过卷积和M个残差块，捕获空间相关性。在分布上采样模块，每个网格区域需要划分为N<em>N个区域，所以分布上采样主要是改变特征图的大小，从原来的$F</em>I<em>J变成F</em>NI<em>NJ$，对于每个网格区域，经过分布上采样模块，输出的$N</em>N$的flow分布概率。原始的输入$X_c的维度是I<em>J$，经过近邻上采样，会将原始的输入$变成维度为NI</em>NJ$,就是将每个区域的$flow复制N<em>N份$，然后和分布上采样输出的概率相乘，得到每个细粒度区域的flow。<br>**需要注意外部因素，输入是一个向量，经过特征提取模块，输出也是一个向量，为了将外部因素和粗粒度级的flow和细粒度级的flow拼接，也需要将外部因素reshape成$I</em>J或NI<em>NJ$的形状。我原先以为是将外部因素复制$I</em>J或NJ<em>NJ$份，其实并不是，是使用reshape函数。*</em>  </p><p><img src="/2019/08/02/论文总结/urbanFM.png" alt=""></p><h2><span id="48-deep-multi-view-spatial-temporal-network-for-taxi-demand-prediction2018aaai">4.8. Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction（2018AAAI）</span></h2><p>将一个城市进行网格划分，时间段：30min。预测一个网格区域的taxi demand。<br><strong>注意：根据多个区域，多个时间段，预测一个区域，一个区域的taxi demand</strong><br>根据前$t-h,….t$个时间段的taxi demand和外部因素，预测第$t+1$个时间段的taxi demand。<br><img src="/2019/08/02/论文总结/taxi-demand.png" alt=""><br>文章的标题是multi-view分别是spatial view、temporal view和semantic view(城市功能)，其中spatial view考虑的是target的邻近区域，但是有些区域离target很远，但是城市功能（居民区、商业区）和target相似，通过semantic view来捕获。<br><strong>1. Spatial view：Local CNN</strong><br>仅考虑空间近邻的区域，邻居区域大小$S<em>S,例如7</em>7$，通道数为1，表示taxi demand，表示为$Y^{i,0}_t \in R^{S \times S \times 1}$，经过K个卷积层，输出变成$Y^{i,K}_t \in R^{S \times S \times \lambda}$，然后reshape成一个向量维度为$S^2\lambda$，输入到全连接$FC中，输出一个d维的向量$。时间段有$t-h,….t$，每个时间段的$S<em>S</em>1$的网格都输入到Conv中，然后再经过全连接$FC$,所以最终输出$t-h,….t$个时间步，每个时间步是$d维。$<br><strong>细节：对于城市的边界区域，使用0来填充邻居。</strong><br><strong>2. Temporal view：LSTM</strong><br>经过spatial view输出每个时间步的表示，再和每个时间步的外部信息(天气，hour of day，day of week)拼接，共同输入到LSTM中，最终输出最后一个时间步的隐藏状态。<br><strong>3. Semantic View：Structural Embedding</strong><br>根据区域之间的城市功能相似性来构建graph，图中的节点是所有的区域，共$L个$，边：2个区域之间的相似性。相似性的计算是通过$Dynamic \quad Time \quad Warping \quad (DTW)$。 下图中给出了2个区域相似性的计算公式。根据区域$i 和 j$在工作日的taxi demand的时间序列，计算2个时间序列的相似性，即2个区域的相似性。根据区域间的相关性构建了一个全连接图$G$,使用$Embed$嵌入层,本文使用$LINE对图中的节点进行嵌入$，得到每个节点的低维特征表示，然后再次送入全连接中。<br><strong>注意：构建的是一个全连接图，即任意2个节点之间都有边，因为任意2个节点都可以达到。</strong><br><img src="/2019/08/02/论文总结/DTW.png" alt=""><br><strong>4. Prediction Component</strong><br>将LSTM中最后一个时间步的隐藏状态和target区域的节点表示$m^i$拼接，送入到全连接中，经过$sigmoid函数，$最终输出的值在[0,1]之间，然后再反归一化得到真实的taxi demand。<br><img src="/2019/08/02/论文总结/prediction.png" alt="">     </p><p><strong>5. Loss Function</strong><br><strong>注意：损失函数有参考意义。</strong>   </p><p>损失函数中包含2部分，一个是输出的taxi demand的均方差，一个是MAPE，前面更关注一些大的值，为了避免模型被一些大的值控制，后面加入MAPE。<br><img src="/2019/08/02/论文总结/loss.png" alt=""> </p><p><img src="/2019/08/02/论文总结/DMVST-Net.png" alt=""><br><strong>6. 数据集</strong><br>使用广州2个月的taxi数据，$区域划分20<em>20，每个区域700m</em>700m$，<br>（1）使用$Min-Max归一化为[0,1]之间，同时也对y进行归一化到[0,1]之间。$模型预测的输出也在$[0,1]之间，$然后对$y$使用反归一化得到真实的taxi demand。<br>（2）邻居大小设置为$9*9$<br>（3）时间段：30min，根据前8个时间段(4h)预测下一个时间段<br>（4）最后FC的激活函数是$Sigmoid$，其余FC的激活函数是$Relu$</p><h2><span id="49-urban-traffic-prediction-from-spatio-temporal-data-using-deep-meta-learning2019kdd郑宇">4.9. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning（2019KDD郑宇）</span></h2><p>通过时空数据，使用深度元学习，进行城市交通预测<br>论文代码：<a href="https://github.com/panzheyi/ST-MetaNet" target="_blank" rel="noopener">https://github.com/panzheyi/ST-MetaNet</a><br><strong>Abstract</strong><br>&ensp;&ensp;&ensp;&ensp;预测城市traffic有以下挑战：（1）复杂的时空相关性，（2）时空相关性的多样性，每个location的POI和路网信息都不一样。提出deep-meta-learning模型（深度元学习），叫做ST-MetaNet，同时预测所有location的traffic，使用seq2seq架构，包含encoder来学习历史信息，decoder来一步接一步的预测，encoder和decoder有相同的架构，都包含RNN来编码历史traffic数据，一个meta graph attention来捕获各种的空间关系，一个meta RNN来考虑各种的时间相关性。     </p><p><strong>Introduction</strong>   </p><ol><li><p>ST相关性的Complex：<br>&ensp;&ensp;&ensp;&ensp;traffic随着location变化，不同的location，traffic也不同。同一个location，不同的时间点的traffic也不一样。构建一个geo-graph表示空间结构，节点：location，边：location之间的关系。<br>&ensp;&ensp;&ensp;&ensp;在空间上，一些location会相互影响，例如图1(a)中的$S3$发生了accident，那么$S1,S2,S4$可能会发生交通阻塞。<br>&ensp;&ensp;&ensp;&ensp;在时间上，一个location的traffic会受到recent或far时间的影响。例如$S4$举办一个演唱会，$S4$的inflow变大，并且会持续一段时间。  </p></li><li><p>ST相关性的Diversity：<br>&ensp;&ensp;&ensp;&ensp;在上面构建的geo-graph中，有节点特征和边特征。节点：location，节点特征：这个location的POI、路的密度。边：location之间的关系。边特征：location的连通性和距离。比如图1(b)和(c)中，$R1和R3$有相同的POI，都是商业区，$R2$是住宅区，所以它们的flow的时间模式也不一样。  </p></li></ol><p><img src="/2019/08/02/论文总结/Urban-Traffic-Prediction-from-Spatio-Temporal-Data-Using-Deep-Meta-Learning-1.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;为了解决以上的挑战，提出<strong>ST-MetaNet</strong>,首先从geo-graph中的节点和边的特征中提取meta knowledge，从中生成预测网络的权重。<br><img src="/2019/08/02/论文总结/Urban-Traffic-Prediction-from-Spatio-Temporal-Data-Using-Deep-Meta-Learning-2.png" alt=""><br>文章的贡献有4个：<br>（1）提出一个新颖的deep meta learning模型，预测城市traffic，ST-MetaNet利用从geo-graph中提取的meta knowledge，生成graph attention network和RNN seq2seq的权重。<br>（2）提出一个meta graph attention网络来建模空间相关性，Attention机制可以捕获location之间的动态关系，attention网络中的权重是从geo-graph的meta knowledge中提取出来的。<br>（3）提出meta gated RNN，生成<br>（4）在traffic flow和traffic speed做实验<br><strong>Preliminaries</strong><br>&ensp;&ensp;&ensp;&ensp;一共有$N_l个location，每个location有N_t个时间步，traffic一共有D_t类$<br>Ubran Traffic：可以表示为以下的张量</p><script type="math/tex; mode=display">X=\left(X_{1}, \ldots, X_{N_{t}}\right) \in \mathbb{R}^{N_{t} \times N_{l} \times D_{t}}</script><p>其中$X_{t}=\left(x_{t}^{(1)}, \ldots, x_{t}^{\left(N_{l}\right)}\right)$表示在时间步$t$所有区域的traffic信息。<br>&ensp;&ensp;&ensp;&ensp;Geo-Graph 特征：分为节点特征和边特征，其中 $G=\{\mathcal{V}, \mathcal{E}\}$ 表示一个有向图，$\mathcal{V} = \{v^{(1)},\ldots,v^{(N_l)}\}$表示所有节点，$\mathcal{E} = \{e^{(ij)} | 1 \leq i, j \leq N_l\}$表示所有的边，使用$\mathcal{N}_i表示节点i的邻居。$<br>&ensp;&ensp;&ensp;&ensp;问题定义：给定前$\tau_{in}$个时间段的$\left(X_{t-\tau_{i n}+1}, \ldots, X_{t}\right)$所有location在所有时间段的traffic特征，和geo-graph特征$\mathcal{G}$，预测在接下来$\tau_{out}$个时间段所有节点的traffic信息，表示为$\left(\hat{Y}_{t+1}, \ldots, \hat{Y}_{t+\tau_{o u t}}\right)$。<br><strong>Methodologies</strong><br>&ensp;&ensp;&ensp;&ensp;ST-MetaNet是Seq2Seq结构，由encoder(蓝色)和decoder(绿色)组成, encoder编码输入序列$\left(X_{t-\tau_{i n}+1}, \ldots, X_{t}\right)$，生成隐藏状态$\{H_{RNN},H_{Meta-RNN}\}$,用来初始化decoder的状态，预测输出序列$\left(\hat{Y}_{t+1}, \ldots, \hat{Y}_{t+\tau_{o u t}}\right)$。<br>&ensp;&ensp;&ensp;&ensp;encoder和decoder有相同的网络架构，包含以下4个组件。<br>（1）RNN：使用RNN来对历史traffic进行嵌入，捕获长期的时间依赖。<br>（2）Meta-knowledge learner：使用2个全连接FCNs，分分别叫做NMK-Learner和EMK-Learner，从节点特征(POI和GPS位置)和边特征(location的道路连通性和距离)学习meta-knowledge，得到的meta-knowledge用来学习GAT和RNN的权重。<br>（3）Meta-GAT：由Meta-Learner和GAT组成，使用FCN作为Meta-Learner，它的输入是所有节点和边的meta knowledge，输出是GAT的权重。Meta-GAT可以捕获多样的空间相关性。<br>（4）Meta-RNN：由Meta-Learner和RNN组成，Meta-Learner是FCN，输入是所有节点的meta knowledge，输出是每一个节点在RNN的权重，Meta-RNN可以捕获多样的时间相关性。</p><p><img src="/2019/08/02/论文总结/Urban-Traffic-Prediction-from-Spatio-Temporal-Data-Using-Deep-Meta-Learning-3.png" alt="">      </p><ol><li>RNN(GRU)组件<br>编码所有的location的traffic信息，RNN网络对所有的location共享相同的参数，每次GRU输入的是一个location所有时间步的traffic信息，输出这个location的隐藏状态，下一次再输入另一个location所有时间步的traffic，所有的location共享GRU的参数。GRU输出所有location的隐藏状态$H_{t}=\left(h_{t}^{(1)}, \ldots, h_{t}^{\left(N_{l}\right)}\right)$  <script type="math/tex; mode=display"> h_{t}^{(i)}=\operatorname{GRU}\left(z_{t}^{(i)}, h_{t-1}^{(i)} | W_{\Omega}, U_{\Omega}, b_{\Omega}\right), \quad \forall i \in\left\{1, \ldots, N_{l}\right\}</script></li><li>Meta-Knowledge Learner<br>提出2个meta-knowledge learner：NMK-Learner和EMK-Learner，就是2个FCN，输入是一个节点或一条边的特征，输出是节点或边的向量嵌入表示，这些嵌入表示被用来生成GAT和RNN的权重，捕获时空相关性。使用NMK$(v^{(i)})$和EMK$(e^{(ij)})$表示节点和边的嵌入表示。</li></ol><h1><span id="5-图卷积">5. 图卷积</span></h1><p><a href="https://echohhhhhh.github.io/2019/03/03/%E5%9B%BE%E5%8D%B7%E7%A7%AF/" target="_blank" rel="noopener">图卷积总结</a>   </p><h2><span id="51-semi-supervised-classification-with-graph-convolutional-networks2017iclr">5.1. Semi-Supervised Classification with Graph Convolutional Networks（2017ICLR）</span></h2><h2><span id="52-diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting2018iclr">5.2. Diffusion Convolutional Recurrent Neural Network Data-Driven Traffic Forecasting（2018ICLR）</span></h2><h2><span id="53-graph-attention-networks2018iclr">5.3. Graph Attention Networks（2018ICLR）</span></h2><h2><span id="54-deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning2018aaai">5.4. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning（2018AAAI）</span></h2><h1><span id="6-traffic-accident预测">6. traffic accident预测</span></h1><h2><span id="61-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">6.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference（2016AAAI）</span></h2><p>&ensp;&ensp;&ensp;&ensp;在这篇论文中，使用数据：7个月的accident数据和1.6 million的用户GPS数据。使用堆叠降噪自编码器SDA来学习用户GPS的层次特征。这些特征被用来accidentrisk level的预测。这个模型一旦训练好，给定用户的移动轨迹，就可以模拟对应的accident risk地图。但是导致accident的因素很多。例如司机行为，天气，道路情况等。其他的研究尽管考虑到这些因素，但是没有揭示accidentrisk随着这些因素的变化。这篇论文的问题就是：能否通过实时的位置数据来评accident risk。商业和娱乐区有较高的accident risk，因为这些区域有较高的人流密度和人流量。<strong>accident因为受到很多因素的影响使得仅仅给定人类的移动情况，变得不好预测。因此我们推断一个accident的risk，而不是这次accident会不会发生。因为这是一个回归问题，而不是分类问题。</strong><br>&ensp;&ensp;&ensp;&ensp;我们的模型利用降噪自编码器来学习人类移动的层次特征表示。在预测accident risk任务中，经过自编码器学出来的人类移动特征比原始数据更有效。最终，根据人类移动数据的实时输入，我们的模型可以仿真大规模的accident risk地图。有high risk的区域会高亮显示。    </p><p>这篇论文的贡献有3个：  </p><ol><li>第一篇在城市级别上预测accident risk。  </li><li>构造深度学习框架  </li><li>在accident risk上的模拟是非常有效的。  </li></ol><p><strong>使用的数据：</strong></p><ul><li>traffic accident 数据。收集了三十万日本从2013.1.1~2013.7.31的traffic accident数据。每条记录包括事故发生的地点和小时，严重程度。其中严重程度被划分为3级，轻度受伤：1，重度受伤：2，致命：3</li><li>人类移动数据。收集了大约1.6million用户的GPS记录，在日本2013.1.1~2013.7.31。  </li></ul><p>accident的risk可以通过事故的频率和严重程度计算。定义risk level=每一个accident的严重程度的和。  时间划分：1个时间作为一个时间段，一天划分24个时间段。空间划分：每个区域500m*500m。时间索引t，空间索引r表示一个区域。即每1个小时统计一次risk level。 同时每小时统计一次该区域的人流密度density。risk level使用$g_{r,t}$表示，人流密度使用$d_{r,t}$表示。问题是通过$d_{r,t}$来预测$g_{r,t}$。每个区域每个时间段的(d,g)作为一个样本。  </p><p><strong>总结</strong>  </p><ul><li>没有考虑时间和空间特征，没有考虑外部因素</li><li>使用的特征太单一，只考虑区域的人流密度</li></ul><h2><span id="62-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">6.2. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction（2018IEEE-ITSC）</span></h2><p>traffic risk受很多因素的影响。例如不同的区域有不同的accident rate，天气因素，交通量，时间因素。本文结合<br>accident，traffic flow，天气，空气质量的历史短期和周期特征本文提出的模型用来预测短期的accident risk。和AAAI2016一样，本文是回归问题，预测accident risk。将accident分为3级。模型输入的特征是最近的traffic accident，traffic flow，weather，和air quality。最近指的是前几个小时或者昨天或者上星期。<br>将城市网格划分，每个网格区域1000m*1000m，每个时间段是30min或者60min。  </p><p>这篇文章是预测一个区域未来n天的accident平均发生频率。输入有2个，第1个是这个区域历史n个时间段发生的accident的次数，第2个是这个区域的经纬度坐标。<br>这篇论文的前身《A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk》比这篇传入的特征更多，但是不明白为啥没中。<br>这篇文章说traffic accident具有day和week周期性。所以考虑了hour，day，week共3个级别的数据。这篇文章是预测1个区域的risk level，和上一篇不同，上一篇是预测frequency。这篇文章使用的特征有，accident risk，traffic flow，holiday，time period（处于1天的哪个时段，论文中将1天分为7个时段），weather，air quality。将这个区域的以上这6个特征拼接在一起，表示为$I_r(t)$。分别获取这个区域hour，day，week共3个级别的$I_r$,作为LSTM的时间步，每个时间步的特征个数是6个特征拼接起来形成的$I_r$。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 论文总结&lt;br&gt;date: 2019-08-02T10:52:51.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;Spatial-Temporal、图卷积、flow-predicting&quot;&gt;&lt;a href=&quot;#Spatial-Temporal、图卷积、flow-predicting&quot; class=&quot;headerlink&quot; title=&quot;  - Spatial-Temporal、图卷积、flow predicting&quot;&gt;&lt;/a&gt;  - Spatial-Temporal、图卷积、flow predicting&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;这学期看了很多论文，下面把看过的论文总结一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/07/21/traffic-accident/"/>
    <id>http://yoursite.com/2019/07/21/traffic-accident/</id>
    <published>2019-07-21T10:35:32.893Z</published>
    <updated>2019-08-02T13:30:26.352Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: traffic accident<br>date: 2019-07-21T18:35:32.000Z<br>tags:</p><h2><span id="-spatial-temporal-forecasting-traffic-accident-event-prediction">  - spatial-temporal、forecasting traffic accident、event prediction</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>以下是关于event prediction的相关论文，主要是traffic accident这一类的事件预测。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-%e8%ae%ba%e6%96%87">2. 论文</a><ul><li><a href="#21-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">2.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference(2016AAAI)</a></li><li><a href="#22-combining-satellite-imagery-and-open-data-to-map-road-safety2017aaai">2.2. Combining Satellite Imagery and Open Data to Map Road Safety(2017AAAI)</a></li><li><a href="#23-a-deep-learning-approach-to-the-prediction-of-short-term-traffic-accident-risk%e6%9c%aa%e4%b8%ad">2.3. A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk(未中)</a></li><li><a href="#24-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">2.4. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction(2018IEEE-ITSC)</a></li><li><a href="#25-%e5%85%b6%e4%bb%96%e9%97%ae%e9%a2%98%e4%ba%a4%e9%80%9a%e4%b8%8a%e7%9a%84%e5%85%b6%e4%bb%96%e4%ba%8b%e4%bb%b6%e5%ae%9a%e4%b9%89">2.5. 其他问题：(交通上的其他事件定义)</a></li><li><a href="#26-%e8%87%aa%e5%b7%b1%e7%9a%84%e6%95%b0%e6%8d%ae%e9%9b%86">2.6. 自己的数据集：</a></li><li><a href="#27-%e5%85%ac%e5%bc%80%e6%95%b0%e6%8d%ae%e9%9b%86">2.7. 公开数据集：</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-论文">2. 论文</span></h1><h2><span id="21-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai">2.1. Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference(2016AAAI)</span></h2><p>论文地址：<br><a href="https://shiba.iis.u-tokyo.ac.jp/song/wp-content/uploads/2017/02/AAAI2016.pdf" target="_blank" rel="noopener">Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference</a><br>目标：使用real-time GPS data 预测所有区域的traffic risk level，回归问题，预测risk的大小，而不是accident会不会发生 。<br><strong>数据集：</strong><br>Traffic accident data和Human mobility data throughout Japan from 2013.1.1~2013.7.31。<br><strong>模型：</strong><br><img src="/2019/07/21/traffic-accident/1.png" alt="traffic-accident/1.png"><br>对Japan的区域进行网格划分，获取每个网格的risk和mobility。使用Denoise Autoencoder模型对mobility进行编码representation，然后输入到Logistic regression层作为预测。<br><strong>总结：</strong></p><ul><li>第一个使用深度学习来预测traffic accident的模型，使用real-time GPS data作为输入。 </li><li>没有考虑到时间和空间的关系；</li><li>特征单一。只考虑了human mobility，可以考虑weather，POI，population，land use等信息。</li></ul><h2><span id="22-combining-satellite-imagery-and-open-data-to-map-road-safety2017aaai">2.2. Combining Satellite Imagery and Open Data to Map Road Safety(2017AAAI)</span></h2><p>论文地址：<br><a href="https://pdfs.semanticscholar.org/ef28/efaa43a05be548ed61d52a6bd590b88e7782.pdf" target="_blank" rel="noopener">Combining Satellite Imagery and Open Data to Map Road Safety</a>   </p><p>直接从原始的satellie image来预测road safety。相同的safety在图像视觉上有一些相同的特点，比如颜色(grey/greed)，路段等。所以图像的特点是road safety的一种体现。<br><img src="/2019/07/21/traffic-accident/2.png" alt="traffic-accident/1.png"><br>traffic accident被分为3类：slight，heavy，fatal。<br><strong>数据集：</strong><br>NYC：收集了14000张卫星图像，每个图像图片的标签是3类accident中的一类<br>Denver：收集了21406张卫星图像，标签是3类中的一类。<br>每张图片是256*256，使用ConvNet来对图像进行分类。使用NYC的卫星图像训练模型，使用训练好的模型对NYC的测试卫星图像进行测试。<br>使用Denver的traffic accident映射到地图上，形成traffic 热力图。使用从NYC训练得到的模型，输入是Denver的卫星图片，输出区域的traffic accident severity。即可以生成地图来表示区域的risk。</p><p><strong>总结：</strong></p><ul><li>第一个使用satellite image来预测city-scale road safety的模型</li><li>仅仅使用satellite image来预测traffic accident，没有考虑时间和空间信息，外部信息。    </li></ul><h2><span id="23-a-deep-learning-approach-to-the-prediction-of-short-term-traffic-accident-risk未中">2.3. A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk(未中)</span></h2><p>论文地址：<br><a href="https://www.researchgate.net/publication/320627131_A_Deep_Learning_Approach_to_the_Prediction_of_Short-term_Traffic_Accident_Risk" target="_blank" rel="noopener">A Deep Learning Approach to the Prediction of Short-term Traffic Accident Risk</a></p><p>这篇论文首先指出了《Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference》AAAI2016的缺点：（1）只考虑了human mobility data，像traffic flow，weather，air quality，regional characteristic这些重要的信息没有考虑。（2）没有考虑traffic的周期pattern。<br>本篇论文收集了big and heterogeneous data related traffic accident。<br><strong>数据集：</strong><br>traffic accident：北京2016年的accident数据，每条记录包含时间，地点，严重程度，分为三类，slight、heavy、fatal<br>traffic flow data：北京2016.8所有的taxi的GPS信息和speed信息。<br>air quality：北京的daily PM2.5信息。<br>weather information：cloudy，sunny…<br>每个区域的risk level是这个区域所有的accident severity的总和。<br>将traffic accident按照时间和空间划分，时间1h为一个slot，空间每个gird大小为1000m<em>1000m。<br>在给定时间t，定义所有区域的时间相关性，<br>从以上这些数据集提取出6个矩阵，分别是<br><img src="/2019/07/21/traffic-accident/4.png" alt="traffic-accident/1.png"><br>将这6个矩阵进行整合成一个矩阵，每个区域每个时间得到一个多源数据的表示。<br><em>*模型：Traffic Accident Risk Prediction Method based on LSTM (TARPML)</em></em><br><img src="/2019/07/21/traffic-accident/5.png" alt="traffic-accident/1.png"><br>有2个input layer，隐藏层有4个LSTM layer和3个fully connected layer，1个output layer，输出risk level。<br>使用LSTM是因为LSTM可以捕捉periodic信息。<br>输入层中的Short-term features是预测时间槽t的前几个小时的特征I，Periodic feature是预测时间槽t的daily和weekly特征。将这三种特征拼接起来，输入到first input layer中。区域的经纬度信息输入到second input layer中， 直接和fully connected layer相连。<br>输入的短期特征是预测时间t前n个小时的特征，n=4.输入的周期特征是预测时间t昨天和上周该时间段前后3个小时的特征。所以输入的特征维度为$(n+2n_d+2n_w+2,6)$。对于一个区域预测时间t的risk level，需要输入的数据是$(n+2n_d+2n_w+2,6)$<br><img src="/2019/07/21/traffic-accident/7.png" alt="traffic-accident/1.png"></p><p><strong>整体架构</strong><br><img src="/2019/07/21/traffic-accident/3.png" alt="traffic-accident/1.png"><br><strong>总结</strong></p><ul><li>使用到的数据集是北京：traffic accident，traffic flow，weather，holiday，air quality数据</li><li>分三种时间模式，recent，daily，weekly，直接concatenate输入到LSTM中。并且把region的经纬度输入到全连接中，相当于位置embedding。</li><li>没有考虑不同区域之间的关系</li><li>想法：（1）回归问题，预测区域的risk level，但是是一个区域还是所有区域，没有想好（2）考虑recent，daily，weekly，使用Attention机制计算三种之间的重要性.（3）对区域进行embedding，（4）考虑不同区域之间的关系.   </li></ul><h2><span id="24-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc">2.4. A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction(2018IEEE-ITSC)</span></h2><p>论文地址：<br><a href="https://arxiv.org/pdf/1710.09543.pdf" target="_blank" rel="noopener">A Deep Learning Approach to the Citywide Traffic<br>Accident Risk Prediction</a><br>这篇论文是上篇论文的修改版本。<br>和上文的改进之处是加入了很多图表对现象进行解释。解释了为什么预测traffic accident分类比回归要难的原因。<br>这里只使用了北京Traffic accident数据，没有使用其他外部数据。<br>在给定时间t，计算所有区域的空间相关性，然后再计算时空相关性。 计算下面2个公式，主要是为了说明traffic accident具有day周期性。所以在本论文中一个time slot=24h。<strong>计算一个区域每天的traffic accident发生的频率作为risk。</strong><br><img src="/2019/07/21/traffic-accident/8.png" alt="traffic-accident/1.png"><br><img src="/2019/07/21/traffic-accident/9.png" alt="traffic-accident/1.png"><br><img src="/2019/07/21/traffic-accident/10.png" alt="traffic-accident/1.png"></p><p>输入序列长度为100，输入每个区域100h的traffic accident frequency，输出是每个区域未来3天的mean frequency，使用所有的区域样本进行训练。测试时，输入是所有区域100h的traffic accident frequency，输出所有区域未来3天的平均frequency。<br><img src="/2019/07/21/traffic-accident/11.png" alt="traffic-accident/1.png"><br><strong>总结：</strong></p><ul><li>只使用了traffic accident数据，没有使用traffic flow，weather，road network等外部信息</li><li>没有考虑空间信息</li></ul><p>路网之间的关系，</p><ul><li>输入：爬取POI数据，关性建模，<strong>区域中的路口和路段之间的关系</strong>，先直观上看事故发生的原因，因素，出租车流量，事故多发，周期性，位置表示，时空融合，时空相关性(图/网格)，</li></ul><h2><span id="25-其他问题交通上的其他事件定义">2.5. 其他问题：(交通上的其他事件定义)</span></h2><ul><li>乘客下单未出现的概率</li><li>共享单车，违章停车事件(郑宇KDD)</li><li>共享单车调度</li><li>闯红灯频率，</li><li>滴滴拼车数量，</li><li>危险系数：急刹车</li></ul><ol><li>原来的问题，在输入上POI，路段关系，丰富输入信息</li><li>模型创新</li><li>定义新问题</li></ol><h2><span id="26-自己的数据集">2.6. 自己的数据集：</span></h2><ul><li>高速公路阻塞数据(计划性/恶劣天气/堵车)  </li><li>高速公路收费数据</li><li>手机信令数据，怎么定义事件，</li><li>滴滴数据</li></ul><h2><span id="27-公开数据集">2.7. 公开数据集：</span></h2><ul><li>共享单车</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: traffic accident&lt;br&gt;date: 2019-07-21T18:35:32.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;spatial-temporal、forecasting-traffic-accident、event-prediction&quot;&gt;&lt;a href=&quot;#spatial-temporal、forecasting-traffic-accident、event-prediction&quot; class=&quot;headerlink&quot; title=&quot;  - spatial-temporal、forecasting traffic accident、event prediction&quot;&gt;&lt;/a&gt;  - spatial-temporal、forecasting traffic accident、event prediction&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;以下是关于event prediction的相关论文，主要是traffic accident这一类的事件预测。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/07/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B8%A9%E5%9D%91/"/>
    <id>http://yoursite.com/2019/07/17/神经网络踩坑/</id>
    <published>2019-07-17T08:08:21.126Z</published>
    <updated>2019-10-28T12:12:10.051Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 神经网络踩坑<br>date: 2019-07-17T16:08:20.000Z<br>tags:</p><h2><span id="-deep-learning-训练经验-踩坑">  - Deep Learning、训练经验、踩坑</span></h2><p>参考资料:<br><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650729285&amp;idx=1&amp;sn=8f78edc716bbd2198cd7b14f62a93298&amp;chksm=871b2f3bb06ca62d60632da0faebbee63068405a5841934ebec300dc4bbace4b5e6f79daaeed&amp;mpshare=1&amp;scene=1&amp;srcid=07263lmMeeAcHLPSZpJXJI3L#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650729285&amp;idx=1&amp;sn=8f78edc716bbd2198cd7b14f62a93298&amp;chksm=871b2f3bb06ca62d60632da0faebbee63068405a5841934ebec300dc4bbace4b5e6f79daaeed&amp;mpshare=1&amp;scene=1&amp;srcid=07263lmMeeAcHLPSZpJXJI3L#rd</a>   </p><a id="more"></a>   <ol><li><strong>Suffle数据集</strong><br>先划分数据集再shuffle。先将数据集划分成训练集、验证集、测试集。然后在DataLoader划分mini-batch时对训练集进行shuffle得到batch。对验证集和测试集不需要shuffle。不对训练集进行shuffle容易造成过拟合。</li><li><strong>归一化</strong><br>先划分数据集，再归一化。将数据划分成训练集，验证集，测试集，然后计算训练集的平均值和标准差。使用训练集的平均值和标准差对验证集和测试集进行归一化。模型不应该知道关于测试集的任何信息，所以要用训练集的均值和标准差对训练集归一化。<br>划分数据集—&gt;归一化—&gt;对训练集shuffle   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">   X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = <span class="number">0.7</span>) <span class="comment">#train 70%, test 30%</span></span><br><span class="line">   ss = StandardScaler()</span><br><span class="line">   ss.fit(X_train)</span><br><span class="line">   X_train_std = ss.transform(X_train)</span><br><span class="line">   X_test_std = ss.transform(X_test)</span><br><span class="line">   ```       </span><br><span class="line">   一般都是把数据归一化成[<span class="number">0</span>,<span class="number">1</span>]或者减去均值除以标准化。默认是对每一列进行归一化，即axis=<span class="number">0</span>。很少用sklearn的标准化方法，都是自己写一个方法用来标准化。</span><br><span class="line">   ![](神经网络踩坑/norm.png)   </span><br><span class="line">   ```python</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">        mean = x.mean(axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> (x - mean) / std</span><br><span class="line">   ``` </span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **batch_size**   </span><br><span class="line">   当数据量较大时，向网络中传入所有的数据来计算loss和梯度，更新参数会造成内存溢出。所以每次向网络中值传入一个batch的数据，说过这一个batch的数据来更新权重，输出这个batch里面所有样本的平均loss。下次再使用另一个batch，更新网络参数，直到所有的数据全都输入，完成一个epoch。</span><br><span class="line"><span class="number">4.</span> **划分数据集**</span><br><span class="line">   如果数据充足的情况下，通常采用均匀随机抽样的方法将数据集划分为<span class="number">3</span>部分，训练集，验证集和测试集，这三个集合不能有交集，常见的比例是<span class="number">8</span>:<span class="number">1</span>:<span class="number">1</span>，<span class="number">6</span>:<span class="number">2</span>:<span class="number">2</span>。需要注意的是，通常都会给定训练集和测试集，而不会给验证集，一般的做法是从训练集中抽取一部分数据作为验证集。   </span><br><span class="line">   在训练时，仅适用训练集的数据进行训练，使用验证集评价模型。当选中最好的模型超参数之后，再使用训练集+验证集来训练模型，以充分利用所有的标注数据，然后再测试集上测试</span><br><span class="line"></span><br><span class="line">   训练模型时，使用一个bacth来训练模型更新模型参数，记录下batch的loss。当训练完一个epcoh时，记录下模型的参数和梯度。并在验证集上计算验证集的误差，在测试集上计算测试集的MAE和MSE。</span><br><span class="line">   ```python</span><br><span class="line">   global_step = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> train_w, train_d, train_r, train_t <span class="keyword">in</span> train_loader:</span><br><span class="line"></span><br><span class="line">            start_time = time()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output = net([train_w, train_d, train_r])</span><br><span class="line">                l = loss_function(output, train_t)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step(train_t.shape[<span class="number">0</span>])</span><br><span class="line">            training_loss = l.mean().asscalar()</span><br><span class="line"></span><br><span class="line">            sw.add_scalar(tag=<span class="string">'training_loss'</span>,</span><br><span class="line">                          value=training_loss,</span><br><span class="line">                          global_step=global_step)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'global step: %s, training loss: %.2f, time: %.2fs'</span></span><br><span class="line">                  % (global_step, training_loss, time() - start_time))</span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># logging the gradients of parameters for checking convergence</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> net.collect_params().items():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                sw.add_histogram(tag=name + <span class="string">"_grad"</span>,</span><br><span class="line">                                 values=param.grad(),</span><br><span class="line">                                 global_step=global_step,</span><br><span class="line">                                 bins=<span class="number">1000</span>)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">"can't plot histogram of &#123;&#125;_grad"</span>.format(name))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute validation loss</span></span><br><span class="line">        compute_val_loss(net, val_loader, loss_function, sw, epoch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># evaluate the model on testing set</span></span><br><span class="line">        evaluate(net, test_loader, true_value, num_of_vertices, sw, epoch)</span><br><span class="line"></span><br><span class="line">        params_filename = os.path.join(params_path,</span><br><span class="line">                                       <span class="string">'%s_epoch_%s.params'</span> % (model_name,</span><br><span class="line">                                                               epoch))</span><br><span class="line">        net.save_parameters(params_filename)</span><br><span class="line">        print(<span class="string">'save parameters to file: %s'</span> % (params_filename))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># close SummaryWriter</span></span><br><span class="line">    sw.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'prediction_filename'</span> <span class="keyword">in</span> training_config:</span><br><span class="line">        prediction_path = training_config[<span class="string">'prediction_filename'</span>]</span><br><span class="line"></span><br><span class="line">        prediction = predict(net, test_loader)</span><br><span class="line"></span><br><span class="line">        np.savez_compressed(</span><br><span class="line">            os.path.normpath(prediction_path),</span><br><span class="line">            prediction=prediction,</span><br><span class="line">            ground_truth=all_data[<span class="string">'test'</span>][<span class="string">'target'</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">   ```    </span><br><span class="line"><span class="number">5.</span> 交叉验证</span><br><span class="line">   原先对交叉验证使用的数据集一直都理解错了。  </span><br><span class="line">   [参考资料](https://blog.csdn.net/qq_24753293/article/details/<span class="number">79970997</span>)</span><br><span class="line">   交叉验证使用的数据集是训练集，而不是全部的数据集。在交叉验证的时候把训练集分成K个集合，其中K<span class="number">-1</span>份用来训练，<span class="number">1</span>份用来验证。    </span><br><span class="line">   ![](神经网络踩坑/cross.png)</span><br><span class="line">   比如使用<span class="number">5</span>折交叉验证，使用不同的<span class="number">5</span>个训练集和测试集，训练得到<span class="number">5</span>个模型，但是我们最后使用的模型并不是这<span class="number">5</span>个模型中的一个。我们仍然认为这<span class="number">5</span>个模型是一个模型，虽然参数不同，只是它们的输入不同而已。交叉验证只是为了验证这个模型的性能，交叉验证的目的并不是为了得到最终的模型。</span><br><span class="line">   假设我们有<span class="number">2</span>个模型：线性回归和MLP。怎么说哪个模型更好呢？我们可以使用K折交叉验证来证明哪个模型更好，一旦我们选择了更好模型，例如MLP,那我们就用全部的数据来训练这个模型。</span><br><span class="line">   先使用网格搜索选择超参数，然后使用交叉验证输出这个模型的预测结果。</span><br><span class="line">   交叉验证有<span class="number">2</span>个用处：</span><br><span class="line">   - 准确的调整模型的超参数。超参数不同模型就不同。使用交叉验证来选出最好的超参数。</span><br><span class="line">   - 比如分类问题，有多个算法，逻辑回归，决策树，聚类等方法，不确定使用哪个方法时，可以使用交叉验证。  </span><br><span class="line"><span class="number">6.</span> 数据输入和输出</span><br><span class="line">   （<span class="number">1</span>）在gloun中Dense的输入是二维的，(batch_size,feature)，比如输入是(<span class="number">64</span>,<span class="number">120</span>)表示一个batch有<span class="number">64</span>个样本，每个样本有<span class="number">120</span>个特征。如果训练集中的X不是二维的，可以使用reshape()将X变换成(<span class="number">-1</span>,全连接输入单元个数)</span><br><span class="line">   （<span class="number">2</span>）卷积神经网络，卷积的输入和输出形状是(batch_size,通道,高,宽)，如果后面接的是全连接，就要转换成二维(batch_size,每个样本特征=通道\*高*宽)，但是不需要人去手动转换形状，Dense会自动转换。如果是keras，从卷积层到全连接层，形状不会自动转变，所以需要自己加一个`Flatten()`层。  </span><br><span class="line">   （<span class="number">3</span>）如果是一个分类问题，比如mnist数字识别，最后一层是一个神经单元个数为<span class="number">10</span>的全连接层，然后把输送入到softmax，将每一行的<span class="number">10</span>个值都变成在[<span class="number">0</span>,<span class="number">1</span>]之间小数。损失函数是交叉熵损失损失。在gluon中，最后一层Dense只需要指定输出神经单元个数即可，即`Dense(<span class="number">10</span>)`，在预测的时候，输出predict，这时的predict并没有归一化到[<span class="number">0</span>,<span class="number">1</span>]的范围内，我们直接把predict和true_label输入到loss中，在loss函数中，才会对predict进行softmax计算，将predict归一化到[<span class="number">0</span>,<span class="number">1</span>]范围内。   </span><br><span class="line">   在keras中，和gluon不同，会在最后一层的输出指定softmax激活函数，即`Dense(<span class="number">10</span>,activation=<span class="string">'softmax'</span>)`。</span><br><span class="line"></span><br><span class="line">   ![](神经网络踩坑/conv.png)  </span><br><span class="line">   （<span class="number">3</span>）循环神经网络的输入形状为(时间步数，batch_size，特征个数)   </span><br><span class="line">   [通俗易懂的RNN图解](https://www.zhihu.com/question/<span class="number">41949741</span>)</span><br><span class="line"><span class="number">7.</span> 激活函数，在使用激活函数的时候，一般都是</span><br><span class="line">   net.add(nn.Dense(<span class="number">10</span>,activation=<span class="string">'relu'</span>)),在定义层的时候直接加上activation，</span><br><span class="line">   也可以使用,但是不常用</span><br><span class="line">   net.add(nn.Dense(<span class="number">10</span>),</span><br><span class="line">           nn.Activation(<span class="string">'relu'</span>) </span><br><span class="line">    )</span><br><span class="line">    或者net.add(nn.Conv2D(channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">    只有当在该层和激活函数之间有其余的操作时，才会分开写，例如在卷积计算之后，激活函数之前加上批量归一化层，写成</span><br><span class="line">    ```python   </span><br><span class="line">    net.add(nn.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>))  </span><br><span class="line">        或者  </span><br><span class="line">        n.Dense(<span class="number">120</span>),</span><br><span class="line">        BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>)</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>GPU运行程序<br>ctx=mx.gpu(2)，下标从0开始<br><strong>需要用到ctx的地方：</strong></p><ul><li>数据集需要放到gpu上。有2种方法。<br>（1）在创建数据的时候，指定ctx，在gpu上创建数据。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">train_loader = gluon.data.DataLoader(</span><br><span class="line">                     gluon.data.ArrayDataset(</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'week'</span>], ctx=ctx),</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'day'</span>], ctx=ctx),</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'recent'</span>], ctx=ctx),</span><br><span class="line">                         nd.array(all_data[<span class="string">'train'</span>][<span class="string">'target'</span>], ctx=ctx)</span><br><span class="line">                     ),</span><br><span class="line">                     batch_size=batch_size,</span><br><span class="line">                     shuffle=<span class="keyword">True</span></span><br><span class="line"> )</span><br><span class="line">```   </span><br><span class="line">（<span class="number">2</span>）在训练的时候，使用as_in_context()将train_loader,val_loader,test_loader,数据拷贝到gpu上   </span><br><span class="line">![](神经网络踩坑/gpu.png)         </span><br><span class="line">- 模型初始化的时候，通过ctx指定gpu设备，将模型参数初始化在gpu上。</span><br><span class="line">```python  </span><br><span class="line"> net = nn.Sequential()</span><br><span class="line"> net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line"> net.initialize(ctx=mx.gpu())</span><br></pre></td></tr></table></figure></li></ul></li><li><p>NDArray和numpy<br>使用gluon运行程序，gluon中的数据结构是NDArray，普通的python程序中的数据是numpy。什么时候用nd.array？什么时候用np.array?</p><ul><li><strong>nd.array</strong><br>（1）在模型内部的运算，使用的都是nd。比如模型的数据输入，在创建DataLoader时，数据需要转换成nd.array()类型。<br>（2）自定义的compute_val_loss()计算验证集的loss时，传入的数据是val_loader，是nd.array类型，但是在返回loss的时候，需要转换成np.array()，<br>（3）自定义的evaluate计算数据，返回的值是np.array()</li><li><strong>np.array</strong><br>（1）在metrics.py中计算MSE，RMSE，MAE等指标时，输出和输出都是np.array类型。</li><li><strong>nd.array和np.array转换</strong><br>（1）nd.array—&gt;np.array:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">10</span>)</span><br><span class="line">b = a.asnumpy()</span><br></pre></td></tr></table></figure></li></ul><p>（2）np.array—&gt;nd.array</p><pre><code class="lang-python">c = nd.array(b)</code></pre></li><li>tensorboard使用   <ul><li>在训练集上每次epoch之后，验证模型在验证集上的平均loss，对验证集上的每个batch中的每个样本都求出一个loss，将所有样本的loss放在list中，最后求list的平均值得到验证集的平均loss。</li><li>在训练集上每次epoch之后，写一个evaluate函数，验证模型在测试集上的RMSE或MSE等指标。<br>tensorboard中tag相同的会被显示在同一张图中。为了显示训练集，验证集和测试集的loss，tag都被设置为loss，但是SummaWriter的logdir不同。  </li></ul></li><li>Dropout的使用<br>丢弃层会将隐藏单元中的值以一定的概率丢弃，即被设置为0，起到正则化的作用，用来应对过拟合。在测试模型时，为了拿到更加确定的结果，一般不使用丢弃法，只在训练模型下才使用dropout。在训练模型时，将靠近输入层的丢弃概率设的小一点。dropout一般放在全连接层后面。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 神经网络踩坑&lt;br&gt;date: 2019-07-17T16:08:20.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;Deep-Learning、训练经验、踩坑&quot;&gt;&lt;a href=&quot;#Deep-Learning、训练经验、踩坑&quot; class=&quot;headerlink&quot; title=&quot;  - Deep Learning、训练经验、踩坑&quot;&gt;&lt;/a&gt;  - Deep Learning、训练经验、踩坑&lt;/h2&gt;&lt;p&gt;参考资料:&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650729285&amp;amp;idx=1&amp;amp;sn=8f78edc716bbd2198cd7b14f62a93298&amp;amp;chksm=871b2f3bb06ca62d60632da0faebbee63068405a5841934ebec300dc4bbace4b5e6f79daaeed&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=07263lmMeeAcHLPSZpJXJI3L#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650729285&amp;amp;idx=1&amp;amp;sn=8f78edc716bbd2198cd7b14f62a93298&amp;amp;chksm=871b2f3bb06ca62d60632da0faebbee63068405a5841934ebec300dc4bbace4b5e6f79daaeed&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=07263lmMeeAcHLPSZpJXJI3L#rd&lt;/a&gt;   &lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/07/16/%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E8%A1%B0%E5%87%8F/"/>
    <id>http://yoursite.com/2019/07/16/梯度爆炸和衰减/</id>
    <published>2019-07-16T03:14:49.985Z</published>
    <updated>2019-10-23T13:37:05.128Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 梯度爆炸和衰减<br>date: 2019-07-16T11:14:49.000Z<br>tags:</p><h2><span id="-反向传播-梯度爆炸-梯度衰减">  - 反向传播、梯度爆炸、梯度衰减</span></h2><a id="more"></a>    <p>参考资料：<a href="https://zhuanlan.zhihu.com/p/33006526" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33006526</a><br><a href="http://wangxin123.com/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#什么是梯度消失和梯度爆炸，分别会引发什么问题" target="_blank" rel="noopener">http://wangxin123.com/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#什么是梯度消失和梯度爆炸，分别会引发什么问题</a></p><h1><span id="为什么使用梯度更新规则">为什么使用梯度更新规则</span></h1><p>&ensp;&ensp;&ensp;&ensp;现在，神经网络的参数都是通过反向传播更新的。深层神经网络由很多分线性层堆叠，每一个非线性层都可以看做是一个非线性函数。神经网络就是一个复合的非线性函数，将输入映射成输出。损失函数就是使关于真实值与预测值之间的误差。通过损失函数对参数求导，得到梯度，梯度的含义就是这个参数对整个网络的影响程度大小。使用梯度下降更新参数。<br>梯度决定了网络（参数）的学习速率。如果梯度出现异常，参数更新出现异常，即神经元失去了学习的能力。<br>如果权重的值都小于1，最终的输出很小，<br>如果权重的值都大于1，最终的输出很大，</p><ul><li>在反向求导时，假设对$W^1$求梯度，如果其他的权重都小于1，求得的梯度很小，出现梯度消失，使用$W-\alpha \Delta W$，权重更新的很慢，训练的难度大大增加。<strong>梯度消失比梯度爆炸更常见</strong>   </li><li>在反向求导时，如果权重大于1，梯度大幅度更新，网络变得很不稳定。较好的情况是网络无法利用训练数据学习，最差的情况是梯度或权重增大溢出，变成网络无法更新的Nan值。<br>不用层的参数更新速率不一样，一般靠近网络输出层的参数更新速度加快，学习的情况较好。靠近网络输入层的参数更新速度慢，学习的很慢，有时候训练了很久，前几层的权重参数值和刚开始初始化的值差不多。因此，梯度消失和爆炸的根本原因在于反向传播法则。<br><img src="/2019/07/16/梯度爆炸和衰减/1.png" alt="">       <h1><span id="激活函数">激活函数</span></h1>（1）在权重更新的时候，需要计算前层的偏导信息，因此如果激活函数选择的不合适，比如sigmoid，梯度消失的很明显。因为sigmoid的导数不会超过0.25，经过链式求导，很容易出现梯度消失。<br><img src="/2019/07/16/梯度爆炸和衰减/sigmoid.png" alt=""><br>（2）tanh比sigmoid好一些，但是它的导数仍然小于1<br><img src="/2019/07/16/梯度爆炸和衰减/tanh.png" alt="">    <h1><span id="初始化缓解梯度消失和爆炸">初始化缓解梯度消失和爆炸</span></h1>使用Xavier初始化：基本思想是通过网络层时，输出和输出的方差相同。Xavier在tanh中表现很好，但在Relu激活函数中表现很差。<br>何凯明提出了针对Relu的初始化方法<br>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification He, K. et al. (2015)<br>该方法集合He initialization，简单思想是：在Relu网络中，假定每一层有一半的神经元被激活，另一半为0，所有，要保持方差不变，只需要在Xavier的基础上再除以2。<br>针对Relu的激活函数，基本使用He initialization。   </li><li>Xavier初始化：tanh，sigmoid激活函数</li><li>He初始化：Relu激活函数<h1><span id="如何判断出现梯度爆炸">如何判断出现梯度爆炸</span></h1>当出现以下信号时，说明出现了梯度爆炸：</li></ul><ol><li>训练过程中，每个节点和层的权重梯度连续大于1</li><li>模型不稳定，梯度显著变化，快速变大</li><li>训练过程中，权重变成了Nan</li><li>权重无法从训练数据中更新</li></ol><h1><span id="如何解决梯度爆炸">如何解决梯度爆炸</span></h1><ol><li>梯度剪切<br>此方法针对梯度爆炸提出来的，基本思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过了这个阈值，就强制限制在这个范围之，这可以防止梯度爆炸。</li><li>权重正则化<br>比较常见的是L1正则化和L2正则化。正则化是通过对网络权重做正则化限制过拟合<br><img src="/2019/07/16/梯度爆炸和衰减/reg.png" alt="">    </li><li>Relu，LeakRelu激活函数<br>如果激活函数的导数为1，就不存在梯度爆炸的问题了。每层网络都可以得到相同的更新速度。在深层网络中使用relu激活函数不会导致梯度消失和爆炸的情况。<br><img src="/2019/07/16/梯度爆炸和衰减/relu.png" alt=""><br>relu的主要贡献在于：<ul><li>解决了梯度消失、爆炸的问题</li><li>计算方便，计算速度快</li><li>加速了网络的训练<br>同时也存在一些缺点：由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）；输出不是以0为中心的</li></ul></li><li>BatchNorm </li><li>RestNet</li><li>LSTM，可以解决梯度消失的问题</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 梯度爆炸和衰减&lt;br&gt;date: 2019-07-16T11:14:49.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;反向传播、梯度爆炸、梯度衰减&quot;&gt;&lt;a href=&quot;#反向传播、梯度爆炸、梯度衰减&quot; class=&quot;headerlink&quot; title=&quot;  - 反向传播、梯度爆炸、梯度衰减&quot;&gt;&lt;/a&gt;  - 反向传播、梯度爆炸、梯度衰减&lt;/h2&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Dataset</title>
    <link href="http://yoursite.com/2019/07/12/Dataset/"/>
    <id>http://yoursite.com/2019/07/12/Dataset/</id>
    <published>2019-07-12T06:22:25.000Z</published>
    <updated>2019-07-15T03:11:24.964Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="数据">数据</span></h1><a id="more"></a>  <p>滴滴数据集：<a href="https://outreach.didichuxing.com/app-vue/dataList" target="_blank" rel="noopener">https://outreach.didichuxing.com/app-vue/dataList</a><br>京东城市计算：<a href="http://urban-computing.com/index-40.htm" target="_blank" rel="noopener">http://urban-computing.com/index-40.htm</a><br>NYC crash数据：<a href="https://data.cityofnewyork.us/Public-Safety/Vision-Zero-View-Data/v7f4-yzyg" target="_blank" rel="noopener">https://data.cityofnewyork.us/Public-Safety/Vision-Zero-View-Data/v7f4-yzyg</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;数据&quot;&gt;&lt;a href=&quot;#数据&quot; class=&quot;headerlink&quot; title=&quot;数据&quot;&gt;&lt;/a&gt;数据&lt;/h1&gt;
    
    </summary>
    
    
      <category term="交通领域数据源" scheme="http://yoursite.com/tags/%E4%BA%A4%E9%80%9A%E9%A2%86%E5%9F%9F%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/07/03/Identity-Mappings-in-Deep-Residual-Networks/"/>
    <id>http://yoursite.com/2019/07/03/Identity-Mappings-in-Deep-Residual-Networks/</id>
    <published>2019-07-03T01:46:08.488Z</published>
    <updated>2019-07-14T04:05:13.320Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: Identity Mappings in Deep Residual Networks<br>date: 2019-07-03T09:46:08.000Z<br>tags:</p><h2><span id="-resnet">  - ResNet</span></h2><h1><span id="简介">简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;论文地址：<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">https://arxiv.org/abs/1603.05027</a><br>&ensp;&ensp;&ensp;&ensp;代码地址： <a href="https://github.com/KaimingHe/resnet-1k-layers" target="_blank" rel="noopener">https://github.com/KaimingHe/resnet-1k-layers</a><br>&ensp;&ensp;&ensp;&ensp;参考资料：<a href="https://www.cnblogs.com/gczr/p/10127723.html" target="_blank" rel="noopener">https://www.cnblogs.com/gczr/p/10127723.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;title: Identity Mappings in Deep Residual Networks&lt;br&gt;date: 2019-07-03T09:46:08.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2&gt;&lt;span id=&quot;-resnet&quot;&gt;  - ResNet&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/07/01/Deep-Spatio-Temporal-Residual-Networks-for-Citywide-Crowd-Flows-Prediction/"/>
    <id>http://yoursite.com/2019/07/01/Deep-Spatio-Temporal-Residual-Networks-for-Citywide-Crowd-Flows-Prediction/</id>
    <published>2019-07-01T13:02:12.664Z</published>
    <updated>2019-07-01T14:22:48.247Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction<br>date: 2019-07-01T21:02:12.000Z<br>tags:</p><h2><span id="-spatial-temporal-resnet-crowd-flows-prediction">  - spatial-temporal、ResNet、Crowd Flows Prediction</span></h2><h1><span id="简介">简介</span></h1><p>参考资料<br><a href="https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/</a><br>城市计算，郑宇发表的论文：<br><a href="https://www.microsoft.com/en-us/research/project/urban-computing/#!publications" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/project/urban-computing/#!publications</a><br><a id="more"></a>   </p><h1><span id="abstract">Abstract</span></h1><p>&ensp;&ensp;&ensp;&ensp;预测人群流量对于公共安全是非常重要的，同时也有挑战，因为涉及到很多复杂的因素。例如区域间交通、时间、以及天气。我们提出了一个基于深度学习的方法：ST-ResNet，预测城市中每个区域的inflow和outflow。基于时空数据，我们设计了端到端结构的ST-ResNet。我们应用残差神经网络框架来建模时间closeness、period和trend的属性。对每一个属性，设计一个残差卷积单元分支，每个残差卷积单元对空间进行建模。三个残差神经网络对每个区域分配不同的权重。残差神经网络的输出再和外部因素（天气）整合，最终预测每个区域的人流量。在北京和NYC2种数据集上做了实验。</p><h1><span id="instruction">Instruction</span></h1><p>&ensp;&ensp;&ensp;&ensp;在这篇论文中，预测2种流量：inflow和outflow。inflow：在给定一个时间间隔中，从其他区域进入到一个区域的所有流量。outflow：在给定时间间隔中，从这个区域离开的流量。这2种流量都标识人迁移的变化。inflow/outflow可以由行人的数量、周围路上车的数量、公共交通系统（bus,metro）上的人数量或者它们的总和（如果都可以获取到）。 例如图1中的(b)，可以通过手机信号来推测人inflow/outflow分别是(3,1)，使用车辆的GPS轨迹，推测处inflow/outflow分别是(0,3)。<br><img src="/2019/07/01/Deep-Spatio-Temporal-Residual-Networks-for-Citywide-Crowd-Flows-Prediction/figure1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;预测城市中每个区域的inflow和outflow有以下3个复杂的因素：</p><ol><li>空间依赖。例如图1中(2)，r2的inflow会受到邻近区域(r1)和偏远区域的outflow影响，相反，r2的outflow也会影响其他区域的inflow。同时r2的inflow也会影响自身的outflow。</li><li>时间依赖。一个区域的流量受到邻近和较远时间段的影响。例如，在上午8点发生交通阻塞将会影响9点的traffic。并且，早上高峰时的交通情况可能其连续工作日的同一时刻相似。当温度变得越来越低，太阳升的越来越晚，人们也起的越来越晚。</li><li>外部因素影响。一些外部因素，像天气，事故可能会影响不同区域的flow。<br>&ensp;&ensp;&ensp;&ensp;为了处理这些挑战，提出ST-RestNet，模型的贡献主要如下：</li></ol><ul><li>ST-ResNet使用基于卷积的残差网络，来建模周围和较远任意2个地区的空间依赖。同时保证模型的准确率不包含在神经网络的深层架构中（？？？）  </li><li>将流量的时间属性总结为3类，分别是closeness、period、trend。ST-ResNet使用3个残差网络分别对这3个属性建模</li><li>ST-ResNet为不同的分支和区域分配不同的权重，动态整合以上3个网络的输出。然后再和外部因素进行整合。</li><li>使用北京出租车轨迹数据和NYC自行车轨迹数据来做实验。有6个baseline。<h1><span id="preliminaries">Preliminaries</span></h1></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction&lt;br&gt;date: 2019-07-01T21:02:12.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;spatial-temporal、ResNet、Crowd-Flows-Prediction&quot;&gt;&lt;a href=&quot;#spatial-temporal、ResNet、Crowd-Flows-Prediction&quot; class=&quot;headerlink&quot; title=&quot;  - spatial-temporal、ResNet、Crowd Flows Prediction&quot;&gt;&lt;/a&gt;  - spatial-temporal、ResNet、Crowd Flows Prediction&lt;/h2&gt;&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;参考资料&lt;br&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/&lt;/a&gt;&lt;br&gt;城市计算，郑宇发表的论文：&lt;br&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/project/urban-computing/#!publications&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.microsoft.com/en-us/research/project/urban-computing/#!publications&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/07/01/Tomcat/"/>
    <id>http://yoursite.com/2019/07/01/Tomcat/</id>
    <published>2019-07-01T00:55:23.474Z</published>
    <updated>2019-07-01T01:23:26.794Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: Tomcat<br>date: 2019-07-01T08:55:23.000Z<br>tags:</p><h2><span id="-jvm-tomcat-内存调优">  - JVM、Tomcat、内存调优</span></h2><h1><span id="简介">简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;<br>参考资料<a href="https://www.cnblogs.com/centos2017/p/9956432.html" target="_blank" rel="noopener">https://www.cnblogs.com/centos2017/p/9956432.html</a><br><a id="more"></a>   </p><h1><span id="内存溢出">内存溢出</span></h1><p>在Tomcat时，常常会遇到内存溢出的错误，主要是以下2种：</p><ul><li>java.lang.OutOfMemoryError: Java heap space   </li><li>java.lang.OutOfMemoryError: PermGen space       </li></ul><h1><span id="原理">原理</span></h1><ul><li>-Xms 为jvm启动时分配的初始内存      比如-Xms200m，表示分配200M</li><li>-Xmx 为jvm运行分配的最大内存        比如-Xms500m，表示jvm进程最多只能够占用500M内存</li><li>-Xss 每个线程堆栈的大小             一般情况下256K是足够了。影响了此进程中并发线程数大小</li><li>-XX  PermSize=64M JVM初始分配的非堆内存</li><li>-XX  MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配<br>首先了解一下JVM内存管理的机制，然后解释每个参数的含义。<br>按照官方的说法：Java虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在Java虚拟机启动时创建的。<br>在JVM中堆之外的内存称为非堆内存（Non-heap memory）。<br>简单来说，堆就是Java代码可及的内存，是留给开发人员使用的，非堆就是JVM留给自己用的。<h2><span id="堆heap内存">堆Heap内存</span></h2></li><li>JVM初始分配的堆内存由-Xms指定，默认是物理内存的1/64；</li><li>JVM最大分配的堆内存由-Xmx指定，默认是物理内存的1/4。<br>默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；<br>空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。<br><strong>因此服务器一般设置-Xms、-Xmx 相等，以避免在每次GC 后调整堆的大小</strong>。<br>说明：如果-Xmx 不指定或者指定偏小，应用可能会导致java.lang.OutOfMemoryError: Java heap space错误，此错误来自JVM，不是Throwable的，无法用try…catch捕捉。   <h2><span id="非堆内存分配">非堆内存分配</span></h2></li><li>JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；</li><li>由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。<br>XX:MaxPermSize设置过小会导致java.lang.OutOfMemoryError: PermGen space 就是内存益出。<br>为什么会内存益出：<br>（1）这一部分内存用于存放Class和Meta的信息，Class在被 Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同。<br>（2）GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS 的话,就很可能出现PermGen space错误。<br>这种错误常见在web服务器对JSP进行pre compile的时候。   <h1><span id="解决方案">解决方案</span></h1>(1) 进入到tomcat的/bin目录下<br>在bin目录下，创建一个新的文件，<br>如果是Linxu或Mac系统，创建setenv.sh<br>如果是Windows系统，创建setenv.bat<br>(2) 添加配置（Linux/Mac）<br>在这个文件中添加以下内容  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export CATALINA_OPTS=&quot;$CATALINA_OPTS -Xms2048m&quot;</span><br><span class="line">export CATALINA_OPTS=&quot;$CATALINA_OPTS -Xmx2048m&quot;</span><br><span class="line">export CATALINA_OPTS=&quot;$CATALINA_OPTS -XX:MaxPermSize=512m&quot;</span><br><span class="line">```       </span><br><span class="line"></span><br><span class="line">如果是Windows系统，使用以下配置</span><br></pre></td></tr></table></figure></li></ul><p>set “JAVA_OPTS=%JAVA_OPTS% -Xms2048m -Xmx2048m-XX:MaxPermSize=512m -server”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(3) 完成以上配置后，启动Tomcat服务可以使用以下2种命令(Linux/Mac)：</span><br></pre></td></tr></table></figure></p><p>cd apache/bin<br>./catalina.sh run或者./startup.sh<br>```        </p><p>如果是Windows系统，使用catalina.bat启动Tomcat服务<br>(4) 查看log日志<br>在日志中，启动Tomcat时，可以看到刚刚配置的参数。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: Tomcat&lt;br&gt;date: 2019-07-01T08:55:23.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;JVM、Tomcat、内存调优&quot;&gt;&lt;a href=&quot;#JVM、Tomcat、内存调优&quot; class=&quot;headerlink&quot; title=&quot;  - JVM、Tomcat、内存调优&quot;&gt;&lt;/a&gt;  - JVM、Tomcat、内存调优&lt;/h2&gt;&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;&lt;br&gt;参考资料&lt;a href=&quot;https://www.cnblogs.com/centos2017/p/9956432.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/centos2017/p/9956432.html&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>gluon编程</title>
    <link href="http://yoursite.com/2019/06/26/gluon%E7%BC%96%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/06/26/gluon编程/</id>
    <published>2019-06-26T10:59:07.000Z</published>
    <updated>2019-06-26T10:59:07.450Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/"/>
    <id>http://yoursite.com/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/</id>
    <published>2019-06-24T00:43:06.723Z</published>
    <updated>2019-07-04T07:30:11.601Z</updated>
    
    <content type="html"><![CDATA[<hr><h2><span id="">{}</span></h2><h1><span id="1-简介">1. 简介</span></h1><p><a href="http://delivery.acm.org/10.1145/3320000/3313730/p717-huang.pdf?ip=218.247.253.153&amp;id=3313730&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1561380383_40e3bd8d678088e9b04173b89f85c49c" target="_blank" rel="noopener">论文出处</a><br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-Keywords">2. Keywords</a></li><li><a href="#3-Abstract">3. Abstract</a></li><li><a href="#4-Introduction">4. Introduction</a></li><li><a href="#5-Problem-Formulation">5. Problem Formulation</a><ul><li><a href="#51-Preliminaries">5.1. Preliminaries</a></li><li><a href="#52-Framework-Overview">5.2. Framework Overview</a></li></ul></li><li><a href="#6-Methodology">6. Methodology</a><ul><li><a href="#61-Context-aware-Recurrent-Framework">6.1. Context-aware Recurrent Framework</a></li><li><a href="#62-Multi-Modal-Pattern-Fusion-Module">6.2. Multi-Modal Pattern Fusion Module</a></li><li><a href="#63-Conclusive-Recurrent-Network">6.3. Conclusive Recurrent Network</a></li><li><a href="#64-Forecasting-and-Model-Inference">6.4. Forecasting and Model Inference</a></li></ul></li><li><a href="#7-Evaluation">7. Evaluation</a><ul><li><a href="#71-Data-Description">7.1. Data Description</a><ul><li><a href="#711-Data-Statistics">7.1.1. Data Statistics</a></li></ul></li><li><a href="#72-Experimental-Setting">7.2. Experimental Setting</a><ul><li><a href="#721-Parameter-Setting">7.2.1. Parameter Setting</a></li><li><a href="#722-Baseline-Methods">7.2.2. Baseline Methods</a></li><li><a href="#723-Evaluation-Protocols">7.2.3. Evaluation Protocols</a></li></ul></li><li><a href="#73-Performance-Comparison">7.3. Performance Comparison</a><ul><li><a href="#731-Overall-ComparisonQ1">7.3.1. Overall Comparison(Q1)</a></li><li><a href="#732-Forecasting-Accuracy-vs-Time-PeriodQ2">7.3.2. Forecasting Accuracy v.s Time Period(Q2)</a></li><li><a href="#733-Forecasting-Accuracy-vs-CategoriesQ3">7.3.3. Forecasting Accuracy v.s Categories(Q3)</a></li></ul></li><li><a href="#74-Component-Wise-Evaluation-of-MiSTQ4">7.4. Component-Wise Evaluation of MiST(Q4)</a></li><li><a href="#75-Effect-of-Spatial-and-Temporal-ScaleQ5">7.5. Effect of Spatial and Temporal Scale(Q5)</a></li><li><a href="#76-Hyperparameters-StudiesQ6">7.6. Hyperparameters Studies(Q6)</a></li><li><a href="#Case-StudyQ7">Case Study(Q7)</a></li></ul></li><li><a href="#Conclusion">Conclusion</a></li><li><a href="#%E5%85%B6%E4%BB%96%E8%AE%BA%E6%96%87">其他论文</a></li><li><a href="#Marco-F1">Marco-F1</a><ul><li><a href="#Micro-F1">Micro-F1</a></li><li><a href="#Macro-F1">Macro-F1</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-keywords">2. Keywords</span></h1><p><strong>异常事件预测、深度神经网络、时空数据挖掘</strong> </p><h1><span id="3-abstract">3. Abstract</span></h1><p>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[应用]</font>城市异常事件，比如犯罪、事故，如果不及时处理的话，会造成人员和财产的损失。如果异常事件能在发生之前自动被预测出来，对很多领域都有重要意义，比如公共秩序维护、灾难控制和人的活动建模。<font color="#FF0000">[挑战]</font>然而，预测不同类型的城市异常事件是非常有挑战的，因为它被很多复杂的因素影响。(i)区域内动态的时间关系；(ii)区域间复杂的空间关系；(iii)潜在的类别之间的关系。<font color="#FF0000">[模型]</font>在这篇论文中，我们研究了一个<strong>Multi-View and Multi-Modal Spatial-Temporal learning多视角和多模态的时空学习框架(MiST)</strong> 来解决以上的挑战，通过增强不同视角（空间、时间和语义）的相关性，和将多模态单元映射到相同的潜在空间。特别的，将多模态模式融合架构和分层循环框架进行整合，MiST可以保留多视角异常事件数据的潜在的结果信息，和自动地学习特定视角表示的重要性。在三个真实数据集上的实验，例如：犯罪数据和城市异常数据，表明我们MiST模型比其他先进的模型效果都好。   </p><h1><span id="4-introduction">4. Introduction</span></h1><p>&ensp;&ensp;&ensp;&ensp;城市异常事件，比如犯罪(抢劫、袭击)和城市异常(道路封锁、噪声)如果不及时处理，对公共安全有很大的风险。据统计，异常造成了很大的损失，因为准确和可靠的预测异常事件是数据驱动的决策者用于减少人和经济的损失迫切的需求。<font color="#FF0000">[应用]</font>例如，在灾难控制中，通过预测未来的异常事件，当地政府可以设计更好的交通规划和移动管理策略来防止严重的社会骚乱。此外，在公共秩序维护上，了解城市每个区域的异常事件潜在的发生模式对人们活动建模和地方推荐任务是非常重要的。在这篇论文中，我们旨在提前预测城市中不同区域不同类型的异常事件，为社会福利给予重大的提高。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[前人工作]</font>前人已经有一些研究关于使用时空数据检测地理异常。大部分这些研究都是通过分析被研究对象的历史轨迹和移动模式，使用统计和数据挖掘的方法来发现异常事件。然而，这些方法并不是预测将来的时间，而是在它们发生之后鉴定是不是异常事件，这会造成信息延迟和缺乏异常处理的提前准备。<br>&ensp;&ensp;&ensp;&ensp;从多个角度，我们确定了建模这种异常事件数据的三个挑战。<font color="#FF0000">[挑战]</font><br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[考虑空间关系]</font>第一，在城市中异常事件的分布是变化的，并且不同区域异常事件的分布是不同的。在这种情况下，异常事件的发生不再是区域独立的，在预测异常事件时，考虑不同区域的空间关系是非常重要的。并且，当建模动态空间关系对时，概率图模型将不再有效，由于概率图模型基于先验假设分布有很多的参数，涉及大量的计算。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[时间动态依赖]</font>第二，异常事件的发生模式经常涉及到随时间变化的潜在因素。例如，工作日的犯罪因果性和周末可能不同。传统的时间序列预测技术，像ARIMA和SVR被限制在线性模型，它仅依赖于单级周期模式。因此，这些方法很难在时间动态上预测异常事件。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[不同类别的异常事件间相互影响]</font>第三，不同类别的异常事件有着显示和隐示的影响。例如，一个区域的抢劫可能会引发该区域的交通堵塞，由于人群的聚集和巡逻的增加。因此，一种类别异常事件的发生不仅仅来源于不同区域之间的空间关系和时间槽之间的时间依赖，还可能来源于不同类别异常事件的相互影响。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[模型3个阶段]</font>受以上挑战的启发，该工作提出了一个通用且灵活的框架：Multi-View Deep Spatial-Temporal Networks(MiST)，从多视角异常事件数据的关系中学习预测结构。特别的，在第一阶段，我们提出了上下文感知context-aware的循环框架从不同的角度来捕获异常事件数据的时间动态性，并且自动提供了某个视角的表示。在第二阶段，为了将区域间的关系、不同类别间的影响和已编码的多维度数据的时间模式整合起来，我们基于attention机制提出了一个模式融合模块，来促进不同视角的融合，并且在预测模型的相应视角，自动地捕获关联区域、时间槽、类别的贡献。为了增强MisT模型的时间序列结构信息和非线性，在最后阶段设计了一个总结性的循环网络模块，对融合嵌入向量的序列模式进行建模。最终的总结潜在表示被喂入一个全连接神经网络来预测未来时间槽的异常事件。<br>&ensp;&ensp;&ensp;&ensp;综上所述，我们贡献主要是：</p><ul><li>我们引入了一个新的多视角和多模态时空学习框架MiST来预测一个城市每个区域不同类型的异常事件。MiST映射所有的空间时间和语义单元到一个潜在空间来保留它们跨模态的相关性。</li><li>我们提出了一个多模态融合模型，和分层循环框架，学习共享在多视角数据中潜在的区域-时间-类别关系，并且自动地调整每个视角中的相关性，以协助预测任务。</li><li>在三个真实世界异常事件数据集，从NYC和Chicago收集的数据集进行试验，MiST一直比其他state-of-the-art方法效果好。     </li></ul><h1><span id="5-problem-formulation">5. Problem Formulation</span></h1><p>&ensp;&ensp;&ensp;&ensp;在这一节，首先引出preliminary和problem。</p><h2><span id="51-preliminaries">5.1. Preliminaries</span></h2><ul><li>定义1 Geographical Region(地理区域)<br>把城市进行网格分区。划分成$I \times J$,有$I$行$J$列，带有经纬度信息。每一个网格被视为一个地理区域，表示为$r_{i,j}$，其中$i和j$是分别是行和列的索引。在这篇论文中，我们使用区域作为最小单元来研究异常事件预测问题。<br>&ensp;&ensp;&ensp;&ensp;我们定义地理区域集合$R=(r_{1,1},…,r_{i,j},…,r_{I,J})$,并且假设有$L$个异常事件类别，$C=(c_1,…,c_l,…,c_L)$,其中$C$表示异常事件类别集合，下标为$l$。给定一个时间窗口$T$,我们分割$T$为不重叠且连续的时间槽$(T=(t_1,…,t_k,…,t_K))$,其中$K$表示时间槽的个数，索引是$k$.<br><strong><script type="math/tex">区域R是I \times J;异常事件类别C，有L个值;     时间T，有K个时间槽</script></strong></li><li>定义2 Abnormal Event Data Source(异常事件数据源)<br>假设一个区域$r_{i,j}$，使用$Y_{i,j}=(y^1_{i,j},…,y^l_{i,j},…,y^L_{i,j}) \in \mathbb{R}^{L \times K}$来表示在区域$r_{i,j}$过去$K$个时间槽发生的所有类型的异常事件。对于$y^l_{i,j} \in \mathbb{R}^K$表示区域$r_{i,j}$在类别$c_l$上从时间$t_1到t_K$的值。在$y^l_{i,j}$中，每一个元素$y^{l,k}_{i,j}$为1如果在区域$r_{i,j}$在时间$t_k$中有类别$c_l$异常事件发生，否则为0。<br>即$Y_{i,j}$是一个矩阵，一共有$L行K列$，每一个元素非0即1，其中每一行表示一种类别，每一列表示一个时间段。</li><li><strong>Problem Statement</strong><br><font color="#FF0000">[任务]</font>给定一个城市区域$R$时间从$t1到t_K$,所有异常事件类别的数据源$Y$，其中$Y$有$I \times J$个矩阵，每个矩阵都是$\mathbb{R}^{L \times K}$。目标是学习一个预测框架来推断一个区域$r_{i,j}$在未来$h$个时间槽，异常事件类别$c_l$是否发生。即计算$(y^{l,(K+h)}_{i,j}|Y_{i,j}=(y^1_{i,j},…,y^L_{i,j}));i,j \in [1,…,I],[1,…,J]$。即给定一个区域历史$K$个时间槽所有类别异常事件发生地数据，来预测这个区域在未来第$K+h$个时间槽，类别$l$事件是否发生，即输出结果是0/1。 </li></ul><h2><span id="52-framework-overview">5.2. Framework Overview</span></h2><p>&ensp;&ensp;&ensp;&ensp;我们提出的MiST模型是一个多层表示学习框架，如figure1所示。在详细介绍模型之前，首先介绍一下模型的输入，然后详细介绍设计的动机。</p><ul><li>定义3 Event Context Tensor(事件上下文张量)<br>给定一个目标区域$r_{i,j}$，使用event context tensor$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$，对这个区域的邻近区域在时间段$t_k$中不同类别的异常事件进行建模。$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$，有3个维度，分别表示$I行J列L个类别$。给定一个时间槽$t_k,\mathcal{A}^k_{i,j,l}$为1如果？？？？？,</li><li><strong>Context-aware Recurrent Framework</strong><br>为了从时间角度，就异常事件分布的动态属性方面表示区域内的相关性，我们提出了基于LSTM的context encoder，将每个时间槽的$\mathcal{A}$展开形成的向量中的每个元素，学习一个潜在表示。从我们的LSTM encoder中学习到的表示，可以对异常事件的时间依赖特性建模，还可以捕获异常事件的局部时间上下文和多层周期模式。  </li><li><strong>Multi-Modal Pattern Fusion Mudule</strong><br>为了捕获异常事件分布，在区域间和不同类别的关系，我们提出了深度融合模块，用于同时对周围地理区域和不同类别的异常事件的固有发生模式进行建模。我们将$K(表示K个时间槽)$个张量$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$按照时间进行排序，然后对于每一个时间槽$t_k$都有一个张量$\mathcal{A}^k_{i,j}$,将它的隐藏向量表示，应用attention机制，从空间-类别视图生成summarized嵌入向量。</li><li><strong>Conclusive Recurrent Networks</strong><br>依赖从空间-时间-类别视图生成的隐藏表示，我们提出一个conclusive recurrent networks来有效地捕获位置、时间、类别多模态的序列模式。最终的spatial-temporal-categorical多视图序列表示被保存在conclusive recurrent network单元格的最终状态，在解码阶段为预测异常发生的概率提供了指导。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure1.png" alt=""><br>输入的数据A是非0即1的张量，表示目标区域和邻居发生异常事件的情况。先选中一个目标区域$r_{i,j}$，找出这个区域的邻居$r_{i\prime,j\prime} \in G(i,j)$。flatten得到的是目标区域和邻居的值，就是0或1值，然后选中一个区域，有K个时间段，得到一个区域在一个异常类别上的时间序列，例如0110…，然后输入到LSTM中，每一步都可以得到一个隐藏状态。所以第一个时间段第一个异常类别，会有很多个隐藏状态，假设目标区域和其邻域共有9个区域，则第一个时间步第一个类别会输出9个隐藏状态。这样每个区域在每个类别上，每个时间步上都会得到一个隐藏状态。只是对一个区域进行建模，没有涉及到邻居和类别。在第二步使用Attention，获取每个区域的得到，就是把这个区域所有的特征全都塞到一个全连接神经网络中，一个区域的特征有3个，LSTM输出的隐藏状态，这个区域的嵌入表示，异常类别的嵌入表示，根据这3个特征得到这个区域的得分，然后将每个区域的得分使用softmax归一化。然后将得分再乘上一个隐藏状态得到每个时间步的表示。再将每个时间步的表示作为一个序列传入到LSTM中，将最终的隐藏状态传入到MLP中。最后预测的值是一个概率，表示这个区域在这个时间段发生这个类别的异常事件的概率。<h1><span id="6-methodology">6. Methodology</span></h1><h2><span id="61-context-aware-recurrent-framework">6.1. Context-aware Recurrent Framework</span></h2>&ensp;&ensp;&ensp;&ensp;在MiST架构中，在异常事件在时间槽$t_1到t_k$的分布，我们首先采用LSTM网络来编码复杂的的区域内相关性。特别，LSTM包含1个记忆细胞状态和3个控制门通过分别执行写、读、重置操作来更新记忆细胞状态。用公式表示，区域$r_{i,j}$和异常类别$c_l$在第$t$个时间槽的隐藏状态$h^t_{i,j,l}$和记忆细胞状态$c^t_{i,j,l}$计算公式如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;其中$W_<em> \in \mathbb{R}^{d_s \times d_s}$表示前一个状态$(i.e., c^{t-1}_{i,j,l} \quad and \quad h^{t-1}_{i,j,l})$到当前状态的转换矩阵，$V_</em> \in \mathbb{R}^{d_x \times d_s}$是从输入到当前状态的转换矩阵，$d_x和d_s$分别表示输入向量的维度和隐藏状态的维度，且$b_<em> \in \mathbb{R}^{d_s}$是偏置向量，$\sigma(.)和\phi(.)$分别表示sigmoid和tanh函数。$\odot$表示元素相乘。分别使用$i^t_{i,j,l},o^t_{i,j,l},f^t_{i,j,l}$表示输入门、输出门、遗忘门。为了简单起见，我们用$h^t_{i,j,l}=LSTM(</em>,c^{t-1}_{i,j,l},h^{t-1}_{i,j,l})$表示上面的式1。当然也存在RNN的一些变体，例如GRU。<h2><span id="62-multi-modal-pattern-fusion-module">6.2. Multi-Modal Pattern Fusion Module</span></h2>&ensp;&ensp;&ensp;&ensp;然后直接或间接地应用RNN来解决异常事件预测问题是直观的。一般的RNN不能处理来自其他地理区域和时间类别的影响因素。因此我们进一步使用attention机制来自适应地捕获空间和类别的动态相关性。Attention机制用来推断训练集不同部分的重要性，让学习算法更加关注重要的部分。Attention机制引入一个context vector建模相关性，让编码器-解码器摆脱定长的内部表示。并且，在融合过程中，为了区分区域和类别，用$e_{r_{i,j}} \in \mathbb{R}^{d_e}$表示区域嵌入，用$e_{c_j} \in \mathbb{R}^{d_e}$表示类别嵌入，这两种嵌入在attention机制中会用到。attention的计算公式如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/2.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;在attention网络中将隐藏表示向量的大小作为attention dimensionality，用$S$表示，其中$d_s$表示LSTM中隐藏状态的维度。$W^k \in \mathbb{R}^{d_s \times S} \quad b^k \in \mathbb{R}^{d_s}$分别表示权重矩阵和偏置向量，将输入映射到隐藏层，得到$\eta^k_{i,j,l}$作为$h^k_{i,j,l}$的隐藏表示。然后我们度量了每个区域$r_{i,j}$每种类别$c_l$的隐藏表示$\eta^k_{i,j,l}$的重要性，归一化得到$\alpha^k_{i,j,l}$。attention中的权重由输入的空间-类别特征$e_{r_{i,j}} \in \mathbb{R}^{d_e},e_{c_j} \in \mathbb{R}^{d_e}$联合决定，在Context-LSTM编码器中编码历史隐藏状态$h^k_{i,j,k}$。在获取attention权重后，在时间段k的输出隐藏表示向量计算如下：<script type="math/tex; mode=display">q^k = \sum_{i,j \in G}\sum_{l=1}^{L} \alpha^k_{i,j,l}h^k_{i,j,l} \tag{3}</script>&ensp;&ensp;&ensp;&ensp;其中$q^k$是$h^k_{i,j,l}$的summarized拼接表示，描述了在区域$r_{i,j}$异常事件的发生，哪个因素更重要。在MiST的训练过程中，带有attention机制的深度融合模块被参数化为前向神经网络，和整个神经网络一起训练。我们提出的方法是非常通用的，可以自动学习不同视图的相关性权重。<h2><span id="63-conclusive-recurrent-network">6.3. Conclusive Recurrent Network</span></h2>&ensp;&ensp;&ensp;&ensp;目前为止，我们已经研究了MiST到的2个组件，(i)从temporal角度，使用context-LSTM建模区域内动态的相关性；(ii)从spatial-categorical角度，使用深度融合模块捕获复杂的区域间和类别见的相关性。经过以上步骤，得到了summarized representation $q^k$，从不同角度使用不同的权重$\alpha^k_{i,j,l}$计算组合表示。<br>&ensp;&ensp;&ensp;&ensp;为了将空间-类别的编码pattern和时间pattern整合在一起，我们提出了用循环神经网络编码多维模式，用潜在空间的表示建模location-time-category之间的关系。在这篇论文中，我们采用LSTM作为循环神经单元，公式如下：<script type="math/tex; mode=display">\xi_k = LSTM(q_{k-1},\xi_{k-1}) \tag{4}</script>&ensp;&ensp;&ensp;&ensp;联合嵌入$\xi$将所有的空间、时间、类别单元映射到一个共同的潜在空间中。提出的conclusive循环神经网络提供了一种灵活的方式让不同的视图彼此合作。将空间、类别上下文信号和时间状态结合，MiST框架可以预测将来异常事件，不仅仅根据时间序列关系，还根据区域间的空间关系和不同类别的共现关系。<h2><span id="64-forecasting-and-model-inference">6.4. Forecasting and Model Inference</span></h2>&ensp;&ensp;&ensp;&ensp;最终，我们利用MLP来解码异常事件出现的概率，通过捕获隐藏向量元素之间的非线性依赖。公式如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/5.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;其中，$N$表示隐藏层的个数，对于层$\psi_n$，$W_n$和$b_n$表示权重矩阵和偏置向量。我们使用$ReLU,\phi(.)$为全连接层的激活函数。使用$\sigma(.)sigmoid$作为输出层的激活函数，值域在(0,1),输出异常事件发生的概率，在区域$r_{i,j}$时间槽$t_k$异常事件类别$c_l$，例如$y^{l,k}_{i,j}$。<br>&ensp;&ensp;&ensp;&ensp;综上所述，我们的异常事件发生预测可以被看做是一个分类问题。我们利用叫啥上作为损失函数。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/6.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;其中，$\hat{y}^{l,k}_{i,j}$表示预测的在区域$r_{i,j}$第$k$个时间段发生第$l$个异常类别事件的概率，$S$是训练集中异常事件的集合。使用Adam优化器来学习参数。<br>算法流程如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/algo.png" alt="">  <h1><span id="7-evaluation">7. Evaluation</span></h1>在三个真实异常事件数据集上做了实验，数据从NYC和Chicago收集，验证模型的有效性和准确率和其他baseline，通过实验回答以下几个问题</li><li>Q1：和其他state-of-the-art预测方法，在预测全市犯罪和不同城市的异常情况时，MiST可以达到与之媲美的准确率吗？</li><li>Q2：在不同的时间段中，MiST一直比其他的算法表现好吗？</li><li>Q3：和其他state-of-the-art技术相比，MiST模型怎么预测不同种类的异常事件</li><li>Q4：MiST使用不同关键组件的组合形成的变体效果怎么样？</li><li>Q5：MiST在不同的空间和时间范围上表现怎么样？</li><li>Q6：不同的参数设置怎么影响MiST的预测效果？</li><li>Q7：当预测城市异常事件时，怎么解释MiST框架捕获的空间和类别维度的动态重要性权重？<h2><span id="71-data-description">7.1. Data Description</span></h2><h3><span id="711-data-statistics">7.1.1. Data Statistics</span></h3>&ensp;&ensp;&ensp;&ensp;我们从NYC和Chicago收集了2种类型的3个异常事件数据，有2个犯罪数据和1个城市异常数据，通过做实验，预测城市的每个区域发生每种城市犯罪和异常事件的可能性。数据集基本统计如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/table1.png" alt=""><br>在我们的实验中，我们重点关注了一些关键类别，把其他的类别看做外部类别。我们也给了不同类型和时间周期的异常事件在地理上的分布，如Figure2所示。</li><li>NYC Crime Data(NYC-C)：这个数据集中有多个类别的犯罪记录。每一个犯罪记录有犯罪类别、经纬度、时间。时间跨度为2015.1~2015.12</li><li>NYC Urban Anomaly Data(NYC-A)：这个数据集时间跨度为2014.1~2014.12，从NYC311个非紧急服务中心收集来的，这里记录了不同类别的城市异常。每个记录都有异常类别、经纬度、时间。</li><li>Chicago Crime Data(CHI-C)：从芝加哥收集的2015.1~2015.12不同种类的犯罪记录，记录的个数和NYC类似。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure2.png" alt=""> <h2><span id="72-experimental-setting">7.2. Experimental Setting</span></h2><h3><span id="721-parameter-setting">7.2.1. Parameter Setting</span></h3>&ensp;&ensp;&ensp;&ensp;在我们的试验中，利用Adam作为优化器，使用Tensorflow实现MiST架构。在LSTM中设置隐藏状态维度$d_s=32$，区域嵌入向量$e_{r_{i,j}}$和类别嵌入向量$e_{c_j}$的维度$d_e=32$，attention的维度$S=32$，MLP的层数为3。batch size=64，学习率=0.001。<h3><span id="722-baseline-methods">7.2.2. Baseline Methods</span></h3></li></ul><p>(i)传统的时间序列预测方法：SVR、ARIMA<br>(ii)传统的有监督学习算法：LR<br>(iii)循环神经网络和它的变体for时空数据预测：ST-RNN 、GRU<br>(iv)先进的神经网络模型for 时间序列和序列模型：RDN、HRN、ARM</p><h3><span id="723-evaluation-protocols">7.2.3. Evaluation Protocols</span></h3><p>&ensp;&ensp;&ensp;&ensp;在实验中，按照时间顺序将数据集划分为训练集(6.5个月)、验证集(0.5个月)和测试集(1个月)。验证集被用来调整超参数，在测试集上进行性能比较。我们把NYC和Chicago划分为248和189个互不相交的区域，每个区域的大小$2km \times 2km$，根据区域划分的结果，我们可以映射每个异常事件(犯罪或城市异常)到一个地理区域中，作为MiST的输入。我们采用2种评价指标来衡量所有的方法。</p><ul><li>(i)使用Macro-F1 和Micro-F1来衡量不同种类犯罪的预测准确率。这2个指标表示了不同类别之间的整体效果。这2个指标的数据定义如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/micro.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/macro.png" alt=""><br>其中$J$是异常事件的种类数。这2个值越高效果越好</li><li>(ii) 使用F1-score和$AUC$来衡量预测一个类别的异常事件发生的准确率。F1和AUC越高，说明预测效果越好。<br>&ensp;&ensp;&ensp;&ensp;为了确保所有方法的性能公平比较，在测试集中预测一段时间连续几天异常事件发生的概率。在评估结果中，一段时间所有天的平均性能作为最终的结果。<h2><span id="73-performance-comparison">7.3. Performance Comparison</span></h2><h3><span id="731-overall-comparisonq1">7.3.1. Overall Comparison(Q1)</span></h3><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/table2.png" alt=""></li></ul><p>&ensp;&ensp;&ensp;&ensp;表2显示了不同城市犯罪和城市异常的预测准确率。总结以下3点：<br>&ensp;&ensp;&ensp;&ensp;第一：MiST比其他神经网络方法效果都好。例如，在预测Chicago犯罪时，MiST比最好的模型RDN Macro-F1和Micrl-F1高9.6%和30.9%。<br>&ensp;&ensp;&ensp;&ensp;第二：神经网络方法比传统的时间序列和有监督学习方法效果好。这是由于（1）传统的时间序列预测方法仅仅强调一个固定的时间模式，而不是时间依赖的演变。（2）神经网络方法使用非线性方法捕获多维空间-时间数据的内在结构，这非常有用。<br>&ensp;&ensp;&ensp;&ensp;第三：在循环神经网络中(ST-LSTM和GRU)和深度序列数据模型方法(RDN、HRN、ARM)效果不分上下。这再一次验证了仅仅考虑时间维度的数据依赖在预测犯罪和城市异常发生时不够的。相反，MiST动态关联潜在的空间、时间、类别的关系，表现了很好的灵活性和优越性。</p><h3><span id="732-forecasting-accuracy-vs-time-periodq2">7.3.2. Forecasting Accuracy v.s Time Period(Q2)</span></h3><p>&ensp;&ensp;&ensp;&ensp;对于MiST和其他的baseline，在不同的训练和测试时间段上做了实验。我们发现MiST在不同的测试时间段上一直保持最好的效果。并且也可以发现在MiST和起亚baseline相比，当滑动训练集和测试集的时间窗口时，MiST的效果更稳定，这说明MiST在学习随着时间动态的异常事件分布时更健壮。</p><h3><span id="733-forecasting-accuracy-vs-categoriesq3">7.3.3. Forecasting Accuracy v.s Categories(Q3)</span></h3><p>&ensp;&ensp;&ensp;&ensp;我们测试了MiST在预测单个异常类别事件的有效性，在NYC的犯罪和异常数据、Chicago的犯罪数据集上，结果如figure3和4所示。发现MiST在所有的类别上都取得了最好的效果。另一个发现是MiST在预测building/Use时效果比ST-RNN高了84.1%左右，这说明MiST在预测稀疏异常类别时表现也很好，解决了数据稀疏问题。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure3.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure4.png" alt=""></p><h2><span id="74-component-wise-evaluation-of-mistq4">7.4. Component-Wise Evaluation of MiST(Q4)</span></h2><p>为了更的理解MiST，对MiST的不同组件进行组合做了实验。</p><ul><li><strong>Spatial-View+Temporal View</strong> $MiST-st$<br>这个变体捕获了空间和时间依赖，不考虑类别的影响</li><li><strong>Category-view+Temporal View</strong>$MiST-ct$<br>这个变体考虑了累呗和时间依赖，不考虑区域间的空间相关性</li><li><strong>Temporal View</strong>$MiST-t$<br>这个变体仅仅使用LSTM和时间attention机制，不考虑空间和类别。      </li></ul><p>&ensp;&ensp;&ensp;&ensp;结果显示使用全部的组件效果最好，这说明使用一个联合框架是很有必要的，同时捕获空间视图（区域间的空间相关性）、时间视图（区域内的时间相关性）、类别视图（类别间的依赖）。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure5.png" alt="">   </p><h2><span id="75-effect-of-spatial-and-temporal-scaleq5">7.5. Effect of Spatial and Temporal Scale(Q5)</span></h2><p>&ensp;&ensp;&ensp;&ensp;进一步研究了空间和时间范围的影响。在event context tensor$\mathcal{A}$中，网格地图的地理范围$G=I \times J$，在我们的实验中$I=J$，循环框架中时间序列长度为$T$。 在十月份的Crime上做了实验，实验结果如图6所示：2个结论，（1） 随着I和J的增大，实验效果也变好。因为每个网格是$2km \times 2km$，I和J增大，说明考虑了更大的地理区域在学习表示时，当I和J为11时，准确率趋于稳定。另一个可能的原因是当考虑更大的地理区域时，需要学习更多的参数，训练MiST更加困难。（2）当时间序列长度$T$变大时，准确率也变得更好。当T=10时趋于稳定。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure6.png" alt="">   </p><h2><span id="76-hyperparameters-studiesq6">7.6. Hyperparameters Studies(Q6)</span></h2><p>&ensp;&ensp;&ensp;&ensp; 为了检验MiST模型的健壮性，设置不同的超参数看预测效果。除了被测试的参数外，其余参数都被设置为默认值。 总体上，发现MiST在两个任务上（预测NYC犯罪和异常事件）对参数不敏感，并且都能达到很好的效果，说明MiST模型的健壮性。并且发现当表示的维度为32时，效果最好。这是因为刚开始，潜在表示的维度变大能够为循环框架和Attention框架提供一个更好的表示，随着参数的增加，可能会造成过拟合。在我们的实验中，为了权衡有效性和计算代价，将表示维度设置为32。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure7.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure8.png" alt=""> </p><h2><span id="case-studyq7">Case Study(Q7)</span></h2><p>&ensp;&ensp;&ensp;&ensp;MiST除了有很好的预测性能，并且在预测一个区域特定类别的异常事件时，能很好的解释空间和类别相关性的重要性。为了说明这点，我们做了实验说明模型的可解释性，在预测NYC盗窃事件时，在一个$5\times 5$的网格中，中间的区域表示目标区域，将attention权重可视化。说明MiST能够动态建模目标区域和其他区域的相关性，并且可以动态建模目标区域的异常类别事件（盗窃）和其他类别的关系。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure9.png" alt="">   </p><h1><span id="conclusion">Conclusion</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文提出了一个新的神经网络架构MiST，从空间-事件-类别维度对城市异常事件的动态模式进行建模。我们整合了循环神经网络和多模态融合模块来建模空间-事件的相关性。在不同的真实数据集上评测模型，结果显示MiST比其他baseline效果都好。<font color="#FF0000">[未来方向]</font>关于我们工作的未来方向。第一，检测不同类别的异常事件发生的因果关系，这对公共政策的制定有用。发现异常事件发生的潜在因素，以及不同类别的异常事件在时空上怎么传播。第二，由于数据的限制，我们只在3个真实数据集上做了实验，实际上，MiST通用且灵活，可以应用到其他多维且有时间戳的序列数据上。</p><h1><span id="其他论文">其他论文</span></h1><p>除了这篇论文之外，作者还发表了3篇关于anomaly方向的论文：</p><ul><li><p>[2016 CIKM]《Crowdsourcing-based urbananomaly prediction system for smart cities》<br>数据集：311 is NYC’s non-emergency service platform.人们可以在这个平台上抱怨周围发生的事情，通过文字、电话或者app，在NYC OpenData可以获取到。<br>Crodsourcing-bases Urban Anomaly Prediction Scheme(CUAPS)给定一个区域，在异常发生之前进行预测。在crowdsourcing data中，结合空间和时间信息进行预测。首先使用贝叶斯推理模型，根据区域的异常分布来鉴别区域之间的依赖性。然后应用一个最优的异常状态预测方案来预测一个区域的异常事件，从这个区域本身的数据和它依赖的区域。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/CUAPS.png" alt=""></p></li><li><p>[2017 ECML] 《Uapd: Predicting urban anomalies from spatial-temporal data》<br>数据集：311 is NYC’s non-emergency service platform（作者提供了整理好的数据和源码）<a href="https://bitbucket.org/xianwu9/uapd/src/master/" target="_blank" rel="noopener">https://bitbucket.org/xianwu9/uapd/src/master/</a><br>和Pittsburgh OpenData portal<br>挑战：时间动态，多维相关，空间、时间、类别。提出模型Urban Anomaly PreDection(UAPD)。首先提出一个概率模型，模型参数通过马尔科夫连推导出来，来检测历史异常记录的变化点，然后最相关的记录被用来预测将来的异常。在第二阶段，从被检测出的变化点开始，使用3维张量建模异常数据，每一维表示区域、时间、类别。然后，分解张量，将每个维度之间的潜在关系合并到张量对应的固有因子上。随后，预测下一时间段的异常变成了一个时间序列预测问题。在第三阶段，利用向量自回归来捕获多个时间序列之间的相互依赖性，从而生成预测结果。</p></li><li>[2018 CIKM] 《DeepCrime:Attentive Hierarchical Recurrent Networks for Crime Prediction》和郑宇联合发表<br>数据集：NYC的Crime记录，<br>DeepCrime，a deep neural network architecture。编码空间、时间、类别到隐藏向量表示中。通过分层循环神经网络捕获异常的动态信息。</li></ul><p>Anomaly领域的其他论文：</p><ul><li>[2013 Ubicomp] 《Flead: Online frequency<br>likelihood estimation anomaly detection for mobile sensing》<br>数据集：手机收集的数据</li><li>[2015 CIKM] 《Profiling pedestrian distribution and anomaly detection in a dynamic environment》<br>数据集：没有说</li><li>[2015 SIGSPATIAL] 郑宇《Detecting collective anomalies from multiple spatio-temporal datasets across different domains》<br>提供了数据集和代码<a href="https://www.microsoft.com/en-us/research/publication/detecting-collective-anomalies-from-multiple-spatio-temporal-datasets-across-different-domains/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2F%3Fid%3D255670" target="_blank" rel="noopener">链接</a><br>数据描述：<br>（1）POI数据：NYC有24031个POI，共14中类别<br>（2）Road network data：在NYC的862个区域中的路段，每个路段有2个终点和一些中间点，还有一些属性，比如级别，速度限制等<br>（3）311data：NYC<br>（4）Taxicab data：在NYC的14000个出租车产生的数据，包括费用和行程数据，行程数据包括：上下车地点和时间，行程的距离和持续时间，出租车ID，乘客个数等。<br>（5）Bike tenting data：自行车租赁数据，NYC的340个自行车站点，大约7000辆车，每一条记录包括时间，车辆ID，站点ID，返还记录。</li><li>[2017 CIKM] 《Spatiotemporal event forecasting from incomplete hyper-local price data》<br>数据集：有6个数据集，来自6个不同的城市，其中2个是商品价格数据，数据从<a href="https://www.premise.com/" target="_blank" rel="noopener">https://www.premise.com/</a>获取。其中4个是美国4个城市房地产短租的价格数据，数据从Airbnb获取</li><li>[2017 KDD] 《Contextual spatial outlier detection with metric learning》<br>一部分数据来源：<a href="http://archive.ics.uci.edu/ml/index.php" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/index.php</a><h1><span id="marco-f1">Marco-F1</span></h1>在二分类任务中，使用Precison，Recall和F1值来评价分类的效果。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/hun.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/f1.png" alt=""><br>F1是针对二分类的，对于多分类，有2个常用的指标，Marco-F1和Micro-F1.   <h2><span id="micro-f1">Micro-F1</span></h2><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/micro.png" alt=""><br>假设对于一个多分类问题，有三个类，分别是1，2，3<br>$TP_i$表示分类$i$的TP<br>$FP_i$表示分类$i$的FP<br>$TN_i$表示分类$i$的TN<br>$FN_i$表示分类$i$的FN<br>接下来，我们来计算Micro的Precison<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/precison-mi.png" alt=""><br>以及Micro的Recall<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/recall-mi.png" alt=""><br>然后计算Micro-F1<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/f1-mi.png" alt="">   <h2><span id="macro-f1">Macro-F1</span></h2>先计算每个类的Precison和Rcall，从而计算出每个类的F1，然后将所有类的F1值平均得到Macro-F1。<br>如果数据集中各个类的分布不均衡的话，建议使用Micro-F1。<br>Macro-F1平等的看待各个类别，它的值更容易受少类别的影响Micro则更容易受常见类别的影响。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;{}&quot;&gt;&lt;/a&gt;{}&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3320000/3313730/p717-huang.pdf?ip=218.247.253.153&amp;amp;id=3313730&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1561380383_40e3bd8d678088e9b04173b89f85c49c&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文出处&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/06/12/Docker/"/>
    <id>http://yoursite.com/2019/06/12/Docker/</id>
    <published>2019-06-12T10:48:24.562Z</published>
    <updated>2019-10-22T14:30:13.853Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: Docker<br>date: 2019-06-12T18:48:24.000Z<br>tags:</p><h2><span id="-docker-镜像">  - Docker、镜像</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖到一个可移植的镜像中，然后发布到Linux或Window系统中，也可以实现虚拟化。容器是完全使用沙箱机制，相互之前不会有任何接口。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-docker">2. Docker</a></li><li><a href="#3-%e9%95%9c%e5%83%8f%e5%ae%b9%e5%99%a8">3. 镜像&amp;容器</a><ul><li><a href="#31-%e9%95%9c%e5%83%8f">3.1. 镜像</a></li><li><a href="#32-%e9%95%9c%e5%83%8f%e5%91%bd%e4%bb%a4">3.2. 镜像命令</a></li><li><a href="#33-%e5%ae%b9%e5%99%a8">3.3. 容器</a></li><li><a href="#34-%e5%ae%b9%e5%99%a8%e5%91%bd%e4%bb%a4">3.4. 容器命令</a></li></ul></li><li><a href="#4-dockerfile">4. Dockerfile</a><ul><li><a href="#41-%e7%9b%b8%e5%85%b3%e6%8c%87%e4%bb%a4">4.1. 相关指令</a></li><li><a href="#42-%e6%9e%84%e5%bb%ba">4.2. 构建</a></li></ul></li><li><a href="#5-%e4%bd%bf%e7%94%a8docker%e8%bf%90%e8%a1%8c%e7%a8%8b%e5%ba%8f">5. 使用Docker运行程序</a></li><li><a href="#6-%e6%9e%84%e5%bb%ba%e8%87%aa%e5%b7%b1%e7%9a%84%e9%95%9c%e5%83%8f">6. 构建自己的镜像</a><ul><li><a href="#61-%e5%88%9b%e5%bb%badockerfile">6.1. 创建Dockerfile</a></li><li><a href="#%e6%9e%84%e5%bb%ba">构建</a></li><li><a href="#%e8%bf%90%e8%a1%8c">运行</a></li></ul></li><li><a href="#docker%e5%b8%b8%e7%94%a8%e5%91%bd%e4%bb%a4">Docker常用命令</a></li><li><a href="#docker%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e8%b7%b5">Docker深度学习实践</a><ul><li><a href="#%e7%ab%af%e5%8f%a3%e6%98%a0%e5%b0%84">端口映射</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-docker">2. Docker</span></h1><p>参考资料：<br><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&amp;mid=2655824742&amp;idx=1&amp;sn=43dcbd8cd3b3e0dc5f06c83a50983420&amp;chksm=bd74e6b18a036fa7e5fe2229b3fa08f5a4d11c6d201deba29c63e61c47ed37b5aa7f22e441d6&amp;scene=0&amp;xtrack=1&amp;key=1873ed4ed1cb893ec82026059f24db129748acc346da2d85cea356373d0c0fd919da6e704f47695c7743b64ff520c46fb51d245dacff68136d667c05e73d963d768ee11171e66dedea24d39bb3d67ced&amp;ascene=1&amp;uin=MTM1ODU4OTIwOA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=84krGCWgmUm73hPCIwVp8NE9B3dpOiU5R1bRm3jvlvv%2FygbqWRm4O%2BYabIzyFhbf" target="_blank" rel="noopener">资料1</a>    </p><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&amp;mid=2655817429&amp;idx=1&amp;sn=f82daff5e9fad66a0e11cdb92c12715e&amp;chksm=bd74c3028a034a14cadf884d97f2a0fc3372d3baa97fb5ae6724e6d1926520eada090c0ffb16&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">资料2</a><br>&ensp;&ensp;&ensp;&ensp;Docker就是一个运行在操作系统上的软件。这个软件上运行很多容器，这些容器相互独立，相互隔离。容器中可以安装很多应用程序。<br>&ensp;&ensp;&ensp;&ensp;我们平时要在Windows上安装Linux系统，都需要先安装一个VMWare，然后在上面安装Linux系统。原理就是虚拟出一套硬件资源，然后在上面运行一个完整的操作系统，再在操作系统上运行所需要的应用程序。在安装虚拟机时，需要先提前给虚拟机分配硬盘，内存等资源。一旦分配，这些资源就被虚拟机全部占用。Docker也可以实现虚拟化。但是Docker没有自己的内核，Docker容器内的应用程序是直接运行在宿主的内核，Docker比传统的虚拟机更轻便。Docker就是一个软件。如果以后再Windows上安装Linux系统，可以先在本地电脑上安装一个Windows版本的Docker，然后</p><h1><span id="3-镜像amp容器">3. 镜像&amp;容器</span></h1><h2><span id="31-镜像">3.1. 镜像</span></h2><p>&ensp;&ensp;&ensp;&ensp;官方定义：Docker镜像是一个只读模板，可以用来创建Docker容器。镜像是一种轻量级的，可执行的独立软件包，软件和依赖的环境可以打包成一个镜像。这个镜像包含某个软件需要的所有内容，包括代码、库、环境变量、配置文件等。<br>&ensp;&ensp;&ensp;&ensp;比如我们开发的Web应用需要JDK，Tomcat，环境变量等。那我们就可以把这些都打包成一个镜像，包括代码+JDK+Tomcat+CentOS系统+各种配置文件等。打包后的镜像如果可以运行，那么这个镜像就可以在任何安装有Docker的电脑上运行。<br>&ensp;&ensp;&ensp;&ensp;任何镜像的创建会基于其他的父镜像，也就是说镜像是一层套一层的。比如一个Tomcat镜像需要运行在CentOS上面，那我们的Tomcat镜像就会基于CentOS镜像创建。</p><h2><span id="32-镜像命令">3.2. 镜像命令</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. docker images：查看本地主机上所有的镜像。注意是本地主机的！这里能看到镜像的名称、版本、id、大小等基本信息，注意这里的 imageID 是镜像的唯一标识！   </span><br><span class="line"></span><br><span class="line">2. docker rmi：删除本地的镜像，如下图所示，可以加上 -f 参数进行强制删除。</span><br><span class="line">这里的 rmi 命令跟 Linux 中的删除命令就很像啦，只是这里加了一个 i 代表 image！   </span><br><span class="line"></span><br><span class="line">3. docker search：根据镜像名称搜索远程仓库中的镜像！   </span><br><span class="line"></span><br><span class="line">4. docker pull：搜索到某个镜像之后就可以从远程拉取镜像啦，有点类似咱们 Git 中的 Pull 命令，当然对应的还有个 dockerpush 的命令。</span><br></pre></td></tr></table></figure><h2><span id="33-容器">3.3. 容器</span></h2><p>&ensp;&ensp;&ensp;&ensp;Docker的容器是<strong>用镜像创建的运行实例</strong>，Docker可以利用容器独立运行一个或一组应用。我们可以使用客户端或API控制容器的启动、开始、停止、删除。每个容器都是相互独立的。上一步创建的镜像是一个静态的文件，这个文件想要运行的话，就要先变成容器。我们可以把容器看做是一个简易版的Linux系统和运行在上面的程序。<br><strong>镜像和容器的关系</strong><br>类似于Java中的类和对象的关系。镜像可以看做一个类，容器是镜像的一个实例。可以根据一个类new很多个实例，new出来的实例就相当于一个个容器。镜像是静态的文件，容器是有生命的个体。   </p><h2><span id="34-容器命令">3.4. 容器命令</span></h2><ul><li>docker pull<br>从远程仓库中拉取镜像</li><li><p>通过镜像创建容器<br><code>docker run [OPTIONS] IMAGE</code>可以基于某个镜像运行一个容器，如果本地有指定的镜像则使用本地的镜像，如果没有则远程拉取然后启动<br><code>docker run -v $PWD:/root -d -ti --runtime=nvidia --rm --name wbbmxnet mxnet/python:1.4.1_gpu_cu90_mkl_py3</code>   </p><h1><span id="4-dockerfile">4. Dockerfile</span></h1><h2><span id="41-相关指令">4.1. 相关指令</span></h2><p>镜像可以从远程仓库中拉取，也可以自己创建一个镜像。Dockerfile是一个包含用户能够构建镜像的所有命令的文本文档，它有自己的语法和命令。Docker能够从Dockerfile中读取指令并自动构建镜像。<br>如果想构建自己的镜像，就要自己写Dockerfile。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-cudnn7-devel</span><br><span class="line"></span><br><span class="line">LABEL SongChao chaosong@bjtu.edu.cn</span><br><span class="line"></span><br><span class="line">RUN apt-get update</span><br><span class="line"></span><br><span class="line">RUN apt-get install -y gcc make build-essential libssl-dev wget curl vim --allow-unauthenticated</span><br><span class="line"></span><br><span class="line">RUN mkdir /root/python3.6</span><br><span class="line"></span><br><span class="line">COPY Python-3.6.8 /root/python3.6/</span><br><span class="line"></span><br><span class="line">WORKDIR /root/python3.6</span><br><span class="line"></span><br><span class="line">RUN ./configure &amp;&amp; make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">RUN wget https://bootstrap.pypa.io/get-pip.py</span><br><span class="line"></span><br><span class="line">RUN python3 get-pip.py</span><br><span class="line"></span><br><span class="line">ENV PYTHONIOENCODING=utf-8</span><br><span class="line"></span><br><span class="line">RUN pip3 install numpy scipy pandas tensorflow-gpu==1.9.0 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">RUN mkdir /workdir</span><br><span class="line"></span><br><span class="line">WORKDIR /workdir</span><br><span class="line">```       </span><br><span class="line">ENV设置环境变量，或者定义变量</span><br><span class="line">在制作dockerfile时，有一些缓存可以删除，可以在RUN里面写相关的操作，这个就只是一层。</span><br><span class="line"></span><br><span class="line">哈哈哈</span><br><span class="line"></span><br><span class="line">`FROM`指定基础镜像，当前镜像是基于哪个镜像创建的，有点类似Java中类继承，FROM指令必须是Dockerfile必须是Dockerfile文件的首条命令。  </span><br><span class="line">`LABEL`给镜像添加元数据，指定作者，邮箱等信息。  </span><br><span class="line"></span><br><span class="line">## 4.2. 构建</span><br><span class="line">Dockerfile的执行顺序从上到下顺序执行，编写好Dockerfile文件后，就需要使用docker build命令对镜像进行构建了。   </span><br><span class="line">`docker build [OPTIONS] PATH | URL | -`  </span><br><span class="line">`docker build -t chaosong/cuda-10.1-cudnn7-devel:with_cuda_samples .</span><br><span class="line">`</span><br><span class="line">-f：指定要使用的 Dockerfile 路径，如果不指定，则在当前工作目录寻找 Dockerfile 文件！   </span><br><span class="line">-t：镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 注意后面的 . , 用于指定镜像构建过程中的上下文环境的目录，.表示当前目录。 </span><br><span class="line"></span><br><span class="line"># 5. 使用Docker运行程序   </span><br><span class="line">1. 登录上网  </span><br><span class="line">   `links 10.1.61.1/a30.htm`   </span><br><span class="line">   登录之后按`Ctrl+C`退出  </span><br><span class="line">2. 在dockerhub上查询镜像</span><br><span class="line">   浏览器进入[docker hub](https://hub.docker.com/)，找到自己需要的镜像，gpu28号服务器Cuda的版本是9.0，下载的镜像需要和服务器上Cuda版本一致。  </span><br><span class="line">3. 拉取镜像</span><br><span class="line">   例如镜像的全称为：</span><br><span class="line">   `mxnet/python:1.4.1_gpu_cu90_mkl_py3`   </span><br><span class="line">   `docker pull mxnet/python:1.4.1_gpu_cu90_mkl_py3`</span><br><span class="line">   这条命令会把镜像下载到服务器上，如果本地已经存在该镜像，docker会在佛那个使用本地的镜像，不再下载。   </span><br><span class="line">4. 启动镜像</span><br><span class="line">   `docker run -v $PWD:/root -d -ti --runtime=nvidia --rm --name wbbmxnet &lt;镜像名:标签&gt;`   </span><br><span class="line">   例如  </span><br><span class="line">   `docker run -v $PWD:/root -d -ti --runtime=nvidia --rm --name wbbmxnet mxnet/python:1.4.1_gpu_cu90_mkl_py3`   </span><br><span class="line">   使用上面的命令可以启动镜像，这里的参数-v通过将宿主机的目录映射到容器内，\$PWD是当前目录，映射目标是容器内的/root目录，这时\$PWD中的目录会全部映射到容器中的/root目录下。</span><br><span class="line">   启动容器后执行</span><br><span class="line">   `cd /root`  </span><br><span class="line">   `ls`  </span><br><span class="line">   可以看到服务器中的目录都映射到容器内，当在容器内创建一个目录时，然后创建一个文件，exit退出容器，可以看到本地服务器也有刚才那个文件。</span><br></pre></td></tr></table></figure><p> mkdir docker_test<br> touch docker_test.txt<br> exit<br> ls<br> ls docker_test</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5. 运行GPU程序  </span><br><span class="line">   先不启动容器，在宿主机上使用  </span><br><span class="line">   `nvidia-smi`查看当前空闲的GPU，使用空闲的GPU运行程序。</span><br><span class="line">   把py程序上传到服务器上，然后启动容器。</span><br></pre></td></tr></table></figure><p> docker run -v $PWD:/root -d -ti —runtime=nvidia —rm —name wbbmxnet mxnet/python:1.4.1_gpu_cu90_mkl_py3<br> cd 进入py程序所在的目录<br> python3 xxx.py</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">分割线   </span><br><span class="line"></span><br><span class="line"># 6. 构建自己的镜像     </span><br><span class="line">## 6.1. 创建Dockerfile   </span><br><span class="line">通过Dockerfile构建自己的镜像。镜像构建时，会一层层的构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是该文件一直跟随镜像。因此在创建镜像的时候，需要小心，每一层尽量只添加该层需要的东西，任何额外的东西应该在该层构建结束前清理掉。  </span><br><span class="line">Docker能够从Dockerfile中读取指令自动的构建镜像。   </span><br><span class="line"></span><br><span class="line">在服务器上创建一个目录  </span><br><span class="line">`mkdir wbb_docker_gcn`   </span><br><span class="line">`cd wbb_docker_gcn`   </span><br><span class="line">可以在这个目录下下载一些创建镜像需要的文件</span><br><span class="line">`git clone https://github.com/NVIDIA/cuda-samples.git`    </span><br><span class="line">然后编写用于构建镜像的Dockerfile</span><br><span class="line">`vi Dockerfile`   </span><br><span class="line">内容如下：</span><br><span class="line">```python</span><br><span class="line">#指定镜像要构建在哪个镜像之上  </span><br><span class="line">#如果程序需要用到GPU，那就一定要构建在nvidia/cuda这个镜像上  </span><br><span class="line">FROM nvidia/cuda:9.0-cudnn7-devel</span><br><span class="line"></span><br><span class="line">#给镜像添加元数据，指定作者邮箱等信息</span><br><span class="line">LABEL WangBeibei 18120408@bjtu.edu.cn    </span><br><span class="line"></span><br><span class="line">#RUN会在当前镜像的最上面创建一个新层，并且能执行任何的命令，</span><br><span class="line">#然后对执行的结果进行提交，提交后的结果镜像在Dockerfile的后续步骤中使用 </span><br><span class="line"></span><br><span class="line">#更新Ubuntu的索引</span><br><span class="line">RUN apt-get update        </span><br><span class="line"></span><br><span class="line">#安装gcc工具</span><br><span class="line">RUN apt-get install -y wget python3-dev gcc git vim &amp;&amp; \</span><br><span class="line">    wget https://bootstrap.pypa.io/get-pip.py &amp;&amp; \</span><br><span class="line">    python3 get-pip.py   </span><br><span class="line"></span><br><span class="line">#在容器上创建一个目录，镜像就安装在该目录下 </span><br><span class="line">RUN mkdir /root/docker_gcn    </span><br><span class="line">   </span><br><span class="line">#关于COPY命令，如果要复制目录的话，COPY命令会把目录里面所有文件赋值到另一个目录下，  </span><br><span class="line">#而不是把这个目录直接复制过去，所以上面先在容器中创建了一个docker_gcn的目录，然后COPY命令将服务器本地的wbb_docker_gcn里面的文件都复制到了/root/docker_gcn/里面</span><br><span class="line">#将服务器本地的文件拷贝到容器中的目录上    </span><br><span class="line">COPY wbb_docker_gcn /root/docker_gcn/      </span><br><span class="line"></span><br><span class="line">#切换到那个目录，如果该目不存在，则创建。WORKDIR是切换当前工作路径，下面的RUN命令都会在这个WORKDIR下面执行 </span><br><span class="line">WORKDIR /root/docker_gcn      </span><br><span class="line"></span><br><span class="line">#下载</span><br><span class="line">```        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">分割线</span><br><span class="line"></span><br><span class="line">## 构建   </span><br><span class="line">Dockerfile是执行是从上到下顺序执行的，每条执行都会创建一个新的镜像层，并对镜像进行提交。编写好Dockerfile文件后，就需要使用dockerbuild命令对镜像进行构建了。   </span><br><span class="line">`docker build [OPTIONS] PATH | URL | -`  </span><br><span class="line">`docker build -t chaosong/cuda-10.1-cudnn7-devel:with_cuda_samples .</span><br><span class="line">`</span><br><span class="line">-f：指定要使用的 Dockerfile 路径，如果不指定，则在当前工作目录寻找 Dockerfile 文件！   </span><br><span class="line">-t：镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 注意后面的 . , 用于指定镜像构建过程中的上下文环境的目录。    </span><br><span class="line"></span><br><span class="line">## 运行   </span><br><span class="line">构建完镜像就可以启动这个容器了，启动完之后就可以运行py脚本   </span><br><span class="line">`docker run -itd --rm --runtime=nvidia -v $PWD:/workdir/ --name wbb songchao/tensorflow:1.9.0_py36_cu90_cudnn7 /bin/bash</span><br><span class="line">`</span><br><span class="line">由于这个命令非常重要，所以下面列出几个比较重要的参数：</span><br><span class="line"></span><br><span class="line">-d：启动容器，并且后台运行（Docker 容器后台运行，就必须要有一个前台进程，容器运行的命令如果不是一直挂起的命令，容器启动后就会自动退出）。使用-d不会进入docker的交互界面，只会返回一个长id。</span><br><span class="line">使用docker ps可以看到对应的短id，使用docker attach 短id，进入到docker的交互界面。</span><br><span class="line"></span><br><span class="line">-i：以交互模式运行容器，通常与 -t 同时使用。</span><br><span class="line"></span><br><span class="line">-t：为容器重新分配一个伪输入终端，通常与 -i 同时使用（容器启动后进入到容器内部的命令窗口）。</span><br><span class="line"></span><br><span class="line">-P：随机端口映射，容器内部端口随机映射到主机的高端口。</span><br><span class="line"></span><br><span class="line">-p：指定端口映射，格式为：主机(宿主)端口：容器端口。</span><br><span class="line"></span><br><span class="line">-v：建立宿主机与容器目录的同步。</span><br><span class="line"></span><br><span class="line">--name=&quot;myTomcat&quot;：为容器指定一个名称（如果不指定，则有个随机的名字）。</span><br><span class="line">其中--rm表示程序运行完，这个容器就删掉了。</span><br><span class="line">-bash表示打开一个命令行</span><br><span class="line"></span><br><span class="line">**Ctrl+P+Q把容器挂在后台**</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Docker常用命令   </span><br><span class="line">1. 列出机器上的镜像   </span><br><span class="line">   ```docker images```   </span><br><span class="line">   ![](Docker/images.png)</span><br><span class="line">2. 拉取镜像  </span><br><span class="line">   ```docker pull mxnet/python:1.4.1_gpu_cu90_mkl_py3</span><br></pre></td></tr></table></figure></li></ul><ol><li><p>将镜像push到个人仓库<br>在上一步从公共仓库拉取了一个镜像   </p><figure class="highlight docker"><figcaption><span>pull mxnet/python:1.1_gpu_cu100_mkl_py3```   </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">   将镜像push到<span class="number">27</span>号服务器上面的仓库  </span><br><span class="line">   ```docker tag source des```   </span><br><span class="line">   例如  </span><br><span class="line">   ```docker tag mxnet/python:<span class="number">1.1</span>_gpu_cu100_mkl_py3 lin-ai-<span class="number">27</span>:<span class="number">5000</span>/mxnet:<span class="number">1.41</span>_cu100_py3```   </span><br><span class="line">   推送镜像   </span><br><span class="line">   ```docker push lin-ai-<span class="number">27</span>:<span class="number">5000</span>/mxnet:<span class="number">1.41</span>_cu100_py3```   </span><br><span class="line">   这样镜像就会上传到<span class="number">27</span>号服务器上面。</span><br><span class="line"><span class="number">4</span>. 查看容器信息</span><br><span class="line">   （<span class="number">1</span>）docker ps：显示当前正在运行的容器，在 PORTS 一列，如果暴露的端口是连续的，还会被合并在一起，例如一个容器暴露了<span class="number">3</span>个 TCP 端口：<span class="number">100</span>，<span class="number">101</span>，<span class="number">102</span>，则会显示为 <span class="number">100</span>-<span class="number">102</span>/tcp。</span><br><span class="line"></span><br><span class="line">   （<span class="number">2</span>）docker ps -a：显示所有的容器，</span><br><span class="line">   容器的状态共有 <span class="number">7</span> 种：created|restarting|running|removing|paused|exited|dead。</span><br><span class="line">   （<span class="number">3</span>）docker ps -n <span class="number">3</span>：显示最后被创建的n个容器</span><br><span class="line">   （<span class="number">4</span>）docker ps -q：只显示正在运行容器的id，在清理容器时非常好用。</span><br><span class="line">   （<span class="number">5</span>）docker ps -s：显示容器文件大小，该命令很实用，可以获得 <span class="number">2</span> 个数值：一个是容器真实增加的大小，一个是整个容器的虚拟大小。</span><br><span class="line"><span class="number">5</span>. 以交互式方式启动容器</span><br></pre></td></tr></table></figure><p>[root@localhost ~]# docker run -it —name centos-test centos:7.4.1708<br>[root@ebd974405f42 /]# </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   以交互式方式启动容器后，docker会随机分配一个容器名，并为容器分配一个短id。</span><br><span class="line">6. 后台运行容器</span><br></pre></td></tr></table></figure><p>[root@localhost ~]# docker run —name centos-deamon -d centos:7.4.1708<br>63e5555c9ed35deb7e32c2a16896e0f806dbcf471d0484bdd904724a3cce542d  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   其中d表示容器在后台运行，使用run命令之后，会把容器挂到后台运行，并且会输出一个长的container id，通过docker ps查看容器的信息，这里输出的是容器id的前12位。</span><br><span class="line">7. 进入到后台运行的容器</span><br><span class="line">   通过docker attach ae60c4b64205连接到正在运行的终端，此时使用exit命令退出容器。  </span><br><span class="line">   </span><br><span class="line">8. 容器的状态</span><br><span class="line">   docker容器有几种状态，分别是created、up、exited、paused。</span><br><span class="line">   （1）我们通过run命令运行容器时，其实是将容器从created到up状态的过程。</span><br><span class="line">   （2）当通过stop命令停止容器时，容器进入exited状态。容器退出后，系统仍然保存该容器实例，即退出的容器仍然会占用系统的硬盘资源，需要使用rm删除该容器才能完全清楚容器的资源占用。容器stop或Ctrl+D时，会保存当前容器的状态之后退出，下次start时会保存上次的关闭时更改，而且每次attach进去的界面是一样的，和第一次run启动一样。</span><br></pre></td></tr></table></figure><p>[root@localhost ~]# docker stop e83cf32fbc22<br>e83cf32fbc22</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（3）重新启动退出的容器  </span><br><span class="line">处于exited状态的容器，可以通过start命令重新启动。</span><br></pre></td></tr></table></figure><p>[root@localhost ~]# docker start e83cf32fbc22<br>e83cf32fbc22</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（4）重启容器</span><br></pre></td></tr></table></figure><p>[root@localhost ~]# docker restart e83cf32fbc22<br>e83cf32fbc22</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">9.  删除容器  </span><br><span class="line">   从上面可以看出，通过stop命令停止容器，容器的相关文件仍然存储在宿主主机中，为了释放这部分空间，需要删除这些容器。</span><br><span class="line">   （1）`docker rm id`可以删除对应id的容器</span><br><span class="line">   （2）批量删除除了运行以外的程序</span><br><span class="line">   `docker rm $(docker ps -a -q)`</span><br><span class="line">   （3）如果要批量删除指定状态的容器  </span><br><span class="line">   `docker rm $(docker ps -a -q  status=exited)`</span><br><span class="line">11. **运行jupyter notebook**  </span><br><span class="line">    （1）在启动容器之前先查看自己在这台服务器上的id，在服务上直接输入```id```就可以查看。在启动容器的时候需要加上```-u 1042```表示哪个这个容器是属于哪个用户的。  </span><br><span class="line">    ![](Docker/uid.png)   </span><br><span class="line">    如果不指定用户，使用```gpustat```查看就会看到当前镜像是属于root用户。容器所挂载的目录所属用户也是root用户。那么你在服务器中去操作操作这个目录就会提示permission denied。例如在使用dicker run命令创建一个容器时，将服务器的deepst这个目录挂在在容器的/root目录下，那么容器中/root目录下面的内容就是服务器中/deepst这个目录下面的内容，并且容器中/root中下面的文件和目录使用-ls -l查看，所属的用户是root用户，那么在服务器中的/deepst目录下面，就没有权限操作当前目录，比如mkdir dira，就会出现permission denied。所以在启动容器的时候，一定要指定```-u 1042```  </span><br><span class="line">    ![](Docker/gpustat.png)   </span><br><span class="line"></span><br><span class="line">    （2）先启动容器</span><br><span class="line">    前面的7000是服务器的端口，后面的7000是容器的端口。这个服务器是共用的，需要找一个没有被占用的端口。容器的端口因为只有自己一个人用，所有没有被占用。</span><br><span class="line">    `docker run -it -p 7000:7000 -v $PWD:/root --runtime=nvidia --rm -u 1042 --name wbbJupyter ufoym/deepo:all-jupyter-py36` </span><br><span class="line">    后面的--rm表示这个退出这个容器时，容器就自动删除了。这个参数可以不指定。</span><br><span class="line">    （2）然后会进入到容器中，</span><br><span class="line">    `cd /root`</span><br><span class="line">    会看到服务器本地的目录   </span><br><span class="line">    （3）在容器中运行jupyter notebook  </span><br><span class="line">    `jupyter notebook --no-browser --ip 0.0.0.0 --port=7000 --allow-root`   </span><br><span class="line">    （4）在putty中添加7000的端口号   </span><br><span class="line">    然后再浏览器中输入[http://localhost:7000/tree](http://localhost:7000/tree)  </span><br><span class="line">    或者不用在putty中添加端口映射，直接在浏览器中输入http://gpu28:7000，这里的7000指的是服务器的端口。 </span><br><span class="line">    （5）如果想要jupyter notebook在后台运行，按Ctrl+P+Q，会退回到服务器</span><br><span class="line">    （6）在网页中查看tensorboard。在上面那个容器中，因为使用了jupyter notebook，不能运行命令。所以需要使用上面那个镜像再开一个容器。tensorboard的端口默认是：6006，使用下面的命令</span><br><span class="line">    ```docker run -it -p 6688:6006 -v $PWD:/root --runtime=nvidia --rm -u 1042 --name wbbtensorboard ufoym/deepo:all-jupyter-py36 bash</span><br></pre></td></tr></table></figure><p> 进入到容器之后，然后使用下面的命令启动tensorboard<br> <code>tensorboard --logdir=.</code><br> 然后在浏览器上<a href="http://gpu28:6688，就可以看到在网页上看到" target="_blank" rel="noopener">http://gpu28:6688，就可以看到在网页上看到</a><br> （6）使用docker ps会看到你的容器正在后台运行<br> （7）使用docker attach 容器的名字/id再次进入到容器中，按Ctrl+C会看到当前正在有jupyter notebook运行<br> （8）发现这个容器中有一个库没有安装，可以在容器中使用pip install在这个容器中安装使用，但是这个容器删除了之后<br> 里面安装的东西也没有了。<br> 有2中方法：一是使用commit从容器生成镜像，但是这种方式不推荐，二是写Dockerfile重新build一个镜像<br> （9）在容器中使用exit退出该容器，再次使用docker ps该容器也消失了</p></li><li>docker commit制作镜像（强烈不建议操作）<br>启动一个容器—》在容器中安装环境—》退出容器—》docker commit制作镜像</li></ol><p>Docker的cuda和服务器的cuda是不冲突的。</p><h1><span id="docker深度学习实践">Docker深度学习实践</span></h1><p>Nvidia-Docker让容器可以使用GPU，使用nvidia docker run来运行程序就可以使用GPU。或者—runtime nvidia</p><h2><span id="端口映射">端口映射</span></h2><ul><li><p>在服务器上开一个jupyter notebook，把服务器的8888映射到本地8888，这样在本地访问8888就可以了。<br>把容器的8888映射到服务器，然后服务器映射到本地，这样在本地8888就可以打开容器中的jupyter。<br>在容器任何非正常关闭的时候，docker会自启，容器也会自己启动。<br>-p 9988:8888，把容器的8888映射到服务器的9988。  </p></li><li><p>使用命令<code>id</code>来查看自己的UID。<br>只要UID相同，权限一定是一样的。不能在镜像中创建user<br>在容器中指定user。在启动容器的时候一定要指定-u<br>-u username或者uid：指定使用某用户启动容器。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: Docker&lt;br&gt;date: 2019-06-12T18:48:24.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;Docker、镜像&quot;&gt;&lt;a href=&quot;#Docker、镜像&quot; class=&quot;headerlink&quot; title=&quot;  - Docker、镜像&quot;&gt;&lt;/a&gt;  - Docker、镜像&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖到一个可移植的镜像中，然后发布到Linux或Window系统中，也可以实现虚拟化。容器是完全使用沙箱机制，相互之前不会有任何接口。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/05/10/CentOs%E7%B3%BB%E7%BB%9Fmatplotlib%E7%94%BB%E5%9B%BE%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"/>
    <id>http://yoursite.com/2019/05/10/CentOs系统matplotlib画图中文乱码/</id>
    <published>2019-05-10T09:15:52.725Z</published>
    <updated>2019-06-26T07:00:06.793Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: CentOs系统matplotlib画图中文乱码<br>date: 2019-05-10T17:15:52.000Z<br>tag:</p><h2><span id="-centos-matplotlib-中文乱码">  - Centos、matplotlib、中文乱码</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;在集群上使用Python中的matplotlib库画图出现中文乱码，记录一下解决方案。   </p><a id="more"></a> <!-- TOC --><ul><li><a href="#1-简介">1. 简介</a></li><li><a href="#2-解决方案">2. 解决方案</a><ul><li><a href="#21-步骤一">2.1. 步骤一</a></li><li><a href="#22-步骤二">2.2. 步骤二</a></li><li><a href="#23-步骤三">2.3. 步骤三</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-解决方案">2. 解决方案</span></h1><h2><span id="21-步骤一">2.1. 步骤一</span></h2><p>&ensp;&ensp;&ensp;&ensp;获取matplotlibrc文件所在的路径，使用jupyter notebook写代码获取路径。我的文件路径在<br>/data/WangBeibei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.matplotlib_fname()</span><br><span class="line">```      </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">       </span><br><span class="line"><span class="comment">## 2.2. 步骤二    </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;</span><br><span class="line">- 到 anaconda 的 matplotlib 中查看是否有 simhei.ttf 字体   </span><br><span class="line">```  </span><br><span class="line">cd /data/WangBeibei/anaconda3/lib/python3<span class="number">.6</span>/site-packages/matplotlib/mpl-data/fonts/ttf    </span><br><span class="line">ls -al | grep simhei </span><br><span class="line">```    </span><br><span class="line">- 如果没有输出任何内容，说明没有simhei字体，下载simhei.ttf文件，并上传到/data/WangBeibei/anaconda3/lib/python3<span class="number">.6</span>/site-packages/matplotlib/mpl-data/fonts/ttf目录下。    </span><br><span class="line">- 修改/data/WangBeibei/anaconda3/lib/python3<span class="number">.6</span>/site-packages/matplotlib/mpl-data/matplotlibrc文件，找到以下<span class="number">3</span>行，改为：  </span><br><span class="line">``` </span><br><span class="line">font.family: sans-serif   </span><br><span class="line">font.sans-serif: simhei,DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif   </span><br><span class="line">axes.unicode_minus: <span class="keyword">False</span><span class="comment">#解决负号'-'显示为方块的问题   </span></span><br><span class="line">```   </span><br><span class="line">- 删除/data/WangBeibei/.cache/matplotlib</span><br></pre></td></tr></table></figure></p><p>rm -r /data/WangBeibei/.cache/matplotlib</p><pre><code>## 2.3. 步骤三   经过以上步骤，再次运行jupyter notebook程序，中文就不会出现乱码。如果还是出现乱码，添加以下两行代码  ```python    import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams[&#39;font.sans-serif&#39;] = [&#39;simhei&#39;]  # 用来正常显示中文标签plt.rcParams[&#39;axes.unicode_minus&#39;] = False  # 用来正常显示负号#显示所有列</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: CentOs系统matplotlib画图中文乱码&lt;br&gt;date: 2019-05-10T17:15:52.000Z&lt;br&gt;tag:&lt;/p&gt;
&lt;h2 id=&quot;Centos、matplotlib、中文乱码&quot;&gt;&lt;a href=&quot;#Centos、matplotlib、中文乱码&quot; class=&quot;headerlink&quot; title=&quot;  - Centos、matplotlib、中文乱码&quot;&gt;&lt;/a&gt;  - Centos、matplotlib、中文乱码&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在集群上使用Python中的matplotlib库画图出现中文乱码，记录一下解决方案。   &lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>gluon环境安装</title>
    <link href="http://yoursite.com/2019/04/27/gluon%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2019/04/27/gluon环境安装/</id>
    <published>2019-04-27T11:59:35.000Z</published>
    <updated>2019-09-12T10:43:34.462Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器上安装gluon环境。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83">2. 虚拟环境</a></li><li><a href="#3-%e4%b8%80%e4%ba%9b%e5%b8%b8%e7%94%a8%e7%9a%84%e5%91%bd%e4%bb%a4">3. 一些常用的命令</a></li><li><a href="#4-openssh">4. openSSH</a></li><li><a href="#5-%e9%97%ae%e9%a2%98">5. 问题</a></li></ul><!-- /TOC --><h1><span id="2-虚拟环境">2. 虚拟环境</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器上可以安装不同的虚拟环境，这些虚拟环境之间互不影响，不同的虚拟环境可以安装不同的python版本，不同的框架。 </p><ol><li>安装Anaconda<br><code>wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh</code><br>下载Anaconda3-5.2.0-Linux-x86_64.sh上传到服务器中，</li><li>运行安装向导<br><code>bash Anaconda3-5.2.0-Linux-x86_64.sh</code><br>为了激活安装， 你应该源~/.bashrc文件：<br><code>source ~/.bashrc</code></li><li>确认安装成功<br><code>conda --version</code><br>然后使用which python查看你当前使用的是哪个python<br>如果输出的目录是data/anaconda/python说明你当前使用的还是服务器自带的python，需要重新练连接一下服务器。<br>如果输出是data/WangBeibei/anaconda/python，说明当前使用是自己安装的anaconda</li><li>配置清华镜像<br>使用conda创建虚拟（运行）环境。conda和pip默认使用国外站点来下载软件，我们可以配置国内镜像来加速下载（国外用户无须此操作）。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># 配置清华conda镜像</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line"># 配置清华PyPI镜像（如无法运行，将pip版本升级到&gt;=10.0.0）</span><br><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">```     </span><br><span class="line">5. `conda env create -f environment.yml`   </span><br><span class="line">创建虚拟环境gluon，安装mxnet框架和一些依赖包。</span><br><span class="line">6.   `conda info -e`   </span><br><span class="line">查看当前服务器上都有哪些虚拟环境,下面截图中显示，当前存在2个虚拟环境，其中带*的是当前正在使用的虚拟环境。  </span><br><span class="line">![](gluon环境安装/conda-info.png)</span><br><span class="line"></span><br><span class="line">7. `screen -S WBB`（超级有用！！！）  </span><br><span class="line">因为是外网服务器，所以网络连接经常断开，连接一断开，运行在上面的程序就不能运行了，所以创建虚拟窗口，在这个虚拟窗口内运行程序，就算网络断开了，程序依然会继续运行</span><br><span class="line"></span><br><span class="line">8. `source activate gluon`   </span><br><span class="line">激活虚拟环境gluon，若要在gluon这个虚拟环境下安装一些库，需要先切换到这个环境下，然后使用conda install xxx或者pip install xxx，优先选择使用conda install xxx。安装完之后可以通过conda list查看当前已经安装的包。</span><br><span class="line"></span><br><span class="line">9. 使用jupyter notebook  </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;第一次使用jupyter notebook，需要映射端口号，默认jupyter notebook的端口号是8888，但是在这个集群上，如果别人已经把8888端口占用了，集群会自动给你分配一个端口号，然后在putty中映射一下这个端口，具体操作如下：</span><br><span class="line">在菜单栏选中change setting，找到Tunnels   </span><br><span class="line">![](gluon环境安装/putty.png)   </span><br><span class="line">![](gluon环境安装/port.png) </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;使用screen创建一个窗口运行jupyter notebook程序的好处，就算ssh和28号服务器的连接断开，jupyter notebook的程序依然可以在后台运行。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;如果在jupyter notebook的程序运行完了，有3中关闭程序的方法：  </span><br><span class="line">1、在jupyter notebook菜单栏，有一个close and hot的按钮  </span><br><span class="line">2、在jupyter notebook上面的菜单中有一个running，shutdown掉程序  </span><br><span class="line">3、screen -r 23560切入到虚拟窗口，然后再这个窗口ctrl+c关闭jupyter notebook进程。不用使用exit，因为使用exit是关闭虚拟窗口，直接按shift+a+d从虚拟窗口中切出，这样这个窗口还是存在的，下一次直接使用screen -ls查看存在的窗口，然后  </span><br><span class="line">source /etc/profile   </span><br><span class="line">source gluon   </span><br><span class="line">jupyter notebook  </span><br><span class="line">再次打开jupyter notebook程序   </span><br><span class="line"></span><br><span class="line">1.  `ctrl+A+D`退出虚拟窗口   </span><br><span class="line">当把程序运行之后，使用以上按钮退出虚拟窗口，这样程序就在后台运行，就算把电脑换机了，程序还是会运行。   </span><br><span class="line">11. `screen -r WBB`   </span><br><span class="line">使用以上命令进入到虚拟窗口中，查看程序运行的结果</span><br><span class="line">12. 下载Tensorflow   </span><br><span class="line">首先到清华的网站下[https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)直接粘贴下面3行命令，</span><br></pre></td></tr></table></figure></li></ol><p>conda config —add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>conda config —add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a><br>conda config —set show_channel_urls yes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">然后使用vi ~/.condarc将里面的-default删除，这样下载TensorFlow就不会使用国外的网站，而是使用清华的镜像</span><br><span class="line">然后使用`conda install tensorflow-gpu`下载TensorFlow框架</span><br><span class="line"></span><br><span class="line">运行TensorFlow程序，指定使用哪一块GPU，如果不指定</span><br><span class="line">使用全部GPU</span><br><span class="line">`CUDA_VISIBLE_DEVICES=5  python train.py`  </span><br><span class="line">直接杀死一个进程  </span><br><span class="line">`kill -9 [PID]`</span><br><span class="line"># 3. 一些常用的命令   </span><br><span class="line">（1）使用清华源下载  </span><br><span class="line">`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pykafka`    </span><br><span class="line">（2）和conda有关  </span><br><span class="line">`conda --version`  </span><br><span class="line">通过使用如下update命令来升级conda：  </span><br><span class="line">`conda update conda`  </span><br><span class="line">```   </span><br><span class="line">pip install 库名</span><br><span class="line">pip install 库名 --upgrade</span><br><span class="line"># 或者</span><br><span class="line">conda install 库名</span><br><span class="line">conda update 库名</span><br><span class="line"></span><br><span class="line"># 更新所有库</span><br><span class="line">conda update --all</span><br><span class="line"></span><br><span class="line"># 更新 conda 自身</span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line"># 更新 anaconda 自身</span><br><span class="line">conda update anaconda</span><br><span class="line"></span><br><span class="line"># 查看已安装的包</span><br><span class="line">conda list</span><br></pre></td></tr></table></figure></p><p>（3）使用conda install xxx或者pip install xxx，优先选择使用conda install xxx。安装完之后可以通过conda list查看当前已经安装的包<br>（4）删除一个虚拟环境<br><code>conda remove -n gluon--all</code><br>为了确定这个名为flowers的环境已经被移除，输入以下命令<code>conda info -e</code> ,会看到已经没有gluon这个环境<br>（5）查看GPU的占用情况<br><code>nvidia-smi</code><br>（6）<code>screen -S jupyter</code><br><code>source activate gluon</code><br><code>source deactivate</code><br><code>jupyter notebook</code><br><code>screen -r jupyter</code><br><code>screen -X -S 122128 quit</code><br>（7）查看当前的进程是谁的<br><code>ps -ef | grep 35230</code><br>（8）在windows在激活虚拟环境<br><code>activate gluon</code><br><code>deactivate gluon</code>    </p><h1><span id="4-openssh">4. openSSH</span></h1><p>Windows10自带了openssh工具，打开powershell使用ssh username@ip就可以连接服务器了</p><h1><span id="5-问题">5. 问题</span></h1><p>我创建了一个gluon的虚拟环境，使用<code>source avtivate gluon</code>激活这个环境时，在这里面装了mxnet框架。但是在screen中启动jupyter notebook时，import mxnet时却报错说no module names mxnet，然后我退出jupyter notebook（仍在screen中），使用which python查看当前使用的python，仍然是base的python，不是env/gluon中的python，因为base中的python没有装mxnet，所以import会出错。那怎么把gluon虚拟环境中的python换成env/gluon中的python呢？<br>参考这个网址：<a href="http://www.pianshen.com/article/2276285026/" target="_blank" rel="noopener">http://www.pianshen.com/article/2276285026/</a><br>在虚拟环境下运行以下命令：<br><code>ipython kernelspec list</code><br>查看jupyter notebook内核指定的python运行环境位置，然后cd到这个目录中，会看到有一个kernel.json文件，使用vi命令编辑这个文件，将python解释器的位置换成<code>/data/WangBeibei/anaconda3/envs/gluon/bin/python</code><br><img src="/2019/04/27/gluon环境安装/python.png" alt=""><br>然后使用<code>source deavtivate</code>断开gluon虚拟环境，在重新激活<code>source avtivate gluon</code>，然后再启动<code>jupyter noteboook</code>就可以了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在服务器上安装gluon环境。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon、jupyter notebook" scheme="http://yoursite.com/tags/gluon%E3%80%81jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/04/24/机器学习/</id>
    <published>2019-04-24T15:52:10.466Z</published>
    <updated>2019-06-26T14:12:34.617Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 机器学习<br>date: 2019-04-24T23:52:10.000Z<br>tags:</p><h2><span id="-特征预处理-模型评估-分类">  - 特征预处理、模型评估、分类</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;最近在做一个分类任务，根据电池的充放电数据，预测电池绝缘报警是否为虚报，就是一个二分类任务。这里使用逻辑回归进行分类。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">2. 数据预处理</a><ul><li><a href="#21-%E7%89%B9%E5%BE%81%E5%80%BC%E8%BF%9E%E7%BB%AD">2.1. 特征值连续</a><ul><li><a href="#211-%E5%BD%92%E4%B8%80%E5%8C%96normalization">2.1.1. 归一化(normalization)</a></li><li><a href="#212-%E6%A0%87%E5%87%86%E5%8C%96standardization">2.1.2. 标准化(standardization)</a></li><li><a href="#213-RobustScaler">2.1.3. RobustScaler</a></li></ul></li><li><a href="#22-%E7%89%B9%E5%BE%81%E5%80%BC%E7%A6%BB%E6%95%A3">2.2. 特征值离散</a></li><li><a href="#23-%E9%A2%84%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4">2.3. 预处理步骤</a></li><li><a href="#24-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81">2.4. 交叉验证</a></li><li><a href="#25-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7">2.5. 模型评价</a></li></ul></li><li><a href="#3-XGBoost">3. XGBoost</a><ul><li><a href="#31-XGBoost%E7%9A%84%E4%BC%98%E5%8A%BF">3.1. XGBoost的优势</a></li><li><a href="#32-%E5%8F%82%E6%95%B0">3.2. 参数</a></li><li><a href="#33-%E8%B0%83%E5%8F%82">3.3. 调参</a></li></ul></li><li><a href="#4-GridSearchCV">4. GridSearchCV</a></li><li><a href="#5-%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98">5. 样本不均衡分类问题</a><ul><li><a href="#51-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B">5.1. 特征工程</a></li></ul></li><li><a href="#6-%E8%BF%87%E9%87%87%E6%A0%B7">6. 过采样</a></li></ul><!-- /TOC --><h1><span id="2-数据预处理">2. 数据预处理</span></h1><p>&ensp;&ensp;&ensp;&ensp;在进行模型训练之前，需要对数据进行预处理。因为多个特征之间的量纲不同，在训练的时候收敛会很慢，所以需要将不同特征值转换为同一量纲。这里将离散特征和连续特征分别处理。   </p><h2><span id="21-特征值连续">2.1. 特征值连续</span></h2><p>&ensp;&ensp;&ensp;&ensp;对于连续值的预处理主要分为2个：归一化和标准化。这2个操作主要是为了使得不同的特征在同一个量纲，对目标的影响是同级的。归一化和标准化都是先对数据先缩小一定的比例，然后再平移。这2者本质上都是对数据进行线性变换，线性变换不会改变原始数据的数值大小排序。即一个数在原始数据最大，经过归一化和标准化这个数还是最大。<a href="https://blog.csdn.net/dujiahei/article/details/86061924" target="_blank" rel="noopener">这篇博客</a>。<br>&ensp;&ensp;&ensp;&ensp;将特征值缩放到相同的区间可以获得性能更好的模型。就梯度下降而言，一个特征值的范围在1-10之间，另一个特征值范围在1-10000之间，训练的目标是最小化平方误差，所以在使用梯度下降算法的过程中，算法会明显偏向第二个特征，因为它的取值范围更大。在K近邻算法中，使用的欧式距离，也会导致偏向第二个特征。<strong>对于决策树和随机森林以及xgboost算法而言，特征缩放对它们没有什么影响，像逻辑回归和支持向量机算法和K近邻，需要对数据进行特征缩放</strong>。  <strong>在分类，聚类算法中，需要使用距离来度量相似性的时候，standardization表现更好</strong>。<br><img src="/2019/04/24/机器学习/表格.png" alt="归一化">   </p><h3><span id="211-归一化normalization">2.1.1. 归一化(normalization)</span></h3><p>&ensp;&ensp;&ensp;&ensp;归一化将每一个属性值映射到[0,1]之间。需要计算训练集的最大值和最小值，当有新样本加入时，需要重新计算最值。 </p><ul><li>特点：多使用于分布有明显边界的情况，如考试成绩，身高，颜色的分布等，都有明显的范围边界，不适用没有范围约定，或者返回非常大的数据。</li><li>缺点：受异常值影响较大。归一化的缩放就是将数据拍扁统一到一个区间中，仅有极值决定，而标准化的缩放更加弹性和动态，和整体的分布有关。归一化只用到了最大值和最小值，而标准化和每一个值有关。   </li></ul><p><img src="/2019/04/24/机器学习/归一化.png" alt="归一化">    </p><h3><span id="212-标准化standardization">2.1.2. 标准化(standardization)</span></h3><p>&ensp;&ensp;&ensp;&ensp; 标准化又叫做Z-score。将所有的数据映射到均值为0，方差为1的正态分布中。  要求原始数据的分布可以近似为正态分布，否则标准化的结果会很差。 标准化表示的是原始值与均值之间差几个标准差，是一个相对值，也有去除量纲的作用，同时还有2个附加好处：均值为0，标准差为1。均值为0的好处是使得数据以0为中心左右分布。</p><ul><li>适用范围：在分类和聚类算法中，需要使用距离来度量相似性时，例如支持向量机，逻辑回归，或者使用PCA进行降维时，Z-score表现更好。 </li><li>推荐先使用标准化。<br><img src="/2019/04/24/机器学习/标准化.png" alt="归一化">         <h3><span id="213-robustscaler">2.1.3. RobustScaler</span></h3>&ensp;&ensp;&ensp;&ensp;在某些情况下，加入数据中有离群点，可以使用standardization进行标准化，但是标准化后的数据并不理想，因为异常点的特征往往在标准化后容易失去离群特征，此时就要使用RobustScaler针对离群点进行标准化处理   。此方法对数据中心化和缩放健壮性有更强的参数控制能力。     <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#RobustScaler标准化</span></span><br><span class="line">robustscaler = preprocessing.RobustScaler()</span><br><span class="line">df_r = robustscaler.fit_transform(df)</span><br><span class="line">df_r = pd.DataFrame(df_r,columns=[<span class="string">'value1_r'</span>,<span class="string">'value2_r'</span>])</span><br><span class="line">df_r.head()</span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line"><span class="comment">## 2.2. 特征值离散   </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;离散值就是特征值是离散的，不是连续的，例如性别是离散值，只有female和male，颜色是离散的。机器学习算法不能直接处理离散值，需要对其进行一些转换。离散值可以是文本(red，black)或者数值（<span class="number">1</span>，<span class="number">2</span>）。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp; 离散数据有<span class="number">2</span>大类：定序(Ordinal)和定类(Nominal)。定序的数据存在一定的顺序意义，例如衣服的尺寸按大小分类(xs,s,m,l),在定类的数据中，属性值之间没有顺序的要求。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;对于定序的数据，没有统一的模块将这些顺序自动转换成映射，可以自定义一些映射规则，比如xs对应<span class="number">1</span>，s对应<span class="number">2</span>，自定义的规则。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;对于文本的定类数据，可以先把文本分类至转换为数字，比如red转换为<span class="number">1</span>，black转换为<span class="number">2</span>，然后对这些数据使用one-hot编码。   </span><br><span class="line">主要是使用LabelEncoder和OneHotEncoder这<span class="number">2</span>个模块。</span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">gle = LabelEncoder()</span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"><span class="comment">#将每个风格属性映射到一个数值(0,1,2,3…)。</span></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment"># OneHotEncoder的transform方法默认返回系数矩阵，调用toarray()方法将系数矩阵转为一般矩阵</span></span><br><span class="line">dis_feature_data = enc.fit_transform(dis_feature_data).toarray()</span><br><span class="line">print(dis_feature_data)</span><br><span class="line">print(dis_feature_data.shape)</span><br><span class="line">```    </span><br><span class="line">除了sklearn中的OneHotEncoder，还可以使用pandas中的get_dummies对离散值进行one-hot编码，比OneHotEncoder好的一点是:转换之后可以直观的看出当前列对应哪个属性。</span><br><span class="line">参考博客：    </span><br><span class="line">[https://blog.csdn.net/wotui1842/article/details/<span class="number">80697444</span>](https://blog.csdn.net/wotui1842/article/details/<span class="number">80697444</span>)      </span><br><span class="line">[https://blog.csdn.net/cymy001/article/details/<span class="number">79154135</span>](https://blog.csdn.net/cymy001/article/details/<span class="number">79154135</span>)   </span><br><span class="line">[https://blog.csdn.net/m0_37324740/article/details/<span class="number">77169771</span>](https://blog.csdn.net/m0_37324740/article/details/<span class="number">77169771</span>)    </span><br><span class="line">[https://blog.csdn.net/wxyangid/article/details/<span class="number">80209156</span>](https://blog.csdn.net/wxyangid/article/details/<span class="number">80209156</span>)   </span><br><span class="line"><span class="comment">## 2.3. 预处理步骤   </span></span><br><span class="line">- 首先使用pandas从csv中读取数据，从数据中取出特征值和目标值，分别存储在X和Y中。</span><br><span class="line">- 从X中取出离散特征值dis_feature，剩下的是连续特征值con_feature。</span><br><span class="line">- 对离散特征值dis_feature进行one-hot编码，形成新的特征值new_dis_feature。然后将新的特征值new_dis_feature和原先的连续特征值con_feature进行拼接形成新的特征值new_X</span><br><span class="line">- 然后对new_X和Y划分为训练集和测试集，然后对训练集进行标准化，使用训练集的均值和标准差再对测试集进行标准化。</span><br><span class="line">- 使用训练集对模型进行训练，对测试集进行验证。       </span><br><span class="line"><span class="comment">## 2.4. 交叉验证   </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;sklearn中有<span class="number">2</span>中交叉验证方法，KFold，StratifiedKFold  </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold,StratifiedKFold</span><br><span class="line">X=np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],</span><br><span class="line">    [<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>],</span><br><span class="line">    [<span class="number">31</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>],</span><br><span class="line">    [<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>],</span><br><span class="line">    [<span class="number">51</span>,<span class="number">52</span>,<span class="number">53</span>,<span class="number">54</span>],</span><br><span class="line">    [<span class="number">61</span>,<span class="number">62</span>,<span class="number">63</span>,<span class="number">64</span>],</span><br><span class="line">    [<span class="number">71</span>,<span class="number">72</span>,<span class="number">73</span>,<span class="number">74</span>]</span><br><span class="line">])</span><br><span class="line"> </span><br><span class="line">y=np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">floder = KFold(n_splits=<span class="number">4</span>,random_state=<span class="number">0</span>,shuffle=<span class="keyword">False</span>)</span><br><span class="line">sfolder = StratifiedKFold(n_splits=<span class="number">4</span>,random_state=<span class="number">0</span>,shuffle=<span class="keyword">False</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sfolder.split(X,y):</span><br><span class="line">    print(<span class="string">'Train: %s | test: %s'</span> % (train, test))</span><br><span class="line">    print(<span class="string">" "</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> floder.split(X,y):</span><br><span class="line">    print(<span class="string">'Train: %s | test: %s'</span> % (train, test))</span><br><span class="line">    print(<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">```   </span><br><span class="line">StratifiedKFold和KFold类似，但是StratifiedKFold是分层采样，确保训练集、测试集各类样本的比例与原始数据集中相同。比如原始数据集中正例:负例=<span class="number">2</span>:<span class="number">1</span>,则训练集和测试集中正例:负例=<span class="number">2</span>:<span class="number">1</span>。   </span><br><span class="line">KFold和enumerate联合使用   </span><br><span class="line">enumerate()函数用于将一个可遍历的数据对象(如列表，元组或str)组合成一个序列索引，同时列出数据和数据下标。一般在<span class="keyword">for</span>循环中使用。   </span><br><span class="line">语法：`enumerate(sequence,[start=<span class="number">0</span>])`   </span><br><span class="line">其中`sequence`表示一个序列，迭代器或可遍历对象，`start`表示下标起始位置   </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">for</span> fold_, (train_, test_) <span class="keyword">in</span> enumerate(kfold.split(X_array, y_array):</span><br><span class="line"><span class="comment">#其中train和test是数据的下标</span></span><br></pre></td></tr></table></figure></li></ul><h2><span id="25-模型评价">2.5. 模型评价</span></h2><ul><li>拟合模型<br>model.fit(X_train, y_train)    </li><li>模型预测，对于分类任务，输出最大可能的类别<br>model.predict(X_train)<br>model.predict(X_test)</li><li>对于分类任务，输出所属每个类别的概率，返回的是一个二维数组，每一行加起来为1<br>prob = model.predict_proba(X_train)<br>model.predict_proba(X_test)<br>获取样本属于正例的概率prob[:,1]   </li><li>获得这个模型的参数<br>model.get_params()    </li><li>为模型进行打分<br>线性回归问题返回预测的确定系数R2<br>逻辑回归（分类）根据给定数据与标签返回分类准确率的均值<br>model.score(X_train, y_train)<br>model.score(X_test, y_test)   </li><li>计算分类准确率,和score返回值一样<br>train_predicted = model.predict(X_train)<br>model.accuracy_score(y_train.flatten(),train_predicted)  </li><li>返回分类准确率，和上面的结果一样<br>np.mean(train_predicted == y_train)<br>np.mean(test_predicted == y_test)   </li><li>召回率<br>precision, recall, F1, _ = precision_recall_fscore_support(y_test, pred_test, average=”binary”)<br>print (“精准率: {0:.2f}. 召回率: {1:.2f}, F1分数: {2:.2f}”.format(precision, recall, F1))    </li><li>AUC&amp;&amp;ROC<br>只针对二分类。通过model.predict_proba(X_test)[:,1]可以获取测试集属于正例的概率，将预测概率从大到小排序，然后以每个预测概率作为阈值，即可得到属于2类的样本数。对应计算每个阈值下的”False Positive Rate”(FPR)和”True Positive Rate”(TPR)，以”False Positive Rate”为横轴，以”True Positive Rate”为纵轴，画出ROC曲线，ROC曲线下的面积就是AUC值。<br>“False Positive Rate”(FPR)=负例被划分为正例个数/真正负例个数（负例被分错的个数/真正负例）<br>“True Positive Rate”(TPR)=正例被划分为正例/真正正例个数（正例被分对的个数/真正正例）<br>当阈值取最大时，所有的样本被分为负样本，对应（0,0），当阈值取最小时，所有的样本被分为正样本，对应于（1,1），随着阈值从最大到最小变化，横坐标和纵坐标都在变大，表示被划分为正例的个数越来越多。<br>AUC用来衡量ROC曲线的好坏。如果分类器能完美的将样本分对，那么AUC=1，如果模型是随机猜测的，那么AUC=0.5，对应着y=x直线。分类器越好，则AUC越大。<br>sklearn给了画ROC曲线的函数。  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fpr, tpr, thresholds=sklearn.metrics.roc_curve(y_true_label,y_prob,pos_label=<span class="keyword">None</span>,sample_weight=<span class="keyword">None</span>,drop_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#其中test_true_label表示数据集真实的标签，&#123;0,1&#125;或&#123;-1,1&#125;</span></span><br><span class="line"><span class="comment">#y_prob表示数据集被分为正例的概率</span></span><br><span class="line"><span class="comment"># 返回值</span></span><br><span class="line"><span class="comment">#thresholds: array, shape = [n_thresholds]所选取的不同的阈值，按照从大到小的排序，阈值越大，横纵坐标越小。</span></span><br><span class="line"><span class="comment">#fpr,tpt：根据 thresholds算出来的横坐标和纵坐标。在此基础上可以画ROC曲线，</span></span><br><span class="line"><span class="comment">#通过auc(fpr,tpr)可以求出AUC的值</span></span><br></pre></td></tr></table></figure></li></ul><h1><span id="3-xgboost">3. XGBoost</span></h1><p>&ensp;&ensp;&ensp;&ensp;XGBoost是一种十分精致的算法，可以处理各种不规则的数据。<br>构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难，因为涉及到很多参数。所以为了提高模型的表现，参数的调整十分必要。</p><h2><span id="31-xgboost的优势">3.1. XGBoost的优势</span></h2><ul><li>正则化<br>正则化防止过拟合，实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。</li><li>缺失值处理<br>XGBoost内置处理缺失值的规则。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值的处理方法。<h2><span id="32-参数">3.2. 参数</span></h2></li></ul><p>&ensp;&ensp;&ensp;&ensp;XGBoost实际上是很多CART树堆叠起来。传入的特征可以含有None值。XGBoost有很多参数，使用GridSearchCV进行网格搜索时比较耗时。  </p><p>使用pip install xgboost安装<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">XGBClassifier(</span><br><span class="line">        base_score=<span class="number">0.5</span>, </span><br><span class="line">        booster=<span class="string">'gbtree'</span>, </span><br><span class="line">        colsample_bylevel=<span class="number">1</span>,</span><br><span class="line">        colsample_bytree=<span class="number">1</span>, </span><br><span class="line">        gamma=<span class="number">0</span>, </span><br><span class="line">        learning_rate=<span class="number">1</span>, </span><br><span class="line">        max_delta_step=<span class="number">0</span>,</span><br><span class="line">        max_depth=<span class="number">2</span>, </span><br><span class="line">        min_child_weight=<span class="number">1</span>, </span><br><span class="line">        missing=<span class="keyword">None</span>, </span><br><span class="line">        n_estimators=<span class="number">2</span>,</span><br><span class="line">        n_jobs=<span class="number">1</span>, </span><br><span class="line">        nthread=<span class="keyword">None</span>, objective=<span class="string">'binary:logistic'</span>, random_state=<span class="number">0</span>,</span><br><span class="line">        reg_alpha=<span class="number">0</span>, </span><br><span class="line">        reg_lambda=<span class="number">1</span>, </span><br><span class="line">        scale_pos_weight=<span class="number">1</span>, </span><br><span class="line">        seed=<span class="keyword">None</span>,</span><br><span class="line">        silent=<span class="keyword">True</span>, </span><br><span class="line">        subsample=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>XGBoost参数有3类：<br><a href="https://www.cnblogs.com/wanglei5205/p/8579244.html" target="_blank" rel="noopener">https://www.cnblogs.com/wanglei5205/p/8579244.html</a><br>（1）通用类别：不需要调整，默认就好：</p><ul><li>booster：[默认gbtree]<br>选择每次迭代的模型，有两种选择：<br>gbtree：基于树的模型<br>gbliner：线性模型</li><li>silent[默认0]<br>当这个参数值为1时，静默模式开启，不会输出任何信息。<br>一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</li><li>nthread[默认值为最大可能的线程数]<br>这个参数用来进行多线程控制，应当输入系统的核数。<br>如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。</li></ul><p>（2）学习目标参数：与任务有关</p><ul><li>objective:损失函数，支持分类/回归<br>[默认reg:linear]，这个参数定义需要被最小化的损失函数。最常用的值有：<br>binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。<br>multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。<br>在这种情况下，你还需要多设一个参数：num_class(类别数目)。<br>multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</li><li><p>eval_metric：评价函数，对于回归问题，默认值是rmse，对于分类问题，默认值是error。<br>典型值有：<br>rmse 均方根误差<br>logloss 负对数似然函数值<br>error 二分类错误率(阈值为0.5)<br>merror 多分类错误率<br>mlogloss 多分类logloss损失函数<br>auc 曲线下面积</p></li><li><p>seed：随机数的种子，默认为0<br>设置它可以复现随机数据的结果，也可以用于调整参数</p></li></ul><p>（3）booster参数：弱学习器参数，需要仔细调整，会影响模型性能<br>学习率和n_estimators具有相反的关系，建议学习率设小，通过交叉验证确定n_estimators</p><ul><li>eta[默认0.3]，和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。     </li></ul><p><strong>和树有关的参数</strong></p><ul><li>min_child_weight[默认1]，最小样本权重的和，用于避免过拟合。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</li><li>max_depth[默认6]，树的最大深度。 用来避免过拟合的。max_depth越大，模型越复杂，学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10      </li><li>gamma[默认0]，Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。   </li><li>subsample[默认1]<br>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。<br>典型值：0.5-1  </li><li>colsample_bytree[默认1]<br>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。<br>典型值：0.5-1  </li></ul><p><strong>和正则化有关的参数</strong></p><ul><li>lambda[默认1]<br>权重的L2正则化项。(和Ridge regression类似)。<br>这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。  </li><li>alpha[默认1]<br>权重的L1正则化项。(和Lasso regression类似)。<br>可以应用在很高维度的情况下，使得算法的速度更快。<br><strong>样本不均衡</strong> </li><li>scale_pos_weight[默认1]<br>正样本占的比重，为1时表示正负样例比重是一样的。当正样本较少时，正样本:负样本=1:9，将scale_pos_weight设置为9，scale_pos_weight=负样本个数/正样本个数。在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。   <h2><span id="33-调参">3.3. 调参</span></h2></li></ul><ol><li><p>先给定一个较高的学习率(learning rate)，一般情况下，学习率为0.1，但是对于不同的问题，理想的学习率在0.05~0.3之间波动。先调节决策树的数量n_estimators</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">20</span>,<span class="number">40</span>, <span class="number">60</span>, <span class="number">80</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">model = XGBClassifier(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">5</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'参数的最佳取值：&#123;0&#125;'</span>.format(optimized_GBM.best_params_))</span><br><span class="line">print(<span class="string">'最佳模型得分:&#123;0&#125;'</span>.format(optimized_GBM.best_score_))</span><br><span class="line">display(pd.DataFrame(optimized_GBM.cv_results_).T)</span><br></pre></td></tr></table></figure></li><li><p>在给定的learning rate和n_eatimators情况下，对决策树特定参数调优(max_depth,min_child_weight,gamma,subsample,colsample_bytree) </p></li><li><p>max_depth和min_child_weight参数调优</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'max_depth'</span>: list(range(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)), <span class="string">'min_child_weight'</span>: list(range(<span class="number">1</span>,<span class="number">7</span>,<span class="number">1</span>))&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">60</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">```   </span><br><span class="line"><span class="number">4.</span> gamma参数调优   </span><br><span class="line">Gamma参数取值范围可以很大，我这里把取值范围设置为<span class="number">5</span>了。你其实也可以取更精确的gamma值。  </span><br><span class="line">  ```python</span><br><span class="line">  cv_params = &#123;<span class="string">'gamma'</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">  other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">350</span>, <span class="string">'max_depth'</span>: <span class="number">3</span>, <span class="string">'min_child_weight'</span>: <span class="number">5</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">  ```   </span><br><span class="line"><span class="number">5.</span> subsamplehe colsample_bytree参数   </span><br><span class="line">   这链各个参数相当于每个树的样本和特征个数。</span><br><span class="line">  ```python</span><br><span class="line">   cv_params = &#123;  </span><br><span class="line">    <span class="string">'subsample'</span>: [i / <span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>, <span class="number">10</span>)],  </span><br><span class="line">    <span class="string">'colsample_bytree'</span>: [i / <span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>, <span class="number">10</span>)]  </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>正则化参数调优<br>下一步应用正则化来降低过拟合。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'reg_alpha'</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">'reg_lambda'</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>学习率调优<br>最后使用较低的学习率</p></li></ol><h1><span id="4-gridsearchcv">4. GridSearchCV</span></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’)</span><br></pre></td></tr></table></figure><p>GridSearchCV参数介绍：</p><ul><li>estimator：使用的分类器，并且传入除需要确定最佳的参数之外的其他参数</li><li>param_grid：值为字典或者列表，即需要最优化的参数的取值，param_grid = {‘n_estimators’:list(range(10,71,10))}</li><li>scoring :准确度评价标准，默认None,表示“GridSearchCV”与“cross_val_score”都会去调用“estimator”自己的“score”；或者如scoring=’roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。scoring参数选择如下：</li><li><p>cv :交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，传入的参数可以是int型，也可以是yield训练/测试数据的生成器。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kflod = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle = <span class="keyword">True</span>,random_state=<span class="number">7</span>)<span class="comment">#将训练/测试数据集划分10个互斥子集，</span></span><br><span class="line">grid_search = GridSearchCV(model,param_grid,scoring = <span class="string">'neg_log_loss'</span>,n_jobs = <span class="number">-1</span>,cv = kflod)</span><br></pre></td></tr></table></figure></li><li><p>refit :默认为True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。</p></li><li>iid:默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。</li><li>verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出</li><li>n_jobs: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。  </li></ul><p><strong>常用的方法</strong></p><ul><li>grid.fit(X, y=None, groups=None, **fit_params)：运行网格搜索，与所有参数组合运行。</li><li><p>cv_results_：旧版本是“grid_scores_”，cv_results_是详尽、升级版。内容较好理解，包含了’mean_test_score’(验证集平均得分)，’rank_test_score’(验证集得分排名)，’params’(dict形式存储所有待选params的组合)，甚至还有在每次划分的交叉验证中的得分（’split0_test_score’、 ‘split1_test_score’等），就是输出的内容稍显臃肿。内容以dict形式输出，我们可以转成DataFrame形式，看起来稍微养眼一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv_result = pd.DataFrame.from_dict(clf.cv_results_)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">means = grid_result.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">'params'</span>]</span><br><span class="line"><span class="keyword">for</span> mean,param <span class="keyword">in</span> zip(means,params):</span><br><span class="line">    print(<span class="string">"%f  with:   %r"</span> % (mean,param))</span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line">    display(pd.DataFrame(grid.cv_results_).T)</span><br><span class="line">    ```   </span><br><span class="line">    参考资料：[https://blog.csdn.net/sinat_32547403/article/details/<span class="number">73008127</span>](https://blog.csdn.net/sinat_32547403/article/details/<span class="number">73008127</span>)</span><br><span class="line">- best_estimator_ : estimator或dict；由搜索选择的估算器，即在左侧数据上给出最高分数（或者如果指定最小损失）的估算器。 如果refit = <span class="keyword">False</span>，则不可用。</span><br><span class="line">- best_params_ : dict；在保持数据上给出最佳结果的参数设置。对于多度量评估，只有在指定了重新指定的情况下才会出现。</span><br><span class="line">- best_score_ : float；best_estimator的平均交叉验证分数，对于多度量评估，只有在指定了重新指定的情况下才会出现。</span><br><span class="line">- get_params（[deep]）：这个和‘best_estimator_ ’这个属性相似，但可以得到这个模型更多的参数</span><br><span class="line">- inverse_transform（Xt）使用找到的最佳参数在分类器上调用inverse_transform。</span><br><span class="line">- predict（X）调用使用最佳找到的参数对估计量进行预测，X：可索引，长度为n_samples；</span><br><span class="line">- score（X, y=<span class="keyword">None</span>）返回给定数据上的分数，X： [n_samples，n_features]输入数据，其中n_samples是样本的数量，n_features是要素的数量。y： [n_samples]或[n_samples，n_output]，可选，相对于X进行分类或回归; 无无监督学习。</span><br><span class="line">  </span><br><span class="line">```python    </span><br><span class="line">cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">100</span>, <span class="number">125</span>, <span class="number">150</span>, <span class="number">175</span>, <span class="number">200</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = XGBClassifier(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">5</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">evalute_result = optimized_GBM.cv_results_</span><br><span class="line"></span><br><span class="line"><span class="comment"># print('每轮迭代运行结果:&#123;0&#125;'.format(evalute_result))</span></span><br><span class="line">print(<span class="string">'参数的最佳取值：&#123;0&#125;'</span>.format(optimized_GBM.best_params_))</span><br><span class="line">print(<span class="string">'最佳模型得分:&#123;0&#125;'</span>.format(optimized_GBM.best_score_))</span><br><span class="line">```      </span><br><span class="line">网格搜索建立在交叉验证的基础上。交叉验证将训练集分成N份，其中N<span class="number">-1</span>份做训练，<span class="number">1</span>份做测试。先选定一个待验证的参数，然后做N次训练和测试，得到平均值，然后再选定下一个参数，做N次训练和测试。     </span><br><span class="line"><span class="comment"># 5. 样本不均衡分类问题      </span></span><br><span class="line">参考资料：[https://github.com/wmlba/innovate2019/blob/master/Credit_Card_Fraud_Detection.ipynb](https://github.com/wmlba/innovate2019/blob/master/Credit_Card_Fraud_Detection.ipynb)   </span><br><span class="line"><span class="comment">## 5.1. 特征工程   </span></span><br><span class="line"><span class="number">1.</span> 特征缩放   </span><br><span class="line">使用归一化或标准化对特征进行缩放，使得不同特征值在同一量纲，   </span><br><span class="line"></span><br><span class="line">```python  </span><br><span class="line"><span class="comment">#使用 sklearn中的 scale 函数</span></span><br><span class="line">minmax_scaler = preprocessing.MinMaxScaler()   <span class="comment">#创建 MinMaxScaler对象</span></span><br><span class="line">df_m1 = minmax_scaler.fit_transform(df)    <span class="comment"># 标准化处理</span></span><br><span class="line">df_m1 = pd.DataFrame(df_m1,columns=[<span class="string">'value1_m'</span>,<span class="string">'value2_m'</span>])</span><br><span class="line">df_m1.head()</span><br><span class="line">```   </span><br><span class="line">```python</span><br><span class="line"><span class="comment">#Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#RobustScaler is robust to outliers.</span></span><br><span class="line">credit_df[<span class="string">'amount_after_scaling'</span>] = RobustScaler().fit_transform(credit_df[<span class="string">'Amount'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">credit_df[<span class="string">'time_after_scaling'</span>] = RobustScaler().fit_transform(credit_df[<span class="string">'Time'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">credit_df.drop([<span class="string">'Time'</span>,<span class="string">'Amount'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Place the class in the begining of the dataframe</span></span><br><span class="line">Class = credit_df[<span class="string">'Class'</span>]</span><br><span class="line">credit_df.drop([<span class="string">'Class'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">credit_df.insert(<span class="number">0</span>, <span class="string">'Class'</span>, Class)</span><br></pre></td></tr></table></figure></li></ul><ol><li>解决样本不均衡问题<br>欠采样或过采样<br><img src="/2019/04/24/机器学习/resample.png" alt="样本不平衡">    </li><li><p>检测和删除异常点   </p></li><li><p>划分数据集<br>划分数据集：训练集，验证集，测试集   </p></li></ol><h1><span id="6-过采样">6. 过采样</span></h1><p>&ensp;&ensp;&ensp;&ensp;分类问题时，样本不均衡，正例和负例的样本数不均衡，为了实现样本均衡，需要对样本比较少的那类数据进行过采样。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 机器学习&lt;br&gt;date: 2019-04-24T23:52:10.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;特征预处理、模型评估、分类&quot;&gt;&lt;a href=&quot;#特征预处理、模型评估、分类&quot; class=&quot;headerlink&quot; title=&quot;  - 特征预处理、模型评估、分类&quot;&gt;&lt;/a&gt;  - 特征预处理、模型评估、分类&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;最近在做一个分类任务，根据电池的充放电数据，预测电池绝缘报警是否为虚报，就是一个二分类任务。这里使用逻辑回归进行分类。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>pyspark</title>
    <link href="http://yoursite.com/2019/04/20/pyspark/"/>
    <id>http://yoursite.com/2019/04/20/pyspark/</id>
    <published>2019-04-20T04:40:56.000Z</published>
    <updated>2019-05-21T12:14:49.501Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;使用Python编写Spark程序<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD">2. 重要概念和术语</a></li><li><a href="#3-%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%BC%8F">3. 执行模式</a><ul><li><a href="#31-standalone%E6%A8%A1%E5%BC%8F">3.1. standalone模式</a></li><li><a href="#32-yarn%E6%A8%A1%E5%BC%8F">3.2. Yarn模式</a></li><li><a href="#33-%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98">3.3. 参数调优</a></li><li><a href="#executor">Executor</a></li></ul></li><li><a href="#4-%E5%88%9B%E5%BB%BAsc">4. 创建sc</a></li><li><a href="#5-rdd%E8%BD%AC%E6%8D%A2">5. RDD转换</a><ul><li><a href="#51-%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C">5.1. 转换操作</a></li><li><a href="#52-%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C">5.2. 行动操作</a></li><li><a href="#53-%E6%8C%81%E4%B9%85%E5%8C%96">5.3. 持久化</a></li></ul></li><li><a href="#6-%E5%88%86%E5%8C%BA">6. 分区</a></li><li><a href="#7-%E5%88%9B%E5%BB%BArdd">7. 创建RDD</a><ul><li><a href="#71-%E9%80%9A%E8%BF%87paralize%E5%88%9B%E5%BB%BArdd">7.1. 通过paralize创建RDD</a></li><li><a href="#72-%E8%AF%BB%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BArdd">7.2. 读文本文件创建RDD</a></li></ul></li><li><a href="#8-map%E5%92%8Cflatmap">8. map和flatMap</a></li><li><a href="#9-flatmap%E5%92%8Cflatmapvalues">9. flatMap和flatMapValues</a></li><li><a href="#10-reducebykey%E5%92%8Cgroupbykey">10. reduceByKey和groupByKey</a></li><li><a href="#11-sortby%E5%92%8Csortbykey">11. sortBy和SortByKey</a></li><li><a href="#12-%E5%B0%86spark%E8%AE%A1%E7%AE%97%E7%9A%84%E7%BB%93%E6%9E%9C%E5%AD%98%E5%82%A8%E5%9C%A8%E6%96%87%E4%BB%B6%E4%B8%AD">12. 将Spark计算的结果存储在文件中</a><ul><li><a href="#121-%E5%86%99%E5%85%A5%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E4%B8%AD">12.1. 写入到服务器本地文件中</a></li><li><a href="#122-%E5%86%99%E5%85%A5%E5%88%B0hdfs%E6%96%87%E4%BB%B6%E4%B8%AD">12.2. 写入到HDFS文件中</a></li><li><a href="#123-%E6%89%93%E5%8D%B0rdd%E5%85%83%E7%B4%A0">12.3. 打印RDD元素</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-重要概念和术语">2. 重要概念和术语</span></h1><ul><li>Master和Worker是物理节点，Driver和Executor是进程。<br>搭建Spark集群的时候我们就已经设置好了Mater节点和Worker节点。一个集群有多个Master节点和多个Worker节点。<br>Master节点常驻Mater守护进程，负责管理worker节点，我们从master节点提交应用。<br>Worker节点常驻Worker守护进程，与Master节点通信，并且管理Executor进程。<br>PS：一台机器可以同时作为master和worker节点（举个例子：你有四台机器，你可以选择一台设置为master节点，然后剩下三台设为worker节点，也可以把四台都设为worker节点，这种情况下，有一个机器既是master节点又是worker节点）</li><li><p>Driver / Driver Program<br>运行main函数并且创建SparkContext的程序。客户端的应用程序，Driver Program类似于wordcount程序中的mian函数。<br>当我们提交应用程序后，便会启动一个对应的Driver进程。Driver会根据我们设置的参数占用一定的资源（主要是CPU核数、内存）。<br>程序启动时，Driver进程首先会向集群资源管理者（Standalone，Mesos，Yarn）申请Spark应用所需的资源，也就是Executor，然后集群管理者会根据Spark应用所设置的参数在各个Worker上分配一定数量的Executor，每个Executor都占用一定数量的CPU和Memory。在申请到应用所需的资源后，Driver就开始调度和执行我们的程序了。Driver进程会把我们编写的Spark程序拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor中执行。<br>Executor进程在Worker节点上，一个Worker可以有多个Executor，每个Executor都有一个进程池，每个进程执行一个task。Executor执行完task之后将结果返回给Driver，每个Executor执行的task属于一个spark程序。此外Executor还有一个功能是为应用程序中的RDD提供内存，RDD是直接缓存在Executor进程内的。<br><a href="https://blog.csdn.net/hongmofang10/article/details/84587262" target="_blank" rel="noopener">这篇博客讲的很好</a><br><a href="https://blog.csdn.net/qq_21383435/article/details/78653427" target="_blank" rel="noopener">通俗易懂</a> </p><pre><code>spark-submit --master yarn --num-executors 32 --executor-memory 8G --executor-cores 8 --jars ../jars/spark-examples_2.10_my_converters_test-1.6.0.jar spark_streaming_all.py</code></pre><p>其中参数的含义：  </p><ul><li>num-executors：创建多少个 executor</li><li>executor-memory：各个 executor 使用的最大内存，不可超过单机的最大可使用内存</li><li>executor-cores：各个 executor 使用的并发线程数目，也即每个 executor 最大可并发执行的 Task 数目</li></ul></li><li><p>Cluster Manager<br>集群的资源管理器，在集群上获取资源的外部服务，例如Standalone，Mesos，Yarn。<br>拿Yarn举例，客户端程序会向Yarn申请运行我这个任务需要多少，多少CPU等，然后Cluster Manager会通过调度告诉客户端可以使用，然后客户端就可以把程序送到每个Worker Node上面执行。    </p><h1><span id="3-执行模式">3. 执行模式</span></h1><p>&ensp;&ensp;&ensp;&ensp;运行spark程序有3种模式，local，standalone，yarn。在使用spark-submit命令提交程序时，需要指定一些参数。   </p></li><li>—master:如spark://host:7077, mesos://host:port, yarn,  yarn-cluster,yarn-client, local   </li><li>—calss CLASS_NAME 应用程序的主类   </li><li>—name NAME 应用程序的名称,这个可以在程序中通过setAppName(“kafka_hbase”)指定  </li><li>—jars JARS 逗号分隔的本地jar包，后面添加jar的路径</li><li>—driver-memory MEM Driver内存，默认1G</li><li>—num-executors NUM，启动的executor的个数，默认为2，在yarn中使用。</li><li>—executor-core NUM，每个executor的核数。在yarn或者standalone下使用</li><li>—executor-memory MEM 每个executor的内存，默认是1G</li><li>—total-executor-cores NUM,所有executor总共的核数，仅仅在mesos或standalone中使用</li><li>driver-cores NUM Driver的核数，默认是1，这个参数只在standalone模式下使用   <h2><span id="31-standalone模式">3.1. standalone模式</span></h2>&ensp;&ensp;&ensp;&ensp;运行一个pyspark程序，使用standalone模式来提交程序，需要使用的参数有：<br>—master spark://hz4:7077<br>—jars xxx1.jar,xxx2.jar<br>不使用—num-executors,这个在yarn中使用<br>—executor-memory MEM,每个executor占用的内存，如果一个executor占用4G，有5个executor，那这个程序占用20G<br>—executor-core NUM，表示每个executor的核数<br><strong>—total-executor-cores NUM</strong>,所有的executor占用的核数。使用total-executor-cores / executor-core得到executor的个数，假设total-executor-cores设置为30，executor-core为6，则表示运行这个程序一共有5个executor，分别在不同worker上。一个worker可以有多个executor。 假设有5个executor，2个worker，那么一个worker上有多个executor。如果不指定—total-executor-cores，程序会把worker上的核全都占用，这样别人提交程序的时候就没有办法运行。<br>&ensp;&ensp;&ensp;&ensp;运行一个程序的命令：spark-submit —master spark://hz4:7077  —executor-memory 4G —executor-cores 6 —total-executor-cores 30 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar spark_streaming.py   <h2><span id="32-yarn模式">3.2. Yarn模式</span></h2>&ensp;&ensp;&ensp;&ensp;yarn模式可以用的参数有：<br>—master yarn<br>—jars xxx1.jar,xxx2.jar<br><strong>—num-executors NUM</strong>, 启动的executor的个数，默认为2，不要使用默认，会很慢。在yarn中使用。yarn资源管理器会在不同的worker上分配executor给程序。<br>—executor-memory MEM,每个executor占用的内存，如果一个executor占用4G，有5个executor，那这个程序占用20G<br>—executor-core NUM，表示每个executor的核数，如果有5个executor，每个executor占用4G，那这个程序运行时占用20G内存。<br>&ensp;&ensp;&ensp;&ensp;运行一个程序的命令：spark-submit —master yarn —num-executors 20 —executor-memory 4G —executor-cores 4 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar spark_streaming.py <h2><span id="33-参数调优">3.3. 参数调优</span></h2>&ensp;&ensp;&ensp;&ensp;</li><li>num-executors：该参数用于设置Spark作业总共要用多少个Executor进程来执行,Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在<br>集群的各个工作节点上，启动相应数量的Executor进程。<br><strong>参数调优建议</strong>：<br>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；<br>设置的太多的话，大部分队列可能无法给予充分的资源。  </li><li>executor-memory：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。<br><strong>参数调优建议</strong>：<br>每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列<br>的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，<br>那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。   </li><li>executor-cores：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个<br>task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。<br><strong>参数调优建议</strong>：<br>Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的<br>Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过<br>队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。  </li><li>driver-memory：该参数用于设置Driver进程的内存。<br><strong>参数调优建议</strong>：<br>Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，<br>那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。  </li><li>—conf spark.default.parallelism：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<br><strong>参数调优建议</strong>：<br>Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量<br>来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会<br>导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的<br>Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍<br>较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。<h2><span id="executor">Executor</span></h2>&ensp;&ensp;&ensp;&ensp;在运行pyspark程序时出错：   Container killed by YARN for exceeding memory limits. 16.9 GB of 16 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead”这个错误总会使你的job夭折。它的意思是：因为超出内存限制，集群停掉了container。<br>Spark的Excutor的Container内存有两大部分组成：Excutor内存和堆外内存。   </li></ul><p><img src="/2019/04/20/pyspark/executor.png" alt="">     </p><p>Spark底层shuffle的传输方式是使用netty传输，netty在进行网络传输的过程会申请堆外内存（netty是零拷贝），所以使用了堆外内存，即spark.yarn.executor.memoryOverhead。<br><strong>Executor内存</strong><br>又spark.executor.memory参数设置，在spark-shell中由—executor-memory指定，分为2部分，shuffle.memoryFraction和storage.memoryFraction。<br><strong>spark.shuffle.memoryFractio</strong><br>该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。<br><strong>参数调优</strong><br>如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br><strong>spark.storage.memoryFractio</strong><br>该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。<br><strong>参数调优</strong><br>如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。         </p><p><strong>spark.yarn.executor.memoryOverhead</strong><br>executor执行的时候，用的内存可能会超过executor-memoy，所以会为executor额外预留一部分内存。spark.yarn.executor.memoryOverhead代表了这部分内存。这个参数如果没有设置，会有一个自动计算公式(位于ClientArguments.scala中)，代码如下：<br>其中，MEMORY_OVERHEAD_FACTOR默认为0.1，executorMemory为设置的executor-memory, MEMORY_OVERHEAD_MIN默认为384m。参数MEMORY_OVERHEAD_FACTOR和MEMORY_OVERHEAD_MIN一般不能直接修改，是Spark代码中直接写死的。</p><p>关于Executor 计算的相关公式，见源码org.apache.spark.deploy.yarn.Clent，org.apache.spark.deploy.yarn.ClentArguments<br>主要部分如下 </p><pre><code class="lang-python">var executorMemory = 1024 // 默认值，1024MBval MEMORY_OVERHEAD_FACTOR = 0.10  // OverHead 比例参数，默认0.1val MEMORY_OVERHEAD_MIN = 384val executorMemoryOverhead = sparkConf.getInt(&quot;spark.yarn.executor.memoryOverhead&quot;,math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))// 假设有设置参数，即获取参数，否则使用executorMemoryOverhead 的默认值val executorMem = args.executorMemory + executorMemoryOverhead// 最终分配的executor 内存为 两部分的和</code></pre><p><strong>解决方案</strong><br>在参数中设置<strong>spark.yarn.executor.memoryOverhead=4096</strong>，单位是MB，一般是2的幂,这里使用4G，默认申请的堆外内存是Executor内存的10%，真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G）</p><p>spark-submit —master yarn —num-executors 20 —executor-memory 4G —executor-cores 4 —conf spark.yarn.executor.memoryOverhead=4096 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar feature_extraction.py    </p><p><strong>executor-memory+spark.yarn.executor.memoryOverhead&lt;MonitorMemory</strong><br>指定的 ExecutorMemory与MemoryOverhead 之和大于 MonitorMemory，则会导致Executor申请失败，程序直接不能运行；若运行过程中，实际使用内存超过上限阈值，Executor进程会被Yarn终止掉（kill）</p><p>在运行程序中发现CPU的占用率不高，，增加num-executors的个数，减少executor-cores的个数<br>参考资料：<br><a href="https://www.cnblogs.com/haozhengfei/p/5fc4a976a864f33587b094f36b72c7d3.html" target="_blank" rel="noopener">https://www.cnblogs.com/haozhengfei/p/5fc4a976a864f33587b094f36b72c7d3.html</a><br><a href="https://blog.csdn.net/hammertank/article/details/48346285" target="_blank" rel="noopener">https://blog.csdn.net/hammertank/article/details/48346285</a><br><a href="http://www.raychase.net/3546" target="_blank" rel="noopener">http://www.raychase.net/3546</a><br><a href="https://www.jianshu.com/p/10e91ace3378" target="_blank" rel="noopener">https://www.jianshu.com/p/10e91ace3378</a></p><h1><span id="4-创建sc">4. 创建sc</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器中的命令行中，输出：pyspark，会打开spark-shell交互窗口，这时spark-shell会自动创建一个sc，不用再创建sc，手动创建了也不能用，会出错。如果在py文件中写程序，首先需要手动创建一个sc。   </p><pre><code class="lang-python">from pyspark import SparkConf, SparkContextconf = SparkConf().set(&quot;spark.executorEnv.PYTHONHASHSEED&quot;, &quot;0&quot;).setAppName(&quot;kafka_hbase&quot;)sc = SparkContext(conf=conf)</code></pre><p>或者使用    </p><pre><code class="lang-python">from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#39;local&#39;).setAppName(&#39;My_App&#39;)sc = SparkContext(conf = conf)</code></pre><p>首先创建一个SparkConf对象来配置应用，然后基于该SparkConf来创建一个SparkContext对象。<br><code>.setMaster()</code>给定了集群的URL，高速spark如何连接到集群上，这里的<code>local</code>表示让spark运行在单机单变成上。<br>也可以是<code>.setMaster(&#39;spark://192.168.1.11:7077&#39;)</code>表示使用standalone运行spark程序。<br><code>.setAppName()</code>给出应用的名字，当连接到一集群上时，这个值可以帮助你找到你的应用。      </p><h1><span id="5-rdd转换">5. RDD转换</span></h1><p>&ensp;&ensp;&ensp;&ensp;RDD被创建好之后，在后续使用过程中有2中操作：</p><ul><li>转换（Transformation）：基于现有的RRD创建一个新的RDD</li><li>行动（Action）：在数据集上进行运算，返回计算值。<h2><span id="51-转换操作">5.1. 转换操作</span></h2>&ensp;&ensp;&ensp;&ensp;对于RDD而言，每一次转换操作都会产生不同的RDD，如果说rdd2 = rdd1.map(lamda x : x+1),rdd1的值不会改变，通过转换操作返回一个新的rdd供下一个转换操作。转换得到的RDD是惰性的，也就是说，整个过程只记录了转换的轨迹，并不会发生真正的计算，只有遇到Action操作时，才会发生真正的计算。开始从血缘关系源头开始，进行物理的转换操作。<br>&ensp;&ensp;&ensp;&ensp;下面列出一些常见的转换（Transformation）操作。</li></ul><ul><li>filter(func)：筛选出满足函数func的元素，并返回一个新的数据集</li><li>map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集</li><li>flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果</li><li>groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</li><li>reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合   <h2><span id="52-行动操作">5.2. 行动操作</span></h2>&ensp;&ensp;&ensp;&ensp;行动操作是真正触发计算的地方。Spark程序执行到行动操作，才会执行真正的计算，从文件中加载数据，完成一次有一次转换操作，最终，完成行动操作得到结果。<strong>在触发Action操作时，开始真正的计算，这时，Spark会把计算分解成多个任务在不同机器上执行，每台机器上运行位于属于它自己的map和reduce，最后把结果返回给Driver Program</strong>。<br>&ensp;&ensp;&ensp;&ensp;下面给出一些常见的行动（Action）操作</li><li>count() 返回数据集中的元素个数</li><li>collect() 以数组的形式返回数据集中的所有元素</li><li>first() 返回数据集中的第一个元素</li><li>take(n) 以数组的形式返回数据集中的前n个元素</li><li>reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</li><li>foreach(func) 将数据集中的每个元素传递到函数func中运行<h2><span id="53-持久化">5.3. 持久化</span></h2>&ensp;&ensp;&ensp;&ensp;在Spark中，RDD采用惰性的机制，每次遇到Action操作，都会从头开始执行计算。如果整个Spark程序只有一次Action操作，当然不会又什么问题。但是，在一些情况下，我们需要对一个RDD多次调用不同的Action，这就意味着，每次调用Action操作，都会触发一次从头开始的计算，代价很大，并且这些Action操作都是对一个RDD而言，所以可以把这个RDD持久化。<br>比如下面是多次对一个RDD进行Action操作   <pre><code class="lang-python">list = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]rdd = sc.parallelize(list)#count()是一个Action操作，触发一次真正从头到尾的计算print(rdd.count())&gt;&gt;&gt;3#collect()也是一个Action()操作，触发一个真正从头到尾的计算print(&#39;,&#39;.join(rdd.collect()))&gt;&gt;&gt;a,b,c</code></pre>&ensp;&ensp;&ensp;&ensp;上面代码执行过程中，前后共触发了2次从头到尾的计算。<br>&ensp;&ensp;&ensp;&ensp;实际上，可以通过持久化(缓存)机制避免这种重复计算的开销。可以使用persist()方法对一个RDD<strong>标记为持久化</strong>，之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算RDD并把它持久化，而是要等到第一个Action操作触发时，才开始计算RDD，并把RDD的内容进行持久化。持久化的RDD将会被保留在计算节点的内存中，以便被后面的Action操作重复使用。<br>&ensp;&ensp;&ensp;&ensp;persist()方法可以传入持久化级别参数。   </li><li>persist(MEMOEY_ONLY)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容。   </li><li>persist(MEMORY_AND_DISK)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存储在硬盘上。</li><li>一般使用cache()方法时，会调用persist(MEMORY_ONLY)   <pre><code class="lang-python">list = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]rdd = sc.parallelize(list)#会调用persist(MEMORY_ONLY)，但是语句执行到这里，并不会缓存rdd的内容，因为这时rdd还没有被计算生成rdd.cache()#count()是一个Action操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache()，把这个rdd的内容放在缓存中。print(rdd.count())&gt;&gt;&gt;3#collect()也是一个Action()操作，不需要触发一个真正从头到尾的计算，只需要重复使用上面缓存中的rdd。print(&#39;,&#39;.join(rdd.collect()))&gt;&gt;&gt;a,b,c</code></pre>最后，可以使用unpersist()方法手动地把持久化的RDD从缓存中移除。    <h1><span id="6-分区">6. 分区</span></h1>&ensp;&ensp;&ensp;&ensp;RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上。RDD的一个分区原则是使得分区的个数尽量等于集群中CPU核心（core）数目。<br>&ensp;&ensp;&ensp;&ensp;对于不同的Spark部署而言（local，Standalone,yarn，Mesos）,都可以通过设置spark.default.parallelism这个参数的值，来配置默认的分区数据，一般而言：</li></ul><ul><li>local模式：默认为本地机器的CPU数目，若设置了local[N],则默认为N</li><li>Standalone和yarn：max(集群中所有CPU核心数目总和,2)作为默认值</li><li>Mesos：默认的分区数为8   <pre><code class="lang-scala">scala&gt;val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt;val rdd = sc.parallelize(array,2) #设置两个分区rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:29</code></pre>&ensp;&ensp;&ensp;&ensp;对于textFile而言，如果没有在方法中指定分区数，则默认为min(defaultParallelism,2)，其中，defaultParallelism对应的就是spark.default.parallelism。<br>&ensp;&ensp;&ensp;&ensp;如果是从HDFS中读取文件，则分区数为文件分片数(比如，128MB/片)。</li></ul><h1><span id="7-创建rdd">7. 创建RDD</span></h1><p>&ensp;&ensp;&ensp;&ensp;创建RDD有2种方式，一种是通过列表创建，一种是通过读取文件创建。<strong>RDD的内部其实是一个Iterator\<string\></string\></strong></p><h2><span id="71-通过paralize创建rdd">7.1. 通过paralize创建RDD</span></h2><pre><code class="lang-python">string=&#39;a\nb\nc\na\nd\ne&#39;b = string.split(&#39;\n&#39;)sc.parallelize(b)</code></pre><p><code>b</code>是一个list列表，通过列表b可以创建一个RDD。      </p><h2><span id="72-读文本文件创建rdd">7.2. 读文本文件创建RDD</span></h2><p>&ensp;&ensp;&ensp;&ensp; 读取文本文件获取RDD，可以从服务器本地读取(其他节点也可以)，也可以从hdfs上读取文件。文本每行的内容以字符串的形式作为RDD的一个元素。<br>从服务器本地读取文件时，需要加上file://</p><pre><code class="lang-python">rdd1 = sc.textFile(&quot;file:///file0/input/test.txt&quot;)</code></pre><p>从HDFS上读取文件   </p><pre><code class="lang-python">rdd1 = sc.textFile(&#39;hdfs://master:8020/pc2/data.csv&#39;)</code></pre><h1><span id="8-map和flatmap">8. map和flatMap</span></h1><p>&ensp;&ensp;&ensp;&ensp;map是对RDD中的每个元素执行一个函数，每个元素返回一个list，然后把每个元素的list再组成一个大的list，例如[[a,a],[b,b]]，然后flatMap就是先对每个元素执行一个函数，每个元素返回一个list，然后把每个元素的list的内容取出来，组成一个大的list，例如[a,a,b,b]。   </p><p>&ensp;&ensp;&ensp;&ensp;<a href="https://www.jianshu.com/p/c76ba3091a21" target="_blank" rel="noopener">这篇博客</a>讲解的比较好。说明flatMap中的函数返回类型一定是一个可迭代的类型，先把元素生成一个列表，然后再把每个列表中的元素取出来拼接成一个大的列表。<br><a href="https://www.4spaces.org/spark-map-flatmap/" target="_blank" rel="noopener">这篇也讲的很好</a>    </p><h1><span id="9-flatmap和flatmapvalues">9. flatMap和flatMapValues</span></h1><p>&ensp;&ensp;&ensp;&ensp;flatMap针对的RDD中的每个元素先做map操作，再做flatten操作，最后形成超大的list返回。flatMapValues只针对元素是<k,v>格式的RDD，原RDD中的key保持不变，只对value进行变换，变换之后的value和原来的key组成新的<k,v1>，作为RDD中的一个元素。参考<a href="http://blog.cheyo.net/172.html" target="_blank" rel="noopener">这篇博客</a></k,v1></k,v></p><h1><span id="10-reducebykey和groupbykey">10. reduceByKey和groupByKey</span></h1><p>&ensp;&ensp;&ensp;&ensp;推荐使用reduceByKey，<a href="https://blog.csdn.net/zongzhiyuan/article/details/49965021" target="_blank" rel="noopener">这篇博客</a>对于两者的区别进行了解释。<br>groupByKey涉及数据的shuffle操作，shuffle是spark重建数据的机制，将来自不同分区的数据进行分组，开销很大。    </p><h1><span id="11-sortby和sortbykey">11. sortBy和SortByKey</span></h1><p>&ensp;&ensp;&ensp;&ensp;sortByKey针对(key,value)对中的key进行排序。<br>&ensp;&ensp;&ensp;&ensp;sortBy可以根据我们需要的值进行排序，不一定是key，比如统计单词出现的次数，然后按照次数进行排序(key,value)，我们就是对value进行排序，可以使用sortBy函数。<br>sortBy()中有3个参数，第一个参数是一个函数，第二个参数是ascending，表示升序还是降序，默认是True(升序)。第三个参数是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等。</p><h1><span id="12-将spark计算的结果存储在文件中">12. 将Spark计算的结果存储在文件中</span></h1><h2><span id="121-写入到服务器本地文件中">12.1. 写入到服务器本地文件中</span></h2><p>&ensp;&ensp;&ensp;&ensp;假设pyspark计算的结果存储在results变量中，然后将<code>results</code>的内容存储在文件中。</p><pre><code class="lang-python"># 将结果写入到服务器本地的文件中filename = &#39;result.txt&#39;with open(filename,&#39;w&#39;) as f:    for line in results:        f.write(line)        f.write(&#39;\n&#39;)</code></pre><h2><span id="122-写入到hdfs文件中">12.2. 写入到HDFS文件中</span></h2><p>&ensp;&ensp;&ensp;&ensp;spark将RDD中的每个元素作为一行写入到文本文件中。在写入到HDFS之前，首先把results中的每个元素转成字符串的形式。<br>&ensp;&ensp;&ensp;&ensp;比如<code>rdd1</code>为<code>[(&#39;b&#39;,3),(&#39;a&#39;,2),(&#39;c&#39;,1)]</code>，<code>rdd1</code>中的每个元素是一个元组，需要把每个元素转换成字符串类型。<br><code>rdd2 = rdd1.map(lamda x: x[0]+&quot;,&quot;+str(x[1]))</code> ,然后使用<code>rdd2.saveAsTextFile(&#39;/tmp/word_count_result&#39;)</code>，把结果存储到<code>word_count_result</code>这个文件中，这个文件没有后缀名。    </p><h2><span id="123-打印rdd元素">12.3. 打印RDD元素</span></h2><p>&ensp;&ensp;&ensp;&ensp;在实际编程中，我们经常需要把RDD中的元素打印输出到屏幕上（标准输出stdout），一般会采用语句rdd.foreach(println)或者rdd.map(println)。当采用本地模式（local）在单机上执行时，这些语句会打印出一个RDD中的所有元素。但是，当采用集群模式执行时，在worker节点上执行打印语句是输出到worker节点的stdout中，而不是输出到任务控制节点Driver Program中，因此，任务控制节点Driver Program中的stdout是不会显示打印语句的这些输出内容的。为了能够把所有worker节点上的打印输出信息也显示到Driver Program中，可以使用collect()方法，比如，rdd.collect().foreach(println)，但是，由于collect()方法会把各个worker节点上的所有RDD元素都抓取到Driver Program中，因此，这可能会导致内存溢出。因此，当你只需要打印RDD的部分元素时，可以采用语句rdd.take(100).foreach(println)。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;使用Python编写Spark程序&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python、spark" scheme="http://yoursite.com/tags/python%E3%80%81spark/"/>
    
  </entry>
  
  <entry>
    <title>Python学习</title>
    <link href="http://yoursite.com/2019/04/15/Python%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/04/15/Python学习/</id>
    <published>2019-04-15T00:25:26.000Z</published>
    <updated>2019-08-23T12:58:47.004Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;学习Python！！！！<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-%e8%a7%84%e8%8c%83">2. 规范</a></li><li><a href="#3-%e5%ba%8f%e5%88%97">3. 序列</a></li><li><a href="#4-%e8%af%8d%e5%85%b8">4. 词典</a></li><li><a href="#5-ndarray%e5%92%8clist">5. ndarray和list</a><ul><li><a href="#51-%e5%88%9b%e5%bb%bandarray">5.1. 创建ndarray</a><ul><li><a href="#511-%e9%80%9a%e8%bf%87nparray">5.1.1. 通过np.array</a></li><li><a href="#512-%e9%80%9a%e8%bf%87nparange">5.1.2. 通过np.arange</a></li><li><a href="#513-list%e5%92%8cndarray%e7%9a%84%e7%b4%a2%e5%bc%95">5.1.3. list和ndarray的索引</a></li><li><a href="#514-npmax">5.1.4. np.max()</a></li></ul></li></ul></li><li><a href="#6-pandas">6. Pandas</a></li></ul><!-- /TOC --><h1><span id="2-规范">2. 规范</span></h1><p>&ensp;&ensp;&ensp;&ensp;</p><ul><li>运算符的左右加空格，例如a + b</li><li>如果有多行赋值，将上下赋值的=对齐<br>num    = 1<br>secNum = 2</li><li>变量的所有字母小写，单词之间用下划线连接，table_name=’test’</li></ul><h1><span id="3-序列">3. 序列</span></h1><p>&ensp;&ensp;&ensp;&ensp;序列是一种容器，是有顺序的数据集合。序列有两种：元组（Tuple）和列表（List）。列表是可变的，元组是不可变的。所以经常会创建空的列表，a=[],而不会创建一个空的元组。<br>&ensp;&ensp;&ensp;&ensp;对序列(元组和列表)范围引用，a[起始,结束,步长]，包含起始，不包含结束。循环获取序列的值：for i in list，这里的a就是值，而不是下标      </p><h1><span id="4-词典">4. 词典</span></h1><p>&ensp;&ensp;&ensp;&ensp;词典中的数据是无序的，不能通过位置下标来获取，   </p><h1><span id="5-ndarray和list">5. ndarray和list</span></h1><p>&ensp;&ensp;&ensp;&ensp;list是Python的内置数据类型，list中的数据类型不必相同。例如：<code>[1,2,&#39;a&#39;,3.9]</code><br>&ensp;&ensp;&ensp;&ensp;首先需要明确的一点是array和ndarray是什么。ndarray是一种类型，array不是一种数据类型，可以通过np.array()来创建一个ndarray的对象。ndarray是numpy的一种数据类型，ndarray中的元素类型必须相同，例如：<code>[1,2,3,4]</code>    </p><h2><span id="51-创建ndarray">5.1. 创建ndarray</span></h2><h3><span id="511-通过nparray">5.1.1. 通过np.array</span></h3><p>&ensp;&ensp;&ensp;&ensp;通过np.array()来创建，传入的参数可以是list，也可以是tuple，使用ndarray的shape属性来获取ndarray的形状。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = np.array((<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>))</span><br><span class="line">c = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br></pre></td></tr></table></figure></p><p>使用reshape改变ndarray的形状<br><code>c.reshape((3,-1))</code>,reshape传入的形状是可以是<br>reshape((3,-1)),<br>reshape([3,1]),<br>reshape(3,-1)</p><h3><span id="512-通过nparange">5.1.2. 通过np.arange</span></h3><p>numpy提供了很多方法直接创建一个ndarray对象.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr1=np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>) <span class="comment">#   </span></span><br><span class="line">arr2=np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)  </span><br><span class="line"><span class="keyword">print</span> (arr1,arr1.dtype)  </span><br><span class="line"><span class="keyword">print</span> (arr2,arr2.dtype)</span><br><span class="line">```  </span><br><span class="line">结果</span><br></pre></td></tr></table></figure></p><p>[1 2 3 4 5 6 7 8 9] int32<br>[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.] float64<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">np.arange(a,b,c)表示产生从a~b，不包括b，间隔为c的一个ndarray，数据类型默认是int32。   </span><br><span class="line">np.linspace(a,b,c)表示把a~b（包括b），平均分成c份。    </span><br><span class="line">np.arange()和range都可以用来生成序列。注意arange是numpy的函数，range可以直接调用。arange和range不同的是：range只能生成int类型，写`rang(1,10,0.1)`是错误的，arange可以生成float类型，可以写成`np.arange(1,10,0.1)`  </span><br><span class="line">![](Python学习/range.png)        </span><br><span class="line">**使用print输出时，list中的元素之间有逗号分开，ndarray元素之间没有逗号**。   </span><br><span class="line">![](Python学习/print.png)  </span><br><span class="line">**虽然有很多产生ndarray类型的方法，但是大部分情况下我们都是从list进行转换生成ndarray。因为我们从文件中读取数据存储在list中，然后转换成ndarray**    </span><br><span class="line">比如定义一个list,a = [1,2,3,4],然后使用np.array(a)将list转换成ndarray类型。   </span><br><span class="line">### 5.1.3. list和ndarray的索引 </span><br><span class="line">定义一个list  </span><br><span class="line">`list1=[[1,2,3],[4,5,6],[7,8,9]]`  </span><br><span class="line">定义一个ndarray   </span><br><span class="line">`arr1 = np.array(list1)`   </span><br><span class="line"></span><br><span class="line">![](Python学习/list.png)</span><br><span class="line">![](Python学习/arr.png)    </span><br><span class="line">ndarray比list的索引方式更多，这也是两者经常遇到的区别。   </span><br><span class="line">**因为list可以存储任意类型的数据，因为list中存储数据存放的地址，简单说就是指针，并非数据，这样保存一个list就太麻烦了，例如list1=[1,2,3,&apos;a&apos;]就需要4个指针和4个数据，增加了存储和CPU消耗**     </span><br><span class="line">### 5.1.4. np.max()</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;numpy常用的统计函数如下：</span><br><span class="line">- np.sum()，返回求和</span><br><span class="line">- np.mean()，返回均值</span><br><span class="line">- np.max()，返回最大值</span><br><span class="line">- np.min()，返回最小值</span><br><span class="line">- np.ptp()，数组沿指定轴返回最大值减去最小值，即（max-min）</span><br><span class="line">- np.std()，返回标准偏差（standard deviation）</span><br><span class="line">- np.var()，返回方差（variance）</span><br><span class="line">- np.cumsum()，返回累加值</span><br><span class="line">- np.cumprod()，返回累乘积值 </span><br><span class="line">注意：在使用以上这些函数时，需要指定axis的方向，若不指定，默认统计整个数组。axis=0表示列，axis=1表示行。一般axis=0比较符合实际情况。   </span><br><span class="line">![](Python学习/max.png) </span><br><span class="line"></span><br><span class="line"># 6. Pandas      </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp; DataFrame根据某一列排序，其中inplace=True表示修改df的值，默认是false，表示不修改df的值，会返回一个排好序的DataFrame。  </span><br><span class="line">```df.sort_values(&quot;数据时间&quot;,inplace=True)```   </span><br><span class="line">排好序的dataframe的index列还是原先dataframe的index。比如下面的图是排序之间的dataframe。   </span><br><span class="line">![](Python学习/排序前.png)    </span><br><span class="line">使用```df.sort_values(&quot;数据时间&quot;,inplace=True)```  按照时间排序，但是index还是原先的index，我想让排序后的dataframe的index从0开始。</span><br><span class="line">![](Python学习/排序后.png)      </span><br><span class="line">```df[2:4]```表示返回第3和4行的数据，即索引为3731，512这2行的数据，而不是返回索引为2和3的数据。 </span><br><span class="line">使用```sort_df.reset_index(drop=True, inplace=True)``` 重新定义索引，使其从0开始。</span><br><span class="line">![](Python学习/reindex.png)      </span><br><span class="line">获取dataframe中的索引  </span><br><span class="line">```firstIndex = df.index.tolist() ```  返回一个list，存储的是dataframe中的索引列表。</span><br><span class="line">```firstIndex = df.index.tolist()[0] ``` 返回的是第一行数据的索引   </span><br><span class="line">```firstIndex = df.index.tolist()[-1] ``` 返回的是最后一行数据的索引</span><br><span class="line">## 获取dataframe中的数据      </span><br><span class="line">![](Python学习/1.png) </span><br><span class="line">1. ```df[1:4]```表示获取表的第2至4行   </span><br><span class="line">![](Python学习/2.png)   </span><br><span class="line">2. df.head()默认返回dataframe中的前5行，如果返回前10行，使用head(10).    </span><br><span class="line">![](Python学习/3.png) </span><br><span class="line">3. 使用```df.iloc[]```和```df.loc[]```获取数据。     </span><br><span class="line">![](Python学习/4.png)   </span><br><span class="line">![](Python学习/5.png) </span><br><span class="line">通过  ```df.iloc[]```传入的参数是数据，而```df.loc[]```传入的参数是字符串索引，除非索引是数字，这时loc[]可以传入数字。  </span><br><span class="line">比如df1.loc[2]表示获取索引为&apos;2&apos;的那一行，而df.iloc[2]表示获取df1的第3行数据，是一个相对位置。  </span><br><span class="line">参考资料：  </span><br><span class="line">[https://www.jb51.net/article/141665.htm](https://www.jb51.net/article/141665.htm)  </span><br><span class="line">## DateFrame常用方法    </span><br><span class="line">- 获取df中某一列特征值的个数   </span><br><span class="line">`credit_df[&apos;Class&apos;].value_counts()`或   </span><br><span class="line">`credit_df[&apos;Class&apos;].value_counts()[0]`   </span><br><span class="line">- 显示df中的详细信息</span><br><span class="line">`df.info()`   </span><br><span class="line">`df.describe()`</span><br><span class="line">- 获取df中的所有列名   </span><br><span class="line">`col_names = list(df.columns.values)`     </span><br><span class="line">- 将df按照某一特征进行分组    </span><br><span class="line">`df.groupby([&apos;total_vol&apos;]).size()`获取每个组中元素的个数   </span><br><span class="line">`df.groupby([&apos;total_vol&apos;,&apos;soc&apos;]).size()`按照多个属性分组   </span><br><span class="line">- 从df中获取样本的特征和标签   </span><br><span class="line">```python   </span><br><span class="line">获取特征</span><br><span class="line">X = df.drop(&quot;误报&quot;,axis = 1)</span><br><span class="line">获取标签</span><br><span class="line">Y = df[&quot;误报&quot;]</span><br><span class="line">```   </span><br><span class="line">- 获取df中的一列或多列     </span><br><span class="line">```python </span><br><span class="line">one_col = df[&apos;total_vol&apos;]</span><br><span class="line">multi_cols_name = [&apos;total_vol&apos;,&apos;soc&apos;,&apos;cur&apos;]   </span><br><span class="line">multi_cols = df[multi_cols_name]   </span><br><span class="line">```   </span><br><span class="line">- 查看df中为空的个数,输出每一列为nan的个数 </span><br><span class="line">`df.isna().sum()`</span><br><span class="line"># Dict   </span><br><span class="line">python创建一个字典有3中方式   </span><br><span class="line">```python</span><br><span class="line">class dict(**kwarg)</span><br><span class="line">class dict(mapping,**kwarg)</span><br><span class="line">class dict(iterable,**kwarg)</span><br><span class="line">```   </span><br><span class="line">其中```**kwarg```是python中可变参数，代表关键字参数，允许你传入0个或任意多个含参数名的参数，这个关键字参数在函数内部自动组装成一个dict。</span><br><span class="line">- class dict(**kwarg)  </span><br><span class="line">通过关键字参数创建一个字典，例如   </span><br><span class="line">```python</span><br><span class="line">d = dict(name=&apos;Tom&apos;,age=23)     </span><br><span class="line">out: &#123;&apos;age&apos;: 23, &apos;name&apos;: &apos;Tom&apos;&#125;</span><br><span class="line"></span><br><span class="line">d = dict(a = 12, b = 13, c = 15)  </span><br><span class="line">out: &#123;&apos;a&apos;: 12, &apos;b&apos;: 13, &apos;c&apos;: 15&#125;</span><br></pre></td></tr></table></figure></p><ul><li>class dict(mapping,<strong>kwarg)<br>通过从一个映射函数对象中构造一个新字典，与dict(</strong>kwarg)函数不一样的地方是参数输入是一个映射类型的函数对象，比如zip函数，map函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">以映射函数方式来构造字典</span><br><span class="line">d2 = dict(zip([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))  </span><br><span class="line">out: &#123;<span class="string">'one'</span>: <span class="number">1</span>, <span class="string">'three'</span>: <span class="number">3</span>, <span class="string">'two'</span>: <span class="number">2</span>&#125;</span><br><span class="line">```    </span><br><span class="line">- <span class="class"><span class="keyword">class</span> <span class="title">dict</span><span class="params">(iterable,**kwarg)</span>   </span></span><br><span class="line"><span class="class">其中<span class="title">iterable</span>表示可迭代对象，可迭代对象可以使用<span class="title">for</span>...<span class="title">in</span>...来遍历，在<span class="title">Pytohn</span>中<span class="title">list</span>，<span class="title">tuple</span>，<span class="title">str</span>，<span class="title">dict</span>，<span class="title">set</span>等都是可迭代对象。创建<span class="title">dict</span>时如果传入的是可迭代对象，则可迭代对象中每一项自身必须是可迭代的，并且每一项只能由有2个对象，第一个对象称为字典的<span class="title">key</span>，第二个对象为<span class="title">key</span>对应的<span class="title">value</span>。如果<span class="title">key</span>有重复，其<span class="title">value</span>为最后重复项的值    </span></span><br><span class="line"><span class="class">![]<span class="params">(Python学习/dict.png)</span>     </span></span><br><span class="line"><span class="class"># 函数参数   </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在python中定义一个函数，可以传入4种参数：  </span><br><span class="line">位置参数，默认参数，关键字参数，可变参数   </span><br><span class="line"><span class="comment">## 位置参数   </span></span><br><span class="line">普通的参数，参数之间是有顺序的， 顺序不能写错 </span><br><span class="line">```python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">```     </span><br><span class="line"><span class="comment">## 关键字参数   </span></span><br><span class="line">函数调用时使用关键字参数来确定传入的参数值，使用关键字参数允许函数调用时参数的顺序和声明的顺序不一致，因为python解释器会根据参数名来匹配参数值。使用key=value格式来指定参数。</span><br><span class="line">```python  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s    </span><br><span class="line">power(<span class="number">5</span>,<span class="number">2</span>)会得到<span class="number">25</span></span><br><span class="line">power(<span class="number">2</span>,<span class="number">5</span>)会得到<span class="number">32</span></span><br><span class="line">power(n=<span class="number">2</span>,x=<span class="number">5</span>)会得到<span class="number">25</span></span><br><span class="line">```   </span><br><span class="line">```python  </span><br><span class="line">可写函数说明</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printinfo</span><span class="params">( name, age )</span>:</span></span><br><span class="line">   <span class="string">"打印任何传入的字符串"</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"名字: "</span>, name)</span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"年龄: "</span>, age)</span><br><span class="line">   <span class="keyword">return</span></span><br><span class="line"> </span><br><span class="line">调用printinfo函数</span><br><span class="line">printinfo( age=<span class="number">50</span>, name=<span class="string">"runoob"</span> )</span><br><span class="line">```  </span><br><span class="line">**注意：关键字参数必须写在位置参数之后，否则会报错**</span><br><span class="line"><span class="comment">## 默认参数    </span></span><br><span class="line">在定义函数时，使用赋值运算符=就为参数设置了一个默认值，默认参数是可选的，就是说可以指定，也可以不指定。当不指定时就使用默认值，如果指定，会覆盖默认值。有了一个默认参数，这样即使传入调用`power(<span class="number">5</span>)`,这样就默认n=<span class="number">2</span>，如果要计算的幂次大于<span class="number">2</span>，就需要明确的指定n的值，`power(<span class="number">5</span>,<span class="number">3</span>)`,这是n=<span class="number">3</span>  </span><br><span class="line">```python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n=<span class="number">2</span>)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">```   </span><br><span class="line">**设置默认参数时，需要注意以下几点：**  </span><br><span class="line"><span class="number">1.</span> 必选参数在前面，默认参数在后面，否则Python的解释器会报错  </span><br><span class="line">使用默认参数的好处是：比如学生注册的时候，需要传入的参数为：姓名，性别，年龄。把年龄设置为默认参数<span class="number">19</span>，这样大部分学生注册时不需要提供年龄，只需要提供<span class="number">2</span>个必须的参数，只有与默认参数不符的学生才提供额外的信息。可见，使用默认参数降低了函数调用的难度，</span><br><span class="line">```python  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enroll</span><span class="params">(name, gender,age=<span class="number">19</span>)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'name:'</span>, name</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'gender:'</span>, gender</span><br><span class="line">```     </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 可变参数   </span></span><br><span class="line"></span><br><span class="line">可变参数就是传入的参数个数是可变的，可以是<span class="number">1</span>个、<span class="number">2</span>个到任意个，还可以是<span class="number">0</span>个。  当函数中有位置参数和可变参数时，位置参数始终在可变参数之前。通常情况下，可变参数会出现在形参的最后，因为它们会把传递给函数的所有剩余参数都收集起来。可变参数之后出现的任何参数都是“强制关键字”参数，也就是说，可变参数之后的参数必须是关键字参数，而不能是位置参数。</span><br><span class="line"><span class="comment">### *args</span></span><br><span class="line">我们以数学题为例子，给定一组数字a，b，c……，请计算a2 + b2 + c2 + ……。   </span><br><span class="line">```python    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> numbers:</span><br><span class="line">        sum = sum + n * n</span><br><span class="line">    <span class="keyword">return</span> sum </span><br><span class="line">```  </span><br><span class="line">在函数内部，参数number接收到的是一个tuple，例如`calc(<span class="number">1</span>,<span class="number">2</span>)`得出来的结果是<span class="number">5</span>，也可以直接传入一个list或者tuple,在list或tuple前面加上一个*，把list或tuple的元素变成可变参数传进去。   </span><br><span class="line">```python   </span><br><span class="line">nums1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">calc(*nums1)</span><br><span class="line"><span class="number">14</span></span><br><span class="line"></span><br><span class="line">nums2 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">calc(*nums1,*nums2)</span><br><span class="line"><span class="number">28</span></span><br><span class="line">```    </span><br><span class="line"><span class="comment">### **args   </span></span><br><span class="line">可变参数允许传入<span class="number">0</span>个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入<span class="number">0</span>个或任意个含参数名称的参数，这些关键字参数函数内部自动组装成一个dict     </span><br><span class="line">```python   </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">person</span><span class="params">(name, age, **kw)</span>:</span></span><br><span class="line">    print( <span class="string">'name:'</span>, name)</span><br><span class="line">    print( <span class="string">'age:'</span>, str(name))</span><br><span class="line">    print( <span class="string">'other:'</span>, kw)</span><br><span class="line">person(<span class="string">'Michael'</span>, <span class="number">30</span>)  </span><br><span class="line">person(<span class="string">'Bob'</span>, <span class="number">35</span>, city=<span class="string">'Beijing'</span>)</span><br><span class="line">kw = &#123;<span class="string">'city'</span>: <span class="string">'Beijing'</span>, <span class="string">'job'</span>: <span class="string">'Engineer'</span>&#125;</span><br><span class="line">person(<span class="string">'Jack'</span>, <span class="number">24</span>, **kw)</span><br><span class="line">```  </span><br><span class="line"><span class="comment">## 总结</span></span><br><span class="line">**在函数定义时，参数的顺序为：位置参数，默认参数，*args，\*\*args**      </span><br><span class="line">**在函数调用时，参数的顺序为位置参数、关键字参数/默认参数，*args，\*\*args**     </span><br><span class="line"><span class="comment"># 文件读取   </span></span><br><span class="line">当一个文件很大时，不能一次性读取所有的内容加载到内存中，需要使用生成器   </span><br><span class="line">```python   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    读取文件</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.readline().strip()</span><br><span class="line">        <span class="keyword">while</span> data:</span><br><span class="line">            <span class="keyword">yield</span> data</span><br><span class="line">            data = f.readline().strip()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> read_file(filename):</span><br><span class="line">        value = json.loads(line)[<span class="string">'data'</span>]</span><br><span class="line">        rowkeys.add(list(value.keys())[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;学习Python！！！！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="编程" scheme="http://yoursite.com/tags/%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CDH集群</title>
    <link href="http://yoursite.com/2019/04/04/CDH%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2019/04/04/CDH集群/</id>
    <published>2019-04-04T14:11:03.000Z</published>
    <updated>2019-05-24T15:52:19.674Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;最近去给甲方安装CDH集群，对于集群的搭建和测试在这里总结一下。<br><a id="more"></a>   </p><!-- TOC --><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6">2. 版本控制</a></li><li><a href="#3-linux%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D">3. Linux目录介绍</a></li><li><a href="#4-%E5%AE%89%E8%A3%85%E5%89%8D%E8%AF%B4%E6%98%8E">4. 安装前说明</a></li><li><a href="#5-%E5%B0%8F%E5%B8%B8%E8%AF%86">5. 小常识</a></li><li><a href="#6-%E5%AE%89%E8%A3%85cdh%E9%9B%86%E7%BE%A4">6. 安装CDH集群</a><ul><li><a href="#61-%E5%85%B3%E9%97%AD%E6%89%80%E6%9C%89%E6%9C%BA%E5%99%A8%E7%9A%84%E9%98%B2%E7%81%AB%E5%A2%99">6.1. 关闭所有机器的防火墙</a></li><li><a href="#62-%E4%BF%AE%E6%94%B9%E6%9C%BA%E5%99%A8%E7%9A%84hosts">6.2. 修改机器的hosts</a></li><li><a href="#63-%E6%9F%A5%E7%9C%8B%E7%BD%91%E7%BB%9C%E6%98%AF%E5%90%A6%E8%BF%9E%E9%80%9A">6.3. 查看网络是否连通</a></li><li><a href="#64-%E7%94%9F%E6%88%90%E4%B8%BB%E8%8A%82%E7%82%B9%E7%9A%84ssh%E5%AF%86%E9%92%A5%E5%B9%B6%E5%88%86%E5%8F%91">6.4. 生成主节点的ssh密钥并分发</a></li><li><a href="#65-%E5%AE%89%E8%A3%85ntp%E6%9C%8D%E5%8A%A1">6.5. 安装ntp服务</a></li><li><a href="#66-%E5%8D%B8%E8%BD%BD%E6%9C%BA%E5%99%A8%E8%87%AA%E5%B8%A6%E7%9A%84openjdk">6.6. 卸载机器自带的openjdk</a></li><li><a href="#67-%E5%AE%89%E8%A3%85jdk">6.7. 安装JDK</a></li><li><a href="#68-%E5%AE%89%E8%A3%85mysql">6.8. 安装MySQL</a></li><li><a href="#69-%E5%88%9B%E5%BB%BAmysql%E6%95%B0%E6%8D%AE%E5%BA%93">6.9. 创建MySQL数据库</a></li><li><a href="#610-%E5%AE%89%E8%A3%85cloudera-manager-server%E5%92%8Cagent">6.10. 安装Cloudera Manager Server和Agent</a></li><li><a href="#611-%E6%89%93%E5%BC%80%E7%BD%91%E9%A1%B5%E9%85%8D%E7%BD%AE">6.11. 打开网页配置</a></li><li><a href="#612-hdfs%E7%9A%84ha%E5%AE%89%E8%A3%85">6.12. HDFS的HA安装</a></li><li><a href="#613-%E5%AE%89%E8%A3%85anaconda">6.13. 安装Anaconda</a></li><li><a href="#614-%E5%AE%89%E8%A3%85sparkstandalone">6.14. 安装Spark（standalone）</a></li><li><a href="#615-%E4%BF%AE%E6%94%B9hdfs%E6%9D%83%E9%99%90">6.15. 修改hdfs权限</a></li><li><a href="#616-%E5%AE%89%E8%A3%85python%E4%B8%89%E6%96%B9%E5%BA%93">6.16. 安装python三方库</a></li><li><a href="#617-%E5%AE%89%E8%A3%85kafka">6.17. 安装kafka</a></li><li><a href="#yarn">yarn</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-版本控制">2. 版本控制</span></h1><div class="table-container"><table><thead><tr><th></th><th>组件</th><th>版本</th></tr></thead><tbody><tr><td></td><td>CenOS</td><td>CenOS7</td></tr><tr><td></td><td>JDK</td><td>JDK1.8</td></tr><tr><td></td><td>CDH集群</td><td>CDH5.7.2</td></tr><tr><td></td><td>CDH-kafka</td><td>CDH-kafka1.2.0</td></tr><tr><td></td><td>Python</td><td>Python3.5 </td></tr></tbody></table></div><p>现在实验室的cdh集群版本是CDH5.7.2,其中每个组件的版本是<br>| 组件 | CDH5.7.2|CDH5.16.1<br>-|-|-|-<br>|Hadoop|2.6.0|2.6.0<br>|HDFS|2.6.0|2.6.0<br>|HBase|1.2.0|1.2.0<br>|Hive|1.1.0|1.1.0<br>|Spark|1.6.0|1.6.0<br>|Kafka|0.10.0|0.10.0<br>|Zookeeper|3.4.5|3.4.5</p><h1><span id="3-linux目录介绍">3. Linux目录介绍</span></h1><div class="table-container"><table><thead><tr><th style="text-align:center">目录</th><th style="text-align:center">说明</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center">bin</td><td style="text-align:center">存储普通用户可执行的指令</td><td style="text-align:center">即使在单用户模式下也能够执行处理</td></tr><tr><td style="text-align:center">dev</td><td style="text-align:center">设备目录</td><td style="text-align:center">所有的硬件设备及周边均放置在这个设备目录中</td></tr><tr><td style="text-align:center">home</td><td style="text-align:center">主要存放用户的个人数据</td><td style="text-align:center">每个用户在home中都有一个文件夹(除root外)，存储每个用户的设置文件，用户的桌面文件夹、用户的数据</td></tr><tr><td style="text-align:center">etc</td><td style="text-align:center">各种配置文件目录</td><td style="text-align:center">大部分配置属性均存放在这里</td></tr><tr><td style="text-align:center">lib/lib64</td><td style="text-align:center">开机时常用的动态链接库</td><td style="text-align:center">bin及sbin指令也会调用对应的lib库</td></tr><tr><td style="text-align:center">opt</td><td style="text-align:center">第三方软件安装目录</td><td style="text-align:center">现在习惯放在/usr/local中</td></tr><tr><td style="text-align:center">run</td><td style="text-align:center">系统运行所需的文件</td><td style="text-align:center">重启后重新生成对一个的目录数据</td></tr><tr><td style="text-align:center">sbin</td><td style="text-align:center">只有root用户才能运行的管理指令</td><td style="text-align:center">跟bin类似，但只属于root管理员</td></tr><tr><td style="text-align:center">tmp</td><td style="text-align:center">存储临时文件目录</td><td style="text-align:center">所有用户对该目录均可读写</td></tr><tr><td style="text-align:center">usr</td><td style="text-align:center">应用程序放置目录</td><td style="text-align:center">/usr/local存储那些手动安装的软件，/usr/bin存储程序，/usr/share存储一些共享数据，例如音乐文件或者图标，/usr/lib存储那些不能直接运行的，但却是很多程序运行所必须的一些函数库文件</td></tr></tbody></table></div><h1><span id="4-安装前说明">4. 安装前说明</span></h1><ul><li>Zookeeper和Kafka部分主节点，要装3台</li><li>HBase和HDFS分主从节点，都需要2个主节点，一个active主节点，一个standby主节点，剩下的机器作为从节点</li><li>关闭防火墙，安装ntp、jdk、mysql，anaconda可以同时进行。   </li><li>本次以3个服务器为例安装CDH集群</li><li>192.168.1.201   node1</li><li>192.168.1.202   node2</li><li>192.168.1.203   node3       </li></ul><h1><span id="5-小常识">5. 小常识</span></h1><ul><li>使用vi命令编辑文件</li><li>键盘a———输入模式，编辑文件</li><li>键盘Esc——修改完之后按Esc退出输入模式</li><li>:wq——-保存，并退出</li><li>:q———不保存，退出    </li></ul><h1><span id="6-安装cdh集群">6. 安装CDH集群</span></h1><h2><span id="61-关闭所有机器的防火墙">6.1. 关闭所有机器的防火墙</span></h2><p><font color="red">每台机器都要执行</font><br>#关闭防火墙<br>systemctl stop firewalld.service<br>#禁止firewall开机启动<br>systemctl disable firewalld.service<br>#关闭selinux<br>vi /etc/selinux/config<br>将SELINUX设置为disabled<br>如下SELINUX=disabled<br>#重启<br>reboot<br>#重启机器后使用root用户查看Selinux状态<br>getenforce</p><h2><span id="62-修改机器的hosts">6.2. 修改机器的hosts</span></h2><p><font color="red">每台机器都要执行</font><br>#使用ip addr查看每台机器的ip地址<br>#修改hosts文件<br>vi /etc/hosts   </p><p><font color="red">在最下面一行添加以下内容</font><br>192.168.1.201 node1<br>192.168.1.202 node2<br>192.168.1.203 node3 </p><h2><span id="63-查看网络是否连通">6.3. 查看网络是否连通</span></h2><p><font color="red">每台机器都要执行</font><br>#在node1上执行<br>ping node2<br>ping node3<br>#在其余两个节点分别ping<br>按Ctrl+C中断ping命令      </p><h2><span id="64-生成主节点的ssh密钥并分发">6.4. 生成主节点的ssh密钥并分发</span></h2><p>生成主节点root账户的ssh密钥，分发至其他机器，要实现免密码登录其他机器的root账户    </p><p><font color="red">只在node1上执行</font><br>#生成ssh密钥（node1上）<br>ssh-keygen -t rsa<br>然后一路回车<br>接下来分发密钥，请仔细观察显示的内容，会让你输入yes和密码<br>ssh-copy-id node1<br>ssh-copy-id node2<br>ssh-copy-id node3   </p><h2><span id="65-安装ntp服务">6.5. 安装ntp服务</span></h2><p><font color="red">每台机器都要执行</font><br>yum install ntpdate<br>在执行这条命令时我的电脑出现以下错误：   </p><p><font color="red">问题：Could not resolve host: mirrorlist.centos.org Centos 7</font><br><strong>解决方案</strong>：<a href="https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7" target="_blank" rel="noopener">https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7</a>   </p><p><font color="red">只在node1上执行</font><br>yum install ntp     </p><p><font color="red">只在node1上执行</font><br>vi /etc/ntp.conf<br>注释以下4行，在前面加#<br>server 0.centos.pool.ntp.org iburst<br>server 1.centos.pool.ntp.org iburst<br>server 2.centos.pool.ntp.org iburst<br>server 3.centos.pool.ntp.org iburst<br>在最下面加上<br>restrict default ignore<br>restrict <font color="red">192.168.1.0</font>   mask 255.255.255.0<br>nomodify notrap<br>server 127.127.1.0   </p><p><font color="red">注意：192.168.1.0是这3台机器ip地址的前3位，最后一位是0</font><br>#重启ntp服务<br>service ntpd restart<br>#设置ntp服务器开机自动启动<br>chkconfig ntpd on       </p><p><font color="red">只在node2和node3执行</font><br>#以下为客户端的配置（除node1外的node2和node3），设定每天00:00向服务器(node1)同步时间，并写入日志<br>crontab -e<br>#输入以下内容后保存并退出<br>0 0 <em> </em> <em> /usr/sbin/ntpdate <em>*node1</em></em>&gt;&gt; /root/ntpd.log   </p><p><font color="red">只在node2和node3执行</font><br>ntpdate node1</p><h2><span id="66-卸载机器自带的openjdk">6.6. 卸载机器自带的openjdk</span></h2><p>！！！！！！！一定要卸载    </p><h2><span id="67-安装jdk">6.7. 安装JDK</span></h2><p>安装自己的jdk到/opt/java/下面，如/opt/java/jdk1.8.0_90    </p><p><font color="red">只在node1上执行</font><br>首先使用filezilla把cdh_deployment压缩包上传到/usr下面，然后再opt下面创建一个java文件夹。<br>将jdk的安装包拷贝到/opt/java/下面<br>cp /usr/CDH_deployment/jdk-8u11-linux-x64.tar.gz /opt/java/<br>#解压<br>tar -zxvf jdk-8u11-linux-x64.tar.gz<br>#修改环境变量<br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH<br>#刷新配置文件<br>source /etc/profile<br>#复制jdk到其他服务器上<br>scp -r /opt/java/jdk1.8.0_11/ node2:/opt/java/<br>scp -r /opt/java/jdk1.8.0_11/ node3:/opt/java/      </p><p><font color="red">在node2执行</font><br>// WangBeibei-DC-2 上<br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH  </p><p><font color="red">在node3执行</font><br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH<br>测试java -version<br>看到java的版本说明安装成功  </p><p><font color="red">在每台上执行</font><br>mkdir /usr/java<br>ln -s /opt/java/jdk1.8.0_90/  /usr/java/default    </p><h2><span id="68-安装mysql">6.8. 安装MySQL</span></h2><p><font color="red">只在node1上执行</font><br>yum remove mysql mysql-server mysql-libs compat-mysql51<br>rm -rf /var/lib/mysql<br>rm -rf /etc/my.cnf<br>将mysql.jar拷贝到/usr/local下面<br>解压<br>tar -zxvf /opt/mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz<br>// 改名为mysql<br>mv mysql-5.6.37-linux-glibc2.12-x86_64 mysql<br>// 删除安装包<br>rm mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz<br>// 修改环境变量<br>vi /etc/profile<br>在最下面添加<br>export MYSQL_HOME=/usr/lcoal/mysql<br>export PATH=\$MYSQL_HOME/bin:\$PATH<br>// 刷新环境变量<br>source /etc/profile<br>将服务文件mysql.server拷贝到init.d下<br>cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql.server<br>MySQL开机自启，赋予可执行权限<br>chmod +x /etc/init.d/mysql.server<br>添加服务<br>chkconfig —add mysql.server<br>显示服务列表<br>chkconfig —list<br>如果看到mysql的服务，并且3、4、5都是on的话则成功。如果mysql.server的 3, 4, 5 不是on，使用下面的命令给他变成on:<br>chkconfig —level 345 mysql.server on<br>// 新建mysql 用户<br>groupadd mysql 在/etc/group 中可以看到<br>useradd -r -g mysql -s /bin/false mysql 在/etc/passwd 中可以看到<br>cd /usr/local/mysql<br>chown -R mysql:mysql .<br>scripts/mysql_install_db —user=mysql<br>// 修改当前目录拥有者为root 用户<br>chown -R root .<br>// 修改当前data 目录拥有者为mysql 用户<br>chown -R mysql data<br>// 新建一个虚拟窗口，叫mysql<br>screen -S mysql<br>bin/mysqld_safe —user=mysql &amp;<br>// 退出虚拟窗口<br>Ctrl+A+D<br>cd /usr/local/mysql<br>// 登陆mysql<br>bin/mysql<br>// 登陆成功后退出即可<br>exit;<br>// 进行root 账户密码的修改等操作<br>bin/mysql_secure_installation<br>首先要求输入root 密码，由于我们没有设置过root 密码，括号里面说了，如果没有root 密码就直接按回车。<br>是否设定root 密码，选y，设定密码为cluster，是否移除匿名用户：y。然后有个是否关闭root 账户的远程<br>登录，选n，删除test 这个数据库？y，更新权限？y，然后ok。<br>cp support-files/mysql.server /etc/init.d/mysql.server<br>// 进入mysql 虚拟窗口<br>screen -r mysql<br>// 查看mysql 的进程号<br>ps -ef | grep mysql<br>// 如果有的话就kill 掉，保证mysql已经中断运行了，一般kill 掉/usr/local/mysql/bin/mysqld 开头的即可<br>kill 进程号<br>// 关闭虚拟窗口<br>exit<br>// 启动mysql<br>/etc/init.d/mysql.server start -user=mysql<br>exit<br>还需要配置一下访问权限，#授权root用户在主节点拥有所有数据库的访问权限：<br>$ mysql -u root -p<br>mysql&gt; GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’%’ IDENTIFIED BY ‘cluster’ WITH GRANT OPTION;<br>mysql&gt; FLUSH PRIVILEGES;    </p><h2><span id="69-创建mysql数据库">6.9. 创建MySQL数据库</span></h2><p>MySQL中root账户执行：<br>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;   </p><h2><span id="610-安装cloudera-manager-server和agent">6.10. 安装Cloudera Manager Server和Agent</span></h2><p>（1） <font color="red">在node1执行</font><br>把cloudera-manager-centos7-cm5.7.2_x86_64.tar.gz的压缩包解压到/opt下面<br>（2）<font color="red">在node1执行</font><br>把mysql-connector-java-5.1.43-bin.jar复制到/opt/cm-5.7.2/share/cmf/lib/ 里面<br>（3）<font color="red">在node1执行</font><br>在主节点初始化CM5的数据库：<br>/opt/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh<br>mysql cm -h[mysql数据库的主机名]  -uroot -p[password] —scm-host [cm server的主机名] [cm的数据库] [cm数据库访问用户] [cm数据库访问用户的密码]<br>执行/opt/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -pcluster —scm-host localhost scm scm scm<br>（4）<font color="red">在node1执行</font><br>Agent配置<br>修改 vi /opt/cm-5.7.2/etc/cloudera-scm-agent/config.ini 这里面的server_host，改成自身的机器名node1，也就是指名主节点的机器名<br>（5）<font color="red">在node1执行</font><br>将cm-5.7.2的目录复制到其他机器上，同步Agent到其他节点<br>确保复制到所有的机器上<br>scp -r /opt/cm-5.7.2 root@node2:/opt/<br>scp -r /opt/cm-5.7.2 root@node3:/opt/<br>（6）<font color="red">在所有机器上</font><br>在所有节点创建cloudera-scm用户<br>执行<br>useradd —system —home=/opt/cm-5.7.2/run/cloudera-scm-server/ —no-create-home —shell=/bin/false —comment “Cloudera SCM User” cloudera-scm<br>（7）<font color="red">在node1执行</font><br>执行 mkdir -p /opt/cloudera/parcel-repo/<br>（8）<font color="red">在node1执行</font><br>把<br>CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel，<br>CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha，<br>manifest.json<br>这三个文件，复制到/opt/cloudera/parcel-repo/这里面<br>（9）<font color="red">在node1执行</font><br>ssh node2<br>mkdir /usr/share/java<br>把mysql-connector-java-5.1.43-bin.jar复制到/usr/share/java下，并命名为mysql-connector-java.jar<br>cp /opt/cm-5.7.2/share/cmf/lib/   mysql-connector-java-5.1.43-bin.jar /usr/share/java/   mysql-connector-java.jar   </p><p><font color="red">其中machine2是第二台机器，把这个machine2改成其他机器的名字，分别执行一遍</font><br>（10）<font color="red">在node1执行</font><br>执行启动服务端：<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-server start<br>执行启动Agent服务端：<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>启动其他机器的Agent<br>执行：<br>ssh node2<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>ssh node3<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>用这个ssh命令将其他所有机器的agent都启动      </p><p><font color="red">问题：在启动时出错<br>/opt/cm-5.7.0/etc/init.d/cloudera-scm-server start<br>/opt/cm-5.7.0/etc/init.d/cloudera-scm-server: line 109: pstree: command not found   </font><br><strong>解决方案</strong>：因为系统是最小化安装，默认没有安装，运行下面的命令。<br>yum -y install psmisc    </p><h2><span id="611-打开网页配置">6.11. 打开网页配置</span></h2><p>（1）打开浏览器，地址是：主节点的ip:7180，用户名和密码都是admin<br>（2）选第一个免费版！<br>（3）<a href="http://www.cnblogs.com/jasondan/p/4011153.html，" target="_blank" rel="noopener">http://www.cnblogs.com/jasondan/p/4011153.html，</a><br>然后按照那个博客里面的图片安装就行了<br>（4）安装服务的时候，错开机器，别把所有的服务都堆在前几台机器上，zookeeper要3个，安装的时候hive会报错，博客里面写了怎么解决，oozie也会报错，都是一样的解决方法，最好默认不要修改。<br>（5）按照博客可以完成CDH集群的安装<br>（6）问题：<br><img src="/2019/04/04/CDH集群/jdk.png" alt=""><br>解决方案：这里需要强调一下CDH5默认识别的jdk路径为：/usr/java/default<br>没有往/usr/java中添加软链接，而这里默认是去/usr/java/default中找环境变量，才会报找不到java_home。安装jdk的方法:把jdk软连接到/usr/java/default首先查看是否有/usr/java目录，没有的话新建此目录：mkdir /usr/java。然后添加软连接到/usr/java/default，命令如下: ln -s /opt/java/jdk1.8.0_11 /usr/java/default<br>问题：<br><img src="/2019/04/04/CDH集群/hive.png" alt=""><br>解决方案：这里安装Hive的时候可能会报错，因为我们使用了MySql作为hive的元数据存储，hive默认没有带mysql的驱动，通过以下命令拷贝一个就行了：<br>cp /opt/cm-5.7.2/share/cmf/lib/mysql-connector-java-5.1.43-bin.jar /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/hive/lib/</p><h2><span id="612-hdfs的ha安装">6.12. HDFS的HA安装</span></h2><p>HDFS HA的安装：<br><a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12_1，" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12_1，</a><br>看里面的Enabling High Availability and Automatic Failover。按操作安装完后，<a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp，" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp，</a><br>完成里面的Upgrading the Hive Metastore to Use HDFS HA和Configuring Hue to Work with HDFS HA<br>问题：Cloudera Manager （重新）部署集群报错：fail to format namenode<br>解决方案：<a href="https://www.jianshu.com/p/1e8b25e63ab9" target="_blank" rel="noopener">https://www.jianshu.com/p/1e8b25e63ab9</a></p><h2><span id="613-安装anaconda">6.13. 安装Anaconda</span></h2><p>首先创建一个file0的目录，在这个目录下面运行下面的命令：<br>bash Anaconda3-4.1.1-Linux-x86_64.sh<br>这样Anaconda3就安装在file0下面。<br><a href="https://blog.csdn.net/m0_37548423/article/details/81173678" target="_blank" rel="noopener">https://blog.csdn.net/m0_37548423/article/details/81173678</a>    </p><p>在所有机器上安装anaconda4.2.0，结尾最后一步，是否添加至环境变量，选择no<br>使用which python查看使用的是哪个版本的python，运行程序的时候要用<br>/file0/anaconda3/bin/python user.py    </p><h2><span id="614-安装sparkstandalone">6.14. 安装Spark（standalone）</span></h2><p>（1）选择这个添加服务，安装Spark (standalone)<br> <img src="/2019/04/04/CDH集群/spark1.png" alt=""><br>（2）点击spark<br><img src="/2019/04/04/CDH集群/spark2.png" alt=""><br>（3）点击配置<br> <img src="/2019/04/04/CDH集群/spark3.png" alt=""></p><p>（4）搜索栏输入spark-env.sh<br>export PYSPARK_PYTHON=/file0/anaconda3/bin/python<br>export PYSPARK_DRIVER_PYTHON=/file0/anaconda3/bin/ipython<br>export PYTHONHASHSEED=0<br><img src="/2019/04/04/CDH集群/spark4.png" alt=""></p><p>找到这个服务高级配置的代码段，改成这个样子，把python的路径指名为anaconda的python路径<br>（5）按照上一步把 Spark (standalone) 的spark-env也改成上面的样子   </p><h2><span id="615-修改hdfs权限">6.15. 修改hdfs权限</span></h2><p>主节点执行：<br>sudo -u hdfs hdfs dfs -mkdir /user/root<br>sudo -u hdfs hdfs dfs -chown root /user/root<br>sudo -u hdfs hdfs dfs -chmod -R 777 /user    </p><h2><span id="616-安装python三方库">6.16. 安装python三方库</span></h2><p>主节点执行：<br>/file0/anaconda3/bin/pip install<br>—index-url=file:///file0/CDH_deployment/pypi/simple pymysql happybase pykafka    </p><h2><span id="617-安装kafka">6.17. 安装kafka</span></h2><p>Kafka的安装过程参照：<br><a href="https://jingyan.baidu.com/article/e9fb46e139dead7521f7662e.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/e9fb46e139dead7521f7662e.html</a><br>里面要求下载的四个文件我已经下载好了，其中一个叫manifest.json的文件，我重命名为了manifest_kafka.json，这个重命名是因为防止和之前的那个manifest冲突，直接按照第18步，将两个kafka的parcel文件和这个manifest拷到那个目录里面就行，按照博客里面的要求安装即可。  </p><p><font color="red">注意：只设置kafka broker，不设置Kafka MirrorMaker<br>安装的时候会配置kafka，配置完后会启动kafka，kafka一定启动不了，右下角有个重试按钮，这时候需要再开一个管理界面，像上面配置spark一样，配置kafka，如下图 </font><br><img src="/2019/04/04/CDH集群/kafka1.png" alt=""><br>分别点进三个超链接，选择左上角的配置，针对每个broker进行配置。<br>需要配置的是两项：</p><ol><li>Broker ID：可以机器顺序分别改成1，2，3</li><li>Java Heap Size of Broker：改成1G<br>然后回到刚才安装的那个界面，点击重试。<h2><span id="yarn">yarn</span></h2>CDH 5.9 以前的版本，如果使用 python3 且使用 yarn 作为master，需手动修复CDH 集群的bug。CDH 5.9 以前的版本在使用 yarn 作为 spark master 的时候，如果使用 python3，会出现 yarn 內部 topology.py 这个文件引发的 bug。这个文件是 python2 的语法，我们使用 python3 运行任务的时候，python3 的解释器在处理这个文件时会出错。<br>解决方案是：将这个文件重写为 python3 的版本，每次在重启 yarn 之后，将这个文件复制到所有机器<br>的 /etc/hadoop/conf.cloudera.yarn/ 目录下。<br><strong>以下操作在所有机器上都要操作，并使用root用户，不可以使用普通用户。如果/etc/hadoop/conf.cloudera.yarn目录不存在，先创建一个同名目录，然后将topology.py复制到该目录下</strong>。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;最近去给甲方安装CDH集群，对于集群的搭建和测试在这里总结一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CDH集群、Linux" scheme="http://yoursite.com/tags/CDH%E9%9B%86%E7%BE%A4%E3%80%81Linux/"/>
    
  </entry>
  
</feed>
