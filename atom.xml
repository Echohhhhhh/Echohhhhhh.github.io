<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Echo&#39;s blog</title>
  
  <subtitle>远方到底有多远</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-26T01:58:55.077Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Echo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/"/>
    <id>http://yoursite.com/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/</id>
    <published>2019-06-24T00:43:06.723Z</published>
    <updated>2019-06-26T01:58:55.077Z</updated>
    
    <content type="html"><![CDATA[<hr><h2><span id="">{}</span></h2><h1><span id="1-简介">1. 简介</span></h1><p><a href="http://delivery.acm.org/10.1145/3320000/3313730/p717-huang.pdf?ip=218.247.253.153&amp;id=3313730&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1561380383_40e3bd8d678088e9b04173b89f85c49c" target="_blank" rel="noopener">论文出处</a><br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-Keywords">2. Keywords</a></li><li><a href="#3-Abstract">3. Abstract</a></li><li><a href="#4-Introduction">4. Introduction</a></li><li><a href="#5-Problem-Formulation">5. Problem Formulation</a><ul><li><a href="#51-Preliminaries">5.1. Preliminaries</a></li><li><a href="#52-Framework-Overview">5.2. Framework Overview</a></li></ul></li><li><a href="#6-Methodology">6. Methodology</a><ul><li><a href="#61-Context-aware-Recurrent-Framework">6.1. Context-aware Recurrent Framework</a></li><li><a href="#62-Multi-Modal-Pattern-Fusion-Module">6.2. Multi-Modal Pattern Fusion Module</a></li><li><a href="#63-Conclusive-Recurrent-Network">6.3. Conclusive Recurrent Network</a></li><li><a href="#64-Forecasting-and-Model-Inference">6.4. Forecasting and Model Inference</a></li></ul></li><li><a href="#7-Evaluation">7. Evaluation</a><ul><li><a href="#71-Data-Description">7.1. Data Description</a><ul><li><a href="#711-Data-Statistics">7.1.1. Data Statistics</a></li></ul></li><li><a href="#72-Experimental-Setting">7.2. Experimental Setting</a><ul><li><a href="#721-Parameter-Setting">7.2.1. Parameter Setting</a></li><li><a href="#722-Baseline-Methods">7.2.2. Baseline Methods</a></li><li><a href="#723-Evaluation-Protocols">7.2.3. Evaluation Protocols</a></li></ul></li><li><a href="#73-Performance-Comparison">7.3. Performance Comparison</a><ul><li><a href="#731-Overall-ComparisonQ1">7.3.1. Overall Comparison(Q1)</a></li><li><a href="#732-Forecasting-Accuracy-vs-Time-PeriodQ2">7.3.2. Forecasting Accuracy v.s Time Period(Q2)</a></li><li><a href="#733-Forecasting-Accuracy-vs-CategoriesQ3">7.3.3. Forecasting Accuracy v.s Categories(Q3)</a></li></ul></li><li><a href="#74-Component-Wise-Evaluation-of-MiSTQ4">7.4. Component-Wise Evaluation of MiST(Q4)</a></li><li><a href="#75-Effect-of-Spatial-and-Temporal-ScaleQ5">7.5. Effect of Spatial and Temporal Scale(Q5)</a></li><li><a href="#76-Hyperparameters-StudiesQ6">7.6. Hyperparameters Studies(Q6)</a></li><li><a href="#Case-StudyQ7">Case Study(Q7)</a></li></ul></li><li><a href="#Conclusion">Conclusion</a></li></ul><!-- /TOC --><h1><span id="2-keywords">2. Keywords</span></h1><p><strong>异常事件预测、深度神经网络、时空数据挖掘</strong> </p><h1><span id="3-abstract">3. Abstract</span></h1><p>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[应用]</font>城市异常事件，比如犯罪、事故，如果不及时处理的话，会造成人员和财产的损失。如果异常事件能在发生之前自动被预测出来，对很多领域都有重要意义，比如公共秩序维护、灾难控制和人的活动建模。<font color="#FF0000">[挑战]</font>然而，预测不同类型的城市异常事件是非常有挑战的，因为它被很多复杂的因素影响。(i)区域内动态的时间关系；(ii)区域间复杂的空间关系；(iii)潜在的类别之间的关系。<font color="#FF0000">[模型]</font>在这篇论文中，我们研究了一个<strong>Multi-View and Multi-Modal Spatial-Temporal learning多视角和多模态的时空学习框架(MiST)</strong> 来解决以上的挑战，通过增强不同视角（空间、时间和语义）的相关性，和将多模态单元映射到相同的潜在空间。特别的，将多模态模式融合架构和分层循环框架进行整合，MiST可以保留多视角异常事件数据的潜在的结果信息，和自动地学习特定视角表示的重要性。在三个真实数据集上的实验，例如：犯罪数据和城市异常数据，表明我们MiST模型比其他先进的模型效果都好。   </p><h1><span id="4-introduction">4. Introduction</span></h1><p>&ensp;&ensp;&ensp;&ensp;城市异常事件，比如犯罪(抢劫、袭击)和城市异常(道路封锁、噪声)如果不及时处理，对公共安全有很大的风险。据统计，异常造成了很大的损失，因为准确和可靠的预测异常事件是数据驱动的决策者用于减少人和经济的损失迫切的需求。<font color="#FF0000">[应用]</font>例如，在灾难控制中，通过预测未来的异常事件，当地政府可以设计更好的交通规划和移动管理策略来防止严重的社会骚乱。此外，在公共秩序维护上，了解城市每个区域的异常事件潜在的发生模式对人们活动建模和地方推荐任务是非常重要的。在这篇论文中，我们旨在提前预测城市中不同区域不同类型的异常事件，为社会福利给予重大的提高。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[前人工作]</font>前人已经有一些研究关于使用时空数据检测地理异常。大部分这些研究都是通过分析被研究对象的历史轨迹和移动模式，使用统计和数据挖掘的方法来发现异常事件。然而，这些方法并不是预测将来的时间，而是在它们发生之后鉴定是不是异常事件，这会造成信息延迟和缺乏异常处理的提前准备。<br>&ensp;&ensp;&ensp;&ensp;从多个角度，我们确定了建模这种异常事件数据的三个挑战。<font color="#FF0000">[挑战]</font><br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[考虑空间关系]</font>第一，在城市中异常事件的分布是变化的，并且不同区域异常事件的分布是不同的。在这种情况下，异常事件的发生不再是区域独立的，在预测异常事件时，考虑不同区域的空间关系是非常重要的。并且，当建模动态空间关系对时，概率图模型将不再有效，由于概率图模型基于先验假设分布有很多的参数，涉及大量的计算。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[时间动态依赖]</font>第二，异常事件的发生模式经常涉及到随时间变化的潜在因素。例如，工作日的犯罪因果性和周末可能不同。传统的时间序列预测技术，像ARIMA和SVR被限制在线性模型，它仅依赖于单级周期模式。因此，这些方法很难在时间动态上预测异常事件。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[不同类别的异常事件间相互影响]</font>第三，不同类别的异常事件有着显示和隐示的影响。例如，一个区域的抢劫可能会引发该区域的交通堵塞，由于人群的聚集和巡逻的增加。因此，一种类别异常事件的发生不仅仅来源于不同区域之间的空间关系和时间槽之间的时间依赖，还可能来源于不同类别异常事件的相互影响。<br>&ensp;&ensp;&ensp;&ensp;<font color="#FF0000">[模型3个阶段]</font>受以上挑战的启发，该工作提出了一个通用且灵活的框架：Multi-View Deep Spatial-Temporal Networks(MiST)，从多视角异常事件数据的关系中学习预测结构。特别的，在第一阶段，我们提出了上下文感知context-aware的循环框架从不同的角度来捕获异常事件数据的时间动态性，并且自动提供了某个视角的表示。在第二阶段，为了将区域间的关系、不同类别间的影响和已编码的多维度数据的时间模式整合起来，我们基于attention机制提出了一个模式融合模块，来促进不同视角的融合，并且在预测模型的相应视角，自动地捕获关联区域、时间槽、类别的贡献。为了增强MisT模型的时间序列结构信息和非线性，在最后阶段设计了一个总结性的循环网络模块，对融合嵌入向量的序列模式进行建模。最终的总结潜在表示被喂入一个全连接神经网络来预测未来时间槽的异常事件。<br>&ensp;&ensp;&ensp;&ensp;综上所述，我们贡献主要是：</p><ul><li>我们引入了一个新的多视角和多模态时空学习框架MiST来预测一个城市每个区域不同类型的异常事件。MiST映射所有的空间时间和语义单元到一个潜在空间来保留它们跨模态的相关性。</li><li>我们提出了一个多模态融合模型，和分层循环框架，学习共享在多视角数据中潜在的区域-时间-类别关系，并且自动地调整每个视角中的相关性，以协助预测任务。</li><li>在三个真实世界异常事件数据集，从NYC和Chicago收集的数据集进行试验，MiST一直比其他state-of-the-art方法效果好。<h1><span id="5-problem-formulation">5. Problem Formulation</span></h1>&ensp;&ensp;&ensp;&ensp;在这一节，首先引出preliminary和problem。<h2><span id="51-preliminaries">5.1. Preliminaries</span></h2></li><li>定义1 Geographical Region(地理区域)<br>把城市进行网格分区。划分成$I \times J$,有$I$行$J$列，带有经纬度信息。每一个网格被视为一个地理区域，表示为$r_{i,j}$，其中$i和j$是分别是行和列的索引。在这篇论文中，我们使用区域作为最小单元来研究异常事件预测问题。<br>&ensp;&ensp;&ensp;&ensp;我们定义地理区域集合$R=(r_{1,1},…,r_{i,j},…,r_{I,J})$,并且假设有$L$个异常事件类别，$C=(c_1,…,c_l,…,c_L)$,其中$C$表示异常事件类别集合，下标为$l$。给定一个时间窗口$T$,我们分割$T$为不重叠且连续的时间槽$(T=(t_1,…,t_k,…,t_K))$,其中$K$表示时间槽的个数，索引是$k$.<br><strong><script type="math/tex">区域R是I \times J;异常事件类别C，有L个值;     时间T，有K个时间槽</script></strong></li><li>定义2 Abnormal Event Data Source(异常事件数据源)<br>假设一个区域$r_{i,j}$，使用$Y_{i,j}=(y^1_{i,j},…,y^l_{i,j},…,y^L_{i,j}) \in \mathbb{R}^{L \times K}$来表示在区域$r_{i,j}$过去$K$个时间槽发生的所有类型的异常事件。对于$y^l_{i,j} \in \mathbb{R}^K$表示区域$r_{i,j}$在类别$c_l$上从时间$t_1到t_K$的值。在$y^l_{i,j}$中，每一个元素$y^{l,k}_{i,j}$为1如果在区域$r_{i,j}$在时间$t_k$中有类别$c_l$异常事件发生，否则为0。<br>即$Y_{i,j}$是一个矩阵，一共有$L行K列$，每一个元素非0即1，其中每一行表示一种类别，每一列表示一个时间段。</li><li><strong>Problem Statement</strong><br><font color="#FF0000">[任务]</font>给定一个城市区域$R$时间从$t1到t_K$,所有异常事件类别的数据源$Y$，其中$Y$有$I \times J$个矩阵，每个矩阵都是$\mathbb{R}^{L \times K}$。目标是学习一个预测框架来推断一个区域$r_{i,j}$在未来$h$个时间槽，异常事件类别$c_l$是否发生。即计算$(y^{l,(K+h)}_{i,j}|Y_{i,j}=(y^1_{i,j},…,y^L_{i,j}));i,j \in [1,…,I],[1,…,J]$。即给定一个区域历史$K$个时间槽所有类别异常事件发生地数据，来预测这个区域在未来第$K+h$个时间槽，类别$l$事件是否发生，即输出结果是0/1。 </li></ul><h2><span id="52-framework-overview">5.2. Framework Overview</span></h2><p>&ensp;&ensp;&ensp;&ensp;我们提出的MiST模型是一个多层表示学习框架，如figure1所示。在详细介绍模型之前，首先介绍一下模型的输入，然后详细介绍设计的动机。</p><ul><li>定义3 Event Context Tensor(事件上下文张量)<br>给定一个目标区域$r_{i,j}$，使用event context tensor$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$，对这个区域的邻近区域在时间段$t_k$中不同类别的异常事件进行建模。$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$，有3个维度，分别表示$I行J列L个类别$。给定一个时间槽$t_k,\mathcal{A}^k_{i,j,l}$为1如果？？？？？,</li><li><strong>Context-aware Recurrent Framework</strong><br>为了从时间角度，就异常事件分布的动态属性方面表示区域内的相关性，我们提出了基于LSTM的context encoder，将每个时间槽的$\mathcal{A}$展开形成的向量中的每个元素，学习一个潜在表示。从我们的LSTM encoder中学习到的表示，可以对异常事件的时间依赖特性建模，还可以捕获异常事件的局部时间上下文和多层周期模式。  </li><li><strong>Multi-Modal Pattern Fusion Mudule</strong><br>为了捕获异常事件分布，在区域间和不同类别的关系，我们提出了深度融合模块，用于同时对周围地理区域和不同类别的异常事件的固有发生模式进行建模。我们将$K(表示K个时间槽)$个张量$\mathcal{A}^k_{i,j} \in \mathbb{R}^{I \times J \times L}$按照时间进行排序，然后对于每一个时间槽$t_k$都有一个张量$\mathcal{A}^k_{i,j}$,将它的隐藏向量表示，应用attention机制，从空间-类别视图生成summarized嵌入向量。</li><li><strong>Conclusive Recurrent Networks</strong><br>依赖从空间-时间-类别视图生成的隐藏表示，我们提出一个conclusive recurrent networks来有效地捕获位置、时间、类别多模态的序列模式。最终的spatial-temporal-categorical多视图序列表示被保存在conclusive recurrent network单元格的最终状态，在解码阶段为预测异常发生的概率提供了指导。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure1.png" alt=""><br>输入的数据X是(异常类别，经纬度，时间戳)。先选中一个目标区域$r_{i,j}$，找出这个区域的邻居$r_{i\prime,j\prime} \in G(i,j)$，然后对于一种类别的异常事件，使用LSTM，每一步$t_k$的输入为$X[i^\prime,j^\prime,l^\prime,t^\prime]$。输入到LSTM的是目标区域$t_{i,j}$的邻居，每一个邻居在一个类别一个时间段都会产生一个隐藏状态。首先我们先确定一个邻居，再确定一个异常事件类别，在K个时段段中使用LSTM，每一个时间段都会产生一个隐藏状态。等到该目标区域的所有邻居都遍历结束后，？？？<h1><span id="6-methodology">6. Methodology</span></h1><h2><span id="61-context-aware-recurrent-framework">6.1. Context-aware Recurrent Framework</span></h2>&ensp;&ensp;&ensp;&ensp;在MiST架构中，在异常事件在时间槽$t_1到t_k$的分布，我们首先采用LSTM网络来编码复杂的的区域内相关性。特别，LSTM包含1个记忆细胞状态和3个控制门通过分别执行写、读、重置操作来更新记忆细胞状态。用公式表示，区域$r_{i,j}$和异常类别$c_l$在第$t$个时间槽的隐藏状态$h^t_{i,j,l}$和记忆细胞状态$c^t_{i,j,l}$计算公式如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;其中$W_<em> \in \mathbb{R}^{d_s \times d_s}$表示前一个状态$(i.e., c^{t-1}_{i,j,l} \quad and \quad h^{t-1}_{i,j,l})$到当前状态的转换矩阵，$V_</em> \in \mathbb{R}^{d_x \times d_s}$是从输入到当前状态的转换矩阵，$d_x和d_s$分别表示输入向量的维度和隐藏状态的维度，且$b_<em> \in \mathbb{R}^{d_s}$是偏置向量，$\sigma(.)和\phi(.)$分别表示sigmoid和tanh函数。$\odot$表示元素相乘。分别使用$i^t_{i,j,l},o^t_{i,j,l},f^t_{i,j,l}$表示输入门、输出门、遗忘门。为了简单起见，我们用$h^t_{i,j,l}=LSTM(</em>,c^{t-1}_{i,j,l},h^{t-1}_{i,j,l})$表示上面的式1。当然也存在RNN的一些变体，例如GRU。<h2><span id="62-multi-modal-pattern-fusion-module">6.2. Multi-Modal Pattern Fusion Module</span></h2>&ensp;&ensp;&ensp;&ensp;然后直接或间接地应用RNN来解决异常事件预测问题是直观的。一般的RNN不能处理来自其他地理区域和时间类别的影响因素。因此我们进一步使用attention机制来自适应地捕获空间和类别的动态相关性。Attention机制用来推断训练集不同部分的重要性，让学习算法更加关注重要的部分。Attention机制引入一个context vector建模相关性，让编码器-解码器摆脱定长的内部表示。并且，在融合过程中，为了区分区域和类别，用$e_{r_{i,j}} \in \mathbb{R}^{d_e}$表示区域嵌入，用$e_{c_j} \in \mathbb{R}^{d_e}$表示类别嵌入，这两种嵌入在attention机制中会用到。attention的计算公式如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/2.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;在attention网络中将隐藏表示向量的大小作为attention dimensionality，用$S$表示，其中$d_s$表示LSTM中隐藏状态的维度。$W^k \in \mathbb{R}^{d_s \times S} \quad b^k \in \mathbb{R}^{d_s}$分别表示权重矩阵和偏置向量，将输入映射到隐藏层，得到$\eta^k_{i,j,l}$作为$h^k_{i,j,l}$的隐藏表示。然后我们度量了每个区域$r_{i,j}$每种类别$c_l$的隐藏表示$\eta^k_{i,j,l}$的重要性，归一化得到$\alpha^k_{i,j,l}$。attention中的权重由输入的空间-类别特征$e_{r_{i,j}} \in \mathbb{R}^{d_e},e_{c_j} \in \mathbb{R}^{d_e}$联合决定，在Context-LSTM编码器中编码历史隐藏状态$h^k_{i,j,k}$。在获取attention权重后，在时间段k的输出隐藏表示向量计算如下：<script type="math/tex; mode=display">q^k = \sum_{i,j \in G}\sum_{l=1}^{L} \alpha^k_{i,j,l}h^k_{i,j,l} \tag{3}</script>&ensp;&ensp;&ensp;&ensp;其中$q^k$是$h^k_{i,j,l}$的summarized拼接表示，描述了在区域$r_{i,j}$异常事件的发生，哪个因素更重要。在MiST的训练过程中，带有attention机制的深度融合模块被参数化为前向神经网络，和整个神经网络一起训练。我们提出的方法是非常通用的，可以自动学习不同视图的相关性权重。<h2><span id="63-conclusive-recurrent-network">6.3. Conclusive Recurrent Network</span></h2>&ensp;&ensp;&ensp;&ensp;目前为止，我们已经研究了MiST到的2个组件，(i)从temporal角度，使用context-LSTM建模区域内动态的相关性；(ii)从spatial-categorical角度，使用深度融合模块捕获复杂的区域间和类别见的相关性。经过以上步骤，得到了summarized representation $q^k$，从不同角度使用不同的权重$\alpha^k_{i,j,l}$计算组合表示。<br>&ensp;&ensp;&ensp;&ensp;为了将空间-类别的编码pattern和时间pattern整合在一起，我们提出了用循环神经网络编码多维模式，用潜在空间的表示建模location-time-category之间的关系。在这篇论文中，我们采用LSTM作为循环神经单元，公式如下：<script type="math/tex; mode=display">\xi_k = LSTM(q_{k-1},\xi_{k-1}) \tag{4}</script>&ensp;&ensp;&ensp;&ensp;联合嵌入$\xi$将所有的空间、时间、类别单元映射到一个共同的潜在空间中。提出的conclusive循环神经网络提供了一种灵活的方式让不同的视图彼此合作。将空间、类别上下文信号和时间状态结合，MiST框架可以预测将来异常事件，不仅仅根据时间序列关系，还根据区域间的空间关系和不同类别的共现关系。<h2><span id="64-forecasting-and-model-inference">6.4. Forecasting and Model Inference</span></h2>&ensp;&ensp;&ensp;&ensp;最终，我们利用MLP来解码异常事件出现的概率，通过捕获隐藏向量元素之间的非线性依赖。公式如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/5.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;其中，$N$表示隐藏层的个数，对于层$\psi_n$，$W_n$和$b_n$表示权重矩阵和偏置向量。我们使用$ReLU,\phi(.)$为全连接层的激活函数。使用$\sigma(.)sigmoid$作为输出层的激活函数，值域在(0,1),输出异常事件发生的概率，在区域$r_{i,j}$时间槽$t_k$异常事件类别$c_l$，例如$y^{l,k}_{i,j}$。<br>&ensp;&ensp;&ensp;&ensp;综上所述，我们的异常事件发生预测可以被看做是一个分类问题。我们利用叫啥上作为损失函数。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/6.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;其中，$\hat{y}^{l,k}_{i,j}$表示预测的在区域$r_{i,j}$第$k$个时间段发生第$l$个异常类别事件的概率，$S$是训练集中异常事件的集合。使用Adam优化器来学习参数。<br>算法流程如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/algo.png" alt="">  <h1><span id="7-evaluation">7. Evaluation</span></h1>在三个真实异常事件数据集上做了实验，数据从NYC和Chicago收集，验证模型的有效性和准确率和其他baseline，通过实验回答以下几个问题</li><li>Q1：和其他state-of-the-art预测方法，在预测全市犯罪和不同城市的异常情况时，MiST可以达到与之媲美的准确率吗？</li><li>Q2：在不同的时间段中，MiST一直比其他的算法表现好吗？</li><li>Q3：和其他state-of-the-art技术相比，MiST模型怎么预测不同种类的异常事件</li><li>Q4：MiST使用不同关键组件的组合形成的变体效果怎么样？</li><li>Q5：MiST在不同的空间和时间范围上表现怎么样？</li><li>Q6：不同的参数设置怎么影响MiST的预测效果？</li><li>Q7：当预测城市异常事件时，怎么解释MiST框架捕获的空间和类别维度的动态重要性权重？<h2><span id="71-data-description">7.1. Data Description</span></h2><h3><span id="711-data-statistics">7.1.1. Data Statistics</span></h3>&ensp;&ensp;&ensp;&ensp;我们从NYC和Chicago收集了2种类型的3个异常事件数据，有2个犯罪数据和1个城市异常数据，通过做实验，预测城市的每个区域发生每种城市犯罪和异常事件的可能性。数据集基本统计如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/table1.png" alt=""><br>在我们的实验中，我们重点关注了一些关键类别，把其他的类别看做外部类别。我们也给了不同类型和时间周期的异常事件在地理上的分布，如Figure2所示。</li><li>NYC Crime Data(NYC-C)：这个数据集中有多个类别的犯罪记录。每一个犯罪记录有犯罪类别、经纬度、时间。时间跨度为2015.1~2015.12</li><li>NYC Urban Anomaly Data(NYC-A)：这个数据集时间跨度为2014.1~2014.12，从NYC311个非紧急服务中心收集来的，这里记录了不同类别的城市异常。每个记录都有异常类别、经纬度、时间。</li><li>Chicago Crime Data(CHI-C)：从芝加哥收集的2015.1~2015.12不同种类的犯罪记录，记录的个数和NYC类似。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure2.png" alt=""> <h2><span id="72-experimental-setting">7.2. Experimental Setting</span></h2><h3><span id="721-parameter-setting">7.2.1. Parameter Setting</span></h3>&ensp;&ensp;&ensp;&ensp;在我们的试验中，利用Adam作为优化器，使用Tensorflow实现MiST架构。在LSTM中设置隐藏状态维度$d_s=32$，区域嵌入向量$e_{r_{i,j}}$和类别嵌入向量$e_{c_j}$的维度$d_e=32$，attention的维度$S=32$，MLP的层数为3。batch size=64，学习率=0.001。<h3><span id="722-baseline-methods">7.2.2. Baseline Methods</span></h3></li></ul><p>(i)传统的时间序列预测方法：SVR、ARIMA<br>(ii)传统的有监督学习算法：LR<br>(iii)循环神经网络和它的变体for时空数据预测：ST-RNN 、GRU<br>(iv)先进的神经网络模型for 时间序列和序列模型：RDN、HRN、ARM</p><h3><span id="723-evaluation-protocols">7.2.3. Evaluation Protocols</span></h3><p>&ensp;&ensp;&ensp;&ensp;在实验中，按照时间顺序将数据集划分为训练集(6.5个月)、验证集(0.5个月)和测试集(1个月)。验证集被用来调整超参数，在测试集上进行性能比较。我们把NYC和Chicago划分为248和189个互不相交的区域，每个区域的大小$2km \times 2km$，根据区域划分的结果，我们可以映射每个异常事件(犯罪或城市异常)到一个地理区域中，作为MiST的输入。我们采用2种评价指标来衡量所有的方法。</p><ul><li>(i)使用Macro-F1 和Micro-F1来衡量不同种类犯罪的预测准确率。这2个指标表示了不同类别之间的整体效果。这2个指标的数据定义如下：<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/micro.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/macro.png" alt=""><br>其中$J$是异常事件的种类数。这2个值越高效果越好</li><li>(ii) 使用F1-score和$AUC$来衡量预测一个类别的异常事件发生的准确率。F1和AUC越高，说明预测效果越好。<br>&ensp;&ensp;&ensp;&ensp;为了确保所有方法的性能公平比较，在测试集中预测一段时间连续几天异常事件发生的概率。在评估结果中，一段时间所有天的平均性能作为最终的结果。<h2><span id="73-performance-comparison">7.3. Performance Comparison</span></h2><h3><span id="731-overall-comparisonq1">7.3.1. Overall Comparison(Q1)</span></h3><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/table2.png" alt=""></li></ul><p>&ensp;&ensp;&ensp;&ensp;表2显示了不同城市犯罪和城市异常的预测准确率。总结以下3点：<br>&ensp;&ensp;&ensp;&ensp;第一：MiST比其他神经网络方法效果都好。例如，在预测Chicago犯罪时，MiST比最好的模型RDN Macro-F1和Micrl-F1高9.6%和30.9%。<br>&ensp;&ensp;&ensp;&ensp;第二：神经网络方法比传统的时间序列和有监督学习方法效果好。这是由于（1）传统的时间序列预测方法仅仅强调一个固定的时间模式，而不是时间依赖的演变。（2）神经网络方法使用非线性方法捕获多维空间-时间数据的内在结构，这非常有用。<br>&ensp;&ensp;&ensp;&ensp;第三：在循环神经网络中(ST-LSTM和GRU)和深度序列数据模型方法(RDN、HRN、ARM)效果不分上下。这再一次验证了仅仅考虑时间维度的数据依赖在预测犯罪和城市异常发生时不够的。相反，MiST动态关联潜在的空间、时间、类别的关系，表现了很好的灵活性和优越性。</p><h3><span id="732-forecasting-accuracy-vs-time-periodq2">7.3.2. Forecasting Accuracy v.s Time Period(Q2)</span></h3><p>&ensp;&ensp;&ensp;&ensp;对于MiST和其他的baseline，在不同的训练和测试时间段上做了实验。我们发现MiST在不同的测试时间段上一直保持最好的效果。并且也可以发现在MiST和起亚baseline相比，当滑动训练集和测试集的时间窗口时，MiST的效果更稳定，这说明MiST在学习随着时间动态的异常事件分布时更健壮。</p><h3><span id="733-forecasting-accuracy-vs-categoriesq3">7.3.3. Forecasting Accuracy v.s Categories(Q3)</span></h3><p>&ensp;&ensp;&ensp;&ensp;我们测试了MiST在预测单个异常类别事件的有效性，在NYC的犯罪和异常数据、Chicago的犯罪数据集上，结果如figure3和4所示。发现MiST在所有的类别上都取得了最好的效果。另一个发现是MiST在预测building/Use时效果比ST-RNN高了84.1%左右，这说明MiST在预测稀疏异常类别时表现也很好，解决了数据稀疏问题。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure3.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure4.png" alt=""></p><h2><span id="74-component-wise-evaluation-of-mistq4">7.4. Component-Wise Evaluation of MiST(Q4)</span></h2><p>为了更的理解MiST，对MiST的不同组件进行组合做了实验。</p><ul><li><strong>Spatial-View+Temporal View</strong> $MiST-st$<br>这个变体捕获了空间和时间依赖，不考虑类别的影响</li><li><strong>Category-view+Temporal View</strong>$MiST-ct$<br>这个变体考虑了累呗和时间依赖，不考虑区域间的空间相关性</li><li><strong>Temporal View</strong>$MiST-t$<br>这个变体仅仅使用LSTM和时间attention机制，不考虑空间和类别。      </li></ul><p>&ensp;&ensp;&ensp;&ensp;结果显示使用全部的组件效果最好，这说明使用一个联合框架是很有必要的，同时捕获空间视图（区域间的空间相关性）、时间视图（区域内的时间相关性）、类别视图（类别间的依赖）。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure5.png" alt="">   </p><h2><span id="75-effect-of-spatial-and-temporal-scaleq5">7.5. Effect of Spatial and Temporal Scale(Q5)</span></h2><p>&ensp;&ensp;&ensp;&ensp;进一步研究了空间和时间范围的影响。在event context tensor$\mathcal{A}$中，网格地图的地理范围$G=I \times J$，在我们的实验中$I=J$，循环框架中时间序列长度为$T$。 在十月份的Crime上做了实验，实验结果如图6所示：2个结论，（1） 随着I和J的增大，实验效果也变好。因为每个网格是$2km \times 2km$，I和J增大，说明考虑了更大的地理区域在学习表示时，当I和J为11时，准确率趋于稳定。另一个可能的原因是当考虑更大的地理区域时，需要学习更多的参数，训练MiST更加困难。（2）当时间序列长度$T$变大时，准确率也变得更好。当T=10时趋于稳定。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure6.png" alt="">   </p><h2><span id="76-hyperparameters-studiesq6">7.6. Hyperparameters Studies(Q6)</span></h2><p>&ensp;&ensp;&ensp;&ensp; 为了检验MiST模型的健壮性，设置不同的超参数看预测效果。除了被测试的参数外，其余参数都被设置为默认值。 总体上，发现MiST在两个任务上（预测NYC犯罪和异常事件）对参数不敏感，并且都能达到很好的效果，说明MiST模型的健壮性。并且发现当表示的维度为32时，效果最好。这是因为刚开始，潜在表示的维度变大能够为循环框架和Attention框架提供一个更好的表示，随着参数的增加，可能会造成过拟合。在我们的实验中，为了权衡有效性和计算代价，将表示维度设置为32。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure7.png" alt=""><br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure8.png" alt=""> </p><h2><span id="case-studyq7">Case Study(Q7)</span></h2><p>&ensp;&ensp;&ensp;&ensp;MiST除了有很好的预测性能，并且在预测一个区域特定类别的异常时间时，能很好的解释空间和类别相关性的重要性。为了说明这点，我们做了实验说明模型的可解释性，在预测NYC盗窃事件时，在一个$5\times 5$的网格中，中间的区域表示目标区域，将attention权重可视化。说明MiST能够动态建模目标区域和其他区域的相关性，并且可以动态建模目标区域的异常类别事件（盗窃）和其他类别的关系。<br><img src="/2019/06/24/MiST-A-Multiview-and-Multimodal-Spatial-Temporal-Learning-Framework-for-Citywide-Abnormal-Event-Forecasting/figure9.png" alt="">   </p><h1><span id="conclusion">Conclusion</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文提出了一个新的神经网络架构MiST，从空间-事件-类别维度对城市异常事件的动态模式进行建模。我们整合了循环神经网络和多模态融合模块来建模空间-事件的相关性。在不同的真实数据集上评测模型，结果显示MiST比其他baseline效果都好。关于我们工作的未来方向。第一，检测不同类别的异常事件发生的因果关系，这对公共政策的制定有用。发现异常事件发生的潜在因素，以及不同类别的异常事件在时空上怎么传播。第二，由于数据的限制，我们只在3个真实数据集上做了实验，实际上，MiST通用且灵活，可以应用到其他多维且有时间戳的序列数据上。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;{}&quot;&gt;&lt;/a&gt;{}&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3320000/3313730/p717-huang.pdf?ip=218.247.253.153&amp;amp;id=3313730&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=BF85BBA5741FDC6E%2EB8E1436BD1CE5062%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1561380383_40e3bd8d678088e9b04173b89f85c49c&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文出处&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/06/12/Docker/"/>
    <id>http://yoursite.com/2019/06/12/Docker/</id>
    <published>2019-06-12T10:48:24.562Z</published>
    <updated>2019-06-23T12:03:17.854Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: Docker<br>date: 2019-06-12T18:48:24.000Z<br>tags:</p><h2><span id="-docker-镜像">  - Docker、镜像</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖到一个可移植的镜像中，然后发布到Linux或Window系统中，也可以实现虚拟化。容器是完全使用沙箱机制，相互之前不会有任何接口。<br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-Docker">2. Docker</a></li><li><a href="#3-%E9%95%9C%E5%83%8F%E5%AE%B9%E5%99%A8">3. 镜像&amp;容器</a><ul><li><a href="#31-%E9%95%9C%E5%83%8F">3.1. 镜像</a></li><li><a href="#32-%E9%95%9C%E5%83%8F%E5%91%BD%E4%BB%A4">3.2. 镜像命令</a></li><li><a href="#33-%E5%AE%B9%E5%99%A8">3.3. 容器</a></li><li><a href="#34-%E5%AE%B9%E5%99%A8%E5%91%BD%E4%BB%A4">3.4. 容器命令</a></li></ul></li><li><a href="#4-Dockerfile">4. Dockerfile</a><ul><li><a href="#41-%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4">4.1. 相关指令</a></li><li><a href="#42-%E6%9E%84%E5%BB%BA">4.2. 构建</a></li></ul></li><li><a href="#5-%E4%BD%BF%E7%94%A8Docker%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F">5. 使用Docker运行程序</a></li><li><a href="#6-%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E9%95%9C%E5%83%8F">6. 构建自己的镜像</a><ul><li><a href="#61-%E5%88%9B%E5%BB%BADockerfile">6.1. 创建Dockerfile</a></li><li><a href="#%E6%9E%84%E5%BB%BA">构建</a></li><li><a href="#%E8%BF%90%E8%A1%8C">运行</a></li></ul></li><li><a href="#Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4">Docker常用命令</a></li></ul><!-- /TOC --><h1><span id="2-docker">2. Docker</span></h1><p>参考资料：<br><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&amp;mid=2655824742&amp;idx=1&amp;sn=43dcbd8cd3b3e0dc5f06c83a50983420&amp;chksm=bd74e6b18a036fa7e5fe2229b3fa08f5a4d11c6d201deba29c63e61c47ed37b5aa7f22e441d6&amp;scene=0&amp;xtrack=1&amp;key=1873ed4ed1cb893ec82026059f24db129748acc346da2d85cea356373d0c0fd919da6e704f47695c7743b64ff520c46fb51d245dacff68136d667c05e73d963d768ee11171e66dedea24d39bb3d67ced&amp;ascene=1&amp;uin=MTM1ODU4OTIwOA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=84krGCWgmUm73hPCIwVp8NE9B3dpOiU5R1bRm3jvlvv%2FygbqWRm4O%2BYabIzyFhbf" target="_blank" rel="noopener">资料1</a>    </p><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&amp;mid=2655817429&amp;idx=1&amp;sn=f82daff5e9fad66a0e11cdb92c12715e&amp;chksm=bd74c3028a034a14cadf884d97f2a0fc3372d3baa97fb5ae6724e6d1926520eada090c0ffb16&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">资料2</a><br>&ensp;&ensp;&ensp;&ensp;Docker就是一个运行在操作系统上的软件。这个软件上运行很多容器，这些容器相互独立，相互隔离。容器中可以安装很多应用程序。<br>&ensp;&ensp;&ensp;&ensp;我们平时要在Windows上安装Linux系统，都需要先安装一个VMWare，然后在上面安装Linux系统。原理就是虚拟出一套硬件资源，然后在上面运行一个完整的操作系统，再在操作系统上运行所需要的应用程序。在安装虚拟机时，需要先提前给虚拟机分配硬盘，内存等资源。一旦分配，这些资源就被虚拟机全部占用。Docker也可以实现虚拟化。但是Docker没有自己的内核，Docker容器内的应用程序是直接运行在宿主的内核，Docker比传统的虚拟机更轻便。Docker就是一个软件。如果以后再Windows上安装Linux系统，可以先在本地电脑上安装一个Windows版本的Docker，然后</p><h1><span id="3-镜像amp容器">3. 镜像&amp;容器</span></h1><h2><span id="31-镜像">3.1. 镜像</span></h2><p>&ensp;&ensp;&ensp;&ensp;官方定义：Docker镜像是一个只读模板，可以用来创建Docker容器。镜像是一种轻量级的，可执行的独立软件包，软件和依赖的环境可以打包成一个镜像。这个镜像包含某个软件需要的所有内容，包括代码、库、环境变量、配置文件等。<br>&ensp;&ensp;&ensp;&ensp;比如我们开发的Web应用需要JDK，Tomcat，环境变量等。那我们就可以把这些都打包成一个镜像，包括代码+JDK+Tomcat+CentOS系统+各种配置文件等。打包后的镜像如果可以运行，那么这个镜像就可以在任何安装有Docker的电脑上运行。<br>&ensp;&ensp;&ensp;&ensp;任何镜像的创建会基于其他的父镜像，也就是说镜像是一层套一层的。比如一个Tomcat镜像需要运行在CentOS上面，那我们的Tomcat镜像就会基于CentOS镜像创建。</p><h2><span id="32-镜像命令">3.2. 镜像命令</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. docker images：查看本地主机上所有的镜像。注意是本地主机的！这里能看到镜像的名称、版本、id、大小等基本信息，注意这里的 imageID 是镜像的唯一标识！   </span><br><span class="line"></span><br><span class="line">2. docker rmi：删除本地的镜像，如下图所示，可以加上 -f 参数进行强制删除。</span><br><span class="line">这里的 rmi 命令跟 Linux 中的删除命令就很像啦，只是这里加了一个 i 代表 image！   </span><br><span class="line"></span><br><span class="line">3. docker search：根据镜像名称搜索远程仓库中的镜像！   </span><br><span class="line"></span><br><span class="line">4. docker pull：搜索到某个镜像之后就可以从远程拉取镜像啦，有点类似咱们 Git 中的 Pull 命令，当然对应的还有个 dockerpush 的命令。</span><br></pre></td></tr></table></figure><h2><span id="33-容器">3.3. 容器</span></h2><p>&ensp;&ensp;&ensp;&ensp;Docker的容器是<strong>用镜像创建的运行实例</strong>，Docker可以利用容器独立运行一个或一组应用。我们可以使用客户端或API控制容器的启动、开始、停止、删除。每个容器都是相互独立的。上一步创建的镜像是一个静态的文件，这个文件想要运行的话，就要先变成容器。我们可以把容器看做是一个简易版的Linux系统和运行在上面的程序。<br><strong>镜像和容器的关系</strong><br>类似于Java中的类和对象的关系。镜像可以看做一个类，容器是镜像的一个实例。可以根据一个类new很多个实例，new出来的实例就相当于一个个容器。镜像是静态的文件，容器是有生命的个体。   </p><h2><span id="34-容器命令">3.4. 容器命令</span></h2><ul><li>docker pull<br>从远程仓库中拉取镜像</li><li><p>通过镜像创建容器<br><code>docker run [OPTIONS] IMAGE</code>可以基于某个镜像运行一个容器，如果本地有指定的镜像则使用本地的镜像，如果没有则远程拉取然后启动<br><code>docker run -v $PWD:/root -ti --runtime=nvidia --rm mxnet/python:1.4.1_gpu_cu90_mkl_py3</code>   </p><h1><span id="4-dockerfile">4. Dockerfile</span></h1><h2><span id="41-相关指令">4.1. 相关指令</span></h2><p>镜像可以从远程仓库中拉取，也可以自己创建一个镜像。Dockerfile是一个包含用户能够构建镜像的所有命令的文本文档，它有自己的语法和命令。Docker能够从Dockerfile中读取指令并自动构建镜像。<br>如果想构建自己的镜像，就要自己写Dockerfile。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia/cuda:9.0-cudnn7-devel</span><br><span class="line"></span><br><span class="line">LABEL SongChao chaosong@bjtu.edu.cn</span><br><span class="line"></span><br><span class="line">RUN apt-get update</span><br><span class="line"></span><br><span class="line">RUN apt-get install -y gcc make build-essential libssl-dev wget curl vim --allow-unauthenticated</span><br><span class="line"></span><br><span class="line">RUN mkdir /root/python3.6</span><br><span class="line"></span><br><span class="line">COPY Python-3.6.8 /root/python3.6/</span><br><span class="line"></span><br><span class="line">WORKDIR /root/python3.6</span><br><span class="line"></span><br><span class="line">RUN ./configure &amp;&amp; make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">RUN wget https://bootstrap.pypa.io/get-pip.py</span><br><span class="line"></span><br><span class="line">RUN python3 get-pip.py</span><br><span class="line"></span><br><span class="line">ENV PYTHONIOENCODING=utf-8</span><br><span class="line"></span><br><span class="line">RUN pip3 install numpy scipy pandas tensorflow-gpu==1.9.0 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">RUN mkdir /workdir</span><br><span class="line"></span><br><span class="line">WORKDIR /workdir</span><br><span class="line">```  </span><br><span class="line">`FROM`指定基础镜像，当前镜像是基于哪个镜像创建的，有点类似Java中类继承，FROM指令必须是Dockerfile必须是Dockerfile文件的首条命令。  </span><br><span class="line">`LABEL`给镜像添加元数据，指定作者，邮箱等信息。</span><br><span class="line">## 4.2. 构建</span><br><span class="line">Dockerfile的执行顺序从上到下顺序执行，编写好Dockerfile文件后，就需要使用docker build命令对镜像进行构建了。   </span><br><span class="line">`docker build [OPTIONS] PATH | URL | -`  </span><br><span class="line">`docker build -t chaosong/cuda-10.1-cudnn7-devel:with_cuda_samples .</span><br><span class="line">`</span><br><span class="line">-f：指定要使用的 Dockerfile 路径，如果不指定，则在当前工作目录寻找 Dockerfile 文件！   </span><br><span class="line">-t：镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 注意后面的 . , 用于指定镜像构建过程中的上下文环境的目录。 </span><br><span class="line"></span><br><span class="line"># 5. 使用Docker运行程序   </span><br><span class="line">1. 登录上网  </span><br><span class="line">   `links 10.1.61.1/a30.htm`   </span><br><span class="line">   登录之后按`Ctrl+C`退出  </span><br><span class="line">2. 在dockerhub上查询镜像</span><br><span class="line">   浏览器进入[docker hub](https://hub.docker.com/)，找到自己需要的镜像，gpu28号服务器Cuda的版本是9.0，下载的镜像需要和服务器上Cuda版本一致。  </span><br><span class="line">3. 拉取镜像</span><br><span class="line">   例如镜像的全称为：</span><br><span class="line">   `mxnet/python:1.4.1_gpu_cu90_mkl_py3`   </span><br><span class="line">   `docker pull mxnet/python:1.4.1_gpu_cu90_mkl_py3`</span><br><span class="line">   这条命令会把镜像下载到服务器上，如果本地已经存在该镜像，docker会在佛那个使用本地的镜像，不再下载。   </span><br><span class="line">4. 启动镜像</span><br><span class="line">   `docker run -v $PWD:/root -ti --runtime=nvidia --rm &lt;镜像名:标签&gt;`   </span><br><span class="line">   例如  </span><br><span class="line">   `docker run -v $PWD:/root -ti --runtime=nvidia --rm mxnet/python:1.4.1_gpu_cu90_mkl_py3`   </span><br><span class="line">   使用上面的命令可以启动镜像，这里的参数-v通过将宿主机的目录映射到容器内，\$PWD是当前目录，映射目标是容器内的/root目录，这时\$PWD中的目录会全部映射到容器中的/root目录下。</span><br><span class="line">   启动容器后执行</span><br><span class="line">   `cd /root`  </span><br><span class="line">   `ls`  </span><br><span class="line">   可以看到服务器中的目录都映射到容器内，当在容器内创建一个目录时，然后创建一个文件，exit退出容器，可以看到本地服务器也有刚才那个文件。</span><br></pre></td></tr></table></figure><p> mkdir docker_test<br> touch docker_test.txt<br> exit<br> ls<br> ls docker_test</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5. 运行GPU程序  </span><br><span class="line">   先不启动容器，在宿主机上使用  </span><br><span class="line">   `nvidia-smi`查看当前空闲的GPU，使用空闲的GPU运行程序。</span><br><span class="line">   把py程序上传到服务器上，然后启动容器。</span><br></pre></td></tr></table></figure><p> docker run -v $PWD:/root -ti —runtime=nvidia —rm mxnet/python:1.4.1_gpu_cu90_mkl_py3<br> cd 进入py程序所在的目录<br> python3 xxx.py</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 6. 构建自己的镜像     </span><br><span class="line">## 6.1. 创建Dockerfile   </span><br><span class="line">通过Dockerfile构建自己的镜像。镜像构建时，会一层层的构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是该文件一直跟随镜像。因此在创建镜像的时候，需要小心，每一层尽量只添加该层需要的东西，任何额外的东西应该在该层构建结束前清理掉。  </span><br><span class="line">Docker能够从Dockerfile中读取指令自动的构建镜像。   </span><br><span class="line"></span><br><span class="line">在服务器上创建一个目录  </span><br><span class="line">`mkdir wbb_docker_gcn`   </span><br><span class="line">`cd wbb_docker_gcn`   </span><br><span class="line">可以在这个目录下下载一些创建镜像需要的文件</span><br><span class="line">`git clone https://github.com/NVIDIA/cuda-samples.git`    </span><br><span class="line">然后编写用于构建镜像的Dockerfile</span><br><span class="line">`vi Dockerfile`   </span><br><span class="line">内容如下：</span><br></pre></td></tr></table></figure></li></ul><h1><span id="指定镜像要构建在哪个镜像之上">指定镜像要构建在哪个镜像之上</span></h1><h1><span id="如果程序需要用到gpu那就一定要构建在nvidiacuda这个镜像上">如果程序需要用到GPU，那就一定要构建在nvidia/cuda这个镜像上</span></h1><p>FROM nvidia/cuda:9.0-cudnn7-devel</p><h1><span id="给镜像添加元数据指定作者邮箱等信息">给镜像添加元数据，指定作者邮箱等信息</span></h1><p>LABEL WangBeibei 18120408@bjtu.edu.cn    </p><h1><span id="run会在当前镜像的最上面创建一个新层并且能执行任何的命令">RUN会在当前镜像的最上面创建一个新层，并且能执行任何的命令，</span></h1><h1><span id="然后对执行的结果进行提交提交后的结果镜像在dockerfile的后续步骤中使用">然后对执行的结果进行提交，提交后的结果镜像在Dockerfile的后续步骤中使用</span></h1><h1><span id="更新ubuntu的索引">更新Ubuntu的索引</span></h1><p>RUN apt-get update        </p><h1><span id="安装gcc工具">安装gcc工具</span></h1><p>RUN apt-get install -y wget python3-dev gcc git vim &amp;&amp; \<br>    wget <a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="noopener">https://bootstrap.pypa.io/get-pip.py</a> &amp;&amp; \<br>    python3 get-pip.py   </p><h1><span id="在容器上创建一个目录镜像就安装在该目录下">在容器上创建一个目录，镜像就安装在该目录下</span></h1><p>RUN mkdir /root/docker_gcn    </p><h1><span id="关于copy命令如果要复制目录的话copy命令会把目录里面所有文件赋值到另一个目录下">关于COPY命令，如果要复制目录的话，COPY命令会把目录里面所有文件赋值到另一个目录下，</span></h1><h1><span id="而不是把这个目录直接复制过去所以上面先在容器中创建了一个docker_gcn的目录然后copy命令将服务器本地的wbb_docker_gcn里面的文件都复制到了rootdocker_gcn里面">而不是把这个目录直接复制过去，所以上面先在容器中创建了一个docker_gcn的目录，然后COPY命令将服务器本地的wbb_docker_gcn里面的文件都复制到了/root/docker_gcn/里面</span></h1><h1><span id="将服务器本地的文件拷贝到容器中的目录上">将服务器本地的文件拷贝到容器中的目录上</span></h1><p>COPY wbb_docker_gcn /root/docker_gcn/      </p><h1><span id="切换到那个目录如果该目不存在则创建-workdir是切换当前工作路径下面的run命令都会在这个workdir下面执行">切换到那个目录，如果该目不存在，则创建。WORKDIR是切换当前工作路径，下面的RUN命令都会在这个WORKDIR下面执行</span></h1><p>WORKDIR /root/docker_gcn      </p><h1><span id="下载">下载</span></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 构建   </span><br><span class="line">Dockerfile是执行是从上到下顺序执行的，每条执行都会创建一个新的镜像层，并对镜像进行提交。编写好Dockerfile文件后，就需要使用dockerbuild命令对镜像进行构建了。   </span><br><span class="line">`docker build [OPTIONS] PATH | URL | -`  </span><br><span class="line">`docker build -t chaosong/cuda-10.1-cudnn7-devel:with_cuda_samples .</span><br><span class="line">`</span><br><span class="line">-f：指定要使用的 Dockerfile 路径，如果不指定，则在当前工作目录寻找 Dockerfile 文件！   </span><br><span class="line">-t：镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 注意后面的 . , 用于指定镜像构建过程中的上下文环境的目录。    </span><br><span class="line"></span><br><span class="line">## 运行   </span><br><span class="line">构建完镜像就可以启动这个容器了，启动完之后就可以运行py脚本   </span><br><span class="line">`docker run -itd --rm --runtime=nvidia -v $PWD:/workdir/ songchao/tensorflow:1.9.0_py36_cu90_cudnn7 /bin/bash</span><br><span class="line">`</span><br><span class="line">由于这个命令非常重要，所以下面列出几个比较重要的参数：</span><br><span class="line"></span><br><span class="line">-d：启动容器，并且后台运行（Docker 容器后台运行，就必须要有一个前台进程，容器运行的命令如果不是一直挂起的命令，容器启动后就会自动退出）。</span><br><span class="line"></span><br><span class="line">-i：以交互模式运行容器，通常与 -t 同时使用。</span><br><span class="line"></span><br><span class="line">-t：为容器重新分配一个伪输入终端，通常与 -i 同时使用（容器启动后进入到容器内部的命令窗口）。</span><br><span class="line"></span><br><span class="line">-P：随机端口映射，容器内部端口随机映射到主机的高端口。</span><br><span class="line"></span><br><span class="line">-p：指定端口映射，格式为：主机(宿主)端口：容器端口。</span><br><span class="line"></span><br><span class="line">-v：建立宿主机与容器目录的同步。</span><br><span class="line"></span><br><span class="line">--name=&quot;myTomcat&quot;：为容器指定一个名称（如果不指定，则有个随机的名字）。</span><br><span class="line">其中--rm表示程序运行完，这个容器就删掉了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Docker常用命令   </span><br><span class="line">1. 列出机器上的镜像</span><br><span class="line">   docker images </span><br><span class="line">   ![](Docker/images.png)</span><br><span class="line">2. 拉取镜像  </span><br><span class="line">   docker pull mxnet/python:1.4.1_gpu_cu90_mkl_py3</span><br><span class="line">3. 查看容器信息</span><br><span class="line">   （1）docker ps：显示当前正在运行的容器，在 PORTS 一列，如果暴露的端口是连续的，还会被合并在一起，例如一个容器暴露了3个 TCP 端口：100，101，102，则会显示为 100-102/tcp。</span><br><span class="line"></span><br><span class="line">   （2）docker ps -a：显示所有的容器，</span><br><span class="line">   容器的状态共有 7 种：created|restarting|running|removing|paused|exited|dead。</span><br><span class="line">   （3）docker ps -n 3：显示最后被创建的n个容器</span><br><span class="line">   （4）docker ps -q：只显示正在运行容器的id，在清理容器时非常好用。</span><br><span class="line">   （5）docker ps -s：显示容器文件大小，该命令很实用，可以获得 2 个数值：一个是容器真实增加的大小，一个是整个容器的虚拟大小。</span><br><span class="line">4. 以交互式方式启动容器</span><br></pre></td></tr></table></figure><p>   [root@localhost ~]# docker run -it —name centos-test centos:7.4.1708<br>   [root@ebd974405f42 /]#<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   以交互式方式启动容器后，docker会随机分配一个容器名，并为容器分配一个短id。</span><br><span class="line">5. 后台运行容器</span><br></pre></td></tr></table></figure></p><p>   [root@localhost ~]# docker run —name centos-deamon -d centos:7.4.1708<br>   63e5555c9ed35deb7e32c2a16896e0f806dbcf471d0484bdd904724a3cce542d<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   其中d表示容器在后台运行，使用run命令之后，会把容器挂到后台运行，并且会输出一个长的container id，通过docker ps查看容器的信息，这里输出的是容器id的前12位。</span><br><span class="line">6. 进入到后台运行的容器</span><br><span class="line">   通过docker attach ae60c4b64205连接到正在运行的终端，此时使用exit命令退出容器。  </span><br><span class="line">   </span><br><span class="line">7. 容器的状态</span><br><span class="line">   docker容器有几种状态，分别是created、up、exited、paused。</span><br><span class="line">   （1）我们通过run命令运行容器时，其实是将容器从created到up状态的过程。</span><br><span class="line">   （2）当通过stop命令停止容器时，容器进入exited状态。容器退出后，系统仍然保存该容器实例，即退出的容器仍然会占用系统的硬盘资源，需要使用rm删除该容器才能完全清楚容器的资源占用。容器stop或Ctrl+D时，会保存当前容器的状态之后退出，下次start时会保存上次的关闭时更改，而且每次attach进去的界面是一样的，和第一次run启动一样。</span><br></pre></td></tr></table></figure></p><p>   [root@localhost ~]# docker stop e83cf32fbc22<br>   e83cf32fbc22<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（3）重新启动退出的容器  </span><br><span class="line">处于exited状态的容器，可以通过start命令重新启动。</span><br></pre></td></tr></table></figure></p><p>   [root@localhost ~]# docker start e83cf32fbc22<br>   e83cf32fbc22<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（4）重启容器</span><br></pre></td></tr></table></figure></p><p>   [root@localhost ~]# docker restart e83cf32fbc22<br>   e83cf32fbc22<br>   ```</p><ol><li>删除容器<br>从上面可以看出，通过stop命令停止容器，容器的相关文件仍然存储在宿主主机中，为了释放这部分空间，需要删除这些容器。<br>（1）<code>docker rm id</code>可以删除对应id的容器<br>（2）批量删除除了运行以外的程序<br><code>docker rm $(docker ps -a -q)</code><br>（3）如果要批量删除指定状态的容器<br><code>docker rm $(docker ps -a -q  status=exited)</code></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;title: Docker&lt;br&gt;date: 2019-06-12T18:48:24.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2&gt;&lt;span id=&quot;-docker-镜像&quot;&gt;  - Docker、镜像&lt;/span&gt;&lt;/h2&gt;&lt;h1&gt;&lt;span id=&quot;1-简介&quot;&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/05/10/CentOs%E7%B3%BB%E7%BB%9Fmatplotlib%E7%94%BB%E5%9B%BE%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"/>
    <id>http://yoursite.com/2019/05/10/CentOs系统matplotlib画图中文乱码/</id>
    <published>2019-05-10T09:15:52.725Z</published>
    <updated>2019-06-26T02:02:03.639Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: CentOs系统matplotlib画图中文乱码<br>date: 2019-05-10T17:15:52.000Z<br>tag:</p><h2><span id="-centos-matplotlib-中文乱码">  - Centos、matplotlib、中文乱码</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;在集群上使用Python中的matplotlib库画图出现中文乱码，记录一下解决方案。   </p><a id="more"></a> <!-- TOC --><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">2. 解决方案</a><ul><li><a href="#21-%E6%AD%A5%E9%AA%A4%E4%B8%80">2.1. 步骤一</a></li><li><a href="#22-%E6%AD%A5%E9%AA%A4%E4%BA%8C">2.2. 步骤二</a></li><li><a href="#23-%E6%AD%A5%E9%AA%A4%E4%B8%89">2.3. 步骤三</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-解决方案">2. 解决方案</span></h1><h2><span id="21-步骤一">2.1. 步骤一</span></h2><p>&ensp;&ensp;&ensp;&ensp;获取matplotlibrc文件所在的路径，使用jupyter notebook写代码获取路径。我的文件路径在<br>/data/WangBeibei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.matplotlib_fname()</span><br><span class="line">```      </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">       </span><br><span class="line"><span class="comment">## 2.2. 步骤二    </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;</span><br><span class="line">- 到 anaconda 的 matplotlib 中查看是否有 simhei.ttf 字体   </span><br><span class="line">```  </span><br><span class="line">cd /data/WangBeibei/anaconda3/lib/python3<span class="number">.6</span>/site-packages/matplotlib/mpl-data/fonts/ttf    </span><br><span class="line">ls -al | grep simhei </span><br><span class="line">```    </span><br><span class="line">- 如果没有输出任何内容，说明没有simhei字体，下载simhei.ttf文件，并上传到/data/WangBeibei/anaconda3/lib/python3<span class="number">.6</span>/site-packages/matplotlib/mpl-data/fonts/ttf目录下。    </span><br><span class="line">- 修改/data/WangBeibei/anaconda3/lib/python3<span class="number">.6</span>/site-packages/matplotlib/mpl-data/matplotlibrc文件，找到以下<span class="number">3</span>行，改为：  </span><br><span class="line">``` </span><br><span class="line">font.family: sans-serif   </span><br><span class="line">font.sans-serif: simhei,DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif   </span><br><span class="line">axes.unicode_minus: <span class="keyword">False</span><span class="comment">#解决负号'-'显示为方块的问题   </span></span><br><span class="line">```   </span><br><span class="line">- 删除/data/WangBeibei/.cache/matplotlib</span><br></pre></td></tr></table></figure></p><p>rm -r /data/WangBeibei/.cache/matplotlib</p><pre><code>## 2.3. 步骤三   经过以上步骤，再次运行jupyter notebook程序，中文就不会出现乱码。如果还是出现乱码，添加以下两行代码  ```python    import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams[&#39;font.sans-serif&#39;] = [&#39;simhei&#39;]  # 用来正常显示中文标签plt.rcParams[&#39;axes.unicode_minus&#39;] = False  # 用来正常显示负号#显示所有列</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: CentOs系统matplotlib画图中文乱码&lt;br&gt;date: 2019-05-10T17:15:52.000Z&lt;br&gt;tag:&lt;/p&gt;
&lt;h2 id=&quot;Centos、matplotlib、中文乱码&quot;&gt;&lt;a href=&quot;#Centos、matplotlib、中文乱码&quot; class=&quot;headerlink&quot; title=&quot;  - Centos、matplotlib、中文乱码&quot;&gt;&lt;/a&gt;  - Centos、matplotlib、中文乱码&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在集群上使用Python中的matplotlib库画图出现中文乱码，记录一下解决方案。   &lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>gluon环境安装</title>
    <link href="http://yoursite.com/2019/04/27/gluon%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2019/04/27/gluon环境安装/</id>
    <published>2019-04-27T11:59:35.000Z</published>
    <updated>2019-06-24T00:42:10.630Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器上安装gluon环境。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83">2. 虚拟环境</a></li><li><a href="#3-%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4">3. 一些常用的命令</a></li><li><a href="#4-openSSH">4. openSSH</a></li><li><a href="#5-%E9%97%AE%E9%A2%98">5. 问题</a></li></ul><!-- /TOC --><h1><span id="2-虚拟环境">2. 虚拟环境</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器上可以安装不同的虚拟环境，这些虚拟环境之间互不影响，不同的虚拟环境可以安装不同的python版本，不同的框架。 </p><ol><li>安装Anaconda<br><code>wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh</code><br>下载Anaconda3-5.2.0-Linux-x86_64.sh上传到服务器中，</li><li>运行安装向导<br><code>bash Anaconda3-5.2.0-Linux-x86_64.sh</code><br>为了激活安装， 你应该源~/.bashrc文件：<br><code>source ~/.bashrc</code></li><li>确认安装成功<br><code>conda --version</code><br>然后使用which python查看你当前使用的是哪个python<br>如果输出的目录是data/anaconda/python说明你当前使用的还是服务器自带的python，需要重新练连接一下服务器。<br>如果输出是data/WangBeibei/anaconda/python，说明当前使用是自己安装的anaconda</li><li>配置清华镜像<br>使用conda创建虚拟（运行）环境。conda和pip默认使用国外站点来下载软件，我们可以配置国内镜像来加速下载（国外用户无须此操作）。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># 配置清华conda镜像</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line"># 配置清华PyPI镜像（如无法运行，将pip版本升级到&gt;=10.0.0）</span><br><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">```     </span><br><span class="line">5. `conda env create -f environment.yml`   </span><br><span class="line">创建虚拟环境gluon，安装mxnet框架和一些依赖包。</span><br><span class="line">6.   `conda info -e`   </span><br><span class="line">查看当前服务器上都有哪些虚拟环境,下面截图中显示，当前存在2个虚拟环境，其中带*的是当前正在使用的虚拟环境。  </span><br><span class="line">![](gluon环境安装/conda-info.png)</span><br><span class="line"></span><br><span class="line">7. `screen -S WBB`（超级有用！！！）  </span><br><span class="line">因为是外网服务器，所以网络连接经常断开，连接一断开，运行在上面的程序就不能运行了，所以创建虚拟窗口，在这个虚拟窗口内运行程序，就算网络断开了，程序依然会继续运行</span><br><span class="line"></span><br><span class="line">8. `source activate gluon`   </span><br><span class="line">激活虚拟环境gluon，若要在gluon这个虚拟环境下安装一些库，需要先切换到这个环境下，然后使用conda install xxx或者pip install xxx，优先选择使用conda install xxx。安装完之后可以通过conda list查看当前已经安装的包。</span><br><span class="line"></span><br><span class="line">9. 使用jupyter notebook  </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;第一次使用jupyter notebook，需要映射端口号，默认jupyter notebook的端口号是8888，但是在这个集群上，如果别人已经把8888端口占用了，集群会自动给你分配一个端口号，然后在putty中映射一下这个端口，具体操作如下：</span><br><span class="line">在菜单栏选中change setting，找到Tunnels   </span><br><span class="line">![](gluon环境安装/putty.png)   </span><br><span class="line">![](gluon环境安装/port.png) </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;使用screen创建一个窗口运行jupyter notebook程序的好处，就算ssh和28号服务器的连接断开，jupyter notebook的程序依然可以在后台运行。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;如果在jupyter notebook的程序运行完了，有3中关闭程序的方法：  </span><br><span class="line">1、在jupyter notebook菜单栏，有一个close and hot的按钮  </span><br><span class="line">2、在jupyter notebook上面的菜单中有一个running，shutdown掉程序  </span><br><span class="line">3、screen -r 23560切入到虚拟窗口，然后再这个窗口ctrl+c关闭jupyter notebook进程。不用使用exit，因为使用exit是关闭虚拟窗口，直接按shift+a+d从虚拟窗口中切出，这样这个窗口还是存在的，下一次直接使用screen -ls查看存在的窗口，然后  </span><br><span class="line">source /etc/profile   </span><br><span class="line">source gluon   </span><br><span class="line">jupyter notebook  </span><br><span class="line">再次打开jupyter notebook程序   </span><br><span class="line"></span><br><span class="line">1.  `ctrl+A+D`退出虚拟窗口   </span><br><span class="line">当把程序运行之后，使用以上按钮退出虚拟窗口，这样程序就在后台运行，就算把电脑换机了，程序还是会运行。   </span><br><span class="line">11. `screen -r WBB`   </span><br><span class="line">使用以上命令进入到虚拟窗口中，查看程序运行的结果</span><br><span class="line">12. 下载Tensorflow   </span><br><span class="line">首先到清华的网站下[https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)直接粘贴下面3行命令，</span><br></pre></td></tr></table></figure></li></ol><p>conda config —add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>conda config —add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a><br>conda config —set show_channel_urls yes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">然后使用vi ~/.condarc将里面的-default删除，这样下载TensorFlow就不会使用国外的网站，而是使用清华的镜像</span><br><span class="line">然后使用`conda install tensorflow-gpu`下载TensorFlow框架</span><br><span class="line"></span><br><span class="line">运行TensorFlow程序，指定使用哪一块GPU，如果不指定</span><br><span class="line">使用全部GPU</span><br><span class="line">`CUDA_VISIBLE_DEVICES=5  python train.py`  </span><br><span class="line">直接杀死一个进程  </span><br><span class="line">`kill -9 [PID]`</span><br><span class="line"># 3. 一些常用的命令   </span><br><span class="line">（1）使用清华源下载  </span><br><span class="line">`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pykafka`    </span><br><span class="line">（2）和conda有关  </span><br><span class="line">`conda --version`  </span><br><span class="line">通过使用如下update命令来升级conda：  </span><br><span class="line">`conda update conda`  </span><br><span class="line">```   </span><br><span class="line">pip install 库名</span><br><span class="line">pip install 库名 --upgrade</span><br><span class="line"># 或者</span><br><span class="line">conda install 库名</span><br><span class="line">conda update 库名</span><br><span class="line"></span><br><span class="line"># 更新所有库</span><br><span class="line">conda update --all</span><br><span class="line"></span><br><span class="line"># 更新 conda 自身</span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line"># 更新 anaconda 自身</span><br><span class="line">conda update anaconda</span><br><span class="line"></span><br><span class="line"># 查看已安装的包</span><br><span class="line">conda list</span><br></pre></td></tr></table></figure></p><p>（3）使用conda install xxx或者pip install xxx，优先选择使用conda install xxx。安装完之后可以通过conda list查看当前已经安装的包<br>（4）删除一个虚拟环境<br><code>conda remove -n gluon--all</code><br>为了确定这个名为flowers的环境已经被移除，输入以下命令<code>conda info -e</code> ,会看到已经没有gluon这个环境<br>（5）查看GPU的占用情况<br><code>nvidia-smi</code><br>（6）<code>screen -S jupyter</code><br><code>source activate gluon</code><br><code>source deactivate</code><br><code>jupyter notebook</code><br><code>screen -r jupyter</code><br><code>screen -X -S 122128 quit</code><br>（7）查看当前的进程是谁的<br><code>ps -ef | grep 35230</code><br>（8）在windows在激活虚拟环境<br><code>activate gluon</code><br><code>deactivate gluon</code>    </p><h1><span id="4-openssh">4. openSSH</span></h1><p>Windows10自带了openssh工具，打开powershell使用ssh username@ip就可以连接服务器了</p><h1><span id="5-问题">5. 问题</span></h1><p>我创建了一个gluon的虚拟环境，使用<code>source avtivate gluon</code>激活这个环境时，在这里面装了mxnet框架。但是在screen中启动jupyter notebook时，import mxnet时却报错说no module names mxnet，然后我退出jupyter notebook（仍在screen中），使用which python查看当前使用的python，仍然是base的python，不是env/gluon中的python，因为base中的python没有装mxnet，所以import会出错。那怎么把gluon虚拟环境中的python换成env/gluon中的python呢？<br>参考这个网址：<a href="http://www.pianshen.com/article/2276285026/" target="_blank" rel="noopener">http://www.pianshen.com/article/2276285026/</a><br>在虚拟环境下运行以下命令：<br><code>ipython kernelspec list</code><br>查看jupyter notebook内核指定的python运行环境位置，然后cd到这个目录中，会看到有一个kernel.json文件，使用vi命令编辑这个文件，将python解释器的位置换成<code>/data/WangBeibei/anaconda3/envs/gluon/bin/python</code><br><img src="/2019/04/27/gluon环境安装/python.png" alt=""><br>然后使用<code>source deavtivate</code>断开gluon虚拟环境，在重新激活<code>source avtivate gluon</code>，然后再启动<code>jupyter noteboook</code>就可以了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在服务器上安装gluon环境。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon、jupyter notebook" scheme="http://yoursite.com/tags/gluon%E3%80%81jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/04/24/机器学习/</id>
    <published>2019-04-24T15:52:10.466Z</published>
    <updated>2019-06-26T02:02:41.525Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 机器学习<br>date: 2019-04-24T23:52:10.000Z<br>tags:</p><h2><span id="-特征预处理-模型评估-分类">  - 特征预处理、模型评估、分类</span></h2><h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;最近在做一个分类任务，根据电池的充放电数据，预测电池绝缘报警是否为虚报，就是一个二分类任务。这里使用逻辑回归进行分类。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">2. 数据预处理</a><ul><li><a href="#21-%E7%89%B9%E5%BE%81%E5%80%BC%E8%BF%9E%E7%BB%AD">2.1. 特征值连续</a><ul><li><a href="#211-%E5%BD%92%E4%B8%80%E5%8C%96normalization">2.1.1. 归一化(normalization)</a></li><li><a href="#212-%E6%A0%87%E5%87%86%E5%8C%96standardization">2.1.2. 标准化(standardization)</a></li><li><a href="#213-RobustScaler">2.1.3. RobustScaler</a></li></ul></li><li><a href="#22-%E7%89%B9%E5%BE%81%E5%80%BC%E7%A6%BB%E6%95%A3">2.2. 特征值离散</a></li><li><a href="#23-%E9%A2%84%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4">2.3. 预处理步骤</a></li><li><a href="#24-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81">2.4. 交叉验证</a></li><li><a href="#25-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7">2.5. 模型评价</a></li></ul></li><li><a href="#3-XGBoost">3. XGBoost</a><ul><li><a href="#31-XGBoost%E7%9A%84%E4%BC%98%E5%8A%BF">3.1. XGBoost的优势</a></li><li><a href="#32-%E5%8F%82%E6%95%B0">3.2. 参数</a></li><li><a href="#33-%E8%B0%83%E5%8F%82">3.3. 调参</a></li></ul></li><li><a href="#4-GridSearchCV">4. GridSearchCV</a></li><li><a href="#5-%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98">5. 样本不均衡分类问题</a><ul><li><a href="#51-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B">5.1. 特征工程</a></li></ul></li><li><a href="#6-%E8%BF%87%E9%87%87%E6%A0%B7">6. 过采样</a></li></ul><!-- /TOC --><h1><span id="2-数据预处理">2. 数据预处理</span></h1><p>&ensp;&ensp;&ensp;&ensp;在进行模型训练之前，需要对数据进行预处理。因为多个特征之间的量纲不同，在训练的时候收敛会很慢，所以需要将不同特征值转换为同一量纲。这里将离散特征和连续特征分别处理。   </p><h2><span id="21-特征值连续">2.1. 特征值连续</span></h2><p>&ensp;&ensp;&ensp;&ensp;对于连续值的预处理主要分为2个：归一化和标准化。这2个操作主要是为了使得不同的特征在同一个量纲，对目标的影响是同级的。归一化和标准化都是先对数据先缩小一定的比例，然后再平移。这2者本质上都是对数据进行线性变换，线性变换不会改变原始数据的数值大小排序。即一个数在原始数据最大，经过归一化和标准化这个数还是最大。<a href="https://blog.csdn.net/dujiahei/article/details/86061924" target="_blank" rel="noopener">这篇博客</a>。<br>&ensp;&ensp;&ensp;&ensp;将特征值缩放到相同的区间可以获得性能更好的模型。就梯度下降而言，一个特征值的范围在1-10之间，另一个特征值范围在1-10000之间，训练的目标是最小化平方误差，所以在使用梯度下降算法的过程中，算法会明显偏向第二个特征，因为它的取值范围更大。在K近邻算法中，使用的欧式距离，也会导致偏向第二个特征。<strong>对于决策树和随机森林以及xgboost算法而言，特征缩放对它们没有什么影响，像逻辑回归和支持向量机算法和K近邻，需要对数据进行特征缩放</strong>。  <strong>在分类，聚类算法中，需要使用距离来度量相似性的时候，standardization表现更好</strong>。<br><img src="/2019/04/24/机器学习/表格.png" alt="归一化">   </p><h3><span id="211-归一化normalization">2.1.1. 归一化(normalization)</span></h3><p>&ensp;&ensp;&ensp;&ensp;归一化将每一个属性值映射到[0,1]之间。需要计算训练集的最大值和最小值，当有新样本加入时，需要重新计算最值。 </p><ul><li>特点：多使用于分布有明显边界的情况，如考试成绩，身高，颜色的分布等，都有明显的范围边界，不适用没有范围约定，或者返回非常大的数据。</li><li>缺点：受异常值影响较大。归一化的缩放就是将数据拍扁统一到一个区间中，仅有极值决定，而标准化的缩放更加弹性和动态，和整体的分布有关。归一化只用到了最大值和最小值，而标准化和每一个值有关。   </li></ul><p><img src="/2019/04/24/机器学习/归一化.png" alt="归一化">    </p><h3><span id="212-标准化standardization">2.1.2. 标准化(standardization)</span></h3><p>&ensp;&ensp;&ensp;&ensp; 标准化又叫做Z-score。将所有的数据映射到均值为0，方差为1的正态分布中。  要求原始数据的分布可以近似为正态分布，否则标准化的结果会很差。 标准化表示的是原始值与均值之间差几个标准差，是一个相对值，也有去除量纲的作用，同时还有2个附加好处：均值为0，标准差为1。均值为0的好处是使得数据以0为中心左右分布。</p><ul><li>适用范围：在分类和聚类算法中，需要使用距离来度量相似性时，例如支持向量机，逻辑回归，或者使用PCA进行降维时，Z-score表现更好。 </li><li>推荐先使用标准化。<br><img src="/2019/04/24/机器学习/标准化.png" alt="归一化">         <h3><span id="213-robustscaler">2.1.3. RobustScaler</span></h3>&ensp;&ensp;&ensp;&ensp;在某些情况下，加入数据中有离群点，可以使用standardization进行标准化，但是标准化后的数据并不理想，因为异常点的特征往往在标准化后容易失去离群特征，此时就要使用RobustScaler针对离群点进行标准化处理   。此方法对数据中心化和缩放健壮性有更强的参数控制能力。     <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#RobustScaler标准化</span></span><br><span class="line">robustscaler = preprocessing.RobustScaler()</span><br><span class="line">df_r = robustscaler.fit_transform(df)</span><br><span class="line">df_r = pd.DataFrame(df_r,columns=[<span class="string">'value1_r'</span>,<span class="string">'value2_r'</span>])</span><br><span class="line">df_r.head()</span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line"><span class="comment">## 2.2. 特征值离散   </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;离散值就是特征值是离散的，不是连续的，例如性别是离散值，只有female和male，颜色是离散的。机器学习算法不能直接处理离散值，需要对其进行一些转换。离散值可以是文本(red，black)或者数值（<span class="number">1</span>，<span class="number">2</span>）。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp; 离散数据有<span class="number">2</span>大类：定序(Ordinal)和定类(Nominal)。定序的数据存在一定的顺序意义，例如衣服的尺寸按大小分类(xs,s,m,l),在定类的数据中，属性值之间没有顺序的要求。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;对于定序的数据，没有统一的模块将这些顺序自动转换成映射，可以自定义一些映射规则，比如xs对应<span class="number">1</span>，s对应<span class="number">2</span>，自定义的规则。</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;对于文本的定类数据，可以先把文本分类至转换为数字，比如red转换为<span class="number">1</span>，black转换为<span class="number">2</span>，然后对这些数据使用one-hot编码。   </span><br><span class="line">主要是使用LabelEncoder和OneHotEncoder这<span class="number">2</span>个模块。</span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">gle = LabelEncoder()</span><br><span class="line">genre_labels = gle.fit_transform(vg_df[<span class="string">'Genre'</span>])</span><br><span class="line"><span class="comment">#将每个风格属性映射到一个数值(0,1,2,3…)。</span></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment"># OneHotEncoder的transform方法默认返回系数矩阵，调用toarray()方法将系数矩阵转为一般矩阵</span></span><br><span class="line">dis_feature_data = enc.fit_transform(dis_feature_data).toarray()</span><br><span class="line">print(dis_feature_data)</span><br><span class="line">print(dis_feature_data.shape)</span><br><span class="line">```    </span><br><span class="line">除了sklearn中的OneHotEncoder，还可以使用pandas中的get_dummies对离散值进行one-hot编码，比OneHotEncoder好的一点是:转换之后可以直观的看出当前列对应哪个属性。</span><br><span class="line">参考博客：    </span><br><span class="line">[https://blog.csdn.net/wotui1842/article/details/<span class="number">80697444</span>](https://blog.csdn.net/wotui1842/article/details/<span class="number">80697444</span>)      </span><br><span class="line">[https://blog.csdn.net/cymy001/article/details/<span class="number">79154135</span>](https://blog.csdn.net/cymy001/article/details/<span class="number">79154135</span>)   </span><br><span class="line">[https://blog.csdn.net/m0_37324740/article/details/<span class="number">77169771</span>](https://blog.csdn.net/m0_37324740/article/details/<span class="number">77169771</span>)    </span><br><span class="line">[https://blog.csdn.net/wxyangid/article/details/<span class="number">80209156</span>](https://blog.csdn.net/wxyangid/article/details/<span class="number">80209156</span>)   </span><br><span class="line"><span class="comment">## 2.3. 预处理步骤   </span></span><br><span class="line">- 首先使用pandas从csv中读取数据，从数据中取出特征值和目标值，分别存储在X和Y中。</span><br><span class="line">- 从X中取出离散特征值dis_feature，剩下的是连续特征值con_feature。</span><br><span class="line">- 对离散特征值dis_feature进行one-hot编码，形成新的特征值new_dis_feature。然后将新的特征值new_dis_feature和原先的连续特征值con_feature进行拼接形成新的特征值new_X</span><br><span class="line">- 然后对new_X和Y划分为训练集和测试集，然后对训练集进行标准化，使用训练集的均值和标准差再对测试集进行标准化。</span><br><span class="line">- 使用训练集对模型进行训练，对测试集进行验证。       </span><br><span class="line"><span class="comment">## 2.4. 交叉验证   </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;sklearn中有<span class="number">2</span>中交叉验证方法，KFold，StratifiedKFold  </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold,StratifiedKFold</span><br><span class="line">X=np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],</span><br><span class="line">    [<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>],</span><br><span class="line">    [<span class="number">31</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>],</span><br><span class="line">    [<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>],</span><br><span class="line">    [<span class="number">51</span>,<span class="number">52</span>,<span class="number">53</span>,<span class="number">54</span>],</span><br><span class="line">    [<span class="number">61</span>,<span class="number">62</span>,<span class="number">63</span>,<span class="number">64</span>],</span><br><span class="line">    [<span class="number">71</span>,<span class="number">72</span>,<span class="number">73</span>,<span class="number">74</span>]</span><br><span class="line">])</span><br><span class="line"> </span><br><span class="line">y=np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">floder = KFold(n_splits=<span class="number">4</span>,random_state=<span class="number">0</span>,shuffle=<span class="keyword">False</span>)</span><br><span class="line">sfolder = StratifiedKFold(n_splits=<span class="number">4</span>,random_state=<span class="number">0</span>,shuffle=<span class="keyword">False</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sfolder.split(X,y):</span><br><span class="line">    print(<span class="string">'Train: %s | test: %s'</span> % (train, test))</span><br><span class="line">    print(<span class="string">" "</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> floder.split(X,y):</span><br><span class="line">    print(<span class="string">'Train: %s | test: %s'</span> % (train, test))</span><br><span class="line">    print(<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">```   </span><br><span class="line">StratifiedKFold和KFold类似，但是StratifiedKFold是分层采样，确保训练集、测试集各类样本的比例与原始数据集中相同。比如原始数据集中正例:负例=<span class="number">2</span>:<span class="number">1</span>,则训练集和测试集中正例:负例=<span class="number">2</span>:<span class="number">1</span>。   </span><br><span class="line">KFold和enumerate联合使用   </span><br><span class="line">enumerate()函数用于将一个可遍历的数据对象(如列表，元组或str)组合成一个序列索引，同时列出数据和数据下标。一般在<span class="keyword">for</span>循环中使用。   </span><br><span class="line">语法：`enumerate(sequence,[start=<span class="number">0</span>])`   </span><br><span class="line">其中`sequence`表示一个序列，迭代器或可遍历对象，`start`表示下标起始位置   </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">for</span> fold_, (train_, test_) <span class="keyword">in</span> enumerate(kfold.split(X_array, y_array):</span><br><span class="line"><span class="comment">#其中train和test是数据的下标</span></span><br></pre></td></tr></table></figure></li></ul><h2><span id="25-模型评价">2.5. 模型评价</span></h2><ul><li>拟合模型<br>model.fit(X_train, y_train)    </li><li>模型预测，对于分类任务，输出最大可能的类别<br>model.predict(X_train)<br>model.predict(X_test)</li><li>对于分类任务，输出所属每个类别的概率，返回的是一个二维数组，每一行加起来为1<br>prob = model.predict_proba(X_train)<br>model.predict_proba(X_test)<br>获取样本属于正例的概率prob[:,1]   </li><li>获得这个模型的参数<br>model.get_params()    </li><li>为模型进行打分<br>线性回归问题返回预测的确定系数R2<br>逻辑回归（分类）根据给定数据与标签返回分类准确率的均值<br>model.score(X_train, y_train)<br>model.score(X_test, y_test)   </li><li>计算分类准确率,和score返回值一样<br>train_predicted = model.predict(X_train)<br>model.accuracy_score(y_train.flatten(),train_predicted)  </li><li>返回分类准确率，和上面的结果一样<br>np.mean(train_predicted == y_train)<br>np.mean(test_predicted == y_test)   </li><li>召回率<br>precision, recall, F1, _ = precision_recall_fscore_support(y_test, pred_test, average=”binary”)<br>print (“精准率: {0:.2f}. 召回率: {1:.2f}, F1分数: {2:.2f}”.format(precision, recall, F1))    </li><li>AUC&amp;&amp;ROC<br>只针对二分类。通过model.predict_proba(X_test)[:,1]可以获取测试集属于正例的概率，将预测概率从大到小排序，然后以每个预测概率作为阈值，即可得到属于2类的样本数。对应计算每个阈值下的”False Positive Rate”(FPR)和”True Positive Rate”(TPR)，以”False Positive Rate”为横轴，以”True Positive Rate”为纵轴，画出ROC曲线，ROC曲线下的面积就是AUC值。<br>“False Positive Rate”(FPR)=负例被划分为正例个数/真正负例个数（负例被分错的个数/真正负例）<br>“True Positive Rate”(TPR)=正例被划分为正例/真正正例个数（正例被分对的个数/真正正例）<br>当阈值取最大时，所有的样本被分为负样本，对应（0,0），当阈值取最小时，所有的样本被分为正样本，对应于（1,1），随着阈值从最大到最小变化，横坐标和纵坐标都在变大，表示被划分为正例的个数越来越多。<br>AUC用来衡量ROC曲线的好坏。如果分类器能完美的将样本分对，那么AUC=1，如果模型是随机猜测的，那么AUC=0.5，对应着y=x直线。分类器越好，则AUC越大。<br>sklearn给了画ROC曲线的函数。  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fpr, tpr, thresholds=sklearn.metrics.roc_curve(y_true_label,y_prob,pos_label=<span class="keyword">None</span>,sample_weight=<span class="keyword">None</span>,drop_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#其中test_true_label表示数据集真实的标签，&#123;0,1&#125;或&#123;-1,1&#125;</span></span><br><span class="line"><span class="comment">#y_prob表示数据集被分为正例的概率</span></span><br><span class="line"><span class="comment"># 返回值</span></span><br><span class="line"><span class="comment">#thresholds: array, shape = [n_thresholds]所选取的不同的阈值，按照从大到小的排序，阈值越大，横纵坐标越小。</span></span><br><span class="line"><span class="comment">#fpr,tpt：根据 thresholds算出来的横坐标和纵坐标。在此基础上可以画ROC曲线，</span></span><br><span class="line"><span class="comment">#通过auc(fpr,tpr)可以求出AUC的值</span></span><br></pre></td></tr></table></figure></li></ul><h1><span id="3-xgboost">3. XGBoost</span></h1><p>&ensp;&ensp;&ensp;&ensp;XGBoost是一种十分精致的算法，可以处理各种不规则的数据。<br>构造一个使用XGBoost的模型十分简单。但是，提高这个模型的表现就有些困难，因为涉及到很多参数。所以为了提高模型的表现，参数的调整十分必要。</p><h2><span id="31-xgboost的优势">3.1. XGBoost的优势</span></h2><ul><li>正则化<br>正则化防止过拟合，实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。</li><li>缺失值处理<br>XGBoost内置处理缺失值的规则。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值的处理方法。<h2><span id="32-参数">3.2. 参数</span></h2></li></ul><p>&ensp;&ensp;&ensp;&ensp;XGBoost实际上是很多CART树堆叠起来。传入的特征可以含有None值。XGBoost有很多参数，使用GridSearchCV进行网格搜索时比较耗时。  </p><p>使用pip install xgboost安装<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">XGBClassifier(</span><br><span class="line">        base_score=<span class="number">0.5</span>, </span><br><span class="line">        booster=<span class="string">'gbtree'</span>, </span><br><span class="line">        colsample_bylevel=<span class="number">1</span>,</span><br><span class="line">        colsample_bytree=<span class="number">1</span>, </span><br><span class="line">        gamma=<span class="number">0</span>, </span><br><span class="line">        learning_rate=<span class="number">1</span>, </span><br><span class="line">        max_delta_step=<span class="number">0</span>,</span><br><span class="line">        max_depth=<span class="number">2</span>, </span><br><span class="line">        min_child_weight=<span class="number">1</span>, </span><br><span class="line">        missing=<span class="keyword">None</span>, </span><br><span class="line">        n_estimators=<span class="number">2</span>,</span><br><span class="line">        n_jobs=<span class="number">1</span>, </span><br><span class="line">        nthread=<span class="keyword">None</span>, objective=<span class="string">'binary:logistic'</span>, random_state=<span class="number">0</span>,</span><br><span class="line">        reg_alpha=<span class="number">0</span>, </span><br><span class="line">        reg_lambda=<span class="number">1</span>, </span><br><span class="line">        scale_pos_weight=<span class="number">1</span>, </span><br><span class="line">        seed=<span class="keyword">None</span>,</span><br><span class="line">        silent=<span class="keyword">True</span>, </span><br><span class="line">        subsample=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>XGBoost参数有3类：<br><a href="https://www.cnblogs.com/wanglei5205/p/8579244.html" target="_blank" rel="noopener">https://www.cnblogs.com/wanglei5205/p/8579244.html</a><br>（1）通用类别：不需要调整，默认就好：</p><ul><li>booster：[默认gbtree]<br>选择每次迭代的模型，有两种选择：<br>gbtree：基于树的模型<br>gbliner：线性模型</li><li>silent[默认0]<br>当这个参数值为1时，静默模式开启，不会输出任何信息。<br>一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</li><li>nthread[默认值为最大可能的线程数]<br>这个参数用来进行多线程控制，应当输入系统的核数。<br>如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。</li></ul><p>（2）学习目标参数：与任务有关</p><ul><li>objective:损失函数，支持分类/回归<br>[默认reg:linear]，这个参数定义需要被最小化的损失函数。最常用的值有：<br>binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。<br>multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。<br>在这种情况下，你还需要多设一个参数：num_class(类别数目)。<br>multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</li><li><p>eval_metric：评价函数，对于回归问题，默认值是rmse，对于分类问题，默认值是error。<br>典型值有：<br>rmse 均方根误差<br>logloss 负对数似然函数值<br>error 二分类错误率(阈值为0.5)<br>merror 多分类错误率<br>mlogloss 多分类logloss损失函数<br>auc 曲线下面积</p></li><li><p>seed：随机数的种子，默认为0<br>设置它可以复现随机数据的结果，也可以用于调整参数</p></li></ul><p>（3）booster参数：弱学习器参数，需要仔细调整，会影响模型性能<br>学习率和n_estimators具有相反的关系，建议学习率设小，通过交叉验证确定n_estimators</p><ul><li>eta[默认0.3]，和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。     </li></ul><p><strong>和树有关的参数</strong></p><ul><li>min_child_weight[默认1]，最小样本权重的和，用于避免过拟合。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</li><li>max_depth[默认6]，树的最大深度。 用来避免过拟合的。max_depth越大，模型越复杂，学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10      </li><li>gamma[默认0]，Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。   </li><li>subsample[默认1]<br>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。<br>典型值：0.5-1  </li><li>colsample_bytree[默认1]<br>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。<br>典型值：0.5-1  </li></ul><p><strong>和正则化有关的参数</strong></p><ul><li>lambda[默认1]<br>权重的L2正则化项。(和Ridge regression类似)。<br>这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。  </li><li>alpha[默认1]<br>权重的L1正则化项。(和Lasso regression类似)。<br>可以应用在很高维度的情况下，使得算法的速度更快。<br><strong>样本不均衡</strong> </li><li>scale_pos_weight[默认1]<br>正样本占的比重，为1时表示正负样例比重是一样的。当正样本较少时，正样本:负样本=1:9，将scale_pos_weight设置为9，scale_pos_weight=负样本个数/正样本个数。在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。   <h2><span id="33-调参">3.3. 调参</span></h2></li></ul><ol><li><p>先给定一个较高的学习率(learning rate)，一般情况下，学习率为0.1，但是对于不同的问题，理想的学习率在0.05~0.3之间波动。先调节决策树的数量n_estimators</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">20</span>,<span class="number">40</span>, <span class="number">60</span>, <span class="number">80</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">model = XGBClassifier(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">5</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'参数的最佳取值：&#123;0&#125;'</span>.format(optimized_GBM.best_params_))</span><br><span class="line">print(<span class="string">'最佳模型得分:&#123;0&#125;'</span>.format(optimized_GBM.best_score_))</span><br><span class="line">display(pd.DataFrame(optimized_GBM.cv_results_).T)</span><br></pre></td></tr></table></figure></li><li><p>在给定的learning rate和n_eatimators情况下，对决策树特定参数调优(max_depth,min_child_weight,gamma,subsample,colsample_bytree) </p></li><li><p>max_depth和min_child_weight参数调优</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'max_depth'</span>: list(range(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)), <span class="string">'min_child_weight'</span>: list(range(<span class="number">1</span>,<span class="number">7</span>,<span class="number">1</span>))&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">60</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">```   </span><br><span class="line"><span class="number">4.</span> gamma参数调优   </span><br><span class="line">Gamma参数取值范围可以很大，我这里把取值范围设置为<span class="number">5</span>了。你其实也可以取更精确的gamma值。  </span><br><span class="line">  ```python</span><br><span class="line">  cv_params = &#123;<span class="string">'gamma'</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">  other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">350</span>, <span class="string">'max_depth'</span>: <span class="number">3</span>, <span class="string">'min_child_weight'</span>: <span class="number">5</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">  ```   </span><br><span class="line"><span class="number">5.</span> subsamplehe colsample_bytree参数   </span><br><span class="line">   这链各个参数相当于每个树的样本和特征个数。</span><br><span class="line">  ```python</span><br><span class="line">   cv_params = &#123;  </span><br><span class="line">    <span class="string">'subsample'</span>: [i / <span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>, <span class="number">10</span>)],  </span><br><span class="line">    <span class="string">'colsample_bytree'</span>: [i / <span class="number">10.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>, <span class="number">10</span>)]  </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>正则化参数调优<br>下一步应用正则化来降低过拟合。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">'reg_alpha'</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">'reg_lambda'</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>学习率调优<br>最后使用较低的学习率</p></li></ol><h1><span id="4-gridsearchcv">4. GridSearchCV</span></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’)</span><br></pre></td></tr></table></figure><p>GridSearchCV参数介绍：</p><ul><li>estimator：使用的分类器，并且传入除需要确定最佳的参数之外的其他参数</li><li>param_grid：值为字典或者列表，即需要最优化的参数的取值，param_grid = {‘n_estimators’:list(range(10,71,10))}</li><li>scoring :准确度评价标准，默认None,表示“GridSearchCV”与“cross_val_score”都会去调用“estimator”自己的“score”；或者如scoring=’roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。scoring参数选择如下：</li><li><p>cv :交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，传入的参数可以是int型，也可以是yield训练/测试数据的生成器。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kflod = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle = <span class="keyword">True</span>,random_state=<span class="number">7</span>)<span class="comment">#将训练/测试数据集划分10个互斥子集，</span></span><br><span class="line">grid_search = GridSearchCV(model,param_grid,scoring = <span class="string">'neg_log_loss'</span>,n_jobs = <span class="number">-1</span>,cv = kflod)</span><br></pre></td></tr></table></figure></li><li><p>refit :默认为True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。</p></li><li>iid:默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。</li><li>verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出</li><li>n_jobs: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。  </li></ul><p><strong>常用的方法</strong></p><ul><li>grid.fit(X, y=None, groups=None, **fit_params)：运行网格搜索，与所有参数组合运行。</li><li><p>cv_results_：旧版本是“grid_scores_”，cv_results_是详尽、升级版。内容较好理解，包含了’mean_test_score’(验证集平均得分)，’rank_test_score’(验证集得分排名)，’params’(dict形式存储所有待选params的组合)，甚至还有在每次划分的交叉验证中的得分（’split0_test_score’、 ‘split1_test_score’等），就是输出的内容稍显臃肿。内容以dict形式输出，我们可以转成DataFrame形式，看起来稍微养眼一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv_result = pd.DataFrame.from_dict(clf.cv_results_)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">means = grid_result.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">'params'</span>]</span><br><span class="line"><span class="keyword">for</span> mean,param <span class="keyword">in</span> zip(means,params):</span><br><span class="line">    print(<span class="string">"%f  with:   %r"</span> % (mean,param))</span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line">    display(pd.DataFrame(grid.cv_results_).T)</span><br><span class="line">    ```   </span><br><span class="line">    参考资料：[https://blog.csdn.net/sinat_32547403/article/details/<span class="number">73008127</span>](https://blog.csdn.net/sinat_32547403/article/details/<span class="number">73008127</span>)</span><br><span class="line">- best_estimator_ : estimator或dict；由搜索选择的估算器，即在左侧数据上给出最高分数（或者如果指定最小损失）的估算器。 如果refit = <span class="keyword">False</span>，则不可用。</span><br><span class="line">- best_params_ : dict；在保持数据上给出最佳结果的参数设置。对于多度量评估，只有在指定了重新指定的情况下才会出现。</span><br><span class="line">- best_score_ : float；best_estimator的平均交叉验证分数，对于多度量评估，只有在指定了重新指定的情况下才会出现。</span><br><span class="line">- get_params（[deep]）：这个和‘best_estimator_ ’这个属性相似，但可以得到这个模型更多的参数</span><br><span class="line">- inverse_transform（Xt）使用找到的最佳参数在分类器上调用inverse_transform。</span><br><span class="line">- predict（X）调用使用最佳找到的参数对估计量进行预测，X：可索引，长度为n_samples；</span><br><span class="line">- score（X, y=<span class="keyword">None</span>）返回给定数据上的分数，X： [n_samples，n_features]输入数据，其中n_samples是样本的数量，n_features是要素的数量。y： [n_samples]或[n_samples，n_output]，可选，相对于X进行分类或回归; 无无监督学习。</span><br><span class="line">  </span><br><span class="line">```python    </span><br><span class="line">cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">100</span>, <span class="number">125</span>, <span class="number">150</span>, <span class="number">175</span>, <span class="number">200</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, </span><br><span class="line">                <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">0</span>, <span class="string">'reg_alpha'</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = XGBClassifier(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">5</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">evalute_result = optimized_GBM.cv_results_</span><br><span class="line"></span><br><span class="line"><span class="comment"># print('每轮迭代运行结果:&#123;0&#125;'.format(evalute_result))</span></span><br><span class="line">print(<span class="string">'参数的最佳取值：&#123;0&#125;'</span>.format(optimized_GBM.best_params_))</span><br><span class="line">print(<span class="string">'最佳模型得分:&#123;0&#125;'</span>.format(optimized_GBM.best_score_))</span><br><span class="line">```      </span><br><span class="line">网格搜索建立在交叉验证的基础上。交叉验证将训练集分成N份，其中N<span class="number">-1</span>份做训练，<span class="number">1</span>份做测试。先选定一个待验证的参数，然后做N次训练和测试，得到平均值，然后再选定下一个参数，做N次训练和测试。     </span><br><span class="line"><span class="comment"># 5. 样本不均衡分类问题      </span></span><br><span class="line">参考资料：[https://github.com/wmlba/innovate2019/blob/master/Credit_Card_Fraud_Detection.ipynb](https://github.com/wmlba/innovate2019/blob/master/Credit_Card_Fraud_Detection.ipynb)   </span><br><span class="line"><span class="comment">## 5.1. 特征工程   </span></span><br><span class="line"><span class="number">1.</span> 特征缩放   </span><br><span class="line">使用归一化或标准化对特征进行缩放，使得不同特征值在同一量纲，   </span><br><span class="line"></span><br><span class="line">```python  </span><br><span class="line"><span class="comment">#使用 sklearn中的 scale 函数</span></span><br><span class="line">minmax_scaler = preprocessing.MinMaxScaler()   <span class="comment">#创建 MinMaxScaler对象</span></span><br><span class="line">df_m1 = minmax_scaler.fit_transform(df)    <span class="comment"># 标准化处理</span></span><br><span class="line">df_m1 = pd.DataFrame(df_m1,columns=[<span class="string">'value1_m'</span>,<span class="string">'value2_m'</span>])</span><br><span class="line">df_m1.head()</span><br><span class="line">```   </span><br><span class="line">```python</span><br><span class="line"><span class="comment">#Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#RobustScaler is robust to outliers.</span></span><br><span class="line">credit_df[<span class="string">'amount_after_scaling'</span>] = RobustScaler().fit_transform(credit_df[<span class="string">'Amount'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">credit_df[<span class="string">'time_after_scaling'</span>] = RobustScaler().fit_transform(credit_df[<span class="string">'Time'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">credit_df.drop([<span class="string">'Time'</span>,<span class="string">'Amount'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Place the class in the begining of the dataframe</span></span><br><span class="line">Class = credit_df[<span class="string">'Class'</span>]</span><br><span class="line">credit_df.drop([<span class="string">'Class'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">credit_df.insert(<span class="number">0</span>, <span class="string">'Class'</span>, Class)</span><br></pre></td></tr></table></figure></li></ul><ol><li>解决样本不均衡问题<br>欠采样或过采样<br><img src="/2019/04/24/机器学习/resample.png" alt="样本不平衡">    </li><li><p>检测和删除异常点   </p></li><li><p>划分数据集<br>划分数据集：训练集，验证集，测试集   </p></li></ol><h1><span id="6-过采样">6. 过采样</span></h1><p>&ensp;&ensp;&ensp;&ensp;分类问题时，样本不均衡，正例和负例的样本数不均衡，为了实现样本均衡，需要对样本比较少的那类数据进行过采样。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 机器学习&lt;br&gt;date: 2019-04-24T23:52:10.000Z&lt;br&gt;tags:&lt;/p&gt;
&lt;h2 id=&quot;特征预处理、模型评估、分类&quot;&gt;&lt;a href=&quot;#特征预处理、模型评估、分类&quot; class=&quot;headerlink&quot; title=&quot;  - 特征预处理、模型评估、分类&quot;&gt;&lt;/a&gt;  - 特征预处理、模型评估、分类&lt;/h2&gt;&lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;最近在做一个分类任务，根据电池的充放电数据，预测电池绝缘报警是否为虚报，就是一个二分类任务。这里使用逻辑回归进行分类。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>pyspark</title>
    <link href="http://yoursite.com/2019/04/20/pyspark/"/>
    <id>http://yoursite.com/2019/04/20/pyspark/</id>
    <published>2019-04-20T04:40:56.000Z</published>
    <updated>2019-05-21T12:14:49.501Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;使用Python编写Spark程序<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD">2. 重要概念和术语</a></li><li><a href="#3-%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%BC%8F">3. 执行模式</a><ul><li><a href="#31-standalone%E6%A8%A1%E5%BC%8F">3.1. standalone模式</a></li><li><a href="#32-yarn%E6%A8%A1%E5%BC%8F">3.2. Yarn模式</a></li><li><a href="#33-%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98">3.3. 参数调优</a></li><li><a href="#executor">Executor</a></li></ul></li><li><a href="#4-%E5%88%9B%E5%BB%BAsc">4. 创建sc</a></li><li><a href="#5-rdd%E8%BD%AC%E6%8D%A2">5. RDD转换</a><ul><li><a href="#51-%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C">5.1. 转换操作</a></li><li><a href="#52-%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C">5.2. 行动操作</a></li><li><a href="#53-%E6%8C%81%E4%B9%85%E5%8C%96">5.3. 持久化</a></li></ul></li><li><a href="#6-%E5%88%86%E5%8C%BA">6. 分区</a></li><li><a href="#7-%E5%88%9B%E5%BB%BArdd">7. 创建RDD</a><ul><li><a href="#71-%E9%80%9A%E8%BF%87paralize%E5%88%9B%E5%BB%BArdd">7.1. 通过paralize创建RDD</a></li><li><a href="#72-%E8%AF%BB%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BArdd">7.2. 读文本文件创建RDD</a></li></ul></li><li><a href="#8-map%E5%92%8Cflatmap">8. map和flatMap</a></li><li><a href="#9-flatmap%E5%92%8Cflatmapvalues">9. flatMap和flatMapValues</a></li><li><a href="#10-reducebykey%E5%92%8Cgroupbykey">10. reduceByKey和groupByKey</a></li><li><a href="#11-sortby%E5%92%8Csortbykey">11. sortBy和SortByKey</a></li><li><a href="#12-%E5%B0%86spark%E8%AE%A1%E7%AE%97%E7%9A%84%E7%BB%93%E6%9E%9C%E5%AD%98%E5%82%A8%E5%9C%A8%E6%96%87%E4%BB%B6%E4%B8%AD">12. 将Spark计算的结果存储在文件中</a><ul><li><a href="#121-%E5%86%99%E5%85%A5%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E4%B8%AD">12.1. 写入到服务器本地文件中</a></li><li><a href="#122-%E5%86%99%E5%85%A5%E5%88%B0hdfs%E6%96%87%E4%BB%B6%E4%B8%AD">12.2. 写入到HDFS文件中</a></li><li><a href="#123-%E6%89%93%E5%8D%B0rdd%E5%85%83%E7%B4%A0">12.3. 打印RDD元素</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-重要概念和术语">2. 重要概念和术语</span></h1><ul><li>Master和Worker是物理节点，Driver和Executor是进程。<br>搭建Spark集群的时候我们就已经设置好了Mater节点和Worker节点。一个集群有多个Master节点和多个Worker节点。<br>Master节点常驻Mater守护进程，负责管理worker节点，我们从master节点提交应用。<br>Worker节点常驻Worker守护进程，与Master节点通信，并且管理Executor进程。<br>PS：一台机器可以同时作为master和worker节点（举个例子：你有四台机器，你可以选择一台设置为master节点，然后剩下三台设为worker节点，也可以把四台都设为worker节点，这种情况下，有一个机器既是master节点又是worker节点）</li><li><p>Driver / Driver Program<br>运行main函数并且创建SparkContext的程序。客户端的应用程序，Driver Program类似于wordcount程序中的mian函数。<br>当我们提交应用程序后，便会启动一个对应的Driver进程。Driver会根据我们设置的参数占用一定的资源（主要是CPU核数、内存）。<br>程序启动时，Driver进程首先会向集群资源管理者（Standalone，Mesos，Yarn）申请Spark应用所需的资源，也就是Executor，然后集群管理者会根据Spark应用所设置的参数在各个Worker上分配一定数量的Executor，每个Executor都占用一定数量的CPU和Memory。在申请到应用所需的资源后，Driver就开始调度和执行我们的程序了。Driver进程会把我们编写的Spark程序拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor中执行。<br>Executor进程在Worker节点上，一个Worker可以有多个Executor，每个Executor都有一个进程池，每个进程执行一个task。Executor执行完task之后将结果返回给Driver，每个Executor执行的task属于一个spark程序。此外Executor还有一个功能是为应用程序中的RDD提供内存，RDD是直接缓存在Executor进程内的。<br><a href="https://blog.csdn.net/hongmofang10/article/details/84587262" target="_blank" rel="noopener">这篇博客讲的很好</a><br><a href="https://blog.csdn.net/qq_21383435/article/details/78653427" target="_blank" rel="noopener">通俗易懂</a> </p><pre><code>spark-submit --master yarn --num-executors 32 --executor-memory 8G --executor-cores 8 --jars ../jars/spark-examples_2.10_my_converters_test-1.6.0.jar spark_streaming_all.py</code></pre><p>其中参数的含义：  </p><ul><li>num-executors：创建多少个 executor</li><li>executor-memory：各个 executor 使用的最大内存，不可超过单机的最大可使用内存</li><li>executor-cores：各个 executor 使用的并发线程数目，也即每个 executor 最大可并发执行的 Task 数目</li></ul></li><li><p>Cluster Manager<br>集群的资源管理器，在集群上获取资源的外部服务，例如Standalone，Mesos，Yarn。<br>拿Yarn举例，客户端程序会向Yarn申请运行我这个任务需要多少，多少CPU等，然后Cluster Manager会通过调度告诉客户端可以使用，然后客户端就可以把程序送到每个Worker Node上面执行。    </p><h1><span id="3-执行模式">3. 执行模式</span></h1><p>&ensp;&ensp;&ensp;&ensp;运行spark程序有3种模式，local，standalone，yarn。在使用spark-submit命令提交程序时，需要指定一些参数。   </p></li><li>—master:如spark://host:7077, mesos://host:port, yarn,  yarn-cluster,yarn-client, local   </li><li>—calss CLASS_NAME 应用程序的主类   </li><li>—name NAME 应用程序的名称,这个可以在程序中通过setAppName(“kafka_hbase”)指定  </li><li>—jars JARS 逗号分隔的本地jar包，后面添加jar的路径</li><li>—driver-memory MEM Driver内存，默认1G</li><li>—num-executors NUM，启动的executor的个数，默认为2，在yarn中使用。</li><li>—executor-core NUM，每个executor的核数。在yarn或者standalone下使用</li><li>—executor-memory MEM 每个executor的内存，默认是1G</li><li>—total-executor-cores NUM,所有executor总共的核数，仅仅在mesos或standalone中使用</li><li>driver-cores NUM Driver的核数，默认是1，这个参数只在standalone模式下使用   <h2><span id="31-standalone模式">3.1. standalone模式</span></h2>&ensp;&ensp;&ensp;&ensp;运行一个pyspark程序，使用standalone模式来提交程序，需要使用的参数有：<br>—master spark://hz4:7077<br>—jars xxx1.jar,xxx2.jar<br>不使用—num-executors,这个在yarn中使用<br>—executor-memory MEM,每个executor占用的内存，如果一个executor占用4G，有5个executor，那这个程序占用20G<br>—executor-core NUM，表示每个executor的核数<br><strong>—total-executor-cores NUM</strong>,所有的executor占用的核数。使用total-executor-cores / executor-core得到executor的个数，假设total-executor-cores设置为30，executor-core为6，则表示运行这个程序一共有5个executor，分别在不同worker上。一个worker可以有多个executor。 假设有5个executor，2个worker，那么一个worker上有多个executor。如果不指定—total-executor-cores，程序会把worker上的核全都占用，这样别人提交程序的时候就没有办法运行。<br>&ensp;&ensp;&ensp;&ensp;运行一个程序的命令：spark-submit —master spark://hz4:7077  —executor-memory 4G —executor-cores 6 —total-executor-cores 30 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar spark_streaming.py   <h2><span id="32-yarn模式">3.2. Yarn模式</span></h2>&ensp;&ensp;&ensp;&ensp;yarn模式可以用的参数有：<br>—master yarn<br>—jars xxx1.jar,xxx2.jar<br><strong>—num-executors NUM</strong>, 启动的executor的个数，默认为2，不要使用默认，会很慢。在yarn中使用。yarn资源管理器会在不同的worker上分配executor给程序。<br>—executor-memory MEM,每个executor占用的内存，如果一个executor占用4G，有5个executor，那这个程序占用20G<br>—executor-core NUM，表示每个executor的核数，如果有5个executor，每个executor占用4G，那这个程序运行时占用20G内存。<br>&ensp;&ensp;&ensp;&ensp;运行一个程序的命令：spark-submit —master yarn —num-executors 20 —executor-memory 4G —executor-cores 4 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar spark_streaming.py <h2><span id="33-参数调优">3.3. 参数调优</span></h2>&ensp;&ensp;&ensp;&ensp;</li><li>num-executors：该参数用于设置Spark作业总共要用多少个Executor进程来执行,Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在<br>集群的各个工作节点上，启动相应数量的Executor进程。<br><strong>参数调优建议</strong>：<br>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；<br>设置的太多的话，大部分队列可能无法给予充分的资源。  </li><li>executor-memory：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。<br><strong>参数调优建议</strong>：<br>每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列<br>的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，<br>那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。   </li><li>executor-cores：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个<br>task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。<br><strong>参数调优建议</strong>：<br>Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的<br>Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过<br>队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。  </li><li>driver-memory：该参数用于设置Driver进程的内存。<br><strong>参数调优建议</strong>：<br>Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，<br>那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。  </li><li>—conf spark.default.parallelism：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<br><strong>参数调优建议</strong>：<br>Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量<br>来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会<br>导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的<br>Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍<br>较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。<h2><span id="executor">Executor</span></h2>&ensp;&ensp;&ensp;&ensp;在运行pyspark程序时出错：   Container killed by YARN for exceeding memory limits. 16.9 GB of 16 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead”这个错误总会使你的job夭折。它的意思是：因为超出内存限制，集群停掉了container。<br>Spark的Excutor的Container内存有两大部分组成：Excutor内存和堆外内存。   </li></ul><p><img src="/2019/04/20/pyspark/executor.png" alt="">     </p><p>Spark底层shuffle的传输方式是使用netty传输，netty在进行网络传输的过程会申请堆外内存（netty是零拷贝），所以使用了堆外内存，即spark.yarn.executor.memoryOverhead。<br><strong>Executor内存</strong><br>又spark.executor.memory参数设置，在spark-shell中由—executor-memory指定，分为2部分，shuffle.memoryFraction和storage.memoryFraction。<br><strong>spark.shuffle.memoryFractio</strong><br>该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。<br><strong>参数调优</strong><br>如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br><strong>spark.storage.memoryFractio</strong><br>该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。<br><strong>参数调优</strong><br>如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。         </p><p><strong>spark.yarn.executor.memoryOverhead</strong><br>executor执行的时候，用的内存可能会超过executor-memoy，所以会为executor额外预留一部分内存。spark.yarn.executor.memoryOverhead代表了这部分内存。这个参数如果没有设置，会有一个自动计算公式(位于ClientArguments.scala中)，代码如下：<br>其中，MEMORY_OVERHEAD_FACTOR默认为0.1，executorMemory为设置的executor-memory, MEMORY_OVERHEAD_MIN默认为384m。参数MEMORY_OVERHEAD_FACTOR和MEMORY_OVERHEAD_MIN一般不能直接修改，是Spark代码中直接写死的。</p><p>关于Executor 计算的相关公式，见源码org.apache.spark.deploy.yarn.Clent，org.apache.spark.deploy.yarn.ClentArguments<br>主要部分如下 </p><pre><code class="lang-python">var executorMemory = 1024 // 默认值，1024MBval MEMORY_OVERHEAD_FACTOR = 0.10  // OverHead 比例参数，默认0.1val MEMORY_OVERHEAD_MIN = 384val executorMemoryOverhead = sparkConf.getInt(&quot;spark.yarn.executor.memoryOverhead&quot;,math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))// 假设有设置参数，即获取参数，否则使用executorMemoryOverhead 的默认值val executorMem = args.executorMemory + executorMemoryOverhead// 最终分配的executor 内存为 两部分的和</code></pre><p><strong>解决方案</strong><br>在参数中设置<strong>spark.yarn.executor.memoryOverhead=4096</strong>，单位是MB，一般是2的幂,这里使用4G，默认申请的堆外内存是Executor内存的10%，真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G）</p><p>spark-submit —master yarn —num-executors 20 —executor-memory 4G —executor-cores 4 —conf spark.yarn.executor.memoryOverhead=4096 —jars ../jars/spark-examples_2.10_my_converters-1.6.0.jar feature_extraction.py    </p><p><strong>executor-memory+spark.yarn.executor.memoryOverhead&lt;MonitorMemory</strong><br>指定的 ExecutorMemory与MemoryOverhead 之和大于 MonitorMemory，则会导致Executor申请失败，程序直接不能运行；若运行过程中，实际使用内存超过上限阈值，Executor进程会被Yarn终止掉（kill）</p><p>在运行程序中发现CPU的占用率不高，，增加num-executors的个数，减少executor-cores的个数<br>参考资料：<br><a href="https://www.cnblogs.com/haozhengfei/p/5fc4a976a864f33587b094f36b72c7d3.html" target="_blank" rel="noopener">https://www.cnblogs.com/haozhengfei/p/5fc4a976a864f33587b094f36b72c7d3.html</a><br><a href="https://blog.csdn.net/hammertank/article/details/48346285" target="_blank" rel="noopener">https://blog.csdn.net/hammertank/article/details/48346285</a><br><a href="http://www.raychase.net/3546" target="_blank" rel="noopener">http://www.raychase.net/3546</a><br><a href="https://www.jianshu.com/p/10e91ace3378" target="_blank" rel="noopener">https://www.jianshu.com/p/10e91ace3378</a></p><h1><span id="4-创建sc">4. 创建sc</span></h1><p>&ensp;&ensp;&ensp;&ensp;在服务器中的命令行中，输出：pyspark，会打开spark-shell交互窗口，这时spark-shell会自动创建一个sc，不用再创建sc，手动创建了也不能用，会出错。如果在py文件中写程序，首先需要手动创建一个sc。   </p><pre><code class="lang-python">from pyspark import SparkConf, SparkContextconf = SparkConf().set(&quot;spark.executorEnv.PYTHONHASHSEED&quot;, &quot;0&quot;).setAppName(&quot;kafka_hbase&quot;)sc = SparkContext(conf=conf)</code></pre><p>或者使用    </p><pre><code class="lang-python">from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#39;local&#39;).setAppName(&#39;My_App&#39;)sc = SparkContext(conf = conf)</code></pre><p>首先创建一个SparkConf对象来配置应用，然后基于该SparkConf来创建一个SparkContext对象。<br><code>.setMaster()</code>给定了集群的URL，高速spark如何连接到集群上，这里的<code>local</code>表示让spark运行在单机单变成上。<br>也可以是<code>.setMaster(&#39;spark://192.168.1.11:7077&#39;)</code>表示使用standalone运行spark程序。<br><code>.setAppName()</code>给出应用的名字，当连接到一集群上时，这个值可以帮助你找到你的应用。      </p><h1><span id="5-rdd转换">5. RDD转换</span></h1><p>&ensp;&ensp;&ensp;&ensp;RDD被创建好之后，在后续使用过程中有2中操作：</p><ul><li>转换（Transformation）：基于现有的RRD创建一个新的RDD</li><li>行动（Action）：在数据集上进行运算，返回计算值。<h2><span id="51-转换操作">5.1. 转换操作</span></h2>&ensp;&ensp;&ensp;&ensp;对于RDD而言，每一次转换操作都会产生不同的RDD，如果说rdd2 = rdd1.map(lamda x : x+1),rdd1的值不会改变，通过转换操作返回一个新的rdd供下一个转换操作。转换得到的RDD是惰性的，也就是说，整个过程只记录了转换的轨迹，并不会发生真正的计算，只有遇到Action操作时，才会发生真正的计算。开始从血缘关系源头开始，进行物理的转换操作。<br>&ensp;&ensp;&ensp;&ensp;下面列出一些常见的转换（Transformation）操作。</li></ul><ul><li>filter(func)：筛选出满足函数func的元素，并返回一个新的数据集</li><li>map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集</li><li>flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果</li><li>groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</li><li>reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合   <h2><span id="52-行动操作">5.2. 行动操作</span></h2>&ensp;&ensp;&ensp;&ensp;行动操作是真正触发计算的地方。Spark程序执行到行动操作，才会执行真正的计算，从文件中加载数据，完成一次有一次转换操作，最终，完成行动操作得到结果。<strong>在触发Action操作时，开始真正的计算，这时，Spark会把计算分解成多个任务在不同机器上执行，每台机器上运行位于属于它自己的map和reduce，最后把结果返回给Driver Program</strong>。<br>&ensp;&ensp;&ensp;&ensp;下面给出一些常见的行动（Action）操作</li><li>count() 返回数据集中的元素个数</li><li>collect() 以数组的形式返回数据集中的所有元素</li><li>first() 返回数据集中的第一个元素</li><li>take(n) 以数组的形式返回数据集中的前n个元素</li><li>reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</li><li>foreach(func) 将数据集中的每个元素传递到函数func中运行<h2><span id="53-持久化">5.3. 持久化</span></h2>&ensp;&ensp;&ensp;&ensp;在Spark中，RDD采用惰性的机制，每次遇到Action操作，都会从头开始执行计算。如果整个Spark程序只有一次Action操作，当然不会又什么问题。但是，在一些情况下，我们需要对一个RDD多次调用不同的Action，这就意味着，每次调用Action操作，都会触发一次从头开始的计算，代价很大，并且这些Action操作都是对一个RDD而言，所以可以把这个RDD持久化。<br>比如下面是多次对一个RDD进行Action操作   <pre><code class="lang-python">list = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]rdd = sc.parallelize(list)#count()是一个Action操作，触发一次真正从头到尾的计算print(rdd.count())&gt;&gt;&gt;3#collect()也是一个Action()操作，触发一个真正从头到尾的计算print(&#39;,&#39;.join(rdd.collect()))&gt;&gt;&gt;a,b,c</code></pre>&ensp;&ensp;&ensp;&ensp;上面代码执行过程中，前后共触发了2次从头到尾的计算。<br>&ensp;&ensp;&ensp;&ensp;实际上，可以通过持久化(缓存)机制避免这种重复计算的开销。可以使用persist()方法对一个RDD<strong>标记为持久化</strong>，之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算RDD并把它持久化，而是要等到第一个Action操作触发时，才开始计算RDD，并把RDD的内容进行持久化。持久化的RDD将会被保留在计算节点的内存中，以便被后面的Action操作重复使用。<br>&ensp;&ensp;&ensp;&ensp;persist()方法可以传入持久化级别参数。   </li><li>persist(MEMOEY_ONLY)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容。   </li><li>persist(MEMORY_AND_DISK)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存储在硬盘上。</li><li>一般使用cache()方法时，会调用persist(MEMORY_ONLY)   <pre><code class="lang-python">list = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]rdd = sc.parallelize(list)#会调用persist(MEMORY_ONLY)，但是语句执行到这里，并不会缓存rdd的内容，因为这时rdd还没有被计算生成rdd.cache()#count()是一个Action操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache()，把这个rdd的内容放在缓存中。print(rdd.count())&gt;&gt;&gt;3#collect()也是一个Action()操作，不需要触发一个真正从头到尾的计算，只需要重复使用上面缓存中的rdd。print(&#39;,&#39;.join(rdd.collect()))&gt;&gt;&gt;a,b,c</code></pre>最后，可以使用unpersist()方法手动地把持久化的RDD从缓存中移除。    <h1><span id="6-分区">6. 分区</span></h1>&ensp;&ensp;&ensp;&ensp;RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上。RDD的一个分区原则是使得分区的个数尽量等于集群中CPU核心（core）数目。<br>&ensp;&ensp;&ensp;&ensp;对于不同的Spark部署而言（local，Standalone,yarn，Mesos）,都可以通过设置spark.default.parallelism这个参数的值，来配置默认的分区数据，一般而言：</li></ul><ul><li>local模式：默认为本地机器的CPU数目，若设置了local[N],则默认为N</li><li>Standalone和yarn：max(集群中所有CPU核心数目总和,2)作为默认值</li><li>Mesos：默认的分区数为8   <pre><code class="lang-scala">scala&gt;val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt;val rdd = sc.parallelize(array,2) #设置两个分区rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:29</code></pre>&ensp;&ensp;&ensp;&ensp;对于textFile而言，如果没有在方法中指定分区数，则默认为min(defaultParallelism,2)，其中，defaultParallelism对应的就是spark.default.parallelism。<br>&ensp;&ensp;&ensp;&ensp;如果是从HDFS中读取文件，则分区数为文件分片数(比如，128MB/片)。</li></ul><h1><span id="7-创建rdd">7. 创建RDD</span></h1><p>&ensp;&ensp;&ensp;&ensp;创建RDD有2种方式，一种是通过列表创建，一种是通过读取文件创建。<strong>RDD的内部其实是一个Iterator\<string\></string\></strong></p><h2><span id="71-通过paralize创建rdd">7.1. 通过paralize创建RDD</span></h2><pre><code class="lang-python">string=&#39;a\nb\nc\na\nd\ne&#39;b = string.split(&#39;\n&#39;)sc.parallelize(b)</code></pre><p><code>b</code>是一个list列表，通过列表b可以创建一个RDD。      </p><h2><span id="72-读文本文件创建rdd">7.2. 读文本文件创建RDD</span></h2><p>&ensp;&ensp;&ensp;&ensp; 读取文本文件获取RDD，可以从服务器本地读取(其他节点也可以)，也可以从hdfs上读取文件。文本每行的内容以字符串的形式作为RDD的一个元素。<br>从服务器本地读取文件时，需要加上file://</p><pre><code class="lang-python">rdd1 = sc.textFile(&quot;file:///file0/input/test.txt&quot;)</code></pre><p>从HDFS上读取文件   </p><pre><code class="lang-python">rdd1 = sc.textFile(&#39;hdfs://master:8020/pc2/data.csv&#39;)</code></pre><h1><span id="8-map和flatmap">8. map和flatMap</span></h1><p>&ensp;&ensp;&ensp;&ensp;map是对RDD中的每个元素执行一个函数，每个元素返回一个list，然后把每个元素的list再组成一个大的list，例如[[a,a],[b,b]]，然后flatMap就是先对每个元素执行一个函数，每个元素返回一个list，然后把每个元素的list的内容取出来，组成一个大的list，例如[a,a,b,b]。   </p><p>&ensp;&ensp;&ensp;&ensp;<a href="https://www.jianshu.com/p/c76ba3091a21" target="_blank" rel="noopener">这篇博客</a>讲解的比较好。说明flatMap中的函数返回类型一定是一个可迭代的类型，先把元素生成一个列表，然后再把每个列表中的元素取出来拼接成一个大的列表。<br><a href="https://www.4spaces.org/spark-map-flatmap/" target="_blank" rel="noopener">这篇也讲的很好</a>    </p><h1><span id="9-flatmap和flatmapvalues">9. flatMap和flatMapValues</span></h1><p>&ensp;&ensp;&ensp;&ensp;flatMap针对的RDD中的每个元素先做map操作，再做flatten操作，最后形成超大的list返回。flatMapValues只针对元素是<k,v>格式的RDD，原RDD中的key保持不变，只对value进行变换，变换之后的value和原来的key组成新的<k,v1>，作为RDD中的一个元素。参考<a href="http://blog.cheyo.net/172.html" target="_blank" rel="noopener">这篇博客</a></k,v1></k,v></p><h1><span id="10-reducebykey和groupbykey">10. reduceByKey和groupByKey</span></h1><p>&ensp;&ensp;&ensp;&ensp;推荐使用reduceByKey，<a href="https://blog.csdn.net/zongzhiyuan/article/details/49965021" target="_blank" rel="noopener">这篇博客</a>对于两者的区别进行了解释。<br>groupByKey涉及数据的shuffle操作，shuffle是spark重建数据的机制，将来自不同分区的数据进行分组，开销很大。    </p><h1><span id="11-sortby和sortbykey">11. sortBy和SortByKey</span></h1><p>&ensp;&ensp;&ensp;&ensp;sortByKey针对(key,value)对中的key进行排序。<br>&ensp;&ensp;&ensp;&ensp;sortBy可以根据我们需要的值进行排序，不一定是key，比如统计单词出现的次数，然后按照次数进行排序(key,value)，我们就是对value进行排序，可以使用sortBy函数。<br>sortBy()中有3个参数，第一个参数是一个函数，第二个参数是ascending，表示升序还是降序，默认是True(升序)。第三个参数是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等。</p><h1><span id="12-将spark计算的结果存储在文件中">12. 将Spark计算的结果存储在文件中</span></h1><h2><span id="121-写入到服务器本地文件中">12.1. 写入到服务器本地文件中</span></h2><p>&ensp;&ensp;&ensp;&ensp;假设pyspark计算的结果存储在results变量中，然后将<code>results</code>的内容存储在文件中。</p><pre><code class="lang-python"># 将结果写入到服务器本地的文件中filename = &#39;result.txt&#39;with open(filename,&#39;w&#39;) as f:    for line in results:        f.write(line)        f.write(&#39;\n&#39;)</code></pre><h2><span id="122-写入到hdfs文件中">12.2. 写入到HDFS文件中</span></h2><p>&ensp;&ensp;&ensp;&ensp;spark将RDD中的每个元素作为一行写入到文本文件中。在写入到HDFS之前，首先把results中的每个元素转成字符串的形式。<br>&ensp;&ensp;&ensp;&ensp;比如<code>rdd1</code>为<code>[(&#39;b&#39;,3),(&#39;a&#39;,2),(&#39;c&#39;,1)]</code>，<code>rdd1</code>中的每个元素是一个元组，需要把每个元素转换成字符串类型。<br><code>rdd2 = rdd1.map(lamda x: x[0]+&quot;,&quot;+str(x[1]))</code> ,然后使用<code>rdd2.saveAsTextFile(&#39;/tmp/word_count_result&#39;)</code>，把结果存储到<code>word_count_result</code>这个文件中，这个文件没有后缀名。    </p><h2><span id="123-打印rdd元素">12.3. 打印RDD元素</span></h2><p>&ensp;&ensp;&ensp;&ensp;在实际编程中，我们经常需要把RDD中的元素打印输出到屏幕上（标准输出stdout），一般会采用语句rdd.foreach(println)或者rdd.map(println)。当采用本地模式（local）在单机上执行时，这些语句会打印出一个RDD中的所有元素。但是，当采用集群模式执行时，在worker节点上执行打印语句是输出到worker节点的stdout中，而不是输出到任务控制节点Driver Program中，因此，任务控制节点Driver Program中的stdout是不会显示打印语句的这些输出内容的。为了能够把所有worker节点上的打印输出信息也显示到Driver Program中，可以使用collect()方法，比如，rdd.collect().foreach(println)，但是，由于collect()方法会把各个worker节点上的所有RDD元素都抓取到Driver Program中，因此，这可能会导致内存溢出。因此，当你只需要打印RDD的部分元素时，可以采用语句rdd.take(100).foreach(println)。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;使用Python编写Spark程序&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python、spark" scheme="http://yoursite.com/tags/python%E3%80%81spark/"/>
    
  </entry>
  
  <entry>
    <title>Python学习</title>
    <link href="http://yoursite.com/2019/04/15/Python%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/04/15/Python学习/</id>
    <published>2019-04-15T00:25:26.000Z</published>
    <updated>2019-06-10T00:11:39.891Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;学习Python！！！！<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E8%A7%84%E8%8C%83">2. 规范</a></li><li><a href="#3-%E5%BA%8F%E5%88%97">3. 序列</a></li><li><a href="#4-%E8%AF%8D%E5%85%B8">4. 词典</a></li><li><a href="#5-ndarray%E5%92%8Clist">5. ndarray和list</a><ul><li><a href="#51-%E5%88%9B%E5%BB%BAndarray">5.1. 创建ndarray</a><ul><li><a href="#511-%E9%80%9A%E8%BF%87nparray">5.1.1. 通过np.array</a></li><li><a href="#512-%E9%80%9A%E8%BF%87nparange">5.1.2. 通过np.arange</a></li><li><a href="#513-list%E5%92%8Cndarray%E7%9A%84%E7%B4%A2%E5%BC%95">5.1.3. list和ndarray的索引</a></li></ul></li></ul></li><li><a href="#6-pandas">6. Pandas</a></li></ul><!-- /TOC --><h1><span id="2-规范">2. 规范</span></h1><p>&ensp;&ensp;&ensp;&ensp;</p><ul><li>运算符的左右加空格，例如a + b</li><li>如果有多行赋值，将上下赋值的=对齐<br>num    = 1<br>secNum = 2</li><li>变量的所有字母小写，单词之间用下划线连接，table_name=’test’</li></ul><h1><span id="3-序列">3. 序列</span></h1><p>&ensp;&ensp;&ensp;&ensp;序列是一种容器，是有顺序的数据集合。序列有两种：元组（Tuple）和列表（List）。列表是可变的，元组是不可变的。所以经常会创建空的列表，a=[],而不会创建一个空的元组。<br>&ensp;&ensp;&ensp;&ensp;对序列(元组和列表)范围引用，a[起始,结束,步长]，包含起始，不包含结束。循环获取序列的值：for i in list，这里的a就是值，而不是下标      </p><h1><span id="4-词典">4. 词典</span></h1><p>&ensp;&ensp;&ensp;&ensp;词典中的数据是无序的，不能通过位置下标来获取，   </p><h1><span id="5-ndarray和list">5. ndarray和list</span></h1><p>&ensp;&ensp;&ensp;&ensp;list是Python的内置数据类型，list中的数据类型不必相同。例如：<code>[1,2,&#39;a&#39;,3.9]</code><br>&ensp;&ensp;&ensp;&ensp;首先需要明确的一点是array和ndarray是什么。ndarray是一种类型，array不是一种数据类型，可以通过np.array()来创建一个ndarray的对象。ndarray是numpy的一种数据类型，ndarray中的元素类型必须相同，例如：<code>[1,2,3,4]</code>    </p><h2><span id="51-创建ndarray">5.1. 创建ndarray</span></h2><h3><span id="511-通过nparray">5.1.1. 通过np.array</span></h3><p>&ensp;&ensp;&ensp;&ensp;通过np.array()来创建，传入的参数可以是list，也可以是tuple，使用ndarray的shape属性来获取ndarray的形状。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = np.array((<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>))</span><br><span class="line">c = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br></pre></td></tr></table></figure></p><p>使用reshape改变ndarray的形状<br><code>c.reshape((3,-1))</code>,reshape传入的形状是可以是<br>reshape((3,-1)),<br>reshape([3,1]),<br>reshape(3,-1)</p><h3><span id="512-通过nparange">5.1.2. 通过np.arange</span></h3><p>numpy提供了很多方法直接创建一个ndarray对象.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr1=np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>) <span class="comment">#   </span></span><br><span class="line">arr2=np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)  </span><br><span class="line"><span class="keyword">print</span> (arr1,arr1.dtype)  </span><br><span class="line"><span class="keyword">print</span> (arr2,arr2.dtype)</span><br><span class="line">```  </span><br><span class="line">结果</span><br></pre></td></tr></table></figure></p><p>[1 2 3 4 5 6 7 8 9] int32<br>[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.] float64<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">np.arange(a,b,c)表示产生从a~b，不包括b，间隔为c的一个ndarray，数据类型默认是int32。   </span><br><span class="line">np.linspace(a,b,c)表示把a~b（包括b），平均分成c份。    </span><br><span class="line">np.arange()和range都可以用来生成序列。注意arange是numpy的函数，range可以直接调用。arange和range不同的是：range只能生成int类型，写`rang(1,10,0.1)`是错误的，arange可以生成float类型，可以写成`np.arange(1,10,0.1)`  </span><br><span class="line">![](Python学习/range.png)        </span><br><span class="line">**使用print输出时，list中的元素之间有逗号分开，ndarray元素之间没有逗号**。   </span><br><span class="line">![](Python学习/print.png)  </span><br><span class="line">**虽然有很多产生ndarray类型的方法，但是大部分情况下我们都是从list进行转换生成ndarray。因为我们从文件中读取数据存储在list中，然后转换成ndarray**    </span><br><span class="line">比如定义一个list,a = [1,2,3,4],然后使用np.array(a)将list转换成ndarray类型。   </span><br><span class="line">### 5.1.3. list和ndarray的索引 </span><br><span class="line">定义一个list  </span><br><span class="line">`list1=[[1,2,3],[4,5,6],[7,8,9]]`  </span><br><span class="line">定义一个ndarray   </span><br><span class="line">`arr1 = np.array(list1)`   </span><br><span class="line"></span><br><span class="line">![](Python学习/list.png)</span><br><span class="line">![](Python学习/arr.png)    </span><br><span class="line">ndarray比list的索引方式更多，这也是两者经常遇到的区别。   </span><br><span class="line">**因为list可以存储任意类型的数据，因为list中存储数据存放的地址，简单说就是指针，并非数据，这样保存一个list就太麻烦了，例如list1=[1,2,3,&apos;a&apos;]就需要4个指针和4个数据，增加了存储和CPU消耗**         </span><br><span class="line"># 6. Pandas      </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp; DataFrame根据某一列排序，其中inplace=True表示修改df的值，默认是false，表示不修改df的值，会返回一个排好序的DataFrame。  </span><br><span class="line">```df.sort_values(&quot;数据时间&quot;,inplace=True)```   </span><br><span class="line">排好序的dataframe的index列还是原先dataframe的index。比如下面的图是排序之间的dataframe。   </span><br><span class="line">![](Python学习/排序前.png)    </span><br><span class="line">使用```df.sort_values(&quot;数据时间&quot;,inplace=True)```  按照时间排序，但是index还是原先的index，我想让排序后的dataframe的index从0开始。</span><br><span class="line">![](Python学习/排序后.png)      </span><br><span class="line">```df[2:4]```表示返回第3和4行的数据，即索引为3731，512这2行的数据，而不是返回索引为2和3的数据。 </span><br><span class="line">使用```sort_df.reset_index(drop=True, inplace=True)``` 重新定义索引，使其从0开始。</span><br><span class="line">![](Python学习/reindex.png)      </span><br><span class="line">获取dataframe中的索引  </span><br><span class="line">```firstIndex = df.index.tolist() ```  返回一个list，存储的是dataframe中的索引列表。</span><br><span class="line">```firstIndex = df.index.tolist()[0] ``` 返回的是第一行数据的索引   </span><br><span class="line">```firstIndex = df.index.tolist()[-1] ``` 返回的是最后一行数据的索引</span><br><span class="line">## 获取dataframe中的数据      </span><br><span class="line">![](Python学习/1.png) </span><br><span class="line">1. ```df[1:4]```表示获取表的第2至4行   </span><br><span class="line">![](Python学习/2.png)   </span><br><span class="line">2. df.head()默认返回dataframe中的前5行，如果返回前10行，使用head(10).    </span><br><span class="line">![](Python学习/3.png) </span><br><span class="line">3. 使用```df.iloc[]```和```df.loc[]```获取数据。     </span><br><span class="line">![](Python学习/4.png)   </span><br><span class="line">![](Python学习/5.png) </span><br><span class="line">通过  ```df.iloc[]```传入的参数是数据，而```df.loc[]```传入的参数是字符串索引，除非索引是数字，这时loc[]可以传入数字。  </span><br><span class="line">比如df1.loc[2]表示获取索引为&apos;2&apos;的那一行，而df.iloc[2]表示获取df1的第3行数据，是一个相对位置。  </span><br><span class="line">参考资料：  </span><br><span class="line">[https://www.jb51.net/article/141665.htm](https://www.jb51.net/article/141665.htm)  </span><br><span class="line">## DateFrame常用方法    </span><br><span class="line">- 获取df中某一列特征值的个数   </span><br><span class="line">`credit_df[&apos;Class&apos;].value_counts()`或   </span><br><span class="line">`credit_df[&apos;Class&apos;].value_counts()[0]`   </span><br><span class="line">- 显示df中的详细信息</span><br><span class="line">`df.info()`   </span><br><span class="line">`df.describe()`</span><br><span class="line">- 获取df中的所有列名   </span><br><span class="line">`col_names = list(df.columns.values)`     </span><br><span class="line">- 将df按照某一特征进行分组    </span><br><span class="line">`df.groupby([&apos;total_vol&apos;]).size()`获取每个组中元素的个数   </span><br><span class="line">`df.groupby([&apos;total_vol&apos;,&apos;soc&apos;]).size()`按照多个属性分组   </span><br><span class="line">- 从df中获取样本的特征和标签   </span><br><span class="line">```python   </span><br><span class="line">获取特征</span><br><span class="line">X = df.drop(&quot;误报&quot;,axis = 1)</span><br><span class="line">获取标签</span><br><span class="line">Y = df[&quot;误报&quot;]</span><br><span class="line">```   </span><br><span class="line">- 获取df中的一列或多列     </span><br><span class="line">```python </span><br><span class="line">one_col = df[&apos;total_vol&apos;]</span><br><span class="line">multi_cols_name = [&apos;total_vol&apos;,&apos;soc&apos;,&apos;cur&apos;]   </span><br><span class="line">multi_cols = df[multi_cols_name]   </span><br><span class="line">```   </span><br><span class="line">- 查看df中为空的个数,输出每一列为nan的个数 </span><br><span class="line">`df.isna().sum()`</span><br><span class="line"># Dict   </span><br><span class="line">python创建一个字典有3中方式   </span><br><span class="line">```python</span><br><span class="line">class dict(**kwarg)</span><br><span class="line">class dict(mapping,**kwarg)</span><br><span class="line">class dict(iterable,**kwarg)</span><br><span class="line">```   </span><br><span class="line">其中```**kwarg```是python中可变参数，代表关键字参数，允许你传入0个或任意多个含参数名的参数，这个关键字参数在函数内部自动组装成一个dict。</span><br><span class="line">- class dict(**kwarg)  </span><br><span class="line">通过关键字参数创建一个字典，例如   </span><br><span class="line">```python</span><br><span class="line">d = dict(name=&apos;Tom&apos;,age=23)     </span><br><span class="line">out: &#123;&apos;age&apos;: 23, &apos;name&apos;: &apos;Tom&apos;&#125;</span><br><span class="line"></span><br><span class="line">d = dict(a = 12, b = 13, c = 15)  </span><br><span class="line">out: &#123;&apos;a&apos;: 12, &apos;b&apos;: 13, &apos;c&apos;: 15&#125;</span><br></pre></td></tr></table></figure></p><ul><li>class dict(mapping,<strong>kwarg)<br>通过从一个映射函数对象中构造一个新字典，与dict(</strong>kwarg)函数不一样的地方是参数输入是一个映射类型的函数对象，比如zip函数，map函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">以映射函数方式来构造字典</span><br><span class="line">d2 = dict(zip([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))  </span><br><span class="line">out: &#123;<span class="string">'one'</span>: <span class="number">1</span>, <span class="string">'three'</span>: <span class="number">3</span>, <span class="string">'two'</span>: <span class="number">2</span>&#125;</span><br><span class="line">```    </span><br><span class="line">- <span class="class"><span class="keyword">class</span> <span class="title">dict</span><span class="params">(iterable,**kwarg)</span>   </span></span><br><span class="line"><span class="class">其中<span class="title">iterable</span>表示可迭代对象，可迭代对象可以使用<span class="title">for</span>...<span class="title">in</span>...来遍历，在<span class="title">Pytohn</span>中<span class="title">list</span>，<span class="title">tuple</span>，<span class="title">str</span>，<span class="title">dict</span>，<span class="title">set</span>等都是可迭代对象。创建<span class="title">dict</span>时如果传入的是可迭代对象，则可迭代对象中每一项自身必须是可迭代的，并且每一项只能由有2个对象，第一个对象称为字典的<span class="title">key</span>，第二个对象为<span class="title">key</span>对应的<span class="title">value</span>。如果<span class="title">key</span>有重复，其<span class="title">value</span>为最后重复项的值    </span></span><br><span class="line"><span class="class">![]<span class="params">(Python学习/dict.png)</span>     </span></span><br><span class="line"><span class="class"># 函数参数   </span></span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在python中定义一个函数，可以传入4种参数：  </span><br><span class="line">位置参数，默认参数，关键字参数，可变参数   </span><br><span class="line"><span class="comment">## 位置参数   </span></span><br><span class="line">普通的参数，参数之间是有顺序的， 顺序不能写错 </span><br><span class="line">```python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">```     </span><br><span class="line"><span class="comment">## 关键字参数   </span></span><br><span class="line">函数调用时使用关键字参数来确定传入的参数值，使用关键字参数允许函数调用时参数的顺序和声明的顺序不一致，因为python解释器会根据参数名来匹配参数值。使用key=value格式来指定参数。</span><br><span class="line">```python  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s    </span><br><span class="line">power(<span class="number">5</span>,<span class="number">2</span>)会得到<span class="number">25</span></span><br><span class="line">power(<span class="number">2</span>,<span class="number">5</span>)会得到<span class="number">32</span></span><br><span class="line">power(n=<span class="number">2</span>,x=<span class="number">5</span>)会得到<span class="number">25</span></span><br><span class="line">```   </span><br><span class="line">```python  </span><br><span class="line">可写函数说明</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printinfo</span><span class="params">( name, age )</span>:</span></span><br><span class="line">   <span class="string">"打印任何传入的字符串"</span></span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"名字: "</span>, name)</span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"年龄: "</span>, age)</span><br><span class="line">   <span class="keyword">return</span></span><br><span class="line"> </span><br><span class="line">调用printinfo函数</span><br><span class="line">printinfo( age=<span class="number">50</span>, name=<span class="string">"runoob"</span> )</span><br><span class="line">```  </span><br><span class="line">**注意：关键字参数必须写在位置参数之后，否则会报错**</span><br><span class="line"><span class="comment">## 默认参数    </span></span><br><span class="line">在定义函数时，使用赋值运算符=就为参数设置了一个默认值，默认参数是可选的，就是说可以指定，也可以不指定。当不指定时就使用默认值，如果指定，会覆盖默认值。有了一个默认参数，这样即使传入调用`power(<span class="number">5</span>)`,这样就默认n=<span class="number">2</span>，如果要计算的幂次大于<span class="number">2</span>，就需要明确的指定n的值，`power(<span class="number">5</span>,<span class="number">3</span>)`,这是n=<span class="number">3</span>  </span><br><span class="line">```python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power</span><span class="params">(x, n=<span class="number">2</span>)</span>:</span></span><br><span class="line">    s = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">        s = s * x</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">```   </span><br><span class="line">**设置默认参数时，需要注意以下几点：**  </span><br><span class="line"><span class="number">1.</span> 必选参数在前面，默认参数在后面，否则Python的解释器会报错  </span><br><span class="line">使用默认参数的好处是：比如学生注册的时候，需要传入的参数为：姓名，性别，年龄。把年龄设置为默认参数<span class="number">19</span>，这样大部分学生注册时不需要提供年龄，只需要提供<span class="number">2</span>个必须的参数，只有与默认参数不符的学生才提供额外的信息。可见，使用默认参数降低了函数调用的难度，</span><br><span class="line">```python  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enroll</span><span class="params">(name, gender,age=<span class="number">19</span>)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'name:'</span>, name</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'gender:'</span>, gender</span><br><span class="line">```     </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 可变参数   </span></span><br><span class="line"></span><br><span class="line">可变参数就是传入的参数个数是可变的，可以是<span class="number">1</span>个、<span class="number">2</span>个到任意个，还可以是<span class="number">0</span>个。  当函数中有位置参数和可变参数时，位置参数始终在可变参数之前。通常情况下，可变参数会出现在形参的最后，因为它们会把传递给函数的所有剩余参数都收集起来。可变参数之后出现的任何参数都是“强制关键字”参数，也就是说，可变参数之后的参数必须是关键字参数，而不能是位置参数。</span><br><span class="line"><span class="comment">### *args</span></span><br><span class="line">我们以数学题为例子，给定一组数字a，b，c……，请计算a2 + b2 + c2 + ……。   </span><br><span class="line">```python    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> numbers:</span><br><span class="line">        sum = sum + n * n</span><br><span class="line">    <span class="keyword">return</span> sum </span><br><span class="line">```  </span><br><span class="line">在函数内部，参数number接收到的是一个tuple，例如`calc(<span class="number">1</span>,<span class="number">2</span>)`得出来的结果是<span class="number">5</span>，也可以直接传入一个list或者tuple,在list或tuple前面加上一个*，把list或tuple的元素变成可变参数传进去。   </span><br><span class="line">```python   </span><br><span class="line">nums1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">calc(*nums1)</span><br><span class="line"><span class="number">14</span></span><br><span class="line"></span><br><span class="line">nums2 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">calc(*nums1,*nums2)</span><br><span class="line"><span class="number">28</span></span><br><span class="line">```    </span><br><span class="line"><span class="comment">### **args   </span></span><br><span class="line">可变参数允许传入<span class="number">0</span>个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入<span class="number">0</span>个或任意个含参数名称的参数，这些关键字参数函数内部自动组装成一个dict     </span><br><span class="line">```python   </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">person</span><span class="params">(name, age, **kw)</span>:</span></span><br><span class="line">    print( <span class="string">'name:'</span>, name)</span><br><span class="line">    print( <span class="string">'age:'</span>, str(name))</span><br><span class="line">    print( <span class="string">'other:'</span>, kw)</span><br><span class="line">person(<span class="string">'Michael'</span>, <span class="number">30</span>)  </span><br><span class="line">person(<span class="string">'Bob'</span>, <span class="number">35</span>, city=<span class="string">'Beijing'</span>)</span><br><span class="line">kw = &#123;<span class="string">'city'</span>: <span class="string">'Beijing'</span>, <span class="string">'job'</span>: <span class="string">'Engineer'</span>&#125;</span><br><span class="line">person(<span class="string">'Jack'</span>, <span class="number">24</span>, **kw)</span><br><span class="line">```  </span><br><span class="line"><span class="comment">## 总结</span></span><br><span class="line">**在函数定义时，参数的顺序为：位置参数，默认参数，*args，\*\*args**      </span><br><span class="line">**在函数调用时，参数的顺序为位置参数、关键字参数/默认参数，*args，\*\*args**     </span><br><span class="line"><span class="comment"># 文件读取   </span></span><br><span class="line">当一个文件很大时，不能一次性读取所有的内容加载到内存中，需要使用生成器   </span><br><span class="line">```python   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    读取文件</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.readline().strip()</span><br><span class="line">        <span class="keyword">while</span> data:</span><br><span class="line">            <span class="keyword">yield</span> data</span><br><span class="line">            data = f.readline().strip()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;学习Python！！！！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="编程" scheme="http://yoursite.com/tags/%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CDH集群</title>
    <link href="http://yoursite.com/2019/04/04/CDH%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2019/04/04/CDH集群/</id>
    <published>2019-04-04T14:11:03.000Z</published>
    <updated>2019-05-24T15:52:19.674Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;最近去给甲方安装CDH集群，对于集群的搭建和测试在这里总结一下。<br><a id="more"></a>   </p><!-- TOC --><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6">2. 版本控制</a></li><li><a href="#3-linux%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D">3. Linux目录介绍</a></li><li><a href="#4-%E5%AE%89%E8%A3%85%E5%89%8D%E8%AF%B4%E6%98%8E">4. 安装前说明</a></li><li><a href="#5-%E5%B0%8F%E5%B8%B8%E8%AF%86">5. 小常识</a></li><li><a href="#6-%E5%AE%89%E8%A3%85cdh%E9%9B%86%E7%BE%A4">6. 安装CDH集群</a><ul><li><a href="#61-%E5%85%B3%E9%97%AD%E6%89%80%E6%9C%89%E6%9C%BA%E5%99%A8%E7%9A%84%E9%98%B2%E7%81%AB%E5%A2%99">6.1. 关闭所有机器的防火墙</a></li><li><a href="#62-%E4%BF%AE%E6%94%B9%E6%9C%BA%E5%99%A8%E7%9A%84hosts">6.2. 修改机器的hosts</a></li><li><a href="#63-%E6%9F%A5%E7%9C%8B%E7%BD%91%E7%BB%9C%E6%98%AF%E5%90%A6%E8%BF%9E%E9%80%9A">6.3. 查看网络是否连通</a></li><li><a href="#64-%E7%94%9F%E6%88%90%E4%B8%BB%E8%8A%82%E7%82%B9%E7%9A%84ssh%E5%AF%86%E9%92%A5%E5%B9%B6%E5%88%86%E5%8F%91">6.4. 生成主节点的ssh密钥并分发</a></li><li><a href="#65-%E5%AE%89%E8%A3%85ntp%E6%9C%8D%E5%8A%A1">6.5. 安装ntp服务</a></li><li><a href="#66-%E5%8D%B8%E8%BD%BD%E6%9C%BA%E5%99%A8%E8%87%AA%E5%B8%A6%E7%9A%84openjdk">6.6. 卸载机器自带的openjdk</a></li><li><a href="#67-%E5%AE%89%E8%A3%85jdk">6.7. 安装JDK</a></li><li><a href="#68-%E5%AE%89%E8%A3%85mysql">6.8. 安装MySQL</a></li><li><a href="#69-%E5%88%9B%E5%BB%BAmysql%E6%95%B0%E6%8D%AE%E5%BA%93">6.9. 创建MySQL数据库</a></li><li><a href="#610-%E5%AE%89%E8%A3%85cloudera-manager-server%E5%92%8Cagent">6.10. 安装Cloudera Manager Server和Agent</a></li><li><a href="#611-%E6%89%93%E5%BC%80%E7%BD%91%E9%A1%B5%E9%85%8D%E7%BD%AE">6.11. 打开网页配置</a></li><li><a href="#612-hdfs%E7%9A%84ha%E5%AE%89%E8%A3%85">6.12. HDFS的HA安装</a></li><li><a href="#613-%E5%AE%89%E8%A3%85anaconda">6.13. 安装Anaconda</a></li><li><a href="#614-%E5%AE%89%E8%A3%85sparkstandalone">6.14. 安装Spark（standalone）</a></li><li><a href="#615-%E4%BF%AE%E6%94%B9hdfs%E6%9D%83%E9%99%90">6.15. 修改hdfs权限</a></li><li><a href="#616-%E5%AE%89%E8%A3%85python%E4%B8%89%E6%96%B9%E5%BA%93">6.16. 安装python三方库</a></li><li><a href="#617-%E5%AE%89%E8%A3%85kafka">6.17. 安装kafka</a></li><li><a href="#yarn">yarn</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-版本控制">2. 版本控制</span></h1><div class="table-container"><table><thead><tr><th></th><th>组件</th><th>版本</th></tr></thead><tbody><tr><td></td><td>CenOS</td><td>CenOS7</td></tr><tr><td></td><td>JDK</td><td>JDK1.8</td></tr><tr><td></td><td>CDH集群</td><td>CDH5.7.2</td></tr><tr><td></td><td>CDH-kafka</td><td>CDH-kafka1.2.0</td></tr><tr><td></td><td>Python</td><td>Python3.5 </td></tr></tbody></table></div><p>现在实验室的cdh集群版本是CDH5.7.2,其中每个组件的版本是<br>| 组件 | CDH5.7.2|CDH5.16.1<br>-|-|-|-<br>|Hadoop|2.6.0|2.6.0<br>|HDFS|2.6.0|2.6.0<br>|HBase|1.2.0|1.2.0<br>|Hive|1.1.0|1.1.0<br>|Spark|1.6.0|1.6.0<br>|Kafka|0.10.0|0.10.0<br>|Zookeeper|3.4.5|3.4.5</p><h1><span id="3-linux目录介绍">3. Linux目录介绍</span></h1><div class="table-container"><table><thead><tr><th style="text-align:center">目录</th><th style="text-align:center">说明</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center">bin</td><td style="text-align:center">存储普通用户可执行的指令</td><td style="text-align:center">即使在单用户模式下也能够执行处理</td></tr><tr><td style="text-align:center">dev</td><td style="text-align:center">设备目录</td><td style="text-align:center">所有的硬件设备及周边均放置在这个设备目录中</td></tr><tr><td style="text-align:center">home</td><td style="text-align:center">主要存放用户的个人数据</td><td style="text-align:center">每个用户在home中都有一个文件夹(除root外)，存储每个用户的设置文件，用户的桌面文件夹、用户的数据</td></tr><tr><td style="text-align:center">etc</td><td style="text-align:center">各种配置文件目录</td><td style="text-align:center">大部分配置属性均存放在这里</td></tr><tr><td style="text-align:center">lib/lib64</td><td style="text-align:center">开机时常用的动态链接库</td><td style="text-align:center">bin及sbin指令也会调用对应的lib库</td></tr><tr><td style="text-align:center">opt</td><td style="text-align:center">第三方软件安装目录</td><td style="text-align:center">现在习惯放在/usr/local中</td></tr><tr><td style="text-align:center">run</td><td style="text-align:center">系统运行所需的文件</td><td style="text-align:center">重启后重新生成对一个的目录数据</td></tr><tr><td style="text-align:center">sbin</td><td style="text-align:center">只有root用户才能运行的管理指令</td><td style="text-align:center">跟bin类似，但只属于root管理员</td></tr><tr><td style="text-align:center">tmp</td><td style="text-align:center">存储临时文件目录</td><td style="text-align:center">所有用户对该目录均可读写</td></tr><tr><td style="text-align:center">usr</td><td style="text-align:center">应用程序放置目录</td><td style="text-align:center">/usr/local存储那些手动安装的软件，/usr/bin存储程序，/usr/share存储一些共享数据，例如音乐文件或者图标，/usr/lib存储那些不能直接运行的，但却是很多程序运行所必须的一些函数库文件</td></tr></tbody></table></div><h1><span id="4-安装前说明">4. 安装前说明</span></h1><ul><li>Zookeeper和Kafka部分主节点，要装3台</li><li>HBase和HDFS分主从节点，都需要2个主节点，一个active主节点，一个standby主节点，剩下的机器作为从节点</li><li>关闭防火墙，安装ntp、jdk、mysql，anaconda可以同时进行。   </li><li>本次以3个服务器为例安装CDH集群</li><li>192.168.1.201   node1</li><li>192.168.1.202   node2</li><li>192.168.1.203   node3       </li></ul><h1><span id="5-小常识">5. 小常识</span></h1><ul><li>使用vi命令编辑文件</li><li>键盘a———输入模式，编辑文件</li><li>键盘Esc——修改完之后按Esc退出输入模式</li><li>:wq——-保存，并退出</li><li>:q———不保存，退出    </li></ul><h1><span id="6-安装cdh集群">6. 安装CDH集群</span></h1><h2><span id="61-关闭所有机器的防火墙">6.1. 关闭所有机器的防火墙</span></h2><p><font color="red">每台机器都要执行</font><br>#关闭防火墙<br>systemctl stop firewalld.service<br>#禁止firewall开机启动<br>systemctl disable firewalld.service<br>#关闭selinux<br>vi /etc/selinux/config<br>将SELINUX设置为disabled<br>如下SELINUX=disabled<br>#重启<br>reboot<br>#重启机器后使用root用户查看Selinux状态<br>getenforce</p><h2><span id="62-修改机器的hosts">6.2. 修改机器的hosts</span></h2><p><font color="red">每台机器都要执行</font><br>#使用ip addr查看每台机器的ip地址<br>#修改hosts文件<br>vi /etc/hosts   </p><p><font color="red">在最下面一行添加以下内容</font><br>192.168.1.201 node1<br>192.168.1.202 node2<br>192.168.1.203 node3 </p><h2><span id="63-查看网络是否连通">6.3. 查看网络是否连通</span></h2><p><font color="red">每台机器都要执行</font><br>#在node1上执行<br>ping node2<br>ping node3<br>#在其余两个节点分别ping<br>按Ctrl+C中断ping命令      </p><h2><span id="64-生成主节点的ssh密钥并分发">6.4. 生成主节点的ssh密钥并分发</span></h2><p>生成主节点root账户的ssh密钥，分发至其他机器，要实现免密码登录其他机器的root账户    </p><p><font color="red">只在node1上执行</font><br>#生成ssh密钥（node1上）<br>ssh-keygen -t rsa<br>然后一路回车<br>接下来分发密钥，请仔细观察显示的内容，会让你输入yes和密码<br>ssh-copy-id node1<br>ssh-copy-id node2<br>ssh-copy-id node3   </p><h2><span id="65-安装ntp服务">6.5. 安装ntp服务</span></h2><p><font color="red">每台机器都要执行</font><br>yum install ntpdate<br>在执行这条命令时我的电脑出现以下错误：   </p><p><font color="red">问题：Could not resolve host: mirrorlist.centos.org Centos 7</font><br><strong>解决方案</strong>：<a href="https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7" target="_blank" rel="noopener">https://serverfault.com/questions/904304/could-not-resolve-host-mirrorlist-centos-org-centos-7</a>   </p><p><font color="red">只在node1上执行</font><br>yum install ntp     </p><p><font color="red">只在node1上执行</font><br>vi /etc/ntp.conf<br>注释以下4行，在前面加#<br>server 0.centos.pool.ntp.org iburst<br>server 1.centos.pool.ntp.org iburst<br>server 2.centos.pool.ntp.org iburst<br>server 3.centos.pool.ntp.org iburst<br>在最下面加上<br>restrict default ignore<br>restrict <font color="red">192.168.1.0</font>   mask 255.255.255.0<br>nomodify notrap<br>server 127.127.1.0   </p><p><font color="red">注意：192.168.1.0是这3台机器ip地址的前3位，最后一位是0</font><br>#重启ntp服务<br>service ntpd restart<br>#设置ntp服务器开机自动启动<br>chkconfig ntpd on       </p><p><font color="red">只在node2和node3执行</font><br>#以下为客户端的配置（除node1外的node2和node3），设定每天00:00向服务器(node1)同步时间，并写入日志<br>crontab -e<br>#输入以下内容后保存并退出<br>0 0 <em> </em> <em> /usr/sbin/ntpdate <em>*node1</em></em>&gt;&gt; /root/ntpd.log   </p><p><font color="red">只在node2和node3执行</font><br>ntpdate node1</p><h2><span id="66-卸载机器自带的openjdk">6.6. 卸载机器自带的openjdk</span></h2><p>！！！！！！！一定要卸载    </p><h2><span id="67-安装jdk">6.7. 安装JDK</span></h2><p>安装自己的jdk到/opt/java/下面，如/opt/java/jdk1.8.0_90    </p><p><font color="red">只在node1上执行</font><br>首先使用filezilla把cdh_deployment压缩包上传到/usr下面，然后再opt下面创建一个java文件夹。<br>将jdk的安装包拷贝到/opt/java/下面<br>cp /usr/CDH_deployment/jdk-8u11-linux-x64.tar.gz /opt/java/<br>#解压<br>tar -zxvf jdk-8u11-linux-x64.tar.gz<br>#修改环境变量<br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH<br>#刷新配置文件<br>source /etc/profile<br>#复制jdk到其他服务器上<br>scp -r /opt/java/jdk1.8.0_11/ node2:/opt/java/<br>scp -r /opt/java/jdk1.8.0_11/ node3:/opt/java/      </p><p><font color="red">在node2执行</font><br>// WangBeibei-DC-2 上<br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH  </p><p><font color="red">在node3执行</font><br>vi /etc/profile<br>// 添加以下内容<br>export JAVA_HOME=/opt/java/jdk1.8.0_11/<br>export JRE_HOME=/opt/java/jdk1.8.0_11/jre<br>export CLASSPATH=.:\$JAVA_HOME/lib:\$JRE_HOME/lib:\$CLASSPATH<br>export PATH=\$JAVA_HOME/bin:\$JRE_HOME/bin:\$JAVA_HOME:\$PATH<br>测试java -version<br>看到java的版本说明安装成功  </p><p><font color="red">在每台上执行</font><br>mkdir /usr/java<br>ln -s /opt/java/jdk1.8.0_90/  /usr/java/default    </p><h2><span id="68-安装mysql">6.8. 安装MySQL</span></h2><p><font color="red">只在node1上执行</font><br>yum remove mysql mysql-server mysql-libs compat-mysql51<br>rm -rf /var/lib/mysql<br>rm -rf /etc/my.cnf<br>将mysql.jar拷贝到/usr/local下面<br>解压<br>tar -zxvf /opt/mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz<br>// 改名为mysql<br>mv mysql-5.6.37-linux-glibc2.12-x86_64 mysql<br>// 删除安装包<br>rm mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz<br>// 修改环境变量<br>vi /etc/profile<br>在最下面添加<br>export MYSQL_HOME=/usr/lcoal/mysql<br>export PATH=\$MYSQL_HOME/bin:\$PATH<br>// 刷新环境变量<br>source /etc/profile<br>将服务文件mysql.server拷贝到init.d下<br>cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql.server<br>MySQL开机自启，赋予可执行权限<br>chmod +x /etc/init.d/mysql.server<br>添加服务<br>chkconfig —add mysql.server<br>显示服务列表<br>chkconfig —list<br>如果看到mysql的服务，并且3、4、5都是on的话则成功。如果mysql.server的 3, 4, 5 不是on，使用下面的命令给他变成on:<br>chkconfig —level 345 mysql.server on<br>// 新建mysql 用户<br>groupadd mysql 在/etc/group 中可以看到<br>useradd -r -g mysql -s /bin/false mysql 在/etc/passwd 中可以看到<br>cd /usr/local/mysql<br>chown -R mysql:mysql .<br>scripts/mysql_install_db —user=mysql<br>// 修改当前目录拥有者为root 用户<br>chown -R root .<br>// 修改当前data 目录拥有者为mysql 用户<br>chown -R mysql data<br>// 新建一个虚拟窗口，叫mysql<br>screen -S mysql<br>bin/mysqld_safe —user=mysql &amp;<br>// 退出虚拟窗口<br>Ctrl+A+D<br>cd /usr/local/mysql<br>// 登陆mysql<br>bin/mysql<br>// 登陆成功后退出即可<br>exit;<br>// 进行root 账户密码的修改等操作<br>bin/mysql_secure_installation<br>首先要求输入root 密码，由于我们没有设置过root 密码，括号里面说了，如果没有root 密码就直接按回车。<br>是否设定root 密码，选y，设定密码为cluster，是否移除匿名用户：y。然后有个是否关闭root 账户的远程<br>登录，选n，删除test 这个数据库？y，更新权限？y，然后ok。<br>cp support-files/mysql.server /etc/init.d/mysql.server<br>// 进入mysql 虚拟窗口<br>screen -r mysql<br>// 查看mysql 的进程号<br>ps -ef | grep mysql<br>// 如果有的话就kill 掉，保证mysql已经中断运行了，一般kill 掉/usr/local/mysql/bin/mysqld 开头的即可<br>kill 进程号<br>// 关闭虚拟窗口<br>exit<br>// 启动mysql<br>/etc/init.d/mysql.server start -user=mysql<br>exit<br>还需要配置一下访问权限，#授权root用户在主节点拥有所有数据库的访问权限：<br>$ mysql -u root -p<br>mysql&gt; GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’%’ IDENTIFIED BY ‘cluster’ WITH GRANT OPTION;<br>mysql&gt; FLUSH PRIVILEGES;    </p><h2><span id="69-创建mysql数据库">6.9. 创建MySQL数据库</span></h2><p>MySQL中root账户执行：<br>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;   </p><h2><span id="610-安装cloudera-manager-server和agent">6.10. 安装Cloudera Manager Server和Agent</span></h2><p>（1） <font color="red">在node1执行</font><br>把cloudera-manager-centos7-cm5.7.2_x86_64.tar.gz的压缩包解压到/opt下面<br>（2）<font color="red">在node1执行</font><br>把mysql-connector-java-5.1.43-bin.jar复制到/opt/cm-5.7.2/share/cmf/lib/ 里面<br>（3）<font color="red">在node1执行</font><br>在主节点初始化CM5的数据库：<br>/opt/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh<br>mysql cm -h[mysql数据库的主机名]  -uroot -p[password] —scm-host [cm server的主机名] [cm的数据库] [cm数据库访问用户] [cm数据库访问用户的密码]<br>执行/opt/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -pcluster —scm-host localhost scm scm scm<br>（4）<font color="red">在node1执行</font><br>Agent配置<br>修改 vi /opt/cm-5.7.2/etc/cloudera-scm-agent/config.ini 这里面的server_host，改成自身的机器名node1，也就是指名主节点的机器名<br>（5）<font color="red">在node1执行</font><br>将cm-5.7.2的目录复制到其他机器上，同步Agent到其他节点<br>确保复制到所有的机器上<br>scp -r /opt/cm-5.7.2 root@node2:/opt/<br>scp -r /opt/cm-5.7.2 root@node3:/opt/<br>（6）<font color="red">在所有机器上</font><br>在所有节点创建cloudera-scm用户<br>执行<br>useradd —system —home=/opt/cm-5.7.2/run/cloudera-scm-server/ —no-create-home —shell=/bin/false —comment “Cloudera SCM User” cloudera-scm<br>（7）<font color="red">在node1执行</font><br>执行 mkdir -p /opt/cloudera/parcel-repo/<br>（8）<font color="red">在node1执行</font><br>把<br>CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel，<br>CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha，<br>manifest.json<br>这三个文件，复制到/opt/cloudera/parcel-repo/这里面<br>（9）<font color="red">在node1执行</font><br>ssh node2<br>mkdir /usr/share/java<br>把mysql-connector-java-5.1.43-bin.jar复制到/usr/share/java下，并命名为mysql-connector-java.jar<br>cp /opt/cm-5.7.2/share/cmf/lib/   mysql-connector-java-5.1.43-bin.jar /usr/share/java/   mysql-connector-java.jar   </p><p><font color="red">其中machine2是第二台机器，把这个machine2改成其他机器的名字，分别执行一遍</font><br>（10）<font color="red">在node1执行</font><br>执行启动服务端：<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-server start<br>执行启动Agent服务端：<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>启动其他机器的Agent<br>执行：<br>ssh node2<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>ssh node3<br>/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start<br>用这个ssh命令将其他所有机器的agent都启动      </p><p><font color="red">问题：在启动时出错<br>/opt/cm-5.7.0/etc/init.d/cloudera-scm-server start<br>/opt/cm-5.7.0/etc/init.d/cloudera-scm-server: line 109: pstree: command not found   </font><br><strong>解决方案</strong>：因为系统是最小化安装，默认没有安装，运行下面的命令。<br>yum -y install psmisc    </p><h2><span id="611-打开网页配置">6.11. 打开网页配置</span></h2><p>（1）打开浏览器，地址是：主节点的ip:7180，用户名和密码都是admin<br>（2）选第一个免费版！<br>（3）<a href="http://www.cnblogs.com/jasondan/p/4011153.html，" target="_blank" rel="noopener">http://www.cnblogs.com/jasondan/p/4011153.html，</a><br>然后按照那个博客里面的图片安装就行了<br>（4）安装服务的时候，错开机器，别把所有的服务都堆在前几台机器上，zookeeper要3个，安装的时候hive会报错，博客里面写了怎么解决，oozie也会报错，都是一样的解决方法，最好默认不要修改。<br>（5）按照博客可以完成CDH集群的安装<br>（6）问题：<br><img src="/2019/04/04/CDH集群/jdk.png" alt=""><br>解决方案：这里需要强调一下CDH5默认识别的jdk路径为：/usr/java/default<br>没有往/usr/java中添加软链接，而这里默认是去/usr/java/default中找环境变量，才会报找不到java_home。安装jdk的方法:把jdk软连接到/usr/java/default首先查看是否有/usr/java目录，没有的话新建此目录：mkdir /usr/java。然后添加软连接到/usr/java/default，命令如下: ln -s /opt/java/jdk1.8.0_11 /usr/java/default<br>问题：<br><img src="/2019/04/04/CDH集群/hive.png" alt=""><br>解决方案：这里安装Hive的时候可能会报错，因为我们使用了MySql作为hive的元数据存储，hive默认没有带mysql的驱动，通过以下命令拷贝一个就行了：<br>cp /opt/cm-5.7.2/share/cmf/lib/mysql-connector-java-5.1.43-bin.jar /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/hive/lib/</p><h2><span id="612-hdfs的ha安装">6.12. HDFS的HA安装</span></h2><p>HDFS HA的安装：<br><a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12_1，" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_enabling.html#cmug_topic_5_12_1，</a><br>看里面的Enabling High Availability and Automatic Failover。按操作安装完后，<a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp，" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp，</a><br>完成里面的Upgrading the Hive Metastore to Use HDFS HA和Configuring Hue to Work with HDFS HA<br>问题：Cloudera Manager （重新）部署集群报错：fail to format namenode<br>解决方案：<a href="https://www.jianshu.com/p/1e8b25e63ab9" target="_blank" rel="noopener">https://www.jianshu.com/p/1e8b25e63ab9</a></p><h2><span id="613-安装anaconda">6.13. 安装Anaconda</span></h2><p>首先创建一个file0的目录，在这个目录下面运行下面的命令：<br>bash Anaconda3-4.1.1-Linux-x86_64.sh<br>这样Anaconda3就安装在file0下面。<br><a href="https://blog.csdn.net/m0_37548423/article/details/81173678" target="_blank" rel="noopener">https://blog.csdn.net/m0_37548423/article/details/81173678</a>    </p><p>在所有机器上安装anaconda4.2.0，结尾最后一步，是否添加至环境变量，选择no<br>使用which python查看使用的是哪个版本的python，运行程序的时候要用<br>/file0/anaconda3/bin/python user.py    </p><h2><span id="614-安装sparkstandalone">6.14. 安装Spark（standalone）</span></h2><p>（1）选择这个添加服务，安装Spark (standalone)<br> <img src="/2019/04/04/CDH集群/spark1.png" alt=""><br>（2）点击spark<br><img src="/2019/04/04/CDH集群/spark2.png" alt=""><br>（3）点击配置<br> <img src="/2019/04/04/CDH集群/spark3.png" alt=""></p><p>（4）搜索栏输入spark-env.sh<br>export PYSPARK_PYTHON=/file0/anaconda3/bin/python<br>export PYSPARK_DRIVER_PYTHON=/file0/anaconda3/bin/ipython<br>export PYTHONHASHSEED=0<br><img src="/2019/04/04/CDH集群/spark4.png" alt=""></p><p>找到这个服务高级配置的代码段，改成这个样子，把python的路径指名为anaconda的python路径<br>（5）按照上一步把 Spark (standalone) 的spark-env也改成上面的样子   </p><h2><span id="615-修改hdfs权限">6.15. 修改hdfs权限</span></h2><p>主节点执行：<br>sudo -u hdfs hdfs dfs -mkdir /user/root<br>sudo -u hdfs hdfs dfs -chown root /user/root<br>sudo -u hdfs hdfs dfs -chmod -R 777 /user    </p><h2><span id="616-安装python三方库">6.16. 安装python三方库</span></h2><p>主节点执行：<br>/file0/anaconda3/bin/pip install<br>—index-url=file:///file0/CDH_deployment/pypi/simple pymysql happybase pykafka    </p><h2><span id="617-安装kafka">6.17. 安装kafka</span></h2><p>Kafka的安装过程参照：<br><a href="https://jingyan.baidu.com/article/e9fb46e139dead7521f7662e.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/e9fb46e139dead7521f7662e.html</a><br>里面要求下载的四个文件我已经下载好了，其中一个叫manifest.json的文件，我重命名为了manifest_kafka.json，这个重命名是因为防止和之前的那个manifest冲突，直接按照第18步，将两个kafka的parcel文件和这个manifest拷到那个目录里面就行，按照博客里面的要求安装即可。  </p><p><font color="red">注意：只设置kafka broker，不设置Kafka MirrorMaker<br>安装的时候会配置kafka，配置完后会启动kafka，kafka一定启动不了，右下角有个重试按钮，这时候需要再开一个管理界面，像上面配置spark一样，配置kafka，如下图 </font><br><img src="/2019/04/04/CDH集群/kafka1.png" alt=""><br>分别点进三个超链接，选择左上角的配置，针对每个broker进行配置。<br>需要配置的是两项：</p><ol><li>Broker ID：可以机器顺序分别改成1，2，3</li><li>Java Heap Size of Broker：改成1G<br>然后回到刚才安装的那个界面，点击重试。<h2><span id="yarn">yarn</span></h2>CDH 5.9 以前的版本，如果使用 python3 且使用 yarn 作为master，需手动修复CDH 集群的bug。CDH 5.9 以前的版本在使用 yarn 作为 spark master 的时候，如果使用 python3，会出现 yarn 內部 topology.py 这个文件引发的 bug。这个文件是 python2 的语法，我们使用 python3 运行任务的时候，python3 的解释器在处理这个文件时会出错。<br>解决方案是：将这个文件重写为 python3 的版本，每次在重启 yarn 之后，将这个文件复制到所有机器<br>的 /etc/hadoop/conf.cloudera.yarn/ 目录下。<br><strong>以下操作在所有机器上都要操作，并使用root用户，不可以使用普通用户。如果/etc/hadoop/conf.cloudera.yarn目录不存在，先创建一个同名目录，然后将topology.py复制到该目录下</strong>。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;最近去给甲方安装CDH集群，对于集群的搭建和测试在这里总结一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CDH集群、Linux" scheme="http://yoursite.com/tags/CDH%E9%9B%86%E7%BE%A4%E3%80%81Linux/"/>
    
  </entry>
  
  <entry>
    <title>Spark-RDD</title>
    <link href="http://yoursite.com/2019/03/26/Spark-RDD/"/>
    <id>http://yoursite.com/2019/03/26/Spark-RDD/</id>
    <published>2019-03-26T03:38:24.000Z</published>
    <updated>2019-03-28T03:11:32.731Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="简介">简介</span></h1> <a id="more"></a> <h1><span id="解释rdd">解释RDD</span></h1><p>&ensp;&ensp;&ensp;&ensp;RDD从表现形式上类似数据库的视图。可以理解为Java中的一个lis或者数据库中的一张表（或者视图）。Spark对RDD的操作，类似于对SQL中的一张表进行操作。但是RDD类似于数据库中的视图，而不是表，因为RDD是弹性的，就是一个RDD的数据并不一定是物理真实存在的。把一个超大的数据集，切切分分成N个小堆，找M个执行器(M&lt;N)，每一个执行器各自拿一块或者多块数据慢慢计算，等到计算出结果再收集在一起，这就算执行完了。那么Spark做了一项工作就是：凡是能够被我计算的，都要符合我的要求，所以spark无论处理什么数据，都要先整理成一个拥有多个分块的数据集，这个数据集就是RDD。<br>&ensp;&ensp;&ensp;&ensp;当你写一个spark程序时，比如下面的程序，在代码上获取了一个RDD，叫做lines，但是这个RDD不包含任何待处理的数据，真正的数据在执行时才会被加载，加载时数据要么来自spark外，例如hdfs，要么来自spark内部，前提是你已经对它做了cache。</p><pre><code class="lang-java">SparkConf conf = new SparkConf().setAppName(&quot;wordcount&quot;).setMaster(&quot;local&quot;);JavaSparkContext sc = new JavaSparkContext(conf);JavaRDD&lt;String&gt; lines = sc.textFile(&quot;/Users/jianhong1/testfile&quot;);</code></pre><p>&ensp;&ensp;&ensp;&ensp;Spark的计算执行可以认为是这样一个过程：在代码中创建一个RDD，但是这个RDD并不包含数据，只有等到Action算子的时候，开始计算，RDD才会加载数据。从一个RDD中读取数据，然后处理数据    </p><h1><span id="hdfs">HDFS</span></h1><p>&ensp;&ensp;&ensp;&ensp;HDFS分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成了一个集群。这些节点分为主节点和从节点。HDFS以Master-Slave模式运行。主节点也可叫做名称节点(NameNode)，从节点也可叫做数据节点(DataNode)。其中有一个NameNode和多个DataNode。<br>&ensp;&ensp;&ensp;&ensp;NameNode有2个职责：（1）负责客户端请求的响应。（2）用于保存HDFS的元数据信息，比如文件系统的命名空间，块信息。维护着文件系统树以及整棵树所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件(Namespace image)和编辑日志文件(edit log)。FsImage用于维护文件系统的目录结构以及元数据信息，文件与数据块(block)列表的对应关系。操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作。元数据存放在fsimage中，在运行时加载到内存中（读写比较快）。操作日志写到edits中。<br>&ensp;&ensp;&ensp;&ensp;DataNode的职责是：HDFS有多个DataNode。（1）存储管理用户的文件块数据。（2）定期（默认1h）向NameNode汇报自身所持有的block信息(通过心跳信息上报)。当NameNode长时间没有收到DataNode-n的心跳信息，则认为DataNode-n不可用。DataNode提供真实文件数据的存储服务。    </p><ul><li>文件块(block)：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的0偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称为一个Block。</li><li><p>Hdfs块大小如何设定？在hdfs-default.xml中设置。</p><pre><code class="lang-python">&lt;property&gt;&lt;name&gt;dfs.blocksize&lt;/name&gt;   #block块存储的配置信息&lt;value&gt;134217728&lt;/value&gt;   #这里的块的容量最大是128M，请注意&lt;description&gt;    The default block size for new files, in bytes.    You can use the following suffix (case insensitive):    k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),    Or provide complete size in bytes (such as 134217728 for 128 MB).&lt;/description&gt;&lt;/property&gt;</code></pre></li><li><p>HDFS默认Block大小是128MB，以一个256MB的文件为例，一共需要2个Block。不同于普通文件系统的是，HDFS中，如果一个文件小于数据块的大小，并不占用整个数据块存储空间。</p></li><li>Replication，默认是3，在hdfs-site.xml的dfs.replication属性。<br>vi hdfs-site.xml,可以修改，配置文件对全局生效。<pre><code class="lang-python">&lt;configuration&gt; &lt;property&gt;   &lt;name&gt;dfs.replication&lt;/name&gt;   &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;</code></pre><h1><span id="hive和hbase">Hive和HBase</span></h1>Hive：通俗地讲，hive是构建在Hadoop之上的数据仓库。是因为（1）数据存储在HDFS上，（2）数据计算使用MapReduce。Hive是一种构建在Hadoop文件系统上的数据仓库框架，并对存储在HDFS中的数据进行分析和管理。它可以将结构化的数据文件映射成一张数据库表，并提供完整的SQL查询功能，就是把写好的hql转换为map-reduce程序操作用来查询存放在HDFS上的数据。HQL是一种类SQL语言。<br>HBase：通俗地讲，HBase可以认为是HDFS的一个包装，HBase的本质是数据存储，是一个Nosql数据库。HBase部署在HDFS之上，并且克服了HDFS在随机读写方面的缺点。HBase是一种Key/Value系统，它运行在HDFS之上。和Hive不一样，HBase能够在它的数据库上实时运行，而不是运行MapReduce任务。   <h2><span id="应用场景">应用场景</span></h2>Hive使用于一段时间内的数据进行分析查询。例如，用于计算趋势或者网站的日志。Hive不应该用来进行实时查询，因为它需要很长时间才可以返回结果。Hive本身不存储和计算数据，它完全依赖HDFS和MapReduce，Hive中的表只是逻辑的表。<br>HBase非常适合用来进行大数据的实时查询。Facebook用HBase进行消息和实时的分析。HBase的物理表，不是逻辑表，提供一个超大的内存Hash表，搜索引擎通过它来存储索引，方便查询操作。<br>hdfs作为底层存储，hdfs是存储文件的系统，而hbase负责组织文件。hive需要用到hdfs存储文件，需要用到MapReduce计算框架。<br>Hive很适合数据仓库的统计分析，Hive的最佳使用场合是大数据集的批处理作业。例如网络日志分析，一般按照天、周、月、年来进行数据统计。不适合实时查询。例如Hive在几百兆的数据集上执行查询一般都有分钟级别的时间延迟。因此Hive不适合低延迟的应用，例如：联机事务处理。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="RDD" scheme="http://yoursite.com/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Learning</title>
    <link href="http://yoursite.com/2019/03/14/Learning/"/>
    <id>http://yoursite.com/2019/03/14/Learning/</id>
    <published>2019-03-14T14:48:30.000Z</published>
    <updated>2019-04-03T06:59:35.414Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;今天看了一天的spark streaming还是没有把程序调通，很失落，觉得自己怎么就那么笨，有点不开心。在做项目的过程中，学习到很多的东西，在这里记录下来。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-简介">1. 简介</a></li><li><a href="#2-工具使用">2. 工具使用</a><ul><li><a href="#21-git">2.1. Git</a></li><li><a href="#22-使用gpu服务器">2.2. 使用GPU服务器</a></li><li><a href="#23-部署代码到服务器">2.3. 部署代码到服务器</a></li><li><a href="#24-在本地运行gluon">2.4. 在本地运行gluon</a></li><li><a href="#25-修复集群">2.5. 修复集群</a></li><li><a href="#26-运行spark">2.6. 运行Spark</a></li><li><a href="#27-使用vscode">2.7. 使用vsCode</a></li><li><a href="#字节">字节</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-工具使用">2. 工具使用</span></h1><h2><span id="21-git">2.1. Git</span></h2><p>&ensp;&ensp;&ensp;&ensp;Git是一个用来版本控制的软件，在用的过程中涉及到以下步骤</p><ol><li>在服务器上创建仓库。首先在服务器上创建一个文件夹，例如test。cd到这个文件夹下，使用<figure class="highlight plain"><figcaption><span>init```命令，把这个文件夹初始化为一个git仓库，会看到这个文件夹下多了一个.git的文件，然后我们把项目代码拷贝到test这个文件夹下。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2. 在自己的电脑上下载并安装Git软件</span><br><span class="line">3. 从服务器git仓库上把代码clone到本地电脑。首先在自己电脑上，创建一个保存代码的目录，在这个目录下右键选择Git Bash，输入下面的代码</span><br><span class="line">```git clone zz@172.11.11.111:/file0/repo/EQ/```这样服务器git仓库中的东西就完封不动的clone到本地了。</span><br><span class="line">4. 在自己电脑上打开eclipse或者myeclipse，在file中导入这个项目代码，就可以进行修改代码了。</span><br><span class="line">5. pull和push。右键选中这个工程，点击team，就可以对项目进行pull和push。当你在本地修改完代码，本地的git会记录下来这些改动，所以本地文件夹中的git也有原先的历史记录，当push的时候会读取本地的git，找到本地的代码在服务器上的源，然后把改动同步到服务器上的git中。 </span><br><span class="line">6. 除了使用eclipse中的team来pull和push代码，还可以使用命令来进行pull或push。每次改完代码提交的时候，打开EQ这个目录（一定要在这个目录下，因为只有这个目录有git这个文件夹），右键```Git Bash Here```输入以下命令:</span><br><span class="line">```git add –all ```,</span><br><span class="line">```git commit –m “自己随便写，写自己本地改动的地方” ```,</span><br><span class="line">```git push</span><br></pre></td></tr></table></figure></li></ol><p>自己在写代码之前，也是打开EQ这个目录，右键<figure class="highlight plain"><figcaption><span>Bash Here```输入以下命令）:```git pull ```这样就会把别人改动过的代码更新到EQ这个文件夹下。     </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">**问题1**   </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在eclipse中右键点击team--&gt;show in History，会出现所有的改动。点击改动的一个文件，应该会出现2个窗口，一个窗口显示现在的代码，一个窗口显示原先的代码，可以明显的看出哪里进行了改动，但是我的电脑确没有出现历史窗口，只有现在代码的窗口。   </span><br><span class="line">**解决：** 在点击show in history之后右键一个文件，点击“compare with previous version”就会出现历史窗口。    </span><br><span class="line"></span><br><span class="line">**问题2**   </span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在每次写代码之前都要先pull一下，把别人push的代码拉取到本地。但有时候你在写代码之前忘了pull别人的代码，直接在本地电脑上修改，然后你在pull别人的代码时会出现conflict，那是因为你和服务器上的代码修改了同一个地方，git不知道该采用谁的修改。   </span><br><span class="line">**解决:** 首先在eclipse中，右键点击team，把你自己本地的代码先commit，不要push。然后打开vscode，导入和eclispe中一样的代码（和eclipse导入的代码在同一个文件下，带有.git），然后在vscode中点击左侧的第三个图标(git的图标)，然后在```...```中右键从服务器上pull到本地。然后再点击左侧的第一个图标，在这里面如果文件的右侧出现M，说明这个文件被修改过，然后你在这里面进行解决冲突。是采用本地的修改还是采用服务上的修改。等到冲突都解决时保存。然后再回到eclispe，会发现项目已经没有报错了。这时在eclispe中右键team，把代码commit and push到服务器中。</span><br><span class="line">## 2.2. 使用GPU服务器</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;使用putty来连接服务器，使用FileZilla来进行文件传输。在putty中输入服务器的IP地址，然后输入用户名和密码。</span><br><span class="line">1. 查看当前GPU的占用情况   </span><br><span class="line">```nvidia-smi</span><br></pre></td></tr></table></figure></p><ol><li>查看当前的进程是谁的(后面加进程号)   <figure class="highlight ps"><figcaption><span>-ef | grep 35230```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span>. 创建一个jupyter窗口   </span><br><span class="line">```screen -S jupyter```    </span><br><span class="line">然后在这个窗口内输入以下命令就可以打开jupyter notebook进行编程   </span><br><span class="line">```source /etc/profile```   </span><br><span class="line">```source activate insis_template_3.<span class="number">6</span> ```激活环境  </span><br><span class="line">```jupyter notebook ```打开jupyter   </span><br><span class="line"><span class="number">4</span>. 当程序写完时，kill掉窗口    </span><br><span class="line">```screen -X -S <span class="number">122128</span> quit```    </span><br><span class="line">```source deactivate``` </span><br><span class="line"><span class="number">5</span>. 查看当前有哪些窗口</span><br><span class="line">```screen -ls```     </span><br><span class="line"><span class="number">6</span>. 进入到一个窗口中</span><br><span class="line">```screen -r  <span class="number">35230</span>```    </span><br><span class="line">在这个窗口中按ctrl+c，shutdown掉一个jupyter进程，然后<span class="keyword">exit</span>退出这个窗口   </span><br><span class="line"><span class="number">7</span>. 使用screen创建一个窗口运行jupyter notebook程序的好处：就算ssh和<span class="number">28</span>号服务器的连接断开，jupyter notebook的程序依然可以在后台运行。如果你打开的jupyter notebook的程序运行完了，有<span class="number">3</span>种关闭程序的方法：</span><br><span class="line"><span class="number">1</span>、在jupyter notebook菜单栏，有一个close and hot的按钮。</span><br><span class="line"><span class="number">2</span>、在jupyter notebook中running中shutdown掉程序</span><br><span class="line"><span class="number">3</span>、screen -r <span class="number">23560</span>切入到虚拟窗口，然后在这个窗口ctrl+c</span><br><span class="line">关闭jupyter notebook进程。不用使用<span class="keyword">exit</span>，因为使用<span class="keyword">exit</span>是关闭</span><br><span class="line">虚拟窗口，直接按shift+a+d从虚拟窗口中切出，这样这个窗口</span><br><span class="line">还是存在的，下一次直接使用screen -ls查看存在的窗口，然后</span><br><span class="line">```source /etc/profile```， </span><br><span class="line">```source activate insis_template_3.<span class="number">6</span>```， </span><br><span class="line">```jupyter notebook</span><br></pre></td></tr></table></figure></li></ol><p>再次打开jupyter notebook程序</p><h2><span id="搭建深度学习框架">搭建深度学习框架</span></h2><p>&ensp;&ensp;&ensp;&ensp;在GPU服务器上怎么搭建深度学习框架。   </p><ol><li>首先在服务器上搭建自己的python运行环境。即安装Anaconda，安装步骤如下</li><li><p>下载安装脚本：   </p><figure class="highlight plain"><figcaption><span>https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3. 运行安装向导：   </span><br><span class="line">```bash Anaconda3-5.2.0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></li><li><p>为了激活安装， 你应该源~/.bashrc文件：    </p><figure class="highlight plain"><figcaption><span>~/.bashrc```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5. 确认是否安装成功：   </span><br><span class="line">```conda --version</span><br></pre></td></tr></table></figure></li><li><p>然后使用which python查看你当前使用的是哪个python，如果输出的目录是data/anaconda/python说明你当前使用的还是服务器自带的python，需要重新练连接一下服务器。如果输出是data/WangBeibei/anaconda/python，说明当前使用是自己安装的anaconda。</p></li><li><p>然后下载深度学习框架TensorFlow。首先到清华的网站下<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/，直接粘贴下面3行命令，" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/，直接粘贴下面3行命令，</a></p><figure class="highlight plain"><figcaption><span>config --add channels</span><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/```" target="_blank" rel="noopener">link</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">```conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/```   </span><br><span class="line">```conda config --set show_channel_urls yes```   </span><br><span class="line">然后使用vi ~/.condarc将里面的-default删除，这样下载TensorFlow就不会使用国外的网站，而是使用清华的镜像。</span><br><span class="line">8. 然后使用conda install tensorflow-gpu下载TensorFlow框架。</span><br><span class="line">9. 运行TensorFlow程序，指定使用哪一块GPU，如果不指定使用全部GPU。   </span><br><span class="line">```CUDA_VISIBLE_DEVICES=5  python train.py</span><br></pre></td></tr></table></figure></li><li><p>直接杀死一个进程<br><code>kill -9 [PID]</code>    </p><h2><span id="23-部署代码到服务器">2.3. 部署代码到服务器</span></h2></li><li>首先在服务器上安装Tomcat</li><li>使用FileZilla进入到服务器，使用putty进入到服务器。</li><li>使用putty cd到到file0/apache-tomcat下，输入bin/shutdown.sh关掉Tomcat服务器。然后把webapps下的项目删除，进入到logs删除里面的日志。</li><li>使用eclipse把项目导出成war压缩包</li><li>使用FileZilla把war上传到webapps下。</li><li>使用bin/startup.sh启动Tomcat服务器</li><li>查看日志主要在catalina.out这个文件中</li><li><a href="http://hz1:8080/EQ/jsp/login.jsp" target="_blank" rel="noopener">http://hz1:8080/EQ/jsp/login.jsp</a><h2><span id="24-在本地运行gluon">2.4. 在本地运行gluon</span></h2>&ensp;&ensp;&ensp;&ensp;在cmd命令行中，找到存储程序的目录，然后输入activate gluon，然后输入jupyter notebook就可以写程序了<h2><span id="25-修复集群">2.5. 修复集群</span></h2>&ensp;&ensp;&ensp;&ensp;当集群中有个节点不能启动时，可以在虚拟机管理界面把虚拟机启动。<br>在浏览器中输入<a href="https://172.31.43.150" target="_blank" rel="noopener">https://172.31.43.150</a><br>点击“登录到vsphere web client”<br>用户名：administrator@vsphere.local，<br>密码：Lenovo-123，<br>点击右侧的“vCenter清单列表”，然后点击左侧的“虚拟机”，在左侧找到需要重启的虚拟机，单击选中，在右侧“关闭虚拟机电源”，关闭后再点击“打开虚拟机电源”。这样就把虚拟机重启了。还需要启动Cloudera manager agent。使用putty，例如输入hz5，<br>进入到ssh，用户名root，密码hz@bjtu_cit。<br>先和server同步时间ntpdate hz1，<br>然后启动agent：/opt/cm-5.7.2/etc/init.d/cloudera-scm-agent start</li></ol><p>平时就可以使用/vsphere-client/?csp#extensionId%3Dvsphere.core.viVms.domainView</p><h2><span id="26-运行spark">2.6. 运行Spark</span></h2><p>&ensp;&ensp;&ensp;&ensp;不论是用什么语言写的spark程序在运行的时候都要用spark-submit来运行<br>如果是java程序需要指定—class 主程序的报名.类名<br>例如—class test.Test<br>python不需要指定—class这个参数</p><ol><li>local模式：spark-submit —class test.Test —master local[2] mySpark.py    </li><li>standalone模型： spark-submit —class test.Test —master spark://hz4:7077 mySpark.py   </li><li>yarn模式： spark-submit —class test.Test —master yarn mySpark.py<h2><span id="27-使用vscode">2.7. 使用vsCode</span></h2>&ensp;&ensp;&ensp;&ensp;使用vscode编写python程序时，如果出现空格和Tab共用时导致程序出错。需要把所有的Tab换成空格，在vscode最下面有一个蓝色的条纹，在条纹的右边有”空格4”，点击这个字，然后在vscode的最上面会出现几个选项：点击“将缩进转换成空格”这个选项，再运行就没错了。  <h2><span id="字节">字节</span></h2>&ensp;&ensp;&ensp;&ensp;int是4个字节，无论数字多长都是4个字节，如果用string定义数字时，字节的长度就是字符的个数。   <pre><code class="lang-java">int a = 11111111;System.out.println(a+&quot;(int)的字节数：&quot;+Bytes.toBytes(a).length);int b=22;System.out.println(b+&quot;(int)的字节数：&quot;+Bytes.toBytes(b).length);String aString = &quot;11111111&quot;;System.out.println(aString+&quot;(string)的字节数：&quot;+Bytes.toBytes(aString).length);String bString = &quot;22&quot;;System.out.println(bString+&quot;(string)的字节数：&quot;+Bytes.toBytes(bString).length);</code></pre>输出如下所示：<br><img src="/2019/03/14/Learning/bytes.png" alt=""></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;今天看了一天的spark streaming还是没有把程序调通，很失落，觉得自己怎么就那么笨，有点不开心。在做项目的过程中，学习到很多的东西，在这里记录下来。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="集群、python、常识" scheme="http://yoursite.com/tags/%E9%9B%86%E7%BE%A4%E3%80%81python%E3%80%81%E5%B8%B8%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://yoursite.com/2019/03/12/Attention/"/>
    <id>http://yoursite.com/2019/03/12/Attention/</id>
    <published>2019-03-12T02:34:53.000Z</published>
    <updated>2019-03-12T02:34:54.030Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</title>
    <link href="http://yoursite.com/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/"/>
    <id>http://yoursite.com/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/</id>
    <published>2019-03-05T02:51:56.000Z</published>
    <updated>2019-03-10T08:25:31.847Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;今天实验室分享了一篇2019年AAAI的论文：<a href="http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf" target="_blank" rel="noopener">Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand</a>。讲的是通过时空多图卷积来进行打车需求的预测。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E4%BB%BB%E5%8A%A1">2. 任务</a></li><li><a href="#3-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">3. 数据处理</a><ul><li><a href="#31-neighborhood">3.1. Neighborhood</a></li><li><a href="#32-functional-similarity">3.2. Functional similarity</a></li><li><a href="#33-transportation-connectivity">3.3. Transportation connectivity</a></li></ul></li><li><a href="#4-st-mgcn%E6%A8%A1%E5%9E%8B">4. ST-MGCN模型</a><ul><li><a href="#41-temporal-correlation-modeling">4.1. Temporal correlation modeling</a></li><li><a href="#42-multi-graph-convolution">4.2. Multi-Graph Convolution</a></li></ul></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;上学期实验室也分享论文，但是分享之后就忘了。所以决定从这学期开始，对一些我觉得对我有帮助的论文记录下来。<br>&ensp;&ensp;&ensp;&ensp;这篇论文和以前读到的图卷积的论文的不同是：这篇论文构建了多个图。<br>&ensp;&ensp;&ensp;&ensp;在介绍这篇论文之前，先写出自己以前一直没有弄清楚的一个点。<strong>一个图有2个矩阵，一个是邻接矩阵A，一个是图信号矩阵X。邻接矩阵A表示的是这个图的结构，节点与节点之间的连接关系。图信号矩阵表示这个图中每个节点的特征。假设一个图有100个节点，每个节点有3个特征，那么邻接矩阵A的维度是100x100，图信号矩阵X的维度是100x3。在构造下面的3个不同的图时，只是邻接矩阵A不同，图信号矩阵都是一样的。在计算图卷积的时候会同时用到图的邻接矩阵和图信号矩阵。</strong></p><h1><span id="2-任务">2. 任务</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文的任务就是一个区域$T$个时间段的特征(可能有1个特征即打车订单数，也可有多个特征)预测第$T+1$时刻的特征值。     </p><h1><span id="3-数据处理">3. 数据处理</span></h1><p>&ensp;&ensp;&ensp;&ensp;首先这篇论文将一个城市进行网格划分，一个网格表示一个区域region。然后对这些网格构建图。<strong>多图就体现在构建图上，原先的论文是构建一个图，这篇论文构建了3个图</strong>。论文的任务是打车需求量预测。论文中说一个region的打车需求可能和以下3个方面有关系：邻近区域neighborhood，和该区域功能相近的区域，和该区域道路可达的区域。所以论文根据以上3种关系分别构建了3个图。  </p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/道路图.png" alt="">  </p><p>&ensp;&ensp;&ensp;&ensp;例如上图所示，和region1相邻的区域是region2，和region1功能相似的是region3，和region1交通可达的是region4。</p><h2><span id="31-neighborhood">3.1. Neighborhood</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的邻居图用$\mathcal{G}_N=(V,A_N)$表示，其中V表示所有的区域，$A_N$表示区域与区域之间的邻居关系。构建的图一个节点表示一个region，2个区域有边表示这2个区域是邻居，没有边相连表示不是邻居。这样就可以从原始的图中根据邻近关系构建出一张图     </p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/邻居图.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;怎么判断两个区域是否相邻？首先把一个city划分成grid，一个区域周围的8个区域就是这个区域相邻，那么$A_{N,ij}=1$,否则为0。这样就构建了一个大小为$\mathbb{R}^{|V|\times|V|}$的矩阵$A$。</p><h2><span id="32-functional-similarity">3.2. Functional similarity</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的功能相似图用$\mathcal{G}_N=(V,A_S)$表示，其中V表示所有的区域，$A_S$表示区域与区域之间的功能相似关系。构建的图一个节点表示一个region，2个区域有边表示这2个区域之间的相似关系，是一个[0,1]之间的数。计算两个区域之间的相似关系：首先需要知道每个区域的POI向量。POI向量是这个区域在某个类别的POI的个数，例如POI有5类，分别是：学校、公园、医院、商场、居民区，这种区域有1个学校，2个公园、0个医院、3个商场、1个居民区，则POI向量为[1,2,0,3,1]，计算两个region的功能相似性即计算两个POI向量的相似性。</p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/功能图.png" alt="">   </p><h2><span id="33-transportation-connectivity">3.3. Transportation connectivity</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的交通可达图用$\mathcal{G}_N=(V,A_C)$表示，其中V表示所有的区域，$A_C$表示区域与区域之间的可达关系。构建的图一个节点表示一个region，2个区域可达判断的依据是通过OpenStreetMap，看两个区域之间是否有highway，subway，motorway，如果有的话就表示这两个区域交通可达。如果两个<br>region交通可达，则conn($v_i,v_j$)=1,否则为0。然后需要注意的是这里减去了邻居关系。因为考虑了交通可达，不想再次考虑邻居关系。取max为了保证没有负值。</p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/可达图.png" alt="">      </p><h1><span id="4-st-mgcn模型">4. ST-MGCN模型</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文提出的模型是$spatiotemporal \quad   multi-graph \quad convolution \quad network\quad(ST-MGCN)$。首先输入是原始数据，一个城市所有区域的特征，不同的层表示不同的时间，那么输入$X\in\mathbb{R}^{T\times|V|\times{P}}$，每一个时刻有一个图信号矩阵，维度为$|V|\times{P}$，一共有$T$个时刻。然后从原始数据中提取出3张图，分别表示region之间的邻居图、功能相似图、交通可达图。这三张图的维度也是$\mathbb{R}^{T\times|V|\times{P}}$，只是这3张图的邻接矩阵不同。然后经过$contextual \quad   gated \quad recurrent \quad neural\quad nwtwork\quad (CGRNN)$对时间进行建模，然后分别输出一张图，一共输出3张图。然后再用multi-graph convolution对空间进行建模，最终输出一张图，表示下一时刻各个节点的特征。</p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/模型图.png" alt=""><br><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/模型图解释.png" alt="">      </p><h2><span id="41-temporal-correlation-modeling">4.1. Temporal correlation modeling</span></h2><p>&ensp;&ensp;&ensp;&ensp;提出$Contextual \quad Gated \quad Recurrent \quad Neural \quad Network \quad (CGRNN)$来对不同时刻的图信号进行建模。在一个城市的打车需求，在某一个时刻，可以记录每个节点(region)的特征，因为是时间序列数据，就有了时间的维度，这样就可以形成T个图信号。CGRNN在对时间建模时引入了上下文信息contextual information。其中contextual information指的是邻近区域的信息，通过图卷积GCN来获取。模型框架图如下所示： </p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/temporal.png" alt="时间模型">   </p><p>&ensp;&ensp;&ensp;&ensp;有T个时刻，第$t$个时刻的观察值是$X^{(t)}\in\mathbb{R}^{|V|\times{P}}$，其中$P$表示每个节点的特征维度，如果$P=1$表示只有一个特征，仅包含region的打车订单数。  </p><p>&ensp;&ensp;&ensp;&ensp;按照上面的模型图，执行的顺序是：<br>（1）先把原始的图$[X^{(t)},X^{(t+1)},…]\in\mathbb{R}^{T\times|V|\times{P}}$经过一个池化层，把一个特征在所有区域的特征值取平均，这样就变成维度为$\mathbb{R}^{T\times1\times{P}}$，与此同时对原始的图信号$[X^{(t)},X^{(t+1)},…]\in\mathbb{R}^{T\times|V|\times{P}}$中的每个时刻的图信号$\mathbb{R}^{|V|\times{P}}$进行卷积操作，K=K‘，表示得到一个结合K`阶邻居信息图$\mathbb{R}^{|V|\times{P}}$，T个时刻共得到T个$\mathbb{R}^{|V|\times{P}}$,即$\mathbb{R}^{T\times|V|\times{P}}$，再经过一个池化操作，得到一个$\mathbb{R}^{T\times1\times{P}}$，然后把2个$\mathbb{R}^{T\times1\times{P}}$进行拼接concatenate，得到一个$\mathbb{R}^{T\times1\times{2P}}$，每个region的特征维度变成2P，前一个P表示的是自己的特征，后一个P表示的整合邻居后的特征。<br>上述的操作在论文中的式子如下：</p><script type="math/tex; mode=display">\hat{X}^{(t)}=[X^{(t)},F_\mathcal{G}^{K`}(X^{(t)})] \quad for \quad t=1,2,...T \tag{6}</script><script type="math/tex; mode=display">\mathcal{z}^{(t)}=F_{pool}(\hat{X}^{(t)})=\frac{1}{|V|}\sum_{i=1}^{|V|}\hat{X}_{i,;}^{(t)} \quad for \quad t=1,2...T \tag{7}</script><p>论文中的式子是先拼接成$\mathbb{R}^{T\times|V|\times{2P}}$，然后再池化成$\mathbb{R}^{T\times1\times{2P}}$，但是模型图上是先分别池化成$\mathbb{R}^{T\times1\times{P}}$，再拼接成$\mathbb{R}^{T\times1\times{2P}}$<br>（2）然后经过2个全连接层，得到关于时间的attention,s的维度是$s\in\mathbb{R}^T$</p><script type="math/tex; mode=display">s=\sigma(W_2\delta(W_1\mathcal{z}))\tag{8}</script><p>（3）将s和原始图信号进行内积，得到缩放后的图信号，维度为$\mathbb{R}^{T\times|V|\times{P}}$。</p><script type="math/tex; mode=display">\widetilde{X}^{(t)}=X^{(t)}\circ s^{(t)} \quad for\quad t=1,2...T\tag{9}</script><p>（4）最后经过一个共享的RNN神经网络。对于每一个节点，在每一个时刻会有一个P个特征值，经过T个时刻，会形成一个$\mathbb{R}^{T\times{P}}$的矩阵，一共有V个节点，对每一个节点的时间序列都应用这个RNN，最后每个节点都会输出一个隐藏变量$H_i$,V个节点会形成V个隐藏变量，最终会输出一个$\mathbb{R}^{|V|\times{P}}$的矩阵。</p><script type="math/tex; mode=display">H_{i,:}=RNN(\widetilde{X}_{i,:}^{(1)},...,\widetilde{X}_{i,:}^{(T)};W_3) \quad for \quad i=1,2...|V|\tag{10}</script><p>&ensp;&ensp;&ensp;&ensp;一共有三个图，所以经过CGRNN会输出3个$\mathbb{R}^{|V|\times{P}}$的矩阵,这时的邻接矩阵还是输入图的A。    </p><h2><span id="42-multi-graph-convolution">4.2. Multi-Graph Convolution</span></h2><p>&ensp;&ensp;&ensp;&ensp;对时间建模完成之后，接下来使用Multi-graph Convolution对空间进行建模。</p><script type="math/tex; mode=display">X_{l+1}=\sigma(\bigsqcup_{A\in\mathbb{A}}f(A;\theta_i)X_lW_l)</script><p>其中$\bigsqcup$是聚合函数，可以是sum，avg，max等，其中$f(A;\theta_i)$是ChebNet卷积核，是$\sum_{k=0}^K\theta_kL^k$,卷积运算可以写成$g_{\theta}*x=(\sum_{k=0}^K\theta_kL^k)x$，其中$L_{ij}^k=0$表示节点$i$到节点$j$可以在k跳到达，$k$定义了图卷积的感受野。以road connectivity为例，$\mathcal{G}_C=(V,A_C)$中，拉普拉斯矩阵L由邻接矩阵A计算得到，在道路连通图中的邻接矩阵$A_{C,1,4}=1;A_{C,1,6}=0;A_{C,4,6}=1$表示region1和4道路连通，region1和6道路不连通，region4和6道路连通。拉普拉斯矩阵L中，对角线表示这个节点的度，其余值不为0表示这个节点的一阶邻居，道路连通图的拉普拉斯矩阵$L_{C,1,4}^1\ne0;L_{C,1,6}^1=0;L_{C,4,6}^1\ne0$。如果在ChebNet中K=1，则对于region1这个节点来说，经过一次卷积运算后，下一层的输出region1这个节点新的特征中只包含了region1的一阶邻居的信息，不包含region6的信息，因为$L_{C,1,4}^1\ne0;L_{C,1,6}^1=0$，如果ChebNet中的K=2，则region1经过一次图卷积运算，下一层region1的特征包含一阶邻居region4和二阶邻居region6的信息，因为$L_{C,1,4}^2\ne0;L_{C,1,6}^2\ne0$。     </p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/chebnet.png" alt="时间模型">    </p><p>&ensp;&ensp;&ensp;&ensp;这个图中，假设ChebNet的$K=k$,根据卷积的运算公式$g_{\theta}*x=(\sum_{k=0}^K\theta_kL^k)x$，对于上图中，把黑色的点当做中心结点，黄色的点是一阶邻居，红色的点是二阶邻居。蓝色的点是三阶邻居。首先计算$k=1$时，则对于黑色的中心结点，整合了一阶邻居的信息得到一个图信息矩阵。然后计算$k=2$时，则对于黑色的中心结点，结合一阶邻居和二阶邻居的信息得到一个新的图信号矩阵，一直到$K=k$，得到了$k$个图信号矩阵，然后把这$k$个图信号矩阵相加得到下一层黑色的中心结点的表示。如果对$X_l$中的每个节点都执行这样的操作，则经过图卷积，下一层的输出$X_{l+1}$则表示每个节点整合了邻居信息得到的一个新的图信号矩阵。<br>&ensp;&ensp;&ensp;&ensp; 这样每一个图经过一个图卷积得到一个新的图信号矩阵，3个图得到3个新的图信号矩阵，然后再执行$\bigsqcup$聚合操作，在这篇论文中，$\bigsqcup$选择的是sum操作，就是说得到的3个新的图信号矩阵，直接相加就得到一个图，表示最终的预测结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;今天实验室分享了一篇2019年AAAI的论文：&lt;a href=&quot;http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand&lt;/a&gt;。讲的是通过时空多图卷积来进行打车需求的预测。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="Multi-Graph Convolutional" scheme="http://yoursite.com/tags/Multi-Graph-Convolutional/"/>
    
      <category term="Spatiotemporal" scheme="http://yoursite.com/tags/Spatiotemporal/"/>
    
  </entry>
  
  <entry>
    <title>图卷积</title>
    <link href="http://yoursite.com/2019/03/03/%E5%9B%BE%E5%8D%B7%E7%A7%AF/"/>
    <id>http://yoursite.com/2019/03/03/图卷积/</id>
    <published>2019-03-03T04:25:25.000Z</published>
    <updated>2019-03-12T02:35:32.219Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-图卷积">1. 图卷积</span></h1><p>&ensp;&ensp;&ensp;&ensp;看了很久关于图卷积的内容，但总觉得自己理解不深刻，在这里把自己的一些想法写出来，也算把图卷积的内容梳理一下。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E5%9B%BE%E5%8D%B7%E7%A7%AF">1. 图卷积</a></li><li><a href="#2-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">2. 卷积神经网络</a></li><li><a href="#3-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">3. 图卷积神经网络</a><ul><li><a href="#31-%E9%A1%B6%E7%82%B9%E5%9F%9Fvertex-domain">3.1. 顶点域(Vertex Domain)</a><ul><li><a href="#311-%E5%9B%BE%E4%B8%AD%E9%A1%B6%E7%82%B9%E7%9A%84%E9%80%89%E6%8B%A9node-sequence-selection">3.1.1. 图中顶点的选择Node Sequence Selection</a></li><li><a href="#312-%E6%89%BE%E5%88%B0%E4%B8%AD%E5%BF%83%E7%BB%93%E7%82%B9%E7%9A%84%E9%82%BB%E5%9F%9Fneighborhood-assembly">3.1.2. 找到中心结点的邻域Neighborhood Assembly</a></li><li><a href="#313-%E5%9B%BE%E8%A7%84%E8%8C%83%E5%8C%96%E8%BF%87%E7%A8%8Bgraph-normalization">3.1.3. 图规范化过程Graph Normalization</a></li><li><a href="#314-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84convolutional-architecture">3.1.4. 卷积网络结构Convolutional Architecture</a></li><li><a href="#315-%E4%BC%AA%E4%BB%A3%E7%A0%81">3.1.5. 伪代码</a></li><li><a href="#316-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B">3.1.6. 算法流程</a></li></ul></li><li><a href="#32-%E8%B0%B1%E5%9F%9Fspectral-domain">3.2. 谱域Spectral Domain</a><ul><li><a href="#321-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97">3.2.1. 图卷积运算</a></li><li><a href="#322-%E7%AC%AC%E4%B8%80%E4%BB%A3%E5%9B%BE%E5%8D%B7%E7%A7%AFscnn">3.2.2. 第一代图卷积SCNN</a></li><li><a href="#323-%E7%AC%AC%E4%BA%8C%E4%BB%A3%E5%9B%BE%E5%8D%B7%E7%A7%AFchebnet">3.2.3. 第二代图卷积ChebNet</a></li><li><a href="#324-%E7%AC%AC%E4%B8%89%E4%BB%A3%E5%9B%BE%E5%8D%B7%E7%A7%AF">3.2.4. 第三代图卷积</a></li></ul></li></ul></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;在介绍图卷积之前，先介绍一下卷积神经网络。</p><h1><span id="2-卷积神经网络">2. 卷积神经网络</span></h1><p>&ensp;&ensp;&ensp;&ensp;卷积神经网络在图像上用的比较多。因为图像的一些重要的性质是：(1)局部性；(2)稳定性；即平移不变性和有大量相似的碎片；(3)多尺度性，简单的结构组合形成复杂的抽象结构。<br>&ensp;&ensp;&ensp;&ensp;图像本来就是一个规范的形状，可以形式化成一个规则的矩阵，再定义一个卷积核，卷积核在图像矩阵上滑动，把周围的几个像素值整合成一个值，获取了图像的局部性。可能会有多个卷积核，用来识别图像中不同的特征，比如下面例子所示，第一个卷积核用来识别左右的边缘，第二个卷积核用来识别上下的边缘。   </p><p><img src="/2019/03/03/图卷积/多个卷积核.png" alt="">    </p><ul><li>在卷积神经网络中，需要训练的参数是卷积核。</li><li>在卷积神经网络中，卷积层后面通常跟一个池化层，防止参数越来越多。</li><li>卷积核的大小通常是3x3和5x5</li><li>池化层如果是3x3，步长为2，那么图像大小会变成原来的一半，变成原先图像的多少和步长有关。</li><li>在图卷积层的最后一层是全连接层，可以使用1x1的卷积核来代表全连接，比如最后池化层输出是5x5x16，表示一层是5x5，一共有16层(通道)，经过全连接变成一个400x1的向量。    </li></ul><p><img src="/2019/03/03/图卷积/CNN实例.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;使用卷积神经网络对一张图片进行分类时，首先给定这张图片的特征，比如是32x32x3和这样图片的实际类别，如果是10个类，这张图片是第2个类，那么就是一个[0,1,0,…]的向量。将图片输入卷积层中，卷积核刚开始是随机初始化的，输入X和卷积核做卷积操作，经过池化层，再经过一个卷积层和池化层，然后是一个全连接层，最后一个全连接层的输出结点个数是10，表示10个类别，如果输出的结果和实际的类别有偏差，然后通过误差反向传播，更新卷积核，直到误差最小，得到模型参数：卷积核。卷积核的参数通过优化求出才能实现特征提取的作用   </p><p><img src="/2019/03/03/图卷积/CNN训练.png" alt="">      </p><h1><span id="3-图卷积神经网络">3. 图卷积神经网络</span></h1><p>&ensp;&ensp;&ensp;&ensp;有了卷积神经网络，为什么还要引入图卷积神经网络？<br>&ensp;&ensp;&ensp;&ensp;因为卷积神经网络处理的是规则的矩阵，像图像和视频中的像素点都是排列整齐的矩阵，也是论文中提到的欧式结构(Euclidean Structure)。  </p><p><img src="/2019/03/03/图卷积/欧式结构.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;但在科学研究中，还有很多非欧式结构(Non Euclidean Structure)的数据,例如社交网络、信息网络。    </p><p><img src="/2019/03/03/图卷积/非欧式结构.png" alt="">    </p><p>&ensp;&ensp;&ensp;&ensp;实际上，这样的网络结构就是图论中抽象意义上的拓扑图。<br>&ensp;&ensp;&ensp;&ensp;为什么要研究GCN，原因如下三个：</p><ol><li>卷积神经网络CNN无法处理非欧式结构的数据，在非欧式结构数据中，图中每个顶点的相邻顶点个数可能不同，无法用一个同样尺寸的卷积核进行卷积运算。</li><li>由于卷积神经网络CNN无法处理非欧式结构的数据，但是又希望在这样的数据结构上有效地提取空间特征来进行机器学习，所以GCN成为了研究的重点。</li><li>拓扑结构的数据在生活中很常见，社交网络、交通领域都涉及到非欧式结构的数据。<br>&ensp;&ensp;&ensp;&ensp;图卷积网络GCN的本质目的是提取拓扑图的空间特征。图卷积神经网络中有2种：顶点域和谱域。这是提取拓扑图空间特征的两种方式，就是给定非欧式结构数据，从中构建图的两种方法。   <h2><span id="31-顶点域vertex-domain">3.1. 顶点域(Vertex Domain)</span></h2>&ensp;&ensp;&ensp;&ensp;提取拓扑图上的空间特征，就是把每个顶点的邻居找出来。这里的问题是:(1)按照什么条件去找中心顶点的邻居，也就是如何确定感受野？(2)确定了邻居，按照什么方式处理包含不同数据邻居的特征？<br>&ensp;&ensp;&ensp;&ensp;<a href="https://arxiv.org/abs/1605.05273" target="_blank" rel="noopener">Learning Convolutional Neural Networks for Graphs</a>是2016年在ICML中发表的一篇论文，这是<a href="http://www.matlog.net/icml2016_slides.pdf" target="_blank" rel="noopener">PPT讲解</a>。由于CNN并不能有效的处理非欧式结构数据，这篇paper的motivation就是想将CNN在图像上的应用generalize到一般的graph上面。<br>&ensp;&ensp;&ensp;&ensp;本文提到的算法思想是：将一个图结构的数据转化为CNN能够高效处理的结构。处理的过程主要分为三个步骤：(1)从图结构中选出一个固定长度具有代表性的结点序列；(2)对于选出的每一个结点，收集固定大小的邻居集合。(3)对由当前节点及其对应的邻居构成的子图进行规范化，作为卷积结构的输入。算法具体分为4个步骤。<h3><span id="311-图中顶点的选择node-sequence-selection">3.1.1. 图中顶点的选择Node Sequence Selection</span></h3>&ensp;&ensp;&ensp;&ensp;首先对于输入的一个Graph，指定中心顶点的个数，然后确定确定图中的中心结点，确定中心结点主要采取的方法是：centrality，也就是中心化的方法，就是越处于中心位置的点越重要。这里的中心位置不是空间上的概念，应该是度量一个点的关系中的重要性的概念，简单的举例说明。如图5当中的两个图实际上表示的是同一个图，对其中红色标明的两个不同的nodes我们来比较他们的中心位置关系。比较的过程当中，我们计算该node和其余所有nodes的距离关系。我们假设相邻的两个node之间的距离都是1。    </li></ol><p><img src="/2019/03/03/图卷积/2个node.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;那么对于左图的红色node，和它直接相连的node有4个，因此距离+4；再稍微远一点的也就是和它相邻点相邻的有3个，距离+6;依次再相邻的有3个+9；最后还剩下一个最远的+4；因此我们知道该node的总的距离为23。同理我们得到右边的node的距离为3+8+6+8=25。那么很明显node的选择的时候左边的node会被先选出来。<br>&ensp;&ensp;&ensp;&ensp;当然，这只是一种node的排序和选择的方法，其存在的问题也是非常明显的。Paper并没有在这次的工作当中做详细的说明。   </p><h3><span id="312-找到中心结点的邻域neighborhood-assembly">3.1.2. 找到中心结点的邻域Neighborhood Assembly</span></h3><p>&ensp;&ensp;&ensp;&ensp;选出目标node之后，我们之后就要为目标node确定感受野大小。但是在确定之前，我们先构建一个candidate set，然后在从这个candidate set中选择感受野的node。这些感受野的candidate set，称为目标node的neighborhood。如下图所示，为每个目标node选择至少4个node（包括自己，k即为感受野node的个数）。接下来对选出来的每一个中心结点确定一个感受野receptive filed，以便进行卷积操作。但是在这之前，首先找到每个结点的邻域区域（neighborhood filed），然后再从当中确定感受野当中的结点。假设感受野的大小为k，那么对于每个结点会有2种情况：邻域结点的个数不够k个，或者邻域结点个数大于k个。      </p><p><img src="/2019/03/03/图卷积/选邻居.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;如上图所示，选出6个中心结点，对于每个中心结点，首先找到与其直接相邻的结点(被称为1阶邻居)，如果还不够再增加2阶邻居，那么对于1阶邻居已经足够的情况下，先全部放在候选 的区域中，在下一步中通过规范化做最终的选择。   </p><h3><span id="313-图规范化过程graph-normalization">3.1.3. 图规范化过程Graph Normalization</span></h3><p>&ensp;&ensp;&ensp;&ensp;这一步的目的在于将从candidate中选择感受野的node，并确定感受野中node的顺序。最终的结果如下图所示：假设上一步选择邻域过程中一个中心结点的邻居(一阶或者二阶)有N个，那么N可能和感受野大小K不相等。因此，normalize的过程就是要对N个邻居打上排序标签并进行选择，并且按照该顺序映射到向量中。    </p><p><img src="/2019/03/03/图卷积/normalize.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;如果这个中心结点的邻居个数不够k个的话，直接把所有的邻居全部选上，不够补上哑结点(dummy nodes),但还是需要排序的。如果中心结点的邻居个数N大于感受野k，则需要按照排序截断后面的结点。如上图所示，表示从中心结点到选邻居的整个过程。Normalize进行排序之后就能够映射到一个vector中，因此这一步最重要的是对结点进行排序。  </p><p>&ensp;&ensp;&ensp;&ensp;对于任意一个中心结点求解它的感受野的过程。这里的卷积核的大小为4(2x2的卷积核)，因此最终要选出4个邻居，包括中心结点本身。因此，需要给这些结点打标签(排序)。怎样打标签才是最好的？如上图要在7个结点中选出4个结点组成一个含有4个结点的图集合。作者认为，在一种标签下，就是已经给图中的节点排好序了，随机从集合中选出2个图，计算它们在向量空间的图距离和在图空间的图距离的差异的期望，如果这个期望越小那么就表示标签越好。得到最好的标签之后，就能够按着顺序将结点映射到一个有序的向量中，也就得到了感受野。    </p><h3><span id="314-卷积网络结构convolutional-architecture">3.1.4. 卷积网络结构Convolutional Architecture</span></h3><p>&ensp;&ensp;&ensp;&ensp;文章使用的是一个2层的卷积神经网络，将输入转化为一个向量vector之后便可以用来进行卷积操作了。具体的操作所示。     </p><p><img src="/2019/03/03/图卷积/卷积操作.png" alt="">    </p><p>&ensp;&ensp;&ensp;&ensp;首先最底层的灰色块为网络的输入，每一个块表示的是一个node的感知野（receptive field）区域，也是前面求解得到的4个nodes。其中an表示的是每一个node的数据中的一个维度（node如果是彩色图像那就是3维；如果是文字，可能是一个词向量……这里表明数据的维度为n）。粉色的表示卷积核，核的大小为4，但是宽度要和数据维度一样。因此，和每一个node卷季后得到一个值。卷积的步长（stride）为4，表明每一次卷积1个node，stride=4下一次刚好跨到下一个node。（备注：paper 中Figure1 当中，（a）当中的stride=1，但是转化为（b）当中的结构后stride=9）。卷积核的个数为M，表明卷积后得到的特征图的通道数为M，因此最终得到的结果为V1……VM，也就是图的特征表示。有了它便可以进行分类或者是回归的任务了。    </p><h3><span id="315-伪代码">3.1.5. 伪代码</span></h3><p><img src="/2019/03/03/图卷积/算法1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;这个算法用来选择要进行卷积操作的node，其中w为要选择的node的个数。s为stride的大小。其中一个关键在于graph labeling procedure l。labeling算法用来确定一个graph中node的次序。这个算法可以根据node degree来确定，或者根据其他确定centrality的测量方式，比如：between centrality， WL algorithm等。或者其他你认为可行的算法。  </p><p><img src="/2019/03/03/图卷积/算法2.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;这个算法非常简单，就是一个BFS算法，对每个目标node，寻找离node最近的至少k个node。   </p><p>&ensp;&ensp;&ensp;&ensp;这是整个论文的重点，这个步骤的目的在于将目标node无序的neighbors映射为一个有序的vector。这个label要实现一个目的：assign nodes of two different graphs to a similar relative position in the respective adjacency matrices if and only if their structural roles within the graph are similar. 也就是说，对于两个不同的graphs， 来自这两个graph的子结构g1和g2，它们在各自的graph中有相似的结构，那么他们label应该相似。为了解决这个问题，论文中定义了一个optimal graph normalization问题，定义如下：    </p><p><img src="/2019/03/03/图卷积/算法3-1.png" alt="">         </p><p>&ensp;&ensp;&ensp;&ensp;这个等式的解在于寻找一个一个labeling L， 使得从图的集合中任意选取两个图G1和G2，它们在vector space距离差距和它们在graph space的距离差距最小化。但是这个问题是NP-hard的问题，所以作者选择找一个近似解。即它比较了各种labeling方法，并从其中找出最优解。具体如下：   </p><p><img src="/2019/03/03/图卷积/算法3-2.png" alt="">    </p><p>&ensp;&ensp;&ensp;&ensp;在特征选择阶段，只有第一层和传统的CNN有区别，之后的卷积层和传统的一样。下面来举例来说明PATCHY-SAN如何提取顶点特征和边特征。我们假设a_v为顶点特征的个数，a_e为边特征的个数。w为目标node的个数，k为感受野中node的个数。对于每个输入图结构，运用上面的一系列normalization算法，我们可以得到两个tensor（w,k,a_v）和（w,k,k,a_e）,分别对应于顶点特征和边特征。这两个tensor可以被reshape成(wk, a_v)和(wk^2, a_e)，其中a_v和a_e可以分别看成是CNN中channel的个数。现在我们可以对它们做一维度的卷积操作，其中第一个的感受野大小为k，第二个感受野大小为k^2。而之后的卷积层的构造和传统的CNN一样了。     </p><h3><span id="316-算法流程">3.1.6. 算法流程</span></h3><p>输入：任意一张图<br>输出：每个channel输出w个receptive field</p><p>Step1： graph labeling（对图的节点做标记，比如可以用节点的度做标记，做图的划分，也 可以叫做color refinement or vertex classification）<br>文中采用The Weisfeiler-Lehman algorithm做图的划分。由此可以得到每个节点的rank 值（为了不同的图能够有一个规范化的组织方式）</p><p>Step2：对labeling好的节点排序，取前w个节点，作为处理的节点序列。（这样就可以把不 同size的graph，变成同一个size）若不足w个节点，则，在输出中加全零的receptive field，相当于padding</p><p>Step3：采用stride=s来遍历这w个节点。文中s=1（若s）1，为了输出有w个receptive field， 也用step2的方式补全）</p><p>Step4：对遍历到的每个节点v（称作root），采用bfs的方式获得此节点的k个1-neighborhood， 如果不k个，再遍历1-neighborhood的1-neighborhood。直到满足k个，或者所有的 邻居节点都遍历完。此节点和他的k个邻居节点就生成了neighborhood graph。</p><p>Step5： step4就生成了w个（s=1）neighborhood graph。需要对着w个graph 进行labeling， 根据离root节点v的远近来计算每个节点的rank，根据算法4是离v越近，r越小。 如果每个neighborhood graph不足k个节点，用0节点补充</p><p>Step6：规范化step5得到了已经label好的graph，因为需要把它变成injective，使每个节点 的标签唯一，采用nauty的算法通过这w个receptive field就能得到一个w(k+1)维的向量。    </p><h2><span id="32-谱域spectral-domain">3.2. 谱域Spectral Domain</span></h2><p>&ensp;&ensp;&ensp;&ensp;谱域就是GCN的理论基础了。这种思路就是借助图谱的理论来实现卷积操作。<br>&ensp;&ensp;&ensp;&ensp;谱图理论就是借助图的拉普拉斯矩阵的特征值和特征向量来研究图的性质。<br>&ensp;&ensp;&ensp;&ensp;<strong>在这里需要明确一点：谱图和顶点域的本质完全不一样。顶点域其实还是先对图进行处理，然后像类似卷积核在图上滑动来计算。这里谱域就没有卷积核在图上滑动的这个概念了。里面的卷积运算就理解成矩阵之间的运算，不要再去想卷积核滑动的思想了。</strong></p><h3><span id="321-图卷积运算">3.2.1. 图卷积运算</span></h3><p>&ensp;&ensp;&ensp;&ensp;由传统的傅里叶变换得到图上的傅里叶变换，我们不需要知道怎么由传统的傅里叶变换得到图上的傅里叶变换，只需要知道图上的傅里叶变换是$F(f)=U^Tf$，其中f是待变换的函数，$F(f)$是$f$傅里叶变换之后的函数。傅里叶的逆变换为$F^{-1}(f)=Uf$。<br>&ensp;&ensp;&ensp;&ensp;由传统的卷积定理得到图上的卷积，因为在计算传统的卷积时需要用到傅里叶变换，所以在计算图上的卷积时也需要用到图上的傅里叶变换。下图中的$F$表示的是傅里叶变换，$g*f$表示卷积运算，可以看到$g和f$的卷积等于$g$的傅里叶变换和$f$的傅里叶变换乘积的逆傅里叶变换。其中$f$是待卷积的函数,就是一个待卷积的图，$g$就是卷积核。 把图上的傅里叶变换代入到下面中，就可以得到图上的卷积运算。<br>&ensp;&ensp;&ensp;&ensp;首先计算$f$的傅里叶变换为$U^Tf$，卷积核的傅里叶变换写成对角矩阵的形式为:是L的特征值的函数     </p><script type="math/tex; mode=display">\left(\begin{matrix}  \hat{g}(\lambda_1) & &  \\  &  \ddots &  \\  & &  \hat{g}(\lambda_n)\\\end{matrix}\right)</script><p>两者的傅里叶变换乘积即为     </p><script type="math/tex; mode=display">\left(\begin{matrix}  \hat{g}(\lambda_1) & &  \\  &  \ddots &  \\  & &  \hat{g}(\lambda_n)\\\end{matrix}\right)U^Tf</script><p>再乘上$U$求两者傅里叶变换乘积的逆变换，则求出$f和g的卷积$     </p><script type="math/tex; mode=display">(f*g)_G=U\left(\begin{matrix}  \hat{g}(\lambda_1) & &  \\  &  \ddots &  \\  & &  \hat{g}(\lambda_n)\\\end{matrix}\right)U^Tf</script><p><strong>很多论文中会把上式写成$(f*g)_G=U((U^Tg)\odot(U^Tf))$</strong>       </p><p>可以看出$U$为特征向量组成的特征矩阵，$f$为待卷积函数，在图中就是图信号矩阵，重点在于设计可训练、共享参数的卷积核$g$。</p><p><img src="/2019/03/03/图卷积/谱图1.png" alt="">   </p><p><img src="/2019/03/03/图卷积/谱图2.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图3.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图4.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图5.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图6.png" alt="">      </p><p>&ensp;&ensp;&ensp;&ensp;给出一个无向图，可以写出这个图的邻接矩阵$A$,无向图的邻接矩阵是一个对称矩阵，其中对角线上全是0。度矩阵$D$是一个对角矩阵，只有对角线上有值，其余全是0，$D_{i,i}=\sum_jA_{i,j}$，把$A$中的每一行加起来就是这个点的度。拉普拉斯矩阵$L=D-A$，拉普拉斯矩阵是一个对称矩阵，只有中心节点和一阶相连的顶点非0，其余位置全为0。对角线上表示这个节点有几个一阶邻居(不包括自己)，这一行中值为-1的表示是该节点的一阶邻居。  </p><p><img src="/2019/03/03/图卷积/图.png" alt=""></p><p><img src="/2019/03/03/图卷积/矩阵.png" alt=""></p><h3><span id="322-第一代图卷积scnn">3.2.2. 第一代图卷积SCNN</span></h3><p>&ensp;&ensp;&ensp;&ensp;第一代图卷积模型SCNN是在2014年发表在NIPS中的<a href="https://arxiv.org/abs/1312.6203" target="_blank" rel="noopener">Spectral Networks and Deep Locally Connected Networks On Graph</a>提出来的。<br><img src="/2019/03/03/图卷积/SCNN.png" alt="">  </p><p><img src="/2019/03/03/图卷积/第一代.png" alt=""></p><p>&ensp;&ensp;&ensp;&ensp;第一代卷积。谱图卷积就是给定一个图信号，和一个卷积核。图信号就是假设有一个图，如果有200个节点，每个节点有3个特征，x就是一个200<em>3的矩阵，这个矩阵就是图信号矩阵。卷积核就是一个参数𝜃，谱图卷积的定义就是对归一化的拉普拉斯矩阵特征分解，得到特征值组成的矩阵$\Lambda$和特征向量组成的矩阵U，由于L是对称矩阵，所以U-1等于UT。谱图卷积的定义：在欧式空间内卷积的定义是傅里叶变换乘积的逆变换，研究图上的卷积是怎么来的，将图上的信号做一个图傅里叶变换，将卷积核做一个图傅里叶变换，将这两个做一个内积，然后再做一个图上的逆傅里叶变换。所以一个卷积核在图信号的谱图卷积就定义出来了。这个式子直接把卷积核g的傅里叶变换的对角矩阵当做参数，<strong>此时的卷积核不需要训练，因为右式中的所有值都是已知的。U已知，中间的对角矩阵的值</strong> $\theta$ <em>*就是傅里叶算子，也是已知的，图信号矩阵x也是已知的</em></em>。这种方式太简单，这个卷积核不具有局部性，不能捕获局部关系。并且这个式子的时间复杂度很高，是n的立方，第二个是参数过多。所以后人对其进行改进。</p><h3><span id="323-第二代图卷积chebnet">3.2.3. 第二代图卷积ChebNet</span></h3><p><img src="/2019/03/03/图卷积/ChebNet.png" alt="">  </p><p><img src="/2019/03/03/图卷积/改进1.png" alt=""> </p><p>&ensp;&ensp;&ensp;&ensp;第二代图卷积。$g_\theta$是关于特征值的一个函数，以前是直接把特征值变成卷积核，现在把每一个特征值上都乘上一个特征值矩阵的k次幂，然后把0到k-1次幂加到一起，构造成一个新的卷积核。把这个卷积核代入，得到右侧这个式子，右边这个式子不需要做特征值分解，直接把L连乘k次就可以了。<strong>这个卷积核是需要训练的，其中的参数是</strong>$\theta_k$<strong>通过初始化赋值再通过反向传播进行调整</strong>。所以这个一个改进，这个式子比前面那个式子的时间复杂度低。但是右边的这个时间复杂度也不低，因为涉及到L的k次幂，现在就想办法把右边这个式子的时间复杂度降低。   </p><h3><span id="324-第三代图卷积">3.2.4. 第三代图卷积</span></h3><p><img src="/2019/03/03/图卷积/图卷积3.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;ICLR2017<a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">Semi-supervised Classification with Graph Convolutional Networks</a>这篇论文就对上面那个式子进行改进，让K=1，最大特征值=2，这个式子就变成了右边这个式子，继续化简，让$\theta_0$和$\theta_1$互为相反数，称为一个权重，为什么能这么变呢，因为他相信训练的时候卷积核就可以学出互为相反数的参数。这样只考虑一阶邻居，如果堆叠2个卷积层的话，就可以考虑2阶邻居了。    </p><p><img src="/2019/03/03/图卷积/图卷积4.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;然后这篇论文又对刚刚的那个式子作一个归一化的trick，就是把邻接矩阵A加上一个单位矩阵，原先A是一个邻接矩阵，对角线上全为0，现在加上一个1，就是说节点自己到自己是没有边的，现在是自己有一条边又到达了自己，也就是增加一个自连接，然后重新计算一个度矩阵，然后就得到重新归一化的拉普拉斯矩阵，然后代入化成矩阵相乘的形式就是右边这个式子。这样卷积就定义完了，其中输入就是图信号X，卷积核就是Θ，前面的东西就是一个常量，只要拥有这三部分就可以得到GCN的卷积。卷积定义完了，然后加一个relu激活函数，这样一层图卷积就定义完了，如果要叠加一层，就是把上一层图卷积输出后的表示作为图卷积的输入，再做一次卷积。因为这篇论文做的是分类任务，所以要输出每个节点属于某个类的概率，所以用一个softmax就可以输出概率。<br>&ensp;&ensp;&ensp;&ensp;这就是现在谱图卷积计算的式子，实际上就是几个矩阵连乘，并没有卷积核滑动的概念。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-图卷积&quot;&gt;&lt;a href=&quot;#1-图卷积&quot; class=&quot;headerlink&quot; title=&quot;1. 图卷积&quot;&gt;&lt;/a&gt;1. 图卷积&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;看了很久关于图卷积的内容，但总觉得自己理解不深刻，在这里把自己的一些想法写出来，也算把图卷积的内容梳理一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="图卷积" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>拦截器</title>
    <link href="http://yoursite.com/2019/03/01/%E6%8B%A6%E6%88%AA%E5%99%A8/"/>
    <id>http://yoursite.com/2019/03/01/拦截器/</id>
    <published>2019-03-01T06:53:36.000Z</published>
    <updated>2019-03-01T13:43:32.767Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;拦截器是SpringMVC中的一个核心应用组件,主要用于处理多个Controller的共性问题.当我们的请求由DispatcherServlet<strong>派发到具体Controller之前</strong>首先要执行拦截器中一些相关方法,在这些方法中可以对请求进行相应预处理(例如权限检测,参数验证),这些方法可以决定对这个请求进行拦截还是放行。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8E%E8%AF%B7%E6%B1%82">2. 服务器与请求</a><ul><li><a href="#21-%E5%B8%B8%E8%A7%81%E7%9A%84web%E6%9C%8D%E5%8A%A1%E5%99%A8">2.1. 常见的WEB服务器</a></li><li><a href="#22-%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82">2.2. 发送请求</a></li><li><a href="#23-%E9%80%9A%E8%BF%87%E6%B5%8F%E8%A7%88%E5%99%A8%E5%8F%91%E9%80%81url%E8%AF%B7%E6%B1%82">2.3. 通过浏览器发送URL请求</a></li><li><a href="#24-js%E6%96%87%E4%BB%B6%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82">2.4. js文件发送请求</a></li></ul></li><li><a href="#3-%E5%AE%9E%E7%8E%B0%E6%8B%A6%E6%88%AA%E5%99%A8">3. 实现拦截器</a><ul><li><a href="#31-controller">3.1. Controller</a></li><li><a href="#32-interceptor">3.2. Interceptor</a></li><li><a href="#33-mvc%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">3.3. mvc配置文件</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-服务器与请求">2. 服务器与请求</span></h1><h2><span id="21-常见的web服务器">2.1. 常见的WEB服务器</span></h2><ol><li>Toncat服务器：我最常用的服务器，开放源代码的，运行servlet和JSP Web应用软件基于Java，比绝大多数的商业用的软件服务器要好。</li><li>Apache服务器：使用广泛，开源代码，支持多个平台，相对其他服务器占的内存较大，是重量级产品。</li><li>Microsoft IIS服务器：微软的，包括Web服务器，FTP服务器，NNTP服务器和SMTP服务器。需要购买。</li><li>Nginx服务器：俄罗斯的一个站点开发的，相比于Apache服务器，Nginx占用内存小且较稳定。<h2><span id="22-发送请求">2.2. 发送请求</span></h2>&ensp;&ensp;&ensp;&ensp;前端向服务器发送请求有2种，(1)通过浏览器发送请求，(2)进入到系统后，通过js发送请求。<h2><span id="23-通过浏览器发送url请求">2.3. 通过浏览器发送URL请求</span></h2>(1)用户在浏览器上输入网址，包含协议和域名.<br>(2)浏览器获得IP地址，浏览器先找自身缓存是否有记录，没有的话再找操作系统缓存，再没有就请求本地DNS服务器帮忙，本地DNS再找不到再一层层往上，最终浏览器获得对应的IP地址。<br>(3)浏览器发送请求，浏览器根据HTTP协议，给对应IP地址的主机发送请求报文，默认端口为80，报文包括请求内容，浏览器信息，本地缓存，cookie等信息。<br>(4)web服务器接收请求，寻找文件,Tomcat服务器接收到请求，找对应的html文件<br>(5)返回数据，web服务器向浏览器反馈html文件，浏览器进行渲染，页面加载。    <h2><span id="24-js文件发送请求">2.4. js文件发送请求</span></h2>&ensp;&ensp;&ensp;&ensp;在项目中，使用ajax向服务器发送请求，例如xxx.do。<h1><span id="3-实现拦截器">3. 实现拦截器</span></h1>&ensp;&ensp;&ensp;&ensp;拦截器需要实现   HandleInterceptor接口,或者继承HandlerInterceptorAdaptor抽象类;<br>HandlerInterceptor接口的三个方法:   </li><li>preHandle()</li><li>postHandle()</li><li>afterCompletion()   </li></ol><p>&ensp;&ensp;&ensp;&ensp;inceptor的作用是，每次在前端向后台发送一个请求时do,后台都会先经过inceptor中的preHandle这个函数，判断这个请求是否满足要求（是否已经登录，是否是管理员），如果满足要求就返回true，系统会自动把这个do请求提交给controller对应的函数进行处理，controller中的函数调用完之后，再次进入Inception中的postHandle()和afterCompletion()方法中。否则preHandle返回false，不会提交这个请求，不会执行Controller中的函数，也不会执行之后的Inception中的postHandle()和afterCompletion()方法。<br>&ensp;&ensp;&ensp;&ensp;服务器一启动,就会创建拦截器对象;拦截器是单例的,整个过程,拦截器只有一个实例对象。<br>&ensp;&ensp;&ensp;&ensp;项目中需要实现一个登录系统，当用户没有登录时，不能访问系统的主页和其他页面，但是可以访问系统的登录界面，所以需要在mvc.xml中设置一下，不拦截登录的请求。   </p><h2><span id="31-controller">3.1. Controller</span></h2><p>下面是用户登录的Controller实现,当前端访问login.jsp时，这时登录的请求不会被拦截器拦截，会执行login()方法，验证前端用户输入的用户名和密码是否正确，如果正确的话，将userName放入到session中，并返回给前端index，那么界面将会跳转到index.jsp，如果用户名或密码错误，那么返回给前端login，前端界面还是login.jsp。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> UserService userService;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">UserController</span><span class="params">()</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="keyword">this</span>.getClass().getName() + <span class="string">" 初始化"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/login"</span>, method = RequestMethod.POST)</span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">login</span><span class="params">(HttpServletRequest request, HttpSession session)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> SQLException, IOException, NoSuchAlgorithmException, InvalidKeySpecException </span>&#123;</span><br><span class="line"><span class="comment">// userList存储了所有的用户</span></span><br><span class="line"><span class="comment">// 每个用户以HashMap的形式存储</span></span><br><span class="line"><span class="comment">// key分别是："userName"，"password"，"salt"</span></span><br><span class="line">ArrayList&lt;HashMap&lt;String, String&gt;&gt; userList = userService.getUserInfo();</span><br><span class="line"></span><br><span class="line">String input_userName = request.getParameter(<span class="string">"userName"</span>);</span><br><span class="line">String input_password = request.getParameter(<span class="string">"password"</span>);</span><br><span class="line"></span><br><span class="line">PBKDF2Util pbkdf2Util = <span class="keyword">new</span> PBKDF2Util();</span><br><span class="line"><span class="comment">// 判断当前用户输入的用户名和密码是否正确</span></span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; userList.size(); i++) &#123;</span><br><span class="line">HashMap&lt;String, String&gt; oneUser = userList.get(i);</span><br><span class="line">String actual_userName = oneUser.get(<span class="string">"userName"</span>);</span><br><span class="line">String actual_password = oneUser.get(<span class="string">"password"</span>);</span><br><span class="line">String salt = oneUser.get(<span class="string">"salt"</span>);</span><br><span class="line"><span class="keyword">boolean</span> password_match = pbkdf2Util.authenticate(input_password, actual_password, salt);</span><br><span class="line"><span class="keyword">if</span> (input_userName.equals(actual_userName) &amp;&amp; password_match) &#123;</span><br><span class="line">flag = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="comment">// for</span></span><br><span class="line"><span class="keyword">if</span> (flag) &#123;</span><br><span class="line">session.setAttribute(<span class="string">"user"</span>, input_userName);</span><br><span class="line"><span class="keyword">return</span> <span class="string">"&#123;\"status\": 0, \"url\": \"index\"&#125;"</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">request.setAttribute(<span class="string">"msg"</span>, <span class="string">"用户名或密码错误"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="string">"&#123;\"status\": 1, \"url\": \"login\", \"msg\": \"用户名或密码错误\"&#125;"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/logout"</span>)</span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">logout</span><span class="params">(HttpSession session)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="comment">// System.out.println("进入到logout()方法中");</span></span><br><span class="line"><span class="comment">// 清除session的数据</span></span><br><span class="line">session.invalidate();</span><br><span class="line"><span class="keyword">return</span> <span class="string">"&#123;\"status\": 0, \"url\": \"login\"&#125;"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>lll</p><h2><span id="32-interceptor">3.2. Interceptor</span></h2><p>在interceptor中会拦截URL请求，如果session中的用户名为空会重定向到login.jsp。但在在做项目时遇见一个问题，拦截器只能拦截js中的ajax发来的URL请求，不能拦截浏览器发送的URL请求。也就是说如果用户在浏览器中输入index.jsp，不会经过拦截器，如果是js中的ajax发送的请求，会经过拦截器。如果用户没有登录直接在浏览器中输入index.jsp，这时页面依然可以进入到index.jsp，这说明拦截器没有起作用。为了应对这一情况，有三种解决方案：<br>（1）把判断用户是否登录的代码写到了jsp中，在jsp中写java代码需要加上&lt;%%&gt;，在这里判断session中的用户名，如果为空的话，直接重定向到login.jsp，这样用户在未登录的情况下，在浏览器上输入index.jsp，页面不会跳转到index.jsp中，还是在login.jsp中。<br>（2）把所有的jsp文件放在WEB-INF文件里,这样用户是直接不能访问WEB-INF文件下的jsp文件的。spring mvc的理念也是通过controller里的@RequestMapping来请求相关jsp页面，而非用户直接访问jsp页面。也就是说，jsp页面的访问需要通过controller来进行一次请求，因为会拦截对controller的请求，所以也就相当于拦截了jsp页面。如果要做登陆拦截，只需要把登陆页面不拦截，其余页面拦截进行是否登陆的验证即可。<br>（3）jsp如果不放在WEB-INF文件下，spring mvc是无法拦截的，这种情况下需要用最原始的servlet的Filter接口。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;%</span><br><span class="line"><span class="comment">//实现登录检查，如果用户没有登录，重定向到登录界面</span></span><br><span class="line"><span class="comment">//这段代码要加载所有需要验证页面里,使用</span></span><br><span class="line"><span class="comment">//&lt;%@include file="/jsp/navigation.jsp"把登录验证加载其余jsp中</span></span><br><span class="line">Object userName = <span class="string">""</span>;</span><br><span class="line"><span class="keyword">if</span> (session == <span class="keyword">null</span>) &#123;</span><br><span class="line">response.sendRedirect(request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">userName = session.getAttribute(<span class="string">"user"</span>);</span><br><span class="line"><span class="keyword">if</span> (userName == <span class="keyword">null</span>) &#123;</span><br><span class="line">response.sendRedirect(request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">userName = userName.toString();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">%&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginInterceptor</span> <span class="keyword">implements</span> <span class="title">HandlerInterceptor</span> </span>&#123;</span><br><span class="line"><span class="comment">// 步骤1</span></span><br><span class="line"><span class="comment">// 在前端发出一个url(xxx.do)请求时，先执行这个方法，判断当前用户是否为空</span></span><br><span class="line"><span class="comment">// 如果用户已经登录，则返回true,否则返回false</span></span><br><span class="line"><span class="comment">// 只有当该函数返回true时，才会调用controller中对应的函数，</span></span><br><span class="line"><span class="comment">// 返回false不用调用controller中的函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object arg2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">String path = request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>;</span><br><span class="line">HttpSession session = request.getSession(<span class="keyword">false</span>);</span><br><span class="line"><span class="keyword">if</span> (session == <span class="keyword">null</span> || !request.isRequestedSessionIdValid()) &#123;</span><br><span class="line">response.sendRedirect(path);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取登录用户信息</span></span><br><span class="line">String user = session.getAttribute(<span class="string">"user"</span>).toString();</span><br><span class="line"><span class="keyword">if</span> (user == <span class="keyword">null</span>) &#123;</span><br><span class="line">response.sendRedirect(path);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 步骤2</span></span><br><span class="line"><span class="comment">// 当preHandle返回true，调用controller中的函数之后，会执行该函数</span></span><br><span class="line"><span class="comment">// 当preHandle返回false，不会执行该函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postHandle</span><span class="params">(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, ModelAndView arg3)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// System.out.println("拦截后...");</span></span><br><span class="line"><span class="comment">// System.out.println("进入到LoginInterceptor的postHandle()方法中");</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 步骤3</span></span><br><span class="line"><span class="comment">// 当preHandle返回true，调用controller中的函数之后，执行完postHandle，会调用该函数</span></span><br><span class="line"><span class="comment">// 当preHandle返回false，不会执行该函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// System.out.println("页面渲染后...");</span></span><br><span class="line"><span class="comment">// System.out.println("进入到LoginInterceptor的afterCompletion()方法中");</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="33-mvc配置文件">3.3. mvc配置文件</span></h2><p>在mvc.xml配置文件中，需要对拦截器进行配置，因为login请求不需要拦截，所以把这个请求排除，这样当前端访问login.jsp页面时，就会显示出登录的界面。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mvc:interceptors</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 拦截以任意字符结尾的路径 ，匹配所有的路径 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!--/**表示拦截所有的url及其子路径  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mvc:mapping</span> <span class="attr">path</span>=<span class="string">"/**"</span> /&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 登录不进行拦截 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mvc:exclude-mapping</span> <span class="attr">path</span>=<span class="string">"/**/*login*"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"com.hz.EQbigdata.interceptor.LoginInterceptor"</span>&gt;</span><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mvc:interceptors</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>页面加载的顺序：<br>前端输入一个网址，相当于发出一个url，比如querywda.jsp。首先拦截器拦截这个url，判断是否合法，如果合法，会交给controller处理，处理完之后才会显示querywda的界面，调用相应的querywda.js。如果不合法，就应该在inception就把这个请求拦截下来，重定向到login，这样querywda的界面也不会加载出来</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;拦截器是SpringMVC中的一个核心应用组件,主要用于处理多个Controller的共性问题.当我们的请求由DispatcherServlet&lt;strong&gt;派发到具体Controller之前&lt;/strong&gt;首先要执行拦截器中一些相关方法,在这些方法中可以对请求进行相应预处理(例如权限检测,参数验证),这些方法可以决定对这个请求进行拦截还是放行。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="SpringMVC" scheme="http://yoursite.com/tags/SpringMVC/"/>
    
      <category term="拦截器" scheme="http://yoursite.com/tags/%E6%8B%A6%E6%88%AA%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>HBase</title>
    <link href="http://yoursite.com/2019/02/28/HBase/"/>
    <id>http://yoursite.com/2019/02/28/HBase/</id>
    <published>2019-02-28T01:03:10.000Z</published>
    <updated>2019-05-14T13:01:42.030Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;在做项目的过程中用到了HBase，遇到了一些问题，当数据过大的时候，向HBase中会出现热点问题。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0">2. 问题描述</a></li><li><a href="#3-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">3. 解决方案</a><ul><li><a href="#%E9%94%99%E8%AF%AF%E7%9A%84%E9%A2%84%E5%88%86%E5%8C%BA">错误的预分区</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-问题描述">2. 问题描述</span></h1><p>&ensp;&ensp;&ensp;&ensp;HBase默认建表时有一个分区（region），这个region的rowkey是没有边界的，即没有startkey和endkey，<strong>hbase的中的数据是按照字典序排序的</strong>，在数据写入时，所有数据都会写入这个默认的region，随着数据量的不断增加，此region已经不能承受不断增长的数据量，当一个region过大（达到hbase.hregion.max.filesize属性中定义的阈值，默认10GB）时，会进行split，分成2个region。在此过程中，会产生两个问题：</p><ul><li>数据往一个region上写,会有写热点问题。</li><li>region split会消耗宝贵的集群I/O资源。<br>哈哈哈哈    </li></ul><h1><span id="3-解决方案">3. 解决方案</span></h1><p>&ensp;&ensp;&ensp;&ensp;基于此我们可以控制在建表的时候，创建多个空region，并确定每个region的起始和终止rowky，这样只要我们的rowkey设计能均匀的命中各个region，就不会存在写热点问题。自然split的几率也会大大降低。当然随着数据量的不断增长，该split的还是要进行split。像这样预先创建hbase表分区的方式，称之为预分区，下面给出一种预分区的实现方式:<br>&ensp;&ensp;&ensp;&ensp;解决这个问题，关键是要设计出可以让数据分布均匀的rowkey，与关系型数据库一样,rowkey是用来检索记录的主键。访问hbase table中的行，rowkey 可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，rowkey保存为字节数组，存储时，数据按照rowkey的字典序排序存储。<br>预分区的时候首先需要指定按什么来划分rowkey，<br>&ensp;&ensp;&ensp;&ensp;设计的rowkey应该由regionNo+messageId组成。设计rowkey方式：随机数+messageId，如果想让最近的数据快速get到，可以将时间戳加上，原先我们设计的行键是数据产生的时间，格式为2018-01-21 12:23:06,没有设置预分区，这样数据就会出现热点问题。     </p><h2><span id="错误的预分区">错误的预分区</span></h2><p>&ensp;&ensp;&ensp;&ensp;后来采用预分区的方式，按照秒进行预分区，splitKeys={“01|”,”02|”,…”59|”},在设计行键的时候在原先的时间上再添加当前的秒数，例如原先的行键是2018-01-21 12:23:06，现在的行键是062018-01-21 12:23:06，这样在存储的时候行键的前2个字符06，我这里的region是01|到59|开头的，因为hbase的数据是字典序排序的,行键开头为06，06大于05，并且06后面字符的ASCII码小于|，则当前这条数据就会保存到05|~06|这个region里。rowkey组成：秒数+messageId，因为我的messageId都是字母+数字，“|”的ASCII值大于字母、数字。<br>&ensp;&ensp;&ensp;&ensp;下图展示了HBase的分区情况。第一个分区没有startKey，endKey为01|，表示比01|小的行键都存储在这里，那就是以00和01开头的行键都存储在这里，最后一个分区的startKey是59|，表示比59大的存储在这个分区里，但是一分钟内的秒数没有比59大的，所以request一直是0。这份分区是不对的，<strong>正确的分区是splitKeys={“00|”,”01|”,…”58|”}</strong><br><img src="/2019/02/28/HBase/table.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;需要注意的是，行键分配值按照rowkey的前几个字符进行匹配的，并不是按照数的大小。例如分区是 -10,10-20,20-30,30-40,40-50,50-60,60-70,70-80,80-90,90-，如果插入的数据rowkey是80 60 22这种两位数，肯定会落到某个分区，如果rowkey是100 333 9955 555544 66910 这种大于两位值，都会落在最后一个分区，还是只取rowkey的前两位与startkey/endkey对应？答案是：是按前两位匹配rowkey的。   </p><pre><code class="lang-java">private byte[][] getSplitKeys() {        String[] keys = new String[] { &quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;,                &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot; };        byte[][] splitKeys = new byte[keys.length][];        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序        for (int i = 0; i &lt; keys.length; i++) {            rows.add(Bytes.toBytes(keys[i]));        }        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();        int i=0;        while (rowKeyIter.hasNext()) {            byte[] tempRow = rowKeyIter.next();            rowKeyIter.remove();            splitKeys[i] = tempRow;            i++;        }        return splitKeys;}</code></pre><p>需要注意的是，在上面的代码中用treeset对rowkey进行排序，必须要对rowkey排序，否则在调用admin.createTable(tableDescriptor,splitKeys)的时候会出错。创建表的代码如下:       </p><pre><code class="lang-java">/**     * 创建预分区hbase表     * @param tableName 表名     * @param columnFamily 列簇     * @return     */    @SuppressWarnings(&quot;resource&quot;)    public boolean createTableBySplitKeys(String tableName, List&lt;String&gt; columnFamily) {        try {            if (StringUtils.isBlank(tableName) || columnFamily == null                    || columnFamily.size() &lt; 0) {                log.error(&quot;===Parameters tableName|columnFamily should not be null,Please check!===&quot;);            }            HBaseAdmin admin = new HBaseAdmin(conf);            if (admin.tableExists(tableName)) {                return true;            } else {                HTableDescriptor tableDescriptor = new HTableDescriptor(                        TableName.valueOf(tableName));                for (String cf : columnFamily) {                    tableDescriptor.addFamily(new HColumnDescriptor(cf));                }                byte[][] splitKeys = getSplitKeys();                admin.createTable(tableDescriptor,splitKeys);//指定splitkeys                log.info(&quot;===Create Table &quot; + tableName                        + &quot; Success!columnFamily:&quot; + columnFamily.toString()                        + &quot;===&quot;);            }        } catch (MasterNotRunningException e) {            // TODO Auto-generated catch block            log.error(e);            return false;        } catch (ZooKeeperConnectionException e) {            // TODO Auto-generated catch block            log.error(e);            return false;        } catch (IOException e) {            // TODO Auto-generated catch block            log.error(e);            return false;        }        return true;    }</code></pre><p>&ensp;&ensp;&ensp;&ensp;HBase中出现热点问题带来的影响是：（1）在我们的项目中，原先使用一个分区，等到这个分区容量达到阈值时，这个分区开始split，然后数据来的时候就会向第二个分区写数据，不会向第一个region中写数据，所以在某一时候只能向一个region中写数据，这样写的速度会变慢。（2）在读数据的时候，因为行键设置的时间，连续的时间一般存储在一个region中，所以读数据的时候也是从一个region中读取数据，读取的速度也会变慢。项目原先应对取数据慢的问题解决方案使用HBase的scan函数，设置起始和终止的行键，使用scan查询数据。<br>&ensp;&ensp;&ensp;&ensp;按秒对表进行预分区时，就相当于把数据均匀分布在60个region中，存储一段时间的数据时，会同时向60个region中写入数据，取数据的时候也会同时从60个region中取数据。这样取数据的时候就不能使用起止行键用scan来查询数据了，只能使用getRow来查询数据，但是这样查询的性能也不会很差，因为是从60个region中同时查询数据，使用scan的时候是从1个region中查询数据。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在做项目的过程中用到了HBase，遇到了一些问题，当数据过大的时候，向HBase中会出现热点问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/tags/HBase/"/>
    
      <category term="预分区" scheme="http://yoursite.com/tags/%E9%A2%84%E5%88%86%E5%8C%BA/"/>
    
  </entry>
  
  <entry>
    <title>NLP</title>
    <link href="http://yoursite.com/2019/02/15/NLP/"/>
    <id>http://yoursite.com/2019/02/15/NLP/</id>
    <published>2019-02-15T14:36:12.000Z</published>
    <updated>2019-04-07T02:08:21.030Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-词嵌入">1. 词嵌入</span></h1><p>&ensp;&ensp;&ensp;&ensp;<strong>词向量</strong>就是用来表示词的向量，也可以是词的特征向量或表征。把词映射成向量的技术叫<strong>词嵌入</strong>。<br>前几天看到了一篇<a href="https://mp.weixin.qq.com/s/thUjPlkqpu6H_t92SUDtLg" target="_blank" rel="noopener">讲解Word2Vec的推送</a>，讲的超级好，通俗易懂。</p> <a id="more"></a> <!-- TOC --><ul><li><a href="#1-%E8%AF%8D%E5%B5%8C%E5%85%A5">1. 词嵌入</a><ul><li><a href="#11-%E8%B4%9F%E9%87%87%E6%A0%B7">1.1. 负采样</a></li><li><a href="#12-%E5%B1%82%E5%BA%8Fsoftmax">1.2. 层序Softmax</a></li><li><a href="#13-%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4">1.3. 实现步骤</a></li></ul></li><li><a href="#2-seq2seq">2. Seq2Seq</a><ul><li><a href="#21-%E7%BC%96%E7%A0%81%E5%99%A8">2.1. 编码器</a></li><li><a href="#22-%E8%A7%A3%E7%A0%81%E5%99%A8">2.2. 解码器</a></li><li><a href="#23-%E4%BC%98%E7%BC%BA%E7%82%B9">2.3. 优缺点</a></li></ul></li><li><a href="#3-attention">3. Attention</a></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;一种最简单的词嵌入是one-hot向量，每个词使用非0即1的向量表示。这样构造虽然简单，但不是一个好的选择。因为one-hot词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度，因为任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度很难通过one-hot向量准确表示出来。<br>&ensp;&ensp;&ensp;&ensp;word2vec工具的提出正是为了解决上面的问题。2013年Google团队发表了word2vec工具。它将每个词表示成一个<strong>定长的向量，并使得这些向量可以较好地表达不同词之间的相似和类比关系</strong>。word2vev包含了2个模型：跳字模型(skip-gram)和连续词袋模型(continuous bag of words CBOW)。以及2种高效训练的方法：负采样(negative sampling)和层序softmax(hierarchical softmax)。 </p><p>&ensp;&ensp;&ensp;&ensp;skip-gram是给定一个中心词，计算周围词出现的概率。在skip-gram中每个词被表示成2个d维向量。当它为中心词时向量被表示成$v_i\in\mathbb{R}^d$,当它为背景词时向量被表示成$u_i\in\mathbb{R}^d$。给定中心词生成背景词的条件概率可以通过softmax运算得到：    </p><p><img src="/2019/02/15/NLP/式1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;假设中心词是loves，判断根据loves生成背景词son的概率，分子是loves的中心词向量和son的背景词向量做内积，分母是除去loves的所有词的背景词向量分别和loves的中心词向量做内积，再相加。然后分子/分母得到根据loves生成son的概率。<br>&ensp;&ensp;&ensp;&ensp;假设给定一个长度为$T$的文本序列$w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为$m$时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率。</p><p><img src="/2019/02/15/NLP/式2.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;文本序列是预先给定的，已经有一句话了。首先时间$t=1$，确定一个中心词，然后根据这个中心词计算背景词的概率，把2m个概率相乘。然后t+1，再找下一个中心词，再计算这个中心词生成背景词的概率，再把2m个概率相乘，直到中心词到最后一个。</p><p>&ensp;&ensp;&ensp;&ensp;训练skip-gram模型就是为了得到每个词的中心词向量和背景词向量，每个词所对应的的中心词向量和背景词向量是skip-gram的模型参数。训练中通过最大化似然函数来学习模型参数，即最大化似然估计。训练结束后，我们可以得到字典中所有词的中心词向量和背景词向量。在自然语言处理应用中，一般使用skip-gram的中心词向量作为词的表征向量。<br>&ensp;&ensp;&ensp;&ensp;连续词袋模型基于背景词来生成中心词。比如一个句子the man - his son，根据前后4个词预测中间的词。也是通过最大化似然函数来训练得到字典中每个词的中心词向量和背景词向量。和跳字模型不一样的是，我们一般使用连续词袋模型的背景词向量作为词的表征向量。<br>&ensp;&ensp;&ensp;&ensp;但是skip-gram和CBOW的计算开销都比较大，下面介绍2个近似训练法：负采样和层序softmax.通过这2种方法可以减小训练开销。    </p><h2><span id="11-负采样">1.1. 负采样</span></h2><p>&ensp;&ensp;&ensp;&ensp;跳字模型的核心在于使用softmax运算得到给定中心词$w_c$来生成背景词的概率。<br><img src="/2019/02/15/NLP/式1.png" alt=""></p><p>&ensp;&ensp;&ensp;&ensp;由于softmax运算考虑了背景词可能是词典$\mathcal{V}$中的任一词，在计算损失函数时计算了所有背景词的损失。不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的梯度计算开销可能过大。为了降低该计算复杂度，本节将介绍两种近似训练方法，即负采样（negative sampling）或层序softmax（hierarchical softmax）。</p><p>&ensp;&ensp;&ensp;&ensp;在CBOW模型中，已知词$w$的上下文$Context(w)$,需要预测$w$，因此对于$Context(w)$，词$w$就是一个正样本，其他词就是一个负样本。从所有的负样本中选择一个负样本子集。训练的目标就是增大当上下文为$Context(w)$时，中心词$w$出现的概率，并且同时降低负样本的概率。<br>&ensp;&ensp;&ensp;&ensp;对于一个给定的词$w$，怎么生成这个词的负采样子集$NEG(W)$?<br>词典中的词出现的次数有高有低，对于那些高频词，被选为负样本的概率就比较大，对于那些低频词，被选中负样本的概率就小，本质上就是一个<strong>带权采样问题</strong>。对于一对中心词和背景词，随机采样K个负样本，论文中的建议K=5，负样本采样的概率$P(w)$设为$w$词频与总词频之比的$3/4$次方。<br><img src="/2019/02/15/NLP/负采样.png" alt="负采样">   </p><p>&ensp;&ensp;&ensp;&ensp;在训练中，首先给出所有的句子。对于skip-gram模型负采样，给定一个中心词预测周围的词。对于一个句子，首先把语料分割成(context(w),w)样本，对于每一个中心词，都可以在这个句子中找出这个中心词的背景词（周围词）,并在词典中找出这个中心词的负样本（非背景词），一个中心词的负样本论文建议个数为5，就是对于一个中心词找出5个负样本。对语料进行预处理形成以下数据集：一个样本包括一个中心词，它所对应的n个背景词，m个噪声词（负样本）。每个样本的背景词窗口大小可能不一样，即每个中心词的背景词和噪声词的个数可能不一样。<br>&ensp;&ensp;&ensp;&ensp;<strong>负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关</strong>。</p><h2><span id="12-层序softmax">1.2. 层序Softmax</span></h2><p>&ensp;&ensp;&ensp;&ensp;使用哈夫曼二叉树来存储词典，叶子节点就是字典$\mathcal{V}$中的每个词，非叶子节点就是一些隐藏向量。<br>&ensp;&ensp;&ensp;&ensp;<strong>层序Softmax使用了二叉树，并根据根节点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关</strong>。    </p><h2><span id="13-实现步骤">1.3. 实现步骤</span></h2><p>&ensp;&ensp;&ensp;&ensp;给定一个训练集，首先对数据进行处理，给定一个背景词窗口大小，对于每一个中心词，找到中心词在句子中的背景词、噪声词。这就是训练集，其中一个样本是(第i个中心词，n个背景词，m个噪声词)，使用这些样本作为训练。<br>&ensp;&ensp;&ensp;&ensp;嵌入层不需要自己写，直接使用Embedding来定义，根据嵌入层可以获取一个词的词向量。嵌入层有一个嵌入矩阵，输入是词典的大小（词的个数），输出是词向量的纬度。所以嵌入矩阵的维度为(词典大小，词向量维度)。嵌入层的输入是语料库中的词的索引[0,1,2,3…]，输入一个词的索引i,嵌入层返回权重矩阵的第i行作为它的词向量。<br> &ensp;&ensp;&ensp;&ensp;skip-gram的输入是包含中心词索引向量，背景词和噪声词索引向量。这2个向量先通过嵌入层得到词向量，然后输出中心词向量与背景词向量噪声词向量的内积作为中心词的词向量。   </p><h2><span id="发展">发展</span></h2><p> &ensp;&ensp;&ensp;&ensp;在word2vec提出来之后，之后又有了新的发展。在2014年Stanford团队提出了GloVe，在2017年Facebook提出了fastText。其中GloVe提出两个词共现的概率，用词向量表达共现词频的对数。fastText提出每个词都是由子词提出来的，把中心词向量表示成所有子词的词向量的和。</p><h1><span id="2-seq2seq">2. Seq2Seq</span></h1><p>&ensp;&ensp;&ensp;&ensp;原先都是给定一个不定长的序列，输出一个定长的序列。比如给定一个不定长的序列预测下一个词。在自然语言处理中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出也可以是一段不定长的法语文本序列。当输入和输出都是不定长序列时，我们可以使用编码器-解码器（encoder-decoder）或者seq2seq模型。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。    </p><p>&ensp;&ensp;&ensp;&ensp;图10.8描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，我们可以在每个句子后附上特殊符号“\<eos>”（end of sequence）以表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“\<eos>”。图10.8中使用了<strong>编码器在最终时间步的隐藏状态作为输入句子的表征或编码信息</strong>。解码器在各个时间步中使用<strong>输入句子的编码信息c</strong>和<strong>上个时间步的输出</strong>以及<strong>隐藏状态</strong>作为输入。 我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号“\<eos>”。 需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号“\<bos>”（beginning of sequence）。其中每个词用$x_t$向量来表示，$x_t$可以通过预训练好的词向量来获取。</bos></eos></eos></eos></p><p><img src="/2019/02/15/NLP/seq2seq.png" alt=""></p><h2><span id="21-编码器">2.1. 编码器</span></h2><p>&ensp;&ensp;&ensp;&ensp;编码器的作用是把一个不定长的输入序列变成一个定长的背景变量$c$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。背景变量$c=q(h_1,…h_T)$,编码器通过自定义函数$q$将各个时间步的隐藏状态变换成背景变量。   </p><h2><span id="22-解码器">2.2. 解码器</span></h2><p>&ensp;&ensp;&ensp;&ensp;编码器输出的背景变量$c$编码了整个输入序列$x_1,…x_T$的信息。给定训练样板中的输出序列$y_1,…y_{T<code>}$，对每个时间步$t</code>$，解码器输出$y_{t<code>}$的条件概率将基于之前的输出序列$y_1,...y_{t</code>-1}$和背景变量$c$，即$P(y_t<code>|y_1,...y_{t</code>-1},c)$。为此，我们可以使用另一个循环神经网络作为解码器。在时间步$t<code>$，解码器根据背景变量$c$，上一时间步的输出$y_{t</code>-1}$和上一时间步的隐藏变量$s_{t<code>-1}$来生成当前时间步的隐藏变量$s_{t</code>}$。有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算生成当前时间步的输出$y_{t<code>}$。即计算的顺序是：先根据背景变量$c$，上一时间步的输出$y_{t</code>-1}$和上一时间步的隐藏变量$s_{t<code>-1}$来生成当前时间步的隐藏变量$s_{t</code>}$，然后再根据当前时间步的隐藏状态$s_{t<code>}$生成当前时间步的输出$y_{t</code>}$。<br>3个小trick：</p><ul><li>解码器什么时候停止？当预测的下一个词是\<eos>时停止。</eos></li><li>对于解码器，当生成第一个隐藏状态$s_1$时，需要给定$s_0,y_0和c$,其中$y_0=<bos>对应的词向量$</bos></li><li>对于编码器，生成第一个隐藏状态$h_1$时，需要给定当前的输入$x_1和上一时刻的隐藏状态h_0$，$h_0$可以初始化为全零的向量。对于编码器，$s_0$也可以初始化为全零向量。也可以初始化为$s_0=tanh(W\overleftarrow{h_1})$，编码器从右向左输入，最后得到第一个词的隐藏向量，然后用来初始化解码器的$s_0$。<h2><span id="23-优缺点">2.3. 优缺点</span></h2>&ensp;&ensp;&ensp;&ensp;encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量C。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，还有就是先输入的内容携带的信息会被后输入的信息稀释掉，或者说，被覆盖了。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息， 那么解码的准确度自然也就要打个折扣了</li></ul><h1><span id="3-attention">3. Attention</span></h1><p>&ensp;&ensp;&ensp;&ensp;在上面的编码器-解码器中，从编码器传到解码器的背景变量$c$是不变的。就是说解码器在翻译第一个词和第二个词是c是不变的。但是实际情况中，比如英语they are watching。翻译成法语是：IIs regardent。比如在翻译IIS时，和they are更相关，在翻译regardent和watching更相关，所以希望把背景变量$c$设置成一个变化的值。当翻译IIS时，对编码器的隐藏变量$h_1,h_2$更看重，当翻译regardent对隐藏变量$h_3$更看重，所以就需要在解码器中，在不同时间步时，对编码器的隐藏变量$h_1,h_2,h_3$分配不同的权重，加权平均得到背景变量$c$。<br>&ensp;&ensp;&ensp;&ensp;原先解码器隐藏层变量的计算是上一时刻的输出$y_{t<code>-1}$，上一时刻的隐藏状态$s_{t</code>-1}$以及背景变量$c$，在加入attention机制后，这里的背景变量变成了$c_{t`}$，每一步的背景变量都不一样。</p><p><img src="/2019/02/15/NLP/st.png" alt=""><br><img src="/2019/02/15/NLP/attention.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;下面看一下$c_{t<code>}$是怎么设计的。 就是编码器的不同时刻的隐藏状态的加权平均。只是这里的权重$\alpha_{t</code>t}$在每一个时刻是一个变化的值。  注意这里的$t<code>$是输出(解码器)的时间戳，$t$是输入(编码器)的时间戳。首先我们先固定$t</code>$，下面的式子中$t<code>$是不变的。在计算$c_{t</code>}$时，变化$t从1到T$，遍历所有的$h_t$，然后给定一个$h_t$，怎么求$h_t$对应的权重$\alpha_{t`t}$。</p><p><img src="/2019/02/15/NLP/ct.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;下面我们看一下$\alpha_{t<code>t}$是怎么来表示。加权平均就要使所有的权值加起来为1，所以用到softmax运算。softmax中的每一个值是$e$,这个是怎么计算的.$e_{t</code>t}$通过解码器上一时刻的隐藏变量$s_{t`-1}$和当前编码器的隐藏变量$h_t$计算得到。</p><p><img src="/2019/02/15/NLP/alpha.png" alt=""><br><img src="/2019/02/15/NLP/e.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;注意力机制对函数$a$设计有很多。下面是一种设计方法。首先一定要有$s_{t<code>-1}$和$h_t$。然后引入了3个模型参数$v^T,W_s,W_h$,这3个参数通过训练得到。对$s_{t</code>-1}$做一个projection，对$h_t$做一个projection，使得projection之后的向量长度相等，这样就可以加在一起，然后使用tanh激活函数，这时的向量长度还是projection之后的长度，但是我们希望$e_{t`t}$是一个标量，那就再引入向量$v^T$，和右边的向量做一个点乘得到一个标量。其实注意力可以通过多层感知机（全连接层）得到。  </p><p><img src="/2019/02/15/NLP/a.png" alt=""><br>下面介绍计算解码器的隐藏变量时的函数$g$是什么？g可以看到是一个GRU单元。</p><p><img src="/2019/02/15/NLP/g.png" alt=""><br>这里总结一下模型的参数都有哪些：编码器中的W和b，上式中解码器中的W和b，还有计算attention中的$v^T,W_s,W_h$。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-词嵌入&quot;&gt;&lt;a href=&quot;#1-词嵌入&quot; class=&quot;headerlink&quot; title=&quot;1. 词嵌入&quot;&gt;&lt;/a&gt;1. 词嵌入&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;&lt;strong&gt;词向量&lt;/strong&gt;就是用来表示词的向量，也可以是词的特征向量或表征。把词映射成向量的技术叫&lt;strong&gt;词嵌入&lt;/strong&gt;。&lt;br&gt;前几天看到了一篇&lt;a href=&quot;https://mp.weixin.qq.com/s/thUjPlkqpu6H_t92SUDtLg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;讲解Word2Vec的推送&lt;/a&gt;，讲的超级好，通俗易懂。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RNN</title>
    <link href="http://yoursite.com/2019/02/12/RNN/"/>
    <id>http://yoursite.com/2019/02/12/RNN/</id>
    <published>2019-02-12T12:55:39.000Z</published>
    <updated>2019-02-22T11:46:29.842Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-门控循环单元gru">1. 门控循环单元GRU</span></h1><p>&ensp;&ensp;&ensp;&ensp;梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83gru">1. 门控循环单元GRU</a><ul><li><a href="#11-%E5%80%99%E9%80%89%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81">1.1. 候选隐藏状态</a></li><li><a href="#12-%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81">1.2. 隐藏状态</a></li><li><a href="#13-%E6%80%BB%E7%BB%93">1.3. 总结</a></li></ul></li><li><a href="#2-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86lstm">2. 长短期记忆LSTM</a></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;门控循环神经网络的提出(2014年提出)正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。其中，门控循环单元是一种常用的门控循环神经网络。门控循环神经单元引入了重置门和更新门的概念，从而修改了循环神经网络中隐藏状态的计算方式。重置门和更新门的激活函数是sigmoid函数，可以将元素的值变换到0到1之间，因为重置门$R_t$和更新们$Z_t$中每个元素的值域都是[0,1]。   </p><h2><span id="11-候选隐藏状态">1.1. 候选隐藏状态</span></h2><p>&ensp;&ensp;&ensp;&ensp;候选隐藏状态用来辅助后面的隐藏状态计算。重置门为0，意味着重置对应隐藏状态元素为0，即丢弃上一时间步的隐藏状态。如果重置门为1，表示保留上一时间步的隐藏状态。重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃和预测无关的历史信息。   </p><h2><span id="12-隐藏状态">1.2. 隐藏状态</span></h2><p>&ensp;&ensp;&ensp;&ensp;时间步$t$的隐藏状态$H_t$的计算使用当前时间步的更新们$Z_t$来对上一时间步的隐藏状态$H_t-1$和当前时间步的候选隐藏状态$\tilde{H}_{t}$做组合。<br>&ensp;&ensp;&ensp;&ensp;更新门控制了包含当前时间步信息的候选隐藏状态如何流入隐藏状态。   </p><script type="math/tex; mode=display">X_t,H_{t-1},R_t(控制H_{t-1})----->\tilde{H}_t,H_{t-1},Z_t(控制\tilde{H}_t,H_{t-1})----->H_t</script><p>&ensp;&ensp;&ensp;&ensp;假设更新门$Z_t$在t时刻为1，那么时间步$t$的输入信息没有流入当前时间步的隐藏状态$H_t$，实际上，上一时间步的隐藏状态$H_{t-1}$保存并传递到当前时间步$t$。<strong>这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</strong>    </p><h2><span id="13-总结">1.3. 总结</span></h2><ul><li>重置门有助于捕捉时间序列里短期的依赖关系。重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃和预测无关的历史信息。</li><li>更新门有助于捕捉时间序列里长期的依赖关系。更新门$Z_t$在t时刻为1，那么时间步$t$的输入信息没有流入当前时间步的隐藏状态$H_t$，实际上，上一时间步的隐藏状态$H_{t-1}$保存并传递到当前时间步$t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。    </li><li>为什么叫GRU也叫做循环神经网络，因为门控循环单元中上一时间步的隐藏状态会传到当前时间步，体现了循环的性质。    <h1><span id="2-长短期记忆lstm">2. 长短期记忆LSTM</span></h1>&ensp;&ensp;&ensp;&ensp;另外一种常用的门控循环神经网络是LSTM(1997年提出)，比门控循环单元的结构稍微复杂一些。<br>&ensp;&ensp;&ensp;&ensp;GRU中的术语是：重置门，更新门，候选隐藏状态，隐藏状态。<br>&ensp;&ensp;&ensp;&ensp;LSTM的术语是：输入门，遗忘门，输出门，候选记忆细胞(与候选隐藏状态形状相同)，记忆细胞(与隐藏状态形状相同)，隐藏状态。<br>&ensp;&ensp;&ensp;&ensp;输入门$I_t$,遗忘门$F_t$,输出门$O_t$,候选记忆细胞$\tilde{C}_t$,记忆细胞$C_t$。<br>&ensp;&ensp;&ensp;&ensp;其中输入门$I_t$,遗忘门$F_t$,输出门$O_t$,候选记忆细胞$\tilde{C}_t$取决于$X_t和H_{t-1}$,记忆细胞$C_t$取决于$遗忘门F_t,C_{t-1},输入门I_t,\tilde{C}_t$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-门控循环单元GRU&quot;&gt;&lt;a href=&quot;#1-门控循环单元GRU&quot; class=&quot;headerlink&quot; title=&quot;1. 门控循环单元GRU&quot;&gt;&lt;/a&gt;1. 门控循环单元GRU&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon" scheme="http://yoursite.com/tags/gluon/"/>
    
      <category term="RNN" scheme="http://yoursite.com/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>lovely dog</title>
    <link href="http://yoursite.com/2019/02/12/lovely-dog/"/>
    <id>http://yoursite.com/2019/02/12/lovely-dog/</id>
    <published>2019-02-12T08:37:27.000Z</published>
    <updated>2019-02-12T17:12:03.384Z</updated>
    
    <content type="html"><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;记得第一次养小狗狗，还是我上小学的时候，是一个白色的小狗，超级好看，我用香皂给它洗澡，后来家里种小麦的时候我爸用农药拌小麦，小狗吃了一些，就死了(((m-__-)m))。从那之后家里也没养过狗。<br> <a id="more"></a><br> &ensp;&ensp;&ensp;&ensp;前几天去我奶奶家，她有一个小卖部，年级大了，不敢开车，让我弟弟去帮她进货，进货的时候要先把需要进什么货都写下来，我奶奶不识字，我就去帮她写。到她家发现她家的狗不见了，奶奶说前几天小狗和她去曹庄的时候丢了，我问小狗自己找不到家吗？因为小狗一直拴着，没有出去过，所以不记得家。当时觉得挺可惜的，那么好的一个小狗。今天我和我妈去伯党乡洗澡，路上看见一个小花狗，在大路上一直看来往的人，我妈就说这是不是你奶奶家的狗？我当时想，我奶奶家的狗是在曹庄丢的，应该不会在这，说应该不是吧，就走了。走了之后越来越感觉好像就是这只小狗，但当时我妈开车已经走了好远了，也不好意思回去找。突然我妈说：你看，这只小狗追着我们呢。一看，哇！真的是我奶奶家的狗，竟然认识我们，因为我和我妈都经常不在家，小狗也没见过我们几次，竟然记得我们，就跟着我们的车跑。我们停下车，想把它抱到车上，但是它老动，抱不上去，就让它在后面跟着我们的车跑。我们到洗澡的地方，把车停在院子里，考虑要不要把小狗狗锁在车里，怕再次跑丢，当时想的是我们的车就在这，它应该不会走吧。然后我们就去洗澡了，小狗狗在院子里。等我们洗完的时候，叫了好几声都没发现那个小狗，当时还挺自责的，为啥不把它锁在车里呢，小狗又丢了。我姥姥就在洗澡的附近，洗完澡和我妈又开车去我姥姥家了，路上看见小狗就觉得是我家的狗，到我姥姥家也没有找到。中午在我姥姥家吃了饭，待了一会就回家。路上我妈说你看看路上有没有小狗。又走到上午发现小狗的那条路上，不知道小狗从那出来的，看见我们又跟在我们的车后，看见小狗当时好高兴，心想，这只小狗好聪明，找不到我们又回到原来的地方，刚走一会就看见我奶奶开个车在找她家的小狗，我就叫我奶奶，说狗找到啦。小狗看见我奶奶一个劲的往她身上蹭，我奶奶看见小狗，眼睛都红了，对着小狗说：这几天去哪了，也不知道回家。我奶奶把小狗抱上车回家了。<br> &ensp;&ensp;&ensp;&ensp;看到小狗又找到了，真的好高兴。以前看过《忠犬八公的故事》，电影中的小狗的主人因病去世了，但小狗狗每次到下班的点都去地铁站等着主人，但是主人再也不会从出站口出来了。小狗真的好有灵性，我们养只小狗可能觉得好玩，看家，小狗只是我们生活的一部分，但是我们却是它生活的全部。真的不敢想我奶奶家的小狗如果没有找到，又找不到回家的路该怎么办，在外面吃啥睡哪，万幸小狗狗找到了，以后不要再乱跑了，在家陪着奶奶吧。<br>  <img src="/2019/02/12/lovely-dog/dog3.jpg" alt=""><br> <img src="/2019/02/12/lovely-dog/dog1.jpg" alt=""><br> <img src="/2019/02/12/lovely-dog/dog2.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;记得第一次养小狗狗，还是我上小学的时候，是一个白色的小狗，超级好看，我用香皂给它洗澡，后来家里种小麦的时候我爸用农药拌小麦，小狗吃了一些，就死了(((m-__-)m))。从那之后家里也没养过狗。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>gluon_卷积</title>
    <link href="http://yoursite.com/2019/02/05/gluon-%E5%8D%B7%E7%A7%AF/"/>
    <id>http://yoursite.com/2019/02/05/gluon-卷积/</id>
    <published>2019-02-05T07:44:27.000Z</published>
    <updated>2019-02-10T12:53:21.761Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-卷积">1. 卷积</span></h1><p>卷积操作需要有1一个数组和一个卷积核，假设卷积核的形状为pxq，代表卷积核的高和宽。二维卷积层的输入输出用4维表示，格式为(样本，通道，高，宽)<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E5%8D%B7%E7%A7%AF">1. 卷积</a><ul><li><a href="#11-%E5%A1%AB%E5%85%85">1.1. 填充</a></li><li><a href="#12-%E6%AD%A5%E5%B9%85">1.2. 步幅</a></li><li><a href="#13-%E5%B0%8F%E7%BB%93">1.3. 小结</a></li><li><a href="#14-%E9%80%9A%E9%81%93channel">1.4. 通道channel</a></li><li><a href="#15-%E6%B1%A0%E5%8C%96%E5%B1%82">1.5. 池化层</a></li></ul></li><li><a href="#2-cnn%E4%BA%94%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B">2. CNN五大经典模型</a><ul><li><a href="#21-lenet">2.1. LeNet</a></li><li><a href="#22-alexnet">2.2. AlexNet</a></li><li><a href="#23-googlenet">2.3. GoogleNet</a></li><li><a href="#24-vgg">2.4. VGG</a></li><li><a href="#25-nin">2.5. NiN</a></li><li><a href="#26-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82">2.6. 批量归一化层</a></li><li><a href="#27-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet">2.7. 残差网络ResNet</a></li></ul></li></ul><!-- /TOC --><p>假设输入的形状为$n_h,n_w$,卷积核的形状为$k_hxk_w$，那么输出的形状为  </p><script type="math/tex; mode=display">(n_h-k_h+1)*(n_w-k_w+1)</script><p>卷积层的输出形状由输入形状和卷积核窗口形状决定，下面介绍卷积层的两个超参数，填充和步幅。  </p><h2><span id="11-填充">1.1. 填充</span></h2><p>填充通常在输入的高和宽填充0元素，如果在高的<strong>两侧一共</strong>填充$p_h$行，在宽的<strong>两侧一共</strong>填充$p_w$列，那么输出形状为</p><script type="math/tex; mode=display">(n_h-k_h+p_h+1)*(n_w-k_w+p_w+1)</script><h2><span id="12-步幅">1.2. 步幅</span></h2><p>步幅表示卷积核一次移动的个数，当高的步幅为$s_h$,宽的步幅为$s_w$，输出形状为</p><script type="math/tex; mode=display">\left\lfloor(n_h-k_h+p_h+s_h)/s_h\right\rfloor*\left\lfloor(n_w-k_w+p_w+s_w)/s_w\right\rfloor</script><h2><span id="13-小结">1.3. 小结</span></h2><ul><li><strong>填充可以增加输出的高和宽，常用来使输出与输入具有相同的高和宽</strong></li><li><strong>步幅可以减小输出的高和宽，使得输出的高和宽为输入的$1/n$</strong>  <h2><span id="14-通道channel">1.4. 通道channel</span></h2>通道(channel)：每个卷积层中卷积核的数量。<a href="https://blog.csdn.net/sscc_learning/article/details/79814146" target="_blank" rel="noopener">这篇文章</a>关于channel讲的很好<br>下面X(x_in,h,w)<br>K(k_out,k_in,h,w)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = nd.random.uniform(shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = nd.random.uniform(shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="15-池化层">1.5. 池化层</span></h2><p>pooling层(池化层)的输入一般是上一个卷积层，主要有以下2个作用：  </p><ol><li>保留主要的特征，同时减少下一层的参数和计算量，防止过拟合</li><li>保持某种不变性，包括平移，旋转，常用的平均池化和最大池化<br><strong>池化层的输出通道数和输入通道数相同</strong>     <h1><span id="2-cnn五大经典模型">2. CNN五大经典模型</span></h1></li><li>Lenet：1986年</li><li>Alexnet：2012年</li><li>GoogleNet：2014年</li><li>VGG：2014年</li><li>Deep Residual Learning：2015年<h2><span id="21-lenet">2.1. LeNet</span></h2>LeNet交替使用卷积层和最大池化层后接全连接层进行图像分类。网络结构如下所示<br><img src="/2019/02/05/gluon-卷积/LeNet.png" alt="LeNet"> <h2><span id="22-alexnet">2.2. AlexNet</span></h2>2012年，ImageNet比赛冠军的model—AlexNet，以第一作者alex命名。这个model的意义比后面的那些model都大很多。首先它证明了CNN在复杂模型下的有效性，然后GPU实现使得训练在可接受的时间范围内得到结果，让CNN和GPU都火了一把。<br>AlexNet包含8层变换，其中5层卷积和2层全连接层隐藏层，1个全连接输出层。<br>AlexNet将sigmoid激活函数改成了简单的ReLu激活函数。一方面，ReLu激活函数更简单，例如它没有sigmoid激活函数中的求幂运算。另一方面，ReLu激活函数在不同的参数初始化方法下使得模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度为0，从而造成反向传播无法继续更新部分模型参数；而ReLu激活函数在正区间的梯度恒为1.因为，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。<h2><span id="23-googlenet">2.3. GoogleNet</span></h2>2014年的ImageNet图像s识别挑战赛的冠军。<br>GoogleNet中的基础卷积块叫做Inception块。<br><img src="/2019/02/05/gluon-卷积/Inception.png" alt="">  </li></ol><h2><span id="24-vgg">2.4. VGG</span></h2><p><strong>VGG卷积块</strong>的组成规律是：连续使用数个相同的填充为1，窗口形状为3x3的卷积层后接一个步幅为2，窗口形状为2x2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。<br>VGG网络=VGG卷积块+n个全连接层<br>VGG卷积块=n个相同的卷积层+1个最大池化层   </p><h2><span id="25-nin">2.5. NiN</span></h2><p>前面介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以卷积层构成的模块充分抽取空间特征，再以全连接层构成的模块来输出分类结果。其中AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节介绍网络中的网络（NiN），即串联多个由卷积层和全连接层构成的 小网络来构建一个深层网络。<br><img src="/2019/02/05/gluon-卷积/NiN.png" alt=""><br>解决深度为<br><strong>全连接层可以由1x1卷积层充当</strong><br>NiN块是NiN中的基本块。它由一个卷积层加两个充当全连接层的1x1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二个和第三个卷积层的超参数一般是固定的。</p><h2><span id="26-批量归一化层">2.6. 批量归一化层</span></h2><p>标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为0，标准差为1.标准化处理输入数据使各个特征的分布相近：这样往往更容易训练处有效的模型。<br>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化但对深层神经网络来说，即使输入数据已经做了标准化，训练中模型参数的更新依然很容易造成靠近输出层的输出剧烈变化。这种计算数值的不稳定性通常令我们难以训练处有效的深度模型。<br>标准归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，<strong>批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出</strong>，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>BatchNorm主要是让训练收敛更快。</strong><br>对全连接层和卷积层做批量归一化的方法不同。   </p><ul><li>对全连接层做批量归一化<br>权重参数和偏差参数分别为$W和b$，激活函数为$\phi$,批量归一化运算符为$BN$,使用批量归一化的全连接层的输出为   <script type="math/tex; mode=display">\phi(BN(Wx+b))</script></li><li>对卷积层做批量归一化<br>对卷积层来说，批量归一化发生在卷积计算之后，应用激活函数之前。<br>对于前面的模型，我们可以在卷积层或全连接层之后、激活层之前加入批量归一化层，以LeNet为例：</li><li>未加入批量归一化层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(channels=<span class="number">6</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.MaxPool2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">       nn.Conv2D(channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.MaxPool2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">       nn.Dense(<span class="number">120</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.Dense(<span class="number">84</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.Dense(<span class="number">10</span>))</span><br><span class="line">```     </span><br><span class="line">- 加入批量归一化层     </span><br><span class="line">在卷积层或全连接层之后，激活层之前加入批量归一化层</span><br><span class="line">```python</span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2D(<span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Dense(<span class="number">120</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.Dense(<span class="number">84</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="27-残差网络resnet">2.7. 残差网络ResNet</span></h2><p>2015年ImageNet冠军model。<br>深度网络的好处：特征的等级随着网络深度的加深二变高，及其深的深度使得该网络拥有强大的表达能力。<br>但不是网络层数越多，效果就越好。随着网络深度的加深，(1)会出现梯度衰减的问题，在反向传播时，使梯度不断下降直至消失，对于权重的更新会越来越慢，直至不更新。(2)并且较深层网络比较浅的网络有更高的训练误差，称为退化问题。<br>深度残差网络主要思想很简单，就是在标准的前馈卷积网络上，加一个跳跃绕过一些层的连接。每绕过一层就产生一个残差块(residual block)，卷积层预测加输入张量的残差。普通的深度前馈网络难以优化。除了深度，所加层也使得training和validation的错误率增加，即使用上了batch normalization也是如此。残差神经网络由于存在shorcut connections，网络间的数据流通更为顺畅。残差网络结构的解决方案是，增加卷积层输出求和的捷径连接。<br>实验表明，残差网络更容易优化，并且能够通过增加相当的深度来提高准确率。核心是解决了增加深度带来的副作用（退化问题），这样能够通过单纯地增加网络深度，来提高网络性能。   </p><ul><li>网络的深度为什么重要？<br>因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。</li><li>为什么不能简单地增加网络层数？<br>对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。<br>对于该问题的解决方法是正则化初始化和中间的正则化层（Batch Normalization），这样的话可以训练几十层的网络。虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。 作者通过实验：通过浅层网络+ y=x 等同映射构造深层模型，结果深层模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。</li><li>怎么解决退化问题？<br>深度残差网络。如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。但是，如果把网络设计为H(x) = F(x) + x。我们可以转换为学习一个残差函数F(x) = H(x) - x. 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。<br><img src="/2019/02/05/gluon-卷积/ResNet1.png" alt=""><br>二层平原网络我们根据输入$x$,去拟合$H(x)$,$H(x)$是任意一种理想的映射，希望第2层权重输出能够与理想$H(x)$拟合。<br><img src="/2019/02/05/gluon-卷积/ResNet2.png" alt=""><br>为了解决深度神经网络的2个问题，提出残差网络ResNet。<br><img src="/2019/02/05/gluon-卷积/ResNet3.png" alt=""><br>残差是$F(X)$，让$F(x)=0$，这样$H(X)就趋近于x，是一个恒等映射$，输出和输入相等，这样计算增加网络深度，也不会造成训练误差上升（退化问题）。<br><img src="/2019/02/05/gluon-卷积/ResNet4.png" alt=""><br><img src="/2019/02/05/gluon-卷积/ResNet5.png" alt=""><br><img src="/2019/02/05/gluon-卷积/ResNet6.png" alt=""><br>残差网络的基础块是残差块，在残差块中，输入可通过跨层的数据线路更快地向前传播。<br><img src="/2019/02/05/gluon-卷积/ResNet7.png" alt=""></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-卷积&quot;&gt;&lt;a href=&quot;#1-卷积&quot; class=&quot;headerlink&quot; title=&quot;1. 卷积&quot;&gt;&lt;/a&gt;1. 卷积&lt;/h1&gt;&lt;p&gt;卷积操作需要有1一个数组和一个卷积核，假设卷积核的形状为pxq，代表卷积核的高和宽。二维卷积层的输入输出用4维表示，格式为(样本，通道，高，宽)&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon" scheme="http://yoursite.com/tags/gluon/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="卷积" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>再看gluon新收获</title>
    <link href="http://yoursite.com/2019/01/31/%E5%86%8D%E7%9C%8Bgluon%E6%96%B0%E6%94%B6%E8%8E%B7/"/>
    <id>http://yoursite.com/2019/01/31/再看gluon新收获/</id>
    <published>2019-01-31T09:05:01.000Z</published>
    <updated>2019-03-12T04:56:05.529Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-简介">1. 简介</a></li><li><a href="#2-softmax回归">2. Softmax回归</a></li><li><a href="#3-交叉熵损失函数">3. 交叉熵损失函数</a><ul><li><a href="#31-softmax运算步骤">3.1. softmax运算步骤</a></li></ul></li><li><a href="#4-优化算法">4. 优化算法</a><ul><li><a href="#41-梯度下降gd">4.1. 梯度下降GD</a></li><li><a href="#42-随机梯度下降sgd">4.2. 随机梯度下降SGD</a></li><li><a href="#43-小批量随机梯度下降">4.3. 小批量随机梯度下降</a></li><li><a href="#44-学习率">4.4. 学习率</a></li></ul></li><li><a href="#5-batch-size">5. batch size</a><ul><li><a href="#51-总结">5.1. 总结</a></li></ul></li><li><a href="#6-使用gluon定义模型">6. 使用gluon定义模型</a><ul><li><a href="#61-线性回归">6.1. 线性回归</a></li><li><a href="#62-softmax回归">6.2. softmax回归</a></li><li><a href="#63-多层感知机">6.3. 多层感知机</a></li></ul></li><li><a href="#7-过拟合和欠拟合">7. 过拟合和欠拟合</a><ul><li><a href="#71-验证数据集">7.1. 验证数据集</a></li><li><a href="#72-权重衰减">7.2. 权重衰减</a></li><li><a href="#73-丢弃法">7.3. 丢弃法</a></li></ul></li><li><a href="#8-模型参数初始化">8. 模型参数初始化</a><ul><li><a href="#81-随机初始化">8.1. 随机初始化</a></li><li><a href="#82-xavier随机初始化">8.2. Xavier随机初始化</a></li><li><a href="#83-模型参数的延后初始化">8.3. 模型参数的延后初始化</a></li></ul></li><li><a href="#9-gpu计算">9. GPU计算</a></li></ul><!-- /TOC --><h1><span id="2-softmax回归">2. Softmax回归</span></h1><ol><li>Softmax回归是用来分类的，输入的个数表示特征，输出的个数表示类别。</li><li>Softmax运算  <script type="math/tex; mode=display">\hat{y_1},\hat{y_2},\hat{y_3}=softmax(o_1,o_2,o_3)</script>&ensp;&ensp;其中<script type="math/tex; mode=display">\hat{y}_1=\frac{\exp(o_1)}{\sum_{i=1}^3\exp{(o_i)}},\qquad \hat{y}_2=\frac{\exp(o_2)}{\sum_{i=1}^3\exp{(o_i)}},\qquad\hat{y}_3=\frac{\exp(o_3)}{\sum_{i=1}^3\exp{(o_i)}}</script>&ensp;&ensp;&ensp;&ensp;Softmax回归中有Softmax运算才可以使得输出的结果相加为1。如果没有softmax运算，输出结果也是可以用来分类的，例如$y_1=0.1$,$y_2=10$,$y_3=0.1$，最终属于的类别是2。但是如果$y_1=100$,$y_2=10$,$y_3=0.1$，最终属于的类别是1，没有经过softmax运算，会使得输出层的输出值的范围不确定，难以直观判断这些值的意义。<h1><span id="3-交叉熵损失函数">3. 交叉熵损失函数</span></h1>&ensp;&ensp;&ensp;&ensp;分类问题的准确性通过交叉熵损失函数来判定。一个样本真实的分类结果可以用一个向量来表示，其中只有一个是1，其余全为0。第i个样本真实的向量为$\boldsymbol{y}^{(i)}$,预测的向量为$\boldsymbol{\hat{y}^{(i)}}$<br>交叉熵用来评估预测值和真实值之间的差异<script type="math/tex; mode=display">H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\sum_{j=1}^q y_j^{(i)}\log\hat{y}_j^{(i)}</script>&ensp;&ensp;&ensp;&ensp;向量$\boldsymbol{}y^{(i)}$中共有q个元素，其中只有一个元素为1，其余全部为0，于是$H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\log\hat{y}_j^{(i)}$,<strong>交叉熵只关心正确类别的预测概率</strong>，比如样本$i$的真实类别为5，那么只关心预测向量$\hat{y}^{(i)}$中的第5个元素，即样本$i$属于第5个类别的概率。<br>假设训练数据集的样本个数为$n$，交叉熵损失函数定义为<script type="math/tex; mode=display">\ell(\boldsymbol{\Theta})=\frac{1}{n}\sum_{i=1}^nH\left(\boldsymbol{y^{(i)}},\hat{y}^{(i)}\right)</script>&ensp;&ensp;&ensp;&ensp;其中$\Theta$代表模型参数，对于这$n$个样本，每一个样本都求出这个样本的交叉熵。如果是一个样本只属于一个类，那么向量$\boldsymbol{}y^{(i)}$中只有1个为1，其余全为0。交叉熵损失函数可以简写成$\ell(\boldsymbol{\Theta})=-\frac{1}{n}\sum_{i=1}^n\log\hat{y}_j^{(i)}$,若要交叉熵损失函数$\ell(\boldsymbol{\Theta})$最小，就要使$\sum_{i=1}^n\log\hat{y}_j^{(i)}$最大，即最大化$\prod_{i=1}^n\hat{y}_j^{(i)}$，即每个样本属于自己正确类别的联合概率。  <h2><span id="31-softmax运算步骤">3.1. softmax运算步骤</span></h2>&ensp;&ensp;&ensp;&ensp;对于分类问题，输出的结果是$O$，$O$是一个矩阵，其中行数表示样本的个数，列数表示类别的个数，假设有100个样本，5类，$O$是一个100*5的矩阵。通过softmax运算使得一行的和为1，可以直观的看出每个样本属于每个样本的概率大小。<ul><li>首先对矩阵的中每个元素做exp()运算</li><li>计算出每一行的sum()</li><li>然后用一行中的每个元素/该行的sum()  </li></ul></li></ol><p>   <img src="/2019/01/31/再看gluon新收获/softmax运算.png" alt="softmax运算例子"><br>  代码如下<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">  X_exp = X.exp()</span><br><span class="line">  partition = X_exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure></p><h1><span id="4-优化算法">4. 优化算法</span></h1><p>&ensp;&ensp;&ensp;&ensp;优化算法就是用来更新模型参数的一种算法，模型在训练的时候通过反向传播计算梯度，然后更新模型参数，使得模型的损失越来越小，当模型参数不再变化时，训练结束。</p><h2><span id="41-梯度下降gd">4.1. 梯度下降GD</span></h2><p>&ensp;&ensp;&ensp;&ensp;一次迭代中更新一次模型参数，梯度下降在每一次迭代中，使用整个训练数据集来计算梯度，更新一次参数。一个epoch只有一次迭代，下一次epoch再次使用所有的训练数据集更新模型参数。  </p><h2><span id="42-随机梯度下降sgd">4.2. 随机梯度下降SGD</span></h2><p>&ensp;&ensp;&ensp;&ensp;梯度下降每次更新模型参数时都需要遍历所有的data，当数据量太大或者一次无法获取全部数据时，这种方法并不可行。这个问题基本思路是：每次迭代只通过一个随机选取的数据$(x_n,y_n)$来获取梯度，以此对w进行更新，这种方法叫做随机梯度下降。一次迭代使用一个样本更新模型参数，这样一个epoch就需要很多次迭代，每次迭代随机采样一个样本更新模型参数。<br>&ensp;&ensp;&ensp;&ensp;小批量随机梯度下降中，当批量大小为1时是随机梯度下降；当批量大小为训练数据样本数时是梯度下降。当batch size较小时，每次迭代中使用的样本少，导致并行处理和内存使用效率变低。这使得在计算相同数据样本的情况下比使用更大batch size时所花的时间更多，即相同的训练数据，batch size越小，训练时间越长。当批量较大时，每个批量梯度里可能含有更多的冗余信息，为了得到较好的模型参数，批量较大时比批量较小时需要计算的样本数目可能更多，即迭代周期数多。</p><ul><li>相同的训练数据，batch size较小比batch size大时需要的训练时间长。</li><li>相同的训练数据，batch size大时，为了达到和batch size小时一样的训练效果，需要的epoch多。<h2><span id="43-小批量随机梯度下降">4.3. 小批量随机梯度下降</span></h2>小批量随机梯度下降：在每次迭代中，随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，更新模型参数。<br><strong>小批量随机梯度下降的学习率可以在迭代中自我衰减</strong>   <h2><span id="44-学习率">4.4. 学习率</span></h2>&ensp;&ensp;&ensp;&ensp;当学习率很小时，模型参数更新非常慢，训练时间会很长。当学习率很大时，模型可能会越过最优解，导致模型不收敛，训练误差会越来越大，出现nan。当loss出现nan的时候，可以减少学习率。<h1><span id="5-batch-size">5. batch size</span></h1>&ensp;&ensp;&ensp;&ensp;梯度下降是用来寻找模型最佳的模型参数w和b的迭代优化<strong>算法</strong>，通过最小化损失函数(线性回归的平方差误差、softmax的交叉熵损失函数),来寻找w和b。<br>&ensp;&ensp;&ensp;&ensp;只有在数据量比较大的时候，才会用到epoch和batch size和迭代，但这3个词代表什么意思呢？一直不太清楚。  </li><li>epoch：当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个epoch。   </li><li>batch size：当一个完整数据集太大时，不能一次将全部数据输入到神经网络中进行训练，所以需要将完整数据集进行分块，每块样本的个数就是batch size。batch size是为了在内存效率和内存容量之间寻找最佳平衡。</li><li><p>迭代：就是以batch size向神经网络中输入样本，将完整数据集输入到神经网络中所需的次数，即完成一次epoch的次数。迭代数=batch的个数。比如完整数据集2000个样本，每个batch有200个样本，那么共有10个batch，完成一个epoch需要10次迭代。<br>在读取数据的时候传入一个参数batch_size,这个函数返回的X和y分别是含有batch_size个样本的特征和标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">     print(X, y)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;&ensp;&ensp;在进行小批量随机梯度算法中，一个batch size更新一次梯度，如果完整训练集中有2000个样本，一个batch有200个样本，那么一次epoch中更新10次模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">   <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">       <span class="comment"># 这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</span></span><br><span class="line">       param[:] = param - lr * param.grad / batch_size</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期epoch中，会使用所有的训练样本一次</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="comment">#每次读取batch_size个样本的特征和标签，用来训练，一个batch更新一次模型参数</span></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用batch size个样本来更新模型参数w和b</span></span><br><span class="line">    <span class="comment">#一个epoch之后，使用更新后的w和b来计算误差。传入的参数是一个list，里面有所有样本的预测值和真实值，返回的train_l也是一个list，包含每个样本的真实值和预测值的误差。print中输出的是一个标量：train_l.mean().asnumpy()，对于所有的样本的误差求一个平均值输出</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="51-总结">5.1. 总结</span></h2><p> CIFAR10 数据集有 50000 张训练图片，10000 张测试图片。现在选择 Batch Size = 500 对模型进行训练.  </p><ul><li>每个epoch要训练的图片数量：50000</li><li>训练集中具有的batch个数：50000/500=100</li><li>每次epoch需要的batch个数：100</li><li>每次epoch需要的迭代(iteration)个数：100 </li><li>每次epoch中更新模型参数的次数：100</li><li>如果有10个epoch，模型参数更新的次数为：100*10=1000</li><li>一次epoch使用的是全部的训练集50000中图片，下一次epoch中使用的还是这50000张图片，但是对模型参数的权重更新值却是不一样的，因为不同epoch的的模型参数不一样，模型训练的次数越多，损失函数越小，越接近谷底。</li><li>适当增加batch size的优点：<br>（1）提高内存利用率<br>（2）一次epoch的迭代次数减少，相同数据量的处理速度更快，但是达到相同精度所需的epoch越多<br>（3）梯度下降方向准确度增加，训练震荡越小</li><li>减少batch size的缺点<br>（1）小的batch size引入的随机性越大，难以达到收敛<h1><span id="6-使用gluon定义模型">6. 使用gluon定义模型</span></h1><strong>在gluon中无须指定每一层输入的形状，例如线性回归的输入个数，当模型得到数据时，例如执行后面的net(X)时，模型将自动推断出每一层的输入个数</strong><h2><span id="61-线性回归">6.1. 线性回归</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先导入nn模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#导入初始化模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="comment">#导入损失函数模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line"><span class="comment">#先定义一个模型变量net,sequential可以看做是串联各个层的容器，在构造模型时，向该容器依次添加层</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="comment">#全连接层是Dense(),定义全连接层的输出层个数为1</span></span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数：w和b,w初始化为均值为0，标准差为0.01的正太分布，b默认初始化为0</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#平方差损失</span></span><br><span class="line">loss = gloss.L2Loss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD,该优化算法将用来更新通过add添加的层所包含的全部参数</span></span><br><span class="line">tariner = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.03</span>&#125;)</span><br><span class="line"><span class="comment">#在训练模型时，调用Trainer实例的step()函数来更新模型参数w和b</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,num_epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autogard.record():</span><br><span class="line">            <span class="comment">#计算预测值和真实值的误差，l是一个长度为batch_size的数组</span></span><br><span class="line">            l=loss(net(X),y)</span><br><span class="line">        <span class="comment">#因为上面l是一个数组，实际是执行l.sum().backward()，把l变成标量</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#更新w和b，损失函数对w和b求梯度，w=w-r*Δ(l/w)/batch_size,</span></span><br><span class="line">        tariner.step(batch_size)</span><br><span class="line">    <span class="comment">#一个epoch结束，输入所有的训练数据集，更新完w和b，使用更新后的w和b，对所有的数据进行预测，计算预测值和真实值的误差l，这个l是一个len(所有样本)的数组，每个元素表示一个样本的预测值和真实值的误差，print对l求均值l.mean()变成标量</span></span><br><span class="line">    l = loss(net(features),label)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.mean().asnumpy()))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="62-softmax回归">6.2. softmax回归</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(Dense(<span class="number">10</span>))<span class="comment">#输出层有10个神经元，10个类别</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#定义交叉熵损失函数</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD</span></span><br><span class="line">trainer=gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.1</span>&#125;)</span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat,y).sum</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        y=y.astype(<span class="string">'float32'</span>)</span><br><span class="line">        train_l_sum+=l.asscalar()</span><br><span class="line">        train_acc_sum+=(y_hat.argmax(axis=<span class="number">1</span>)=y).sum().acscalar()</span><br><span class="line">        n+=y.size</span><br><span class="line">    test_acc=evaluate_accuracy(test_iter,net)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br></pre></td></tr></table></figure><h2><span id="63-多层感知机">6.3. 多层感知机</span></h2><p>输入层、隐藏层256个节点，输出层10个节点,relu激活函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line">loss = gluon.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.01</span>&#125;)</span><br><span class="line">batch_size=<span class="number">256</span></span><br></pre></td></tr></table></figure></p><h1><span id="7-过拟合和欠拟合">7. 过拟合和欠拟合</span></h1><h2><span id="71-验证数据集">7.1. 验证数据集</span></h2><p>测试数据集只能在所有超参数和模型参数都选定后使用一次。不可以使用测试数据集选择模型参数。所以需要验证集用来选择模型，验证集不参会模型训练。</p><h2><span id="72-权重衰减">7.2. 权重衰减</span></h2><p>权重衰减等于L2范数正则化，用来减少过拟合</p><h2><span id="73-丢弃法">7.3. 丢弃法</span></h2><p>深度学习模型常常使用丢弃法(dropout)来应对过拟合。在训练过程中，对<strong>隐藏层</strong>使用丢弃法，这样隐藏层中的某些神经元将会为0，即被丢弃。下图是一个多层感知机，隐藏层有5个神经元。<br><img src="/2019/01/31/再看gluon新收获/多层感知机.png" alt="多层感知机"><br>其中  </p><script type="math/tex; mode=display">h_i=\phi(x_1w_{1i}+x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i)</script><p>隐藏层计算的结果$h_i$将以$p$的概率被丢弃，即$h_i=0$,丢弃概率$0&lt;=p&lt;=1$。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1$…$h_5$中的任一个都有可能被清零，输出层的计算无法过度依赖隐藏层$h_1$…$h_5$中的任一个，从而在训练模型时起到正则化的作用，用来应对过拟合。<strong>在测试模型时，为了拿到更加确定的结果，一般不使用丢弃法。</strong><br>假设$h_2=0,h_5=0$,使用丢弃法之后的模型为<br><img src="/2019/01/31/再看gluon新收获/丢弃感知机.png" alt="丢弃后的多层感知机"><br><strong>代码实现</strong><br>在Gluon中，只需要在全连接层后面添加Dropout层并指定丢弃概率。在训练模型时，Dropout将以指定的丢失概率随机丢弃上一层的输出元素；在测试模型时，Dropout不起作用。<br><strong>一般在靠近输入层的丢弃率较小</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">        nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure></p><h1><span id="8-模型参数初始化">8. 模型参数初始化</span></h1><h2><span id="81-随机初始化">8.1. 随机初始化</span></h2><p>在Mxnet中，随机初始化通过net.initialize(init.Normal(sigma=0.01))对模型的权重参数w采用正太分布的随机初始化。如果不指定初始化方法，如net.initialize()，默认的初始化方法：权重参数w每个元素随机采样于-0.07到0.07之间的均匀分布，偏差b为0。</p><h2><span id="82-xavier随机初始化">8.2. Xavier随机初始化</span></h2><p>假设某全连接层的输入个数为$a$，输出个数为$b$,Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a+b}},\sqrt{\frac{6}{a+b}})</script><h2><span id="83-模型参数的延后初始化">8.3. 模型参数的延后初始化</span></h2><p>模型net在调用初始化函数 initialize之后，在做前向计算net(X)之前，权重参数的形状出现了0.<br><img src="/2019/01/31/再看gluon新收获/params.png" alt=""><br>在之前使用gluon创建的全连接层都没有指定输入个数，例如使用感知机net里，创建的隐藏层仅仅指定输出大小为256，当调用initialize函数时，由于隐藏层输入个数依然未知，系统无法知道隐藏层权重参数的形状，只有在当我们将形状为(2,20)的输入X传进网络进行前向计算net(X)时，系统才推断该层的权重参数形状为(256,20)，因此，这时候才真正开始初始化参数.     </p><h1><span id="9-gpu计算">9. GPU计算</span></h1><p>使用GPU进行计算，通过ctx指定，NDArray存在内存上，在创建NDArray时可以通过指定ctx在指定的gpu上创建数组<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],ctx=mx.gpu())</span><br><span class="line">b=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>),ctx=mx.gpu(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>同NDArray类似，Gluon的模型也可以在初始化时通过ctx参数指定设备，下面的代码将模型参数初始化在显存上。当输入x是显存上的NDArray时，gluon会在同一块显卡的显存上计算结果。<br><strong>mxnet要求计算的所有输入数据都在内存或同一块显卡的显存上</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line">net.initialize(ctx=mx.gpu())</span><br><span class="line">x=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">8</span>))</span><br><span class="line">y=net(x)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon" scheme="http://yoursite.com/tags/gluon/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
