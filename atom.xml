<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Echo&#39;s blog</title>
  
  <subtitle>远方到底有多远</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-05T07:40:31.791Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Echo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>再看gluon新收获</title>
    <link href="http://yoursite.com/2019/01/31/%E5%86%8D%E7%9C%8Bgluon%E6%96%B0%E6%94%B6%E8%8E%B7/"/>
    <id>http://yoursite.com/2019/01/31/再看gluon新收获/</id>
    <published>2019-01-31T09:05:01.000Z</published>
    <updated>2019-02-05T07:40:31.791Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-简介">1. 简介</a></li><li><a href="#2-softmax回归">2. Softmax回归</a></li><li><a href="#3-交叉熵损失函数">3. 交叉熵损失函数</a></li><li><a href="#4-batch-size">4. batch size</a><ul><li><a href="#41-总结">4.1. 总结</a></li></ul></li><li><a href="#5-使用gluon定义模型">5. 使用gluon定义模型</a><ul><li><a href="#51-线性回归">5.1. 线性回归</a></li><li><a href="#52-softmax回归">5.2. softmax回归</a></li><li><a href="#53-多层感知机">5.3. 多层感知机</a></li></ul></li><li><a href="#6-过拟合和欠拟合">6. 过拟合和欠拟合</a><ul><li><a href="#61-验证数据集">6.1. 验证数据集</a></li><li><a href="#62-权重衰减">6.2. 权重衰减</a></li><li><a href="#63-丢弃法">6.3. 丢弃法</a></li></ul></li><li><a href="#7-模型参数初始化">7. 模型参数初始化</a><ul><li><a href="#71-随机初始化">7.1. 随机初始化</a></li><li><a href="#72-xavier随机初始化">7.2. Xavier随机初始化</a></li><li><a href="#73-模型参数的延后初始化">7.3. 模型参数的延后初始化</a></li></ul></li><li><a href="#8-gpu计算">8. GPU计算</a></li></ul><!-- /TOC --><h1><span id="2-softmax回归">2. Softmax回归</span></h1><ol><li>Softmax回归是用来分类的，输入的个数表示特征，输出的个数表示类别。</li><li>Softmax运算  <script type="math/tex; mode=display">\hat{y_1},\hat{y_2},\hat{y_3}=softmax(o_1,o_2,o_3)</script>&ensp;&ensp;其中<script type="math/tex; mode=display">\hat{y}_1=\frac{\exp(o_1)}{\sum_{i=1}^3\exp{(o_i)}},\qquad \hat{y}_2=\frac{\exp(o_2)}{\sum_{i=1}^3\exp{(o_i)}},\qquad\hat{y}_3=\frac{\exp(o_3)}{\sum_{i=1}^3\exp{(o_i)}}</script>&ensp;&ensp;&ensp;&ensp;Softmax回归中有Softmax运算才可以使得输出的结果相加为1。如果没有softmax运算，输出结果也是可以用来分类的，例如$y_1=0.1$,$y_2=10$,$y_3=0.1$，最终属于的类别是2。但是如果$y_1=100$,$y_2=10$,$y_3=0.1$，最终属于的类别是1，没有经过softmax运算，会使得输出层的输出值的范围不确定，难以直观判断这些值的意义。<h1><span id="3-交叉熵损失函数">3. 交叉熵损失函数</span></h1>&ensp;&ensp;&ensp;&ensp;分类问题的预测效果通过交叉熵损失函数来判定。一个样本真实的分类结果也可以用一个向量来表示，其中只有一个是1，其余全为0。第i个样本真实的向量为$\boldsymbol{y}^{(i)}$,预测的向量为$\boldsymbol{\hat{y}^{(i)}}$<br>交叉熵用来评估预测值和真实值之间的差异<script type="math/tex; mode=display">H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\sum_{j=1}^q y_j^{(i)}\log\hat{y}_j^{(i)}</script>&ensp;&ensp;&ensp;&ensp;向量$\boldsymbol{}y^{(i)}$中共有q个元素，其中只有一个元素为1，其余全部为0，于是$H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\log\hat{y}_j^{(i)}$,交叉熵只关心正确类别的预测概率，比如样本$i$的真实类别为5，那么只关心预测向量$\hat{y}^{(i)}$中的第5个元素，即样本$i$属于第5个类别的概率。<br>假设训练数据集的样本个数为$n$，交叉熵损失函数定义为<script type="math/tex; mode=display">\ell(\boldsymbol{\Theta})=\frac{1}{n}\sum_{i=1}^nH\left(\boldsymbol{y^{(i)}},\hat{y}^{(i)}\right)</script>&ensp;&ensp;&ensp;&ensp;其中$\Theta$代表模型参数，对于这$n$个样本，每一个样本都求出这个样本的交叉熵。如果是一个样本只属于一个类，那么向量$\boldsymbol{}y^{(i)}$中只有1个为1，其余全为0。交叉熵损失函数可以简写成$\ell(\boldsymbol{\Theta})=-\frac{1}{n}\sum_{i=1}^n\log\hat{y}_j^{(i)}$,若要交叉熵损失函数$\ell(\boldsymbol{\Theta})$最小，就要使$\sum_{i=1}^n\log\hat{y}_j^{(i)}$最大，即最大化$\prod_{i=1}^n\hat{y}_j^{(i)}$，即每个样本属于自己正确类别的联合概率。<br>softmax运算步骤<br>&ensp;&ensp;&ensp;&ensp;对于分类问题，输出的结果是$O$，$O$是一个矩阵，其中行数表示样本的个数，列数表示类别的个数，假设有100个样本，5类，$O$是一个100*5的矩阵  <ul><li>首先对矩阵的中每个元素做exp()运算</li><li>计算出每一行的sum()</li><li>然后用一行中的每个元素/该行的sum()<br><img src="/2019/01/31/再看gluon新收获/softmax运算.png" alt="softmax运算例子"><br>代码如下<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">  X_exp = X.exp()</span><br><span class="line">  partition = X_exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><h1><span id="4-batch-size">4. batch size</span></h1><p> &ensp;&ensp;&ensp;&ensp;梯度下降是用来寻找模型最佳的模型参数w和b的迭代优化<strong>算法</strong>，通过最小化损失函数(线性回归的平方差误差、softmax的交叉熵损失函数),来寻找w和b。<br> &ensp;&ensp;&ensp;&ensp;只有在数据量比较大的时候，才会用到epoch和batch size和迭代，但这3个词代表什么意思呢？一直不太清楚。  </p><ul><li>epoch：当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个epoch。   </li><li>batch size：当一个完整数据集太大时，不能一次将全部数据输入到神经网络中进行训练，所以需要将完整数据集进行分块，每块样本的个数就是batch size。batch size是为了在内存效率和内存容量之间寻找最佳平衡。</li><li><p>迭代：就是以batch size向神经网络中输入样本，将完整数据集输入到神经网络中所需的次数，即完成一次epoch的次数。迭代数=batch的个数。比如完整数据集2000个样本，每个batch有200个样本，那么共有10个batch，完成一个epoch需要10次迭代。<br>在读取数据的时候传入一个参数batch_size,这个函数返回的X和y分别是含有batch_size个样本的特征和标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">     print(X, y)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;&ensp;&ensp;在进行小批量随机梯度算法中，一个batch size更新一次梯度，如果完整训练集中有2000个样本，一个batch有200个样本，那么一次epoch中更新10次模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">   <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">       <span class="comment"># 这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</span></span><br><span class="line">       param[:] = param - lr * param.grad / batch_size</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期epoch中，会使用所有的训练样本一次</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="comment">#每次读取batch_size个样本的特征和标签，用来训练，一个batch更新一次模型参数</span></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用batch size个样本来更新模型参数w和b</span></span><br><span class="line">    <span class="comment">#一个epoch之后，使用更新后的w和b来计算误差。传入的参数是一个list，里面有所有样本的预测值和真实值，返回的train_l也是一个list，包含每个样本的真实值和预测值的误差。print中输出的是一个标量：train_l.mean().asnumpy()，对于所有的样本的误差求一个平均值输出</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="41-总结">4.1. 总结</span></h2><p> CIFAR10 数据集有 50000 张训练图片，10000 张测试图片。现在选择 Batch Size = 500 对模型进行训练.  </p><ul><li>每个epoch要训练的图片数量：50000</li><li>训练集中具有的batch个数：50000/500=100</li><li>每次epoch需要的batch个数：100</li><li>每次epoch需要的迭代(iteration)个数：100 </li><li>每次epoch中更新模型参数的次数：100</li><li>如果有10个epoch，模型参数更新的次数为：100*10=1000</li><li>一次epoch使用的是全部的训练集50000中图片，下一次epoch中使用的还是这50000张图片，但是对模型参数的权重更新值却是不一样的，因为不同epoch的的模型参数不一样，模型训练的次数越多，损失函数越小，越接近谷底。</li><li>适当增加batch size的优点：<br>（1）提高内存利用率<br>（2）一次epoch的迭代次数减少，相同数据量的处理速度更快，但是达到相同精度所需的epoch越多<br>（3）梯度下降方向准确度增加，训练震荡越小</li><li>减少batch size的缺点<br>（1）小的batch size引入的随机性越大，难以达到收敛<h1><span id="5-使用gluon定义模型">5. 使用gluon定义模型</span></h1><strong>在gluon中无须指定每一层输入的形状，例如线性回归的输入个数，当模型得到数据时，例如执行后面的net(X)时，模型将自动推断出每一层的输入个数</strong><h2><span id="51-线性回归">5.1. 线性回归</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先导入nn模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#导入初始化模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="comment">#导入损失函数模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line"><span class="comment">#先定义一个模型变量net,sequential可以看做是串联各个层的容器，在构造模型时，向该容器依次添加层</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="comment">#全连接层是Dense(),定义全连接层的输出层个数为1</span></span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数：w和b,w初始化为均值为0，标准差为0.01的正太分布，b默认初始化为0</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#平方差损失</span></span><br><span class="line">loss = gloss.L2Loss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD,该优化算法将用来更新通过add添加的层所包含的全部参数</span></span><br><span class="line">tariner = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.03</span>&#125;)</span><br><span class="line"><span class="comment">#在训练模型时，调用Trainer实例的step()函数来更新模型参数w和b</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,num_epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autogard.record():</span><br><span class="line">            <span class="comment">#计算预测值和真实值的误差，l是一个长度为batch_size的数组</span></span><br><span class="line">            l=loss(net(X),y)</span><br><span class="line">        <span class="comment">#因为上面l是一个数组，实际是执行l.sum().backward()，把l变成标量</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#更新w和b，损失函数对w和b求梯度，w=w-r*Δ(l/w)/batch_size,</span></span><br><span class="line">        tariner.step(batch_size)</span><br><span class="line">    <span class="comment">#一个epoch结束，输入所有的训练数据集，更新完w和b，使用更新后的w和b，对所有的数据进行预测，计算预测值和真实值的误差l，这个l是一个len(所有样本)的数组，每个元素表示一个样本的预测值和真实值的误差，print对l求均值l.mean()变成标量</span></span><br><span class="line">    l = loss(net(features),label)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.mean().asnumpy()))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="52-softmax回归">5.2. softmax回归</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(Dense(<span class="number">10</span>))<span class="comment">#输出层有10个神经元，10个类别</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#定义交叉熵损失函数</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD</span></span><br><span class="line">trainer=gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.1</span>&#125;)</span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat,y).sum</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        y=y.astype(<span class="string">'float32'</span>)</span><br><span class="line">        train_l_sum+=l.asscalar()</span><br><span class="line">        train_acc_sum+=(y_hat.argmax(axis=<span class="number">1</span>)=y).sum().acscalar()</span><br><span class="line">        n+=y.size</span><br><span class="line">    test_acc=evaluate_accuracy(test_iter,net)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br></pre></td></tr></table></figure><h2><span id="53-多层感知机">5.3. 多层感知机</span></h2><p>输入层、隐藏层256个节点，输出层10个节点,relu激活函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line">loss = gluon.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.01</span>&#125;)</span><br><span class="line">batch_size=<span class="number">256</span></span><br></pre></td></tr></table></figure></p><h1><span id="6-过拟合和欠拟合">6. 过拟合和欠拟合</span></h1><h2><span id="61-验证数据集">6.1. 验证数据集</span></h2><p>测试数据集只能在所有超参数和模型参数都选定后使用一次。不可以使用测试数据集选择模型参数。所以需要验证集用来选择模型，验证集不参会模型训练。</p><h2><span id="62-权重衰减">6.2. 权重衰减</span></h2><p>权重衰减等于L2范数正则化，用来减少过拟合</p><h2><span id="63-丢弃法">6.3. 丢弃法</span></h2><p>深度学习模型常常使用丢弃法(dropout)来应对过拟合。在训练过程中，对<strong>隐藏层</strong>使用丢弃法，这样隐藏层中的某些神经元将会为0，即被丢弃。下图是一个多层感知机，隐藏层有5个神经元。<br><img src="/2019/01/31/再看gluon新收获/多层感知机.png" alt="多层感知机"><br>其中  </p><script type="math/tex; mode=display">h_i=\phi(x_1w_{1i}+x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i)</script><p>隐藏层计算的结果$h_i$将以$p$的概率被丢弃，即$h_i=0$,丢弃概率$0&lt;=p&lt;=1$。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1$…$h_5$中的任一个都有可能被清零，输出层的计算无法过度依赖隐藏层$h_1$…$h_5$中的任一个，从而在训练模型时起到正则化的作用，用来应对过拟合。<strong>在测试模型时，为了拿到更加确定的结果，一般不使用丢弃法。</strong><br>假设$h_2=0,h_5=0$,使用丢弃法之后的模型为<br><img src="/2019/01/31/再看gluon新收获/丢弃感知机.png" alt="丢弃后的多层感知机"><br><strong>代码实现</strong><br>在Gluon中，只需要在全连接层后面添加Dropout层并指定丢弃概率。在训练模型时，Dropout将以指定的丢失概率随机丢弃上一层的输出元素；在测试模型时，Dropout不起作用。<br><strong>一般在靠近输入层的丢弃率较小</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">        nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line">``` </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 7. 模型参数初始化</span></span><br><span class="line"><span class="comment">## 7.1. 随机初始化</span></span><br><span class="line">在Mxnet中，随机初始化通过net.initialize(init.Normal(sigma=<span class="number">0.01</span>))对模型的权重参数w采用正太分布的随机初始化。如果不指定初始化方法，如net.initialize()，默认的初始化方法：权重参数w每个元素随机采样于<span class="number">-0.07</span>到<span class="number">0.07</span>之间的均匀分布，偏差b为<span class="number">0</span>。</span><br><span class="line"><span class="comment">## 7.2. Xavier随机初始化</span></span><br><span class="line">假设某全连接层的输入个数为$a$，输出个数为$b$,Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</span><br><span class="line">$$U(-\sqrt&#123;\frac&#123;<span class="number">6</span>&#125;&#123;a+b&#125;&#125;,\sqrt&#123;\frac&#123;<span class="number">6</span>&#125;&#123;a+b&#125;&#125;)$$</span><br><span class="line"><span class="comment">## 7.3. 模型参数的延后初始化</span></span><br><span class="line">模型net在调用初始化函数 initialize之后，在做前向计算net(X)之前，权重参数的形状出现了<span class="number">0.</span>   </span><br><span class="line">![](再看gluon新收获/params.png)   </span><br><span class="line">在之前使用gluon创建的全连接层都没有指定输入个数，例如使用感知机net里，创建的隐藏层仅仅指定输出大小为<span class="number">256</span>，当调用initialize函数时，由于隐藏层输入个数依然未知，系统无法知道隐藏层权重参数的形状，只有在当我们将形状为(<span class="number">2</span>,<span class="number">20</span>)的输入X传进网络进行前向计算net(X)时，系统才推断该层的权重参数形状为(<span class="number">256</span>,<span class="number">20</span>)，因此，这时候才真正开始初始化参数.     </span><br><span class="line"><span class="comment"># 8. GPU计算</span></span><br><span class="line">使用GPU进行计算，通过ctx指定，NDArray存在内存上，在创建NDArray时可以通过指定ctx在指定的gpu上创建数组</span><br><span class="line">```python</span><br><span class="line">a=nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],ctx=mx.gpu())</span><br><span class="line">b=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>),ctx=mx.gpu(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>同NDArray类似，Gluon的模型也可以在初始化时通过ctx参数指定设备，下面的代码将模型参数初始化在显存上。当输入x是显存上的NDArray时，gluon会在同一块显卡的显存上计算结果。<br><strong>mxnet要求计算的所有输入数据都在内存或同一块显卡的显存上</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line">net.initialize(ctx=mx.gpu())</span><br><span class="line">x=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">8</span>))</span><br><span class="line">y=net(x)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>学习Markdown</title>
    <link href="http://yoursite.com/2019/01/21/First-blog/"/>
    <id>http://yoursite.com/2019/01/21/First-blog/</id>
    <published>2019-01-20T16:28:04.000Z</published>
    <updated>2019-02-01T13:30:53.674Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="简介">简介</span></h1><p>Markdown是一种轻量级标记语言，它用简洁的语法代替排版，使我们专心码字。它的目的是实现易读易写，成为一种适用于网络的书写语言。同时，Markdown支持嵌入html标签。<br> <a id="more"></a><br>注意：Markdown使用#、+、*等符号来标记，符号后面必须跟上至少一个空格才有效！  </p><h2><span id="标题">标题</span></h2><p>在标题前面加上1~6个#，依次表示一级标题，二级标题…六级标题</p><blockquote><h1><span id="一级标题">一级标题</span></h1><h2><span id="二级标题">二级标题</span></h2><h3><span id="三级标题">三级标题</span></h3><h6><span id="六级标题">六级标题</span></h6><h2><span id="列表">列表</span></h2><p>Markdown支持有序列表和无序列表<br>无序列表使用-、+、和*作为列表标记<br>使用-作为列表标记</p><ul><li>Red</li><li>Green</li><li>Blue<br>使用+作为列表标记</li></ul><ul><li>Red</li><li>Green</li><li>Blue<br>使用*作为列表标记</li></ul><ul><li>Red</li><li>Green</li><li>Blue<br>有序列表使用数组加英文句点.来表示</li></ul><ol><li>Red</li><li>Green</li><li><p>Blue  </p><h2><span id="引用">引用</span></h2><p>引用用&gt;来表示，引用支持多级引用，标题，列表，代码块，分割线等常规语法。<br>常见的引用写法：<br>这是一段应用    //在&gt;后面有1个空格</p><p> 这是引用的代码块形式  // 在&gt;后面有5个空格</p></li></ol><p>代码例子：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">super</span>.onCreate(savedInstanceState);  </span><br><span class="line">        setContentView(R.layout.activity_main);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>一级引用</p><blockquote><p>二级引用</p><blockquote><p>三级引用</p><ol><li>这是第一行列表项</li><li>这是第二行列表项<h2><span id="强调">强调</span></h2>两个<em>或_代表加粗，一个 </em>或者_代表斜体，<del>代表删除<br><strong>加粗文本</strong> 或者<br><strong>加粗文本</strong><br><em>斜体文本</em> 或者<br>_斜体文本_<br>~~删除文本</del>  <h2><span id="图像与链接">图像与链接</span></h2>图片与链接的语法很像，区别在于一个!,二者格式：<br>图片：<img src="/2019/01/21/First-blog/" alt=""> <img src="/2019/01/21/First-blog/图片地址" alt="图像文本(可忽略)"><br><img src="http://pic6.huitu.com/res/20130116/84481_20130116142820494200_1.jpg" alt=""><br><img src="https://images0.cnblogs.com/blog/404392/201501/122257231047591.jpg" alt="Markdown"><br><strong>在博客中插入本地图片</strong><br>1.修改配置文件_config.yml 里的post_asset_folder:这个选项设置为true<br>2.在你的hexo目录下执行这样一句话npm install hexo-asset-image —save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git（未验证有什么用）<br>3.等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹<br>4.最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：<br>输入![你想输入的替代文字]和(xxxx/图片名.jpg)</li></ol></blockquote></blockquote></blockquote><p> <strong>注意</strong></p><ul><li>导入的图片路径可以使用绝对路径也可以使用相对路径，建议使用相对路径。  </li><li>通常的做法是Markdown文档的同级目录下建立一个pictures文件夹，里面放置所有所需的图片，如果图片多的话，你也可以在pictures文件夹里建立子文件夹归类。</li><li>如果你的markdown在一个文件目录下，需要添加另一个目录下的图片，绝对路径是不可行的。需要 “迂回”<br>所谓 迂回，即需要先用../../命令返回上一文件目录，直至可以顺利找到要添加图片的目录。<br>举个栗子:<br>比如你的markdown在~/Document/mymarkdown/test下，需要添加~/Downloads/Pic/background目录下的sunlight.jpg<br>你需要做的是:先写![]，再加上(../../../Downloads/Pic/background/sunlight.jpg)</li></ul><blockquote><p>链接：<a href=""></a>  <a href="链接地址">链接文本</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener">百度</a> </p><h2><span id="代码">代码</span></h2><p>代码分为行内代码和代码块</p><ul><li>行内代码使用<code>代码</code>标识，可嵌入文本中</li><li>代码块使用4个空格，或者<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">这里是代码</span><br></pre></td></tr></table></figure></li></ul></blockquote><ul><li>代码语法高亮在<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">``` java</span><br><span class="line">protected void onCreate(Bundle savedInstanceState) &#123;  </span><br><span class="line">        super.onCreate(savedInstanceState);  </span><br><span class="line">        setContentView(R.layout.activity_main);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1><span id="表格">表格</span></h1><p>表格对齐方式</p><ul><li>居左：:——</li><li>居中：:——:或——-</li><li>居右：——:<br>例子  <blockquote><p>|标题1|标题2|标题3|<br>|:—-|:—-:|—-:|<br>|居左文本1|居中文本1|居右文本1|<br>|居左文本2|居中文本2|居右文本2|<br>|居左文本3|居中文本3|居右文本3|<br>表头1   | 表头2<br>————|———<br>内容1    | 内容2<br>内容3    | 内容3</p></blockquote></li></ul><h2><span id="分割线">分割线</span></h2><p>在一行中用三个以上的*、-、_、来建立一个分割线，行内不能有东西，也可以在行内插入空格</p><blockquote><h2><span id=""><em>*</em></span></h2><hr><hr><h2><span id="换行">换行</span></h2><p>在行尾添加两个空格加回车表示换行</p><h1><span id="常用弥补markdown的html标签">常用弥补Markdown的html标签</span></h1><h2><span id="字体">字体</span></h2><p><font face="微软雅黑" color="red" size="3">字体及字体颜色和大小</font></p><p><font color="#0000ff">字体颜色</font></p><h2><span id="换行">换行</span></h2><p>使用html标签<code>&lt;br/&gt;</code><br>换行</p><h2><span id="文本对其方式">文本对其方式</span></h2><p></p><p align="left">居左文本</p><p></p><p></p><p align="center">居中文本</p><p></p><p></p><p align="right">居右文本</p><p></p><h2><span id="下划线">下划线</span></h2><p><u>下划线文本</u></p></blockquote>]]></content>
    
    <summary type="html">
    
      学习使用Markdown
    
    </summary>
    
    
      <category term="Markdown" scheme="http://yoursite.com/tags/Markdown/"/>
    
  </entry>
  
</feed>
