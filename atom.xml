<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Echo&#39;s blog</title>
  
  <subtitle>远方到底有多远</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-09T15:33:18.024Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Echo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</title>
    <link href="http://yoursite.com/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/"/>
    <id>http://yoursite.com/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/</id>
    <published>2019-03-05T02:51:56.000Z</published>
    <updated>2019-03-09T15:33:18.024Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;今天实验室分享了一篇2019年AAAI的论文：<a href="http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf" target="_blank" rel="noopener">Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand</a>。讲的是通过时空多图卷积来进行打车需求的预测。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E4%BB%BB%E5%8A%A1">2. 任务</a></li><li><a href="#3-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">3. 数据处理</a><ul><li><a href="#31-neighborhood">3.1. Neighborhood</a></li><li><a href="#32-functional-similarity">3.2. Functional similarity</a></li><li><a href="#33-transportation-connectivity">3.3. Transportation connectivity</a></li></ul></li><li><a href="#st-mgcn%E6%A8%A1%E5%9E%8B">ST-MGCN模型</a></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;上学期实验室也分享论文，但是分享之后就忘了。所以决定从这学期开始，对一些我觉得对我有帮助的论文记录下来。<br>&ensp;&ensp;&ensp;&ensp;这篇论文和以前读到的图卷积的论文的不同是：这篇论文构建了多个图。</p><h1><span id="2-任务">2. 任务</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文的任务就是一个区域$T$个时间段的特征(可能有1个特征即打车订单数，也可有多个特征)预测第$T+1$时刻的特征值。     </p><h1><span id="3-数据处理">3. 数据处理</span></h1><p>&ensp;&ensp;&ensp;&ensp;首先这篇论文将一个城市进行网格划分，一个网格表示一个区域region。然后对这些网格构建图。<strong>多图就体现在构建图上，原先的论文是构建一个图，这篇论文构建了3个图。</strong>论文的任务是打车需求量预测。论文中说一个region的打车需求可能和以下3个方面有关系：邻近区域neighborhood，和该区域功能相近的区域，和该区域道路可达的区域。所以论文根据以上3种关系分别构建了3个图。  </p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/道路图.png" alt="">  </p><p>&ensp;&ensp;&ensp;&ensp;例如上图所示，那region1距离，和region相邻的区域是region2，和region1功能相似的是region3，和region1交通可达的是region4。</p><h2><span id="31-neighborhood">3.1. Neighborhood</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的邻居图用$\mathcal{G}_N=(V,A_N)$表示，其中V表示所有的区域，$A_N$表示区域与区域之间的邻居关系。构建的图一个节点表示一个region，2个区域有边表示这2个区域是邻居，没有边相连表示不是邻居。     </p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/邻居图.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;怎么判断两个区域是否相邻？首先把一个city划分成grid，一个区域周围的8个区域就是这个区域相邻，那么$A_{N,ij}=1$,否则为0。这样就构建了一个大小为$\mathbb{R}^{|V|\times|V|}$的矩阵$A$。</p><h2><span id="32-functional-similarity">3.2. Functional similarity</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的功能相似图用$\mathcal{G}_N=(V,A_S)$表示，其中V表示所有的区域，$A_S$表示区域与区域之间的功能相似关系。构建的图一个节点表示一个region，2个区域有边表示这2个区域之间的相似关系，是一个[0,1]之间的数。计算两个区域之间的相似关系：首先需要知道每个区域的POI向量。POI向量是这个区域在某个类别的POI的个数，例如POI有5类，分别是：学校、公园、医院、商场、居民区，这种区域有1个学校，2个公园、0个医院、3个商场、1个居民区，则POI向量为[1,2,0,3,1]，计算两个region的功能相似性即计算两个POI向量的相似性。</p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/功能图.png" alt="">   </p><h2><span id="33-transportation-connectivity">3.3. Transportation connectivity</span></h2><p>&ensp;&ensp;&ensp;&ensp;构建的交通可达图用$\mathcal{G}_N=(V,A_C)$表示，其中V表示所有的区域，$A_C$表示区域与区域之间的可达关系。构建的图一个节点表示一个region，2个区域可达判断的依据是通过OpenStreetMap，看两个区域之间是否有highway，subway，motorway，如果有的话就表示这两个区域交通可达。如果两个<br>region交通可达，则conn($v_i,v_j$)=1,否则为0。然后需要注意的是这里减去了邻居关系。因为考虑了交通可达，不想再次考虑邻居关系。取max为了保证没有负值。</p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/可达图.png" alt="">      </p><h1><span id="st-mgcn模型">ST-MGCN模型</span></h1><p>&ensp;&ensp;&ensp;&ensp;这篇论文提出的模型是$spatiotemporal \quad   multi-graph \quad convolution \quad network\quad(ST-MGCN)$。首先输入是原始数据，一个城市所有区域的特征，不同的层表示不同的时间，那么输入$X\in\mathbb{R}^{T\times|V|\times{P}}$，然后从原始数据中提取出3张图，分别表示region之间的邻居图、功能相似图、交通可达图。同样每个图</p><p><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/模型图.png" alt=""><br><img src="/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/模型图解释.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;今天实验室分享了一篇2019年AAAI的论文：&lt;a href=&quot;http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand&lt;/a&gt;。讲的是通过时空多图卷积来进行打车需求的预测。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="Multi-Graph Convolutional" scheme="http://yoursite.com/tags/Multi-Graph-Convolutional/"/>
    
      <category term="Spatiotemporal" scheme="http://yoursite.com/tags/Spatiotemporal/"/>
    
  </entry>
  
  <entry>
    <title>图卷积</title>
    <link href="http://yoursite.com/2019/03/03/%E5%9B%BE%E5%8D%B7%E7%A7%AF/"/>
    <id>http://yoursite.com/2019/03/03/图卷积/</id>
    <published>2019-03-03T04:25:25.000Z</published>
    <updated>2019-03-09T15:46:01.880Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-图卷积">1. 图卷积</span></h1><p>&ensp;&ensp;&ensp;&ensp;看了很久关于图卷积的内容，但总觉得自己理解不深刻，在这里把自己的一些想法写出来，也算把图卷积的内容梳理一下。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E5%9B%BE%E5%8D%B7%E7%A7%AF">1. 图卷积</a></li><li><a href="#2-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">2. 卷积神经网络</a></li><li><a href="#3-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">3. 图卷积神经网络</a><ul><li><a href="#31-%E9%A1%B6%E7%82%B9%E5%9F%9Fvertex-domain">3.1. 顶点域(Vertex Domain)</a><ul><li><a href="#311-%E5%9B%BE%E4%B8%AD%E9%A1%B6%E7%82%B9%E7%9A%84%E9%80%89%E6%8B%A9node-sequence-selection">3.1.1. 图中顶点的选择Node Sequence Selection</a></li><li><a href="#312-%E6%89%BE%E5%88%B0%E4%B8%AD%E5%BF%83%E7%BB%93%E7%82%B9%E7%9A%84%E9%82%BB%E5%9F%9Fneighborhood-assembly">3.1.2. 找到中心结点的邻域Neighborhood Assembly</a></li><li><a href="#313-%E5%9B%BE%E8%A7%84%E8%8C%83%E5%8C%96%E8%BF%87%E7%A8%8Bgraph-normalization">3.1.3. 图规范化过程Graph Normalization</a></li><li><a href="#314-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84convolutional-architecture">3.1.4. 卷积网络结构Convolutional Architecture</a></li><li><a href="#315-%E4%BC%AA%E4%BB%A3%E7%A0%81">3.1.5. 伪代码</a></li><li><a href="#316-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B">3.1.6. 算法流程</a></li></ul></li><li><a href="#32-%E8%B0%B1%E5%9F%9Fspectral-domain">3.2. 谱域Spectral Domain</a><ul><li><a href="#321-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97">3.2.1. 图卷积运算</a></li><li><a href="#322-%E7%AC%AC%E4%B8%80%E4%BB%A3%E5%9B%BE%E5%8D%B7%E7%A7%AFscnn">3.2.2. 第一代图卷积SCNN</a></li><li><a href="#323-%E7%AC%AC%E4%BA%8C%E4%BB%A3%E5%9B%BE%E5%8D%B7%E7%A7%AFchebnet">3.2.3. 第二代图卷积ChebNet</a></li><li><a href="#324-%E7%AC%AC%E4%B8%89%E4%BB%A3%E5%9B%BE%E5%8D%B7%E7%A7%AF">3.2.4. 第三代图卷积</a></li></ul></li></ul></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;在介绍图卷积之前，先介绍一下卷积神经网络。</p><h1><span id="2-卷积神经网络">2. 卷积神经网络</span></h1><p>&ensp;&ensp;&ensp;&ensp;卷积神经网络在图像上用的比较多。因为图像的一些重要的性质是：(1)局部性；(2)稳定性；即平移不变性和有大量相似的碎片；(3)多尺度性，简单的结构组合形成复杂的抽象结构。<br>&ensp;&ensp;&ensp;&ensp;图像本来就是一个规范的形状，可以形式化成一个规则的矩阵，再定义一个卷积核，卷积核在图像矩阵上滑动，把周围的几个像素值整合成一个值，获取了图像的局部性。可能会有多个卷积核，用来识别图像中不同的特征，比如下面例子所示，第一个卷积核用来识别左右的边缘，第二个卷积核用来识别上下的边缘。   </p><p><img src="/2019/03/03/图卷积/多个卷积核.png" alt="">    </p><ul><li>在卷积神经网络中，需要训练的参数是卷积核。</li><li>在卷积神经网络中，卷积层后面通常跟一个池化层，防止参数越来越多。</li><li>卷积核的大小通常是3x3和5x5</li><li>池化层如果是3x3，步长为2，那么图像大小会变成原来的一半，变成原先图像的多少和步长有关。</li><li>在图卷积层的最后一层是全连接层，可以使用1x1的卷积核来代表全连接，比如最后池化层输出是5x5x16，表示一层是5x5，一共有16层(通道)，经过全连接变成一个400x1的向量。    </li></ul><p><img src="/2019/03/03/图卷积/CNN实例.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;使用卷积神经网络对一张图片进行分类时，首先给定这张图片的特征，比如是32x32x3和这样图片的实际类别，如果是10个类，这张图片是第2个类，那么就是一个[0,1,0,…]的向量。将图片输入卷积层中，卷积核刚开始是随机初始化的，输入X和卷积核做卷积操作，经过池化层，再经过一个卷积层和池化层，然后是一个全连接层，最后一个全连接层的输出结点个数是10，表示10个类别，如果输出的结果和实际的类别有偏差，然后通过误差反向传播，更新卷积核，直到误差最小，得到模型参数：卷积核。卷积核的参数通过优化求出才能实现特征提取的作用   </p><p><img src="/2019/03/03/图卷积/CNN训练.png" alt="">      </p><h1><span id="3-图卷积神经网络">3. 图卷积神经网络</span></h1><p>&ensp;&ensp;&ensp;&ensp;有了卷积神经网络，为什么还要引入图卷积神经网络？<br>&ensp;&ensp;&ensp;&ensp;因为卷积神经网络处理的是规则的矩阵，像图像和视频中的像素点都是排列整齐的矩阵，也是论文中提到的欧式结构(Euclidean Structure)。  </p><p><img src="/2019/03/03/图卷积/欧式结构.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;但在科学研究中，还有很多非欧式结构(Non Euclidean Structure)的数据,例如社交网络、信息网络。    </p><p><img src="/2019/03/03/图卷积/非欧式结构.png" alt="">    </p><p>&ensp;&ensp;&ensp;&ensp;实际上，这样的网络结构就是图论中抽象意义上的拓扑图。<br>&ensp;&ensp;&ensp;&ensp;为什么要研究GCN，原因如下三个：</p><ol><li>卷积神经网络CNN无法处理非欧式结构的数据，在非欧式结构数据中，图中每个顶点的相邻顶点个数可能不同，无法用一个同样尺寸的卷积核进行卷积运算。</li><li>由于卷积神经网络CNN无法处理非欧式结构的数据，但是又希望在这样的数据结构上有效地提取空间特征来进行机器学习，所以GCN成为了研究的重点。</li><li>拓扑结构的数据在生活中很常见，社交网络、交通领域都涉及到非欧式结构的数据。<br>&ensp;&ensp;&ensp;&ensp;图卷积网络GCN的本质目的是提取拓扑图的空间特征。图卷积神经网络中有2种：顶点域和谱域。这是提取拓扑图空间特征的两种方式，就是给定非欧式结构数据，从中构建图的两种方法。   <h2><span id="31-顶点域vertex-domain">3.1. 顶点域(Vertex Domain)</span></h2>&ensp;&ensp;&ensp;&ensp;提取拓扑图上的空间特征，就是把每个顶点的邻居找出来。这里的问题是:(1)按照什么条件去找中心顶点的邻居，也就是如何确定感受野？(2)确定了邻居，按照什么方式处理包含不同数据邻居的特征？<br>&ensp;&ensp;&ensp;&ensp;<a href="https://arxiv.org/abs/1605.05273" target="_blank" rel="noopener">Learning Convolutional Neural Networks for Graphs</a>是2016年在ICML中发表的一篇论文，这是<a href="http://www.matlog.net/icml2016_slides.pdf" target="_blank" rel="noopener">PPT讲解</a>。由于CNN并不能有效的处理非欧式结构数据，这篇paper的motivation就是想将CNN在图像上的应用generalize到一般的graph上面。<br>&ensp;&ensp;&ensp;&ensp;本文提到的算法思想是：将一个图结构的数据转化为CNN能够高效处理的结构。处理的过程主要分为三个步骤：(1)从图结构中选出一个固定长度具有代表性的结点序列；(2)对于选出的每一个结点，收集固定大小的邻居集合。(3)对由当前节点及其对应的邻居构成的子图进行规范化，作为卷积结构的输入。算法具体分为4个步骤。<h3><span id="311-图中顶点的选择node-sequence-selection">3.1.1. 图中顶点的选择Node Sequence Selection</span></h3>&ensp;&ensp;&ensp;&ensp;首先对于输入的一个Graph，指定中心顶点的个数，然后确定确定图中的中心结点，确定中心结点主要采取的方法是：centrality，也就是中心化的方法，就是越处于中心位置的点越重要。这里的中心位置不是空间上的概念，应该是度量一个点的关系中的重要性的概念，简单的举例说明。如图5当中的两个图实际上表示的是同一个图，对其中红色标明的两个不同的nodes我们来比较他们的中心位置关系。比较的过程当中，我们计算该node和其余所有nodes的距离关系。我们假设相邻的两个node之间的距离都是1。    </li></ol><p><img src="/2019/03/03/图卷积/2个node.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;那么对于左图的红色node，和它直接相连的node有4个，因此距离+4；再稍微远一点的也就是和它相邻点相邻的有3个，距离+6;依次再相邻的有3个+9；最后还剩下一个最远的+4；因此我们知道该node的总的距离为23。同理我们得到右边的node的距离为3+8+6+8=25。那么很明显node的选择的时候左边的node会被先选出来。<br>&ensp;&ensp;&ensp;&ensp;当然，这只是一种node的排序和选择的方法，其存在的问题也是非常明显的。Paper并没有在这次的工作当中做详细的说明。   </p><h3><span id="312-找到中心结点的邻域neighborhood-assembly">3.1.2. 找到中心结点的邻域Neighborhood Assembly</span></h3><p>&ensp;&ensp;&ensp;&ensp;选出目标node之后，我们之后就要为目标node确定感受野大小。但是在确定之前，我们先构建一个candidate set，然后在从这个candidate set中选择感受野的node。这些感受野的candidate set，称为目标node的neighborhood。如下图所示，为每个目标node选择至少4个node（包括自己，k即为感受野node的个数）。接下来对选出来的每一个中心结点确定一个感受野receptive filed，以便进行卷积操作。但是在这之前，首先找到每个结点的邻域区域（neighborhood filed），然后再从当中确定感受野当中的结点。假设感受野的大小为k，那么对于每个结点会有2种情况：邻域结点的个数不够k个，或者邻域结点个数大于k个。      </p><p><img src="/2019/03/03/图卷积/选邻居.png" alt="">   </p><p>&ensp;&ensp;&ensp;&ensp;如上图所示，选出6个中心结点，对于每个中心结点，首先找到与其直接相邻的结点(被称为1阶邻居)，如果还不够再增加2阶邻居，那么对于1阶邻居已经足够的情况下，先全部放在候选 的区域中，在下一步中通过规范化做最终的选择。   </p><h3><span id="313-图规范化过程graph-normalization">3.1.3. 图规范化过程Graph Normalization</span></h3><p>&ensp;&ensp;&ensp;&ensp;这一步的目的在于将从candidate中选择感受野的node，并确定感受野中node的顺序。最终的结果如下图所示：假设上一步选择邻域过程中一个中心结点的邻居(一阶或者二阶)有N个，那么N可能和感受野大小K不相等。因此，normalize的过程就是要对N个邻居打上排序标签并进行选择，并且按照该顺序映射到向量中。    </p><p><img src="/2019/03/03/图卷积/normalize.png" alt="">     </p><p>&ensp;&ensp;&ensp;&ensp;如果这个中心结点的邻居个数不够k个的话，直接把所有的邻居全部选上，不够补上哑结点(dummy nodes),但还是需要排序的。如果中心结点的邻居个数N大于感受野k，则需要按照排序截断后面的结点。如上图所示，表示从中心结点到选邻居的整个过程。Normalize进行排序之后就能够映射到一个vector中，因此这一步最重要的是对结点进行排序。  </p><p>&ensp;&ensp;&ensp;&ensp;对于任意一个中心结点求解它的感受野的过程。这里的卷积核的大小为4(2x2的卷积核)，因此最终要选出4个邻居，包括中心结点本身。因此，需要给这些结点打标签(排序)。怎样打标签才是最好的？如上图要在7个结点中选出4个结点组成一个含有4个结点的图集合。作者认为，在一种标签下，就是已经给图中的节点排好序了，随机从集合中选出2个图，计算它们在向量空间的图距离和在图空间的图距离的差异的期望，如果这个期望越小那么就表示标签越好。得到最好的标签之后，就能够按着顺序将结点映射到一个有序的向量中，也就得到了感受野。    </p><h3><span id="314-卷积网络结构convolutional-architecture">3.1.4. 卷积网络结构Convolutional Architecture</span></h3><p>&ensp;&ensp;&ensp;&ensp;文章使用的是一个2层的卷积神经网络，将输入转化为一个向量vector之后便可以用来进行卷积操作了。具体的操作所示。     </p><p><img src="/2019/03/03/图卷积/卷积操作.png" alt="">    </p><p>&ensp;&ensp;&ensp;&ensp;首先最底层的灰色块为网络的输入，每一个块表示的是一个node的感知野（receptive field）区域，也是前面求解得到的4个nodes。其中an表示的是每一个node的数据中的一个维度（node如果是彩色图像那就是3维；如果是文字，可能是一个词向量……这里表明数据的维度为n）。粉色的表示卷积核，核的大小为4，但是宽度要和数据维度一样。因此，和每一个node卷季后得到一个值。卷积的步长（stride）为4，表明每一次卷积1个node，stride=4下一次刚好跨到下一个node。（备注：paper 中Figure1 当中，（a）当中的stride=1，但是转化为（b）当中的结构后stride=9）。卷积核的个数为M，表明卷积后得到的特征图的通道数为M，因此最终得到的结果为V1……VM，也就是图的特征表示。有了它便可以进行分类或者是回归的任务了。    </p><h3><span id="315-伪代码">3.1.5. 伪代码</span></h3><p><img src="/2019/03/03/图卷积/算法1.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;这个算法用来选择要进行卷积操作的node，其中w为要选择的node的个数。s为stride的大小。其中一个关键在于graph labeling procedure l。labeling算法用来确定一个graph中node的次序。这个算法可以根据node degree来确定，或者根据其他确定centrality的测量方式，比如：between centrality， WL algorithm等。或者其他你认为可行的算法。  </p><p><img src="/2019/03/03/图卷积/算法2.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;这个算法非常简单，就是一个BFS算法，对每个目标node，寻找离node最近的至少k个node。   </p><p>&ensp;&ensp;&ensp;&ensp;这是整个论文的重点，这个步骤的目的在于将目标node无序的neighbors映射为一个有序的vector。这个label要实现一个目的：assign nodes of two different graphs to a similar relative position in the respective adjacency matrices if and only if their structural roles within the graph are similar. 也就是说，对于两个不同的graphs， 来自这两个graph的子结构g1和g2，它们在各自的graph中有相似的结构，那么他们label应该相似。为了解决这个问题，论文中定义了一个optimal graph normalization问题，定义如下：    </p><p><img src="/2019/03/03/图卷积/算法3-1.png" alt="">         </p><p>&ensp;&ensp;&ensp;&ensp;这个等式的解在于寻找一个一个labeling L， 使得从图的集合中任意选取两个图G1和G2，它们在vector space距离差距和它们在graph space的距离差距最小化。但是这个问题是NP-hard的问题，所以作者选择找一个近似解。即它比较了各种labeling方法，并从其中找出最优解。具体如下：   </p><p><img src="/2019/03/03/图卷积/算法3-2.png" alt="">    </p><p>&ensp;&ensp;&ensp;&ensp;在特征选择阶段，只有第一层和传统的CNN有区别，之后的卷积层和传统的一样。下面来举例来说明PATCHY-SAN如何提取顶点特征和边特征。我们假设a_v为顶点特征的个数，a_e为边特征的个数。w为目标node的个数，k为感受野中node的个数。对于每个输入图结构，运用上面的一系列normalization算法，我们可以得到两个tensor（w,k,a_v）和（w,k,k,a_e）,分别对应于顶点特征和边特征。这两个tensor可以被reshape成(wk, a_v)和(wk^2, a_e)，其中a_v和a_e可以分别看成是CNN中channel的个数。现在我们可以对它们做一维度的卷积操作，其中第一个的感受野大小为k，第二个感受野大小为k^2。而之后的卷积层的构造和传统的CNN一样了。     </p><h3><span id="316-算法流程">3.1.6. 算法流程</span></h3><p>输入：任意一张图<br>输出：每个channel输出w个receptive field</p><p>Step1： graph labeling（对图的节点做标记，比如可以用节点的度做标记，做图的划分，也 可以叫做color refinement or vertex classification）<br>文中采用The Weisfeiler-Lehman algorithm做图的划分。由此可以得到每个节点的rank 值（为了不同的图能够有一个规范化的组织方式）</p><p>Step2：对labeling好的节点排序，取前w个节点，作为处理的节点序列。（这样就可以把不 同size的graph，变成同一个size）若不足w个节点，则，在输出中加全零的receptive field，相当于padding</p><p>Step3：采用stride=s来遍历这w个节点。文中s=1（若s）1，为了输出有w个receptive field， 也用step2的方式补全）</p><p>Step4：对遍历到的每个节点v（称作root），采用bfs的方式获得此节点的k个1-neighborhood， 如果不k个，再遍历1-neighborhood的1-neighborhood。直到满足k个，或者所有的 邻居节点都遍历完。此节点和他的k个邻居节点就生成了neighborhood graph。</p><p>Step5： step4就生成了w个（s=1）neighborhood graph。需要对着w个graph 进行labeling， 根据离root节点v的远近来计算每个节点的rank，根据算法4是离v越近，r越小。 如果每个neighborhood graph不足k个节点，用0节点补充</p><p>Step6：规范化step5得到了已经label好的graph，因为需要把它变成injective，使每个节点 的标签唯一，采用nauty的算法通过这w个receptive field就能得到一个w(k+1)维的向量。    </p><h2><span id="32-谱域spectral-domain">3.2. 谱域Spectral Domain</span></h2><p>&ensp;&ensp;&ensp;&ensp;谱域就是GCN的理论基础了。这种思路就是借助图谱的理论来实现卷积操作。<br>&ensp;&ensp;&ensp;&ensp;谱图理论就是借助图的拉普拉斯矩阵的特征值和特征向量来研究图的性质。<br>&ensp;&ensp;&ensp;&ensp;<strong>在这里需要明确一点：谱图和顶点域的本质完全不一样。顶点域其实还是先对图进行处理，然后像类似卷积核在图上滑动来计算。这里谱域就没有卷积核在图上滑动的这个概念了。里面的卷积运算就理解成矩阵之间的运算，不要再去想卷积核滑动的思想了。</strong></p><h3><span id="321-图卷积运算">3.2.1. 图卷积运算</span></h3><p>&ensp;&ensp;&ensp;&ensp;由传统的傅里叶变换得到图上的傅里叶变换，我们不需要知道怎么由传统的傅里叶变换得到图上的傅里叶变换，只需要知道图上的傅里叶变换是$F(f)=U^Tf$，其中f是待变换的函数，$F(f)$是$f$傅里叶变换之后的函数。傅里叶的逆变换为$F^{-1}(f)=Uf$。<br>&ensp;&ensp;&ensp;&ensp;由传统的卷积定理得到图上的卷积，因为在计算传统的卷积时需要用到傅里叶变换，所以在计算图上的卷积时也需要用到图上的傅里叶变换。下图中的$F$表示的是傅里叶变换，$g*f$表示卷积运算，可以看到$g和f$的卷积等于$g$的傅里叶变换和$f$的傅里叶变换乘积的逆傅里叶变换。其中$f$是待卷积的函数,就是一个待卷积的图，$g$就是卷积核。 把图上的傅里叶变换代入到下面中，就可以得到图上的卷积运算。<br>&ensp;&ensp;&ensp;&ensp;首先计算$f$的傅里叶变换为$U^Tf$，卷积核的傅里叶变换写成对角矩阵的形式为     </p><script type="math/tex; mode=display">\left(\begin{matrix}  \hat{g}(\lambda_1) & &  \\  &  \ddots &  \\  & &  \hat{g}(\lambda_n)\\\end{matrix}\right)</script><p>两者的傅里叶变换乘积即为     </p><script type="math/tex; mode=display">\left(\begin{matrix}  \hat{g}(\lambda_1) & &  \\  &  \ddots &  \\  & &  \hat{g}(\lambda_n)\\\end{matrix}\right)U^Tf</script><p>再乘上$U$求两者傅里叶变换乘积的逆变换，则求出$f和g的卷积$     </p><script type="math/tex; mode=display">(f*g)_G=U\left(\begin{matrix}  \hat{g}(\lambda_1) & &  \\  &  \ddots &  \\  & &  \hat{g}(\lambda_n)\\\end{matrix}\right)U^Tf</script><p><strong>很多论文中会把上式写成$(f*g)_G=U((U^Tg)\odot(U^Tf))$</strong>       </p><p>可以看出    $U$为特征向量组成的特征矩阵，$f$为待卷积函数，在图中就是图信号矩阵，重点在于设计可训练、共享参数的卷积核$g$。</p><p><img src="/2019/03/03/图卷积/谱图1.png" alt="">   </p><p><img src="/2019/03/03/图卷积/谱图2.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图3.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图4.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图5.png" alt="">  </p><p><img src="/2019/03/03/图卷积/谱图6.png" alt="">      </p><p>&ensp;&ensp;&ensp;&ensp;给出一个无向图，可以写出这个图的邻接矩阵$A$,无向图的邻接矩阵是一个对称矩阵，其中对角线上全是0。度矩阵$D$是一个对角矩阵，只有对角线上有值，其余全是0，$D_{i,i}=\sum_jA_{i,j}$，把$A$中的每一行加起来就是这个点的度。拉普拉斯矩阵$L=D-A$，拉普拉斯矩阵是一个对称矩阵，只有中心节点和一阶相连的顶点非0，其余位置全为0。对角线上表示这个节点有几个一阶邻居(不包括自己)，这一行中值为-1的表示是该节点的一阶邻居。  </p><p><img src="/2019/03/03/图卷积/图.png" alt=""></p><p><img src="/2019/03/03/图卷积/矩阵.png" alt=""></p><h3><span id="322-第一代图卷积scnn">3.2.2. 第一代图卷积SCNN</span></h3><p>&ensp;&ensp;&ensp;&ensp;第一代图卷积模型SCNN是在2014年发表在NIPS中的<a href="https://arxiv.org/abs/1312.6203" target="_blank" rel="noopener">Spectral Networks and Deep Locally Connected Networks On Graph</a>提出来的。<br><img src="/2019/03/03/图卷积/SCNN.png" alt="">  </p><p><img src="/2019/03/03/图卷积/第一代.png" alt=""></p><p>&ensp;&ensp;&ensp;&ensp;第一代卷积。谱图卷积就是给定一个图信号，和一个卷积核。图信号就是假设有一个图，如果有200个节点，每个节点有3个特征，x就是一个200*3的矩阵，这个矩阵就是图信号矩阵。卷积核就是一个参数𝜃，谱图卷积的定义就是对归一化的拉普拉斯矩阵特征分解，得到特征值组成的矩阵$\Lambda$和特征向量组成的矩阵U，由于L是对称矩阵，所以U-1等于UT。谱图卷积的定义：在欧式空间内卷积的定义是傅里叶变换乘积的逆变换，研究图上的卷积是怎么来的，将图上的信号做一个图傅里叶变换，将卷积核做一个图傅里叶变换，将这两个做一个内积，然后再做一个图上的逆傅里叶变换。所以一个卷积核在图信号的谱图卷积就定义出来了。这个式子的卷积核就是算出L的特征值，组成一个对角矩阵nxn。直接把对角线变成参数，这种方式太简单，这个卷积核不具有局部性，不能捕获局部关系。并且这个式子的时间复杂度很高，是n的立方，第二个是参数过多。所以后人对其进行改进。</p><h3><span id="323-第二代图卷积chebnet">3.2.3. 第二代图卷积ChebNet</span></h3><p><img src="/2019/03/03/图卷积/ChebNet.png" alt="">  </p><p><img src="/2019/03/03/图卷积/改进1.png" alt=""> </p><p>&ensp;&ensp;&ensp;&ensp;第二代图卷积。$g_\theta$是关于特征值的一个函数，以前是直接把特征值变成卷积核，现在把每一个特征值上都乘上一个特征值矩阵的k次幂，然后把0到k-1次幂加到一起，构造成一个新的卷积核。把这个卷积核代入，得到右侧这个式子，右边这个式子不需要做特征值分解，直接把L连乘k次就可以了。所以这个一个改进，这个式子比前面那个式子的时间复杂度低。但是右边的这个时间复杂度也不低，因为涉及到L的k次幂，现在就想办法把右边这个式子的时间复杂度降低。   </p><h3><span id="324-第三代图卷积">3.2.4. 第三代图卷积</span></h3><p><img src="/2019/03/03/图卷积/图卷积3.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;ICLR2017<a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">Semi-supervised Classification with Graph Convolutional Networks</a>这篇论文就对上面那个式子进行改进，让K=1，最大特征值=2，这个式子就变成了右边这个式子，继续化简，让$\theta_0$和$\theta_1$互为相反数，称为一个权重，为什么能这么变呢，因为他相信训练的时候卷积核就可以学出互为相反数的参数。这样只考虑一阶邻居，如果堆叠2个卷积层的话，就可以考虑2阶邻居了。    </p><p><img src="/2019/03/03/图卷积/图卷积4.png" alt=""><br>&ensp;&ensp;&ensp;&ensp;然后这篇论文又对刚刚的那个式子作一个归一化的trick，就是把邻接矩阵A加上一个单位矩阵，原先A是一个邻接矩阵，对角线上全为0，现在加上一个1，就是说节点自己到自己是没有边的，现在是自己有一条边又到达了自己，也就是增加一个自连接，然后重新计算一个度矩阵，然后就得到重新归一化的拉普拉斯矩阵，然后代入化成矩阵相乘的形式就是右边这个式子。这样卷积就定义完了，其中输入就是图信号X，卷积核就是Θ，前面的东西就是一个常量，只要拥有这三部分就可以得到GCN的卷积。卷积定义完了，然后加一个relu激活函数，这样一层图卷积就定义完了，如果要叠加一层，就是把上一层图卷积输出后的表示作为图卷积的输入，再做一次卷积。因为这篇论文做的是分类任务，所以要输出每个节点属于某个类的概率，所以用一个softmax就可以输出概率。<br>&ensp;&ensp;&ensp;&ensp;这就是现在谱图卷积计算的式子，实际上就是几个矩阵连乘，并没有卷积核滑动的概念。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-图卷积&quot;&gt;&lt;a href=&quot;#1-图卷积&quot; class=&quot;headerlink&quot; title=&quot;1. 图卷积&quot;&gt;&lt;/a&gt;1. 图卷积&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;看了很久关于图卷积的内容，但总觉得自己理解不深刻，在这里把自己的一些想法写出来，也算把图卷积的内容梳理一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="图卷积" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>拦截器</title>
    <link href="http://yoursite.com/2019/03/01/%E6%8B%A6%E6%88%AA%E5%99%A8/"/>
    <id>http://yoursite.com/2019/03/01/拦截器/</id>
    <published>2019-03-01T06:53:36.000Z</published>
    <updated>2019-03-01T13:43:32.767Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;拦截器是SpringMVC中的一个核心应用组件,主要用于处理多个Controller的共性问题.当我们的请求由DispatcherServlet<strong>派发到具体Controller之前</strong>首先要执行拦截器中一些相关方法,在这些方法中可以对请求进行相应预处理(例如权限检测,参数验证),这些方法可以决定对这个请求进行拦截还是放行。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8E%E8%AF%B7%E6%B1%82">2. 服务器与请求</a><ul><li><a href="#21-%E5%B8%B8%E8%A7%81%E7%9A%84web%E6%9C%8D%E5%8A%A1%E5%99%A8">2.1. 常见的WEB服务器</a></li><li><a href="#22-%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82">2.2. 发送请求</a></li><li><a href="#23-%E9%80%9A%E8%BF%87%E6%B5%8F%E8%A7%88%E5%99%A8%E5%8F%91%E9%80%81url%E8%AF%B7%E6%B1%82">2.3. 通过浏览器发送URL请求</a></li><li><a href="#24-js%E6%96%87%E4%BB%B6%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82">2.4. js文件发送请求</a></li></ul></li><li><a href="#3-%E5%AE%9E%E7%8E%B0%E6%8B%A6%E6%88%AA%E5%99%A8">3. 实现拦截器</a><ul><li><a href="#31-controller">3.1. Controller</a></li><li><a href="#32-interceptor">3.2. Interceptor</a></li><li><a href="#33-mvc%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">3.3. mvc配置文件</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-服务器与请求">2. 服务器与请求</span></h1><h2><span id="21-常见的web服务器">2.1. 常见的WEB服务器</span></h2><ol><li>Toncat服务器：我最常用的服务器，开放源代码的，运行servlet和JSP Web应用软件基于Java，比绝大多数的商业用的软件服务器要好。</li><li>Apache服务器：使用广泛，开源代码，支持多个平台，相对其他服务器占的内存较大，是重量级产品。</li><li>Microsoft IIS服务器：微软的，包括Web服务器，FTP服务器，NNTP服务器和SMTP服务器。需要购买。</li><li>Nginx服务器：俄罗斯的一个站点开发的，相比于Apache服务器，Nginx占用内存小且较稳定。<h2><span id="22-发送请求">2.2. 发送请求</span></h2>&ensp;&ensp;&ensp;&ensp;前端向服务器发送请求有2种，(1)通过浏览器发送请求，(2)进入到系统后，通过js发送请求。<h2><span id="23-通过浏览器发送url请求">2.3. 通过浏览器发送URL请求</span></h2>(1)用户在浏览器上输入网址，包含协议和域名.<br>(2)浏览器获得IP地址，浏览器先找自身缓存是否有记录，没有的话再找操作系统缓存，再没有就请求本地DNS服务器帮忙，本地DNS再找不到再一层层往上，最终浏览器获得对应的IP地址。<br>(3)浏览器发送请求，浏览器根据HTTP协议，给对应IP地址的主机发送请求报文，默认端口为80，报文包括请求内容，浏览器信息，本地缓存，cookie等信息。<br>(4)web服务器接收请求，寻找文件,Tomcat服务器接收到请求，找对应的html文件<br>(5)返回数据，web服务器向浏览器反馈html文件，浏览器进行渲染，页面加载。    <h2><span id="24-js文件发送请求">2.4. js文件发送请求</span></h2>&ensp;&ensp;&ensp;&ensp;在项目中，使用ajax向服务器发送请求，例如xxx.do。<h1><span id="3-实现拦截器">3. 实现拦截器</span></h1>&ensp;&ensp;&ensp;&ensp;拦截器需要实现   HandleInterceptor接口,或者继承HandlerInterceptorAdaptor抽象类;<br>HandlerInterceptor接口的三个方法:   </li><li>preHandle()</li><li>postHandle()</li><li>afterCompletion()   </li></ol><p>&ensp;&ensp;&ensp;&ensp;inceptor的作用是，每次在前端向后台发送一个请求时do,后台都会先经过inceptor中的preHandle这个函数，判断这个请求是否满足要求（是否已经登录，是否是管理员），如果满足要求就返回true，系统会自动把这个do请求提交给controller对应的函数进行处理，controller中的函数调用完之后，再次进入Inception中的postHandle()和afterCompletion()方法中。否则preHandle返回false，不会提交这个请求，不会执行Controller中的函数，也不会执行之后的Inception中的postHandle()和afterCompletion()方法。<br>&ensp;&ensp;&ensp;&ensp;服务器一启动,就会创建拦截器对象;拦截器是单例的,整个过程,拦截器只有一个实例对象。<br>&ensp;&ensp;&ensp;&ensp;项目中需要实现一个登录系统，当用户没有登录时，不能访问系统的主页和其他页面，但是可以访问系统的登录界面，所以需要在mvc.xml中设置一下，不拦截登录的请求。   </p><h2><span id="31-controller">3.1. Controller</span></h2><p>下面是用户登录的Controller实现,当前端访问login.jsp时，这时登录的请求不会被拦截器拦截，会执行login()方法，验证前端用户输入的用户名和密码是否正确，如果正确的话，将userName放入到session中，并返回给前端index，那么界面将会跳转到index.jsp，如果用户名或密码错误，那么返回给前端login，前端界面还是login.jsp。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> UserService userService;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">UserController</span><span class="params">()</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="keyword">this</span>.getClass().getName() + <span class="string">" 初始化"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/login"</span>, method = RequestMethod.POST)</span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">login</span><span class="params">(HttpServletRequest request, HttpSession session)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> SQLException, IOException, NoSuchAlgorithmException, InvalidKeySpecException </span>&#123;</span><br><span class="line"><span class="comment">// userList存储了所有的用户</span></span><br><span class="line"><span class="comment">// 每个用户以HashMap的形式存储</span></span><br><span class="line"><span class="comment">// key分别是："userName"，"password"，"salt"</span></span><br><span class="line">ArrayList&lt;HashMap&lt;String, String&gt;&gt; userList = userService.getUserInfo();</span><br><span class="line"></span><br><span class="line">String input_userName = request.getParameter(<span class="string">"userName"</span>);</span><br><span class="line">String input_password = request.getParameter(<span class="string">"password"</span>);</span><br><span class="line"></span><br><span class="line">PBKDF2Util pbkdf2Util = <span class="keyword">new</span> PBKDF2Util();</span><br><span class="line"><span class="comment">// 判断当前用户输入的用户名和密码是否正确</span></span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; userList.size(); i++) &#123;</span><br><span class="line">HashMap&lt;String, String&gt; oneUser = userList.get(i);</span><br><span class="line">String actual_userName = oneUser.get(<span class="string">"userName"</span>);</span><br><span class="line">String actual_password = oneUser.get(<span class="string">"password"</span>);</span><br><span class="line">String salt = oneUser.get(<span class="string">"salt"</span>);</span><br><span class="line"><span class="keyword">boolean</span> password_match = pbkdf2Util.authenticate(input_password, actual_password, salt);</span><br><span class="line"><span class="keyword">if</span> (input_userName.equals(actual_userName) &amp;&amp; password_match) &#123;</span><br><span class="line">flag = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="comment">// for</span></span><br><span class="line"><span class="keyword">if</span> (flag) &#123;</span><br><span class="line">session.setAttribute(<span class="string">"user"</span>, input_userName);</span><br><span class="line"><span class="keyword">return</span> <span class="string">"&#123;\"status\": 0, \"url\": \"index\"&#125;"</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">request.setAttribute(<span class="string">"msg"</span>, <span class="string">"用户名或密码错误"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="string">"&#123;\"status\": 1, \"url\": \"login\", \"msg\": \"用户名或密码错误\"&#125;"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/logout"</span>)</span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">logout</span><span class="params">(HttpSession session)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="comment">// System.out.println("进入到logout()方法中");</span></span><br><span class="line"><span class="comment">// 清除session的数据</span></span><br><span class="line">session.invalidate();</span><br><span class="line"><span class="keyword">return</span> <span class="string">"&#123;\"status\": 0, \"url\": \"login\"&#125;"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>lll</p><h2><span id="32-interceptor">3.2. Interceptor</span></h2><p>在interceptor中会拦截URL请求，如果session中的用户名为空会重定向到login.jsp。但在在做项目时遇见一个问题，拦截器只能拦截js中的ajax发来的URL请求，不能拦截浏览器发送的URL请求。也就是说如果用户在浏览器中输入index.jsp，不会经过拦截器，如果是js中的ajax发送的请求，会经过拦截器。如果用户没有登录直接在浏览器中输入index.jsp，这时页面依然可以进入到index.jsp，这说明拦截器没有起作用。为了应对这一情况，有三种解决方案：<br>（1）把判断用户是否登录的代码写到了jsp中，在jsp中写java代码需要加上&lt;%%&gt;，在这里判断session中的用户名，如果为空的话，直接重定向到login.jsp，这样用户在未登录的情况下，在浏览器上输入index.jsp，页面不会跳转到index.jsp中，还是在login.jsp中。<br>（2）把所有的jsp文件放在WEB-INF文件里,这样用户是直接不能访问WEB-INF文件下的jsp文件的。spring mvc的理念也是通过controller里的@RequestMapping来请求相关jsp页面，而非用户直接访问jsp页面。也就是说，jsp页面的访问需要通过controller来进行一次请求，因为会拦截对controller的请求，所以也就相当于拦截了jsp页面。如果要做登陆拦截，只需要把登陆页面不拦截，其余页面拦截进行是否登陆的验证即可。<br>（3）jsp如果不放在WEB-INF文件下，spring mvc是无法拦截的，这种情况下需要用最原始的servlet的Filter接口。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;%</span><br><span class="line"><span class="comment">//实现登录检查，如果用户没有登录，重定向到登录界面</span></span><br><span class="line"><span class="comment">//这段代码要加载所有需要验证页面里,使用</span></span><br><span class="line"><span class="comment">//&lt;%@include file="/jsp/navigation.jsp"把登录验证加载其余jsp中</span></span><br><span class="line">Object userName = <span class="string">""</span>;</span><br><span class="line"><span class="keyword">if</span> (session == <span class="keyword">null</span>) &#123;</span><br><span class="line">response.sendRedirect(request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">userName = session.getAttribute(<span class="string">"user"</span>);</span><br><span class="line"><span class="keyword">if</span> (userName == <span class="keyword">null</span>) &#123;</span><br><span class="line">response.sendRedirect(request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">userName = userName.toString();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">%&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginInterceptor</span> <span class="keyword">implements</span> <span class="title">HandlerInterceptor</span> </span>&#123;</span><br><span class="line"><span class="comment">// 步骤1</span></span><br><span class="line"><span class="comment">// 在前端发出一个url(xxx.do)请求时，先执行这个方法，判断当前用户是否为空</span></span><br><span class="line"><span class="comment">// 如果用户已经登录，则返回true,否则返回false</span></span><br><span class="line"><span class="comment">// 只有当该函数返回true时，才会调用controller中对应的函数，</span></span><br><span class="line"><span class="comment">// 返回false不用调用controller中的函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object arg2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">String path = request.getContextPath() + <span class="string">"/jsp/login.jsp"</span>;</span><br><span class="line">HttpSession session = request.getSession(<span class="keyword">false</span>);</span><br><span class="line"><span class="keyword">if</span> (session == <span class="keyword">null</span> || !request.isRequestedSessionIdValid()) &#123;</span><br><span class="line">response.sendRedirect(path);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取登录用户信息</span></span><br><span class="line">String user = session.getAttribute(<span class="string">"user"</span>).toString();</span><br><span class="line"><span class="keyword">if</span> (user == <span class="keyword">null</span>) &#123;</span><br><span class="line">response.sendRedirect(path);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 步骤2</span></span><br><span class="line"><span class="comment">// 当preHandle返回true，调用controller中的函数之后，会执行该函数</span></span><br><span class="line"><span class="comment">// 当preHandle返回false，不会执行该函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postHandle</span><span class="params">(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, ModelAndView arg3)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// System.out.println("拦截后...");</span></span><br><span class="line"><span class="comment">// System.out.println("进入到LoginInterceptor的postHandle()方法中");</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 步骤3</span></span><br><span class="line"><span class="comment">// 当preHandle返回true，调用controller中的函数之后，执行完postHandle，会调用该函数</span></span><br><span class="line"><span class="comment">// 当preHandle返回false，不会执行该函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// System.out.println("页面渲染后...");</span></span><br><span class="line"><span class="comment">// System.out.println("进入到LoginInterceptor的afterCompletion()方法中");</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="33-mvc配置文件">3.3. mvc配置文件</span></h2><p>在mvc.xml配置文件中，需要对拦截器进行配置，因为login请求不需要拦截，所以把这个请求排除，这样当前端访问login.jsp页面时，就会显示出登录的界面。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mvc:interceptors</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 拦截以任意字符结尾的路径 ，匹配所有的路径 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!--/**表示拦截所有的url及其子路径  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mvc:mapping</span> <span class="attr">path</span>=<span class="string">"/**"</span> /&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 登录不进行拦截 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mvc:exclude-mapping</span> <span class="attr">path</span>=<span class="string">"/**/*login*"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"com.hz.EQbigdata.interceptor.LoginInterceptor"</span>&gt;</span><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mvc:interceptors</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>页面加载的顺序：<br>前端输入一个网址，相当于发出一个url，比如querywda.jsp。首先拦截器拦截这个url，判断是否合法，如果合法，会交给controller处理，处理完之后才会显示querywda的界面，调用相应的querywda.js。如果不合法，就应该在inception就把这个请求拦截下来，重定向到login，这样querywda的界面也不会加载出来</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;拦截器是SpringMVC中的一个核心应用组件,主要用于处理多个Controller的共性问题.当我们的请求由DispatcherServlet&lt;strong&gt;派发到具体Controller之前&lt;/strong&gt;首先要执行拦截器中一些相关方法,在这些方法中可以对请求进行相应预处理(例如权限检测,参数验证),这些方法可以决定对这个请求进行拦截还是放行。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="SpringMVC" scheme="http://yoursite.com/tags/SpringMVC/"/>
    
      <category term="拦截器" scheme="http://yoursite.com/tags/%E6%8B%A6%E6%88%AA%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>HBase</title>
    <link href="http://yoursite.com/2019/02/28/HBase/"/>
    <id>http://yoursite.com/2019/02/28/HBase/</id>
    <published>2019-02-28T01:03:10.000Z</published>
    <updated>2019-03-01T06:49:58.882Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>&ensp;&ensp;&ensp;&ensp;在做项目的过程中用到了HBase，遇到了一些问题，当数据过大的时候，向HBase中会出现热点问题。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E7%AE%80%E4%BB%8B">1. 简介</a></li><li><a href="#2-%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0">2. 问题描述</a></li><li><a href="#3-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">3. 解决方案</a></li></ul><!-- /TOC --><h1><span id="2-问题描述">2. 问题描述</span></h1><p>&ensp;&ensp;&ensp;&ensp;HBase默认建表时有一个分区（region），这个region的rowkey是没有边界的，即没有startkey和endkey，<strong>hbase的中的数据是按照字典序排序的</strong>，在数据写入时，所有数据都会写入这个默认的region，随着数据量的不断增加，此region已经不能承受不断增长的数据量，当一个region过大（达到hbase.hregion.max.filesize属性中定义的阈值，默认10GB）时，会进行split，分成2个region。在此过程中，会产生两个问题：</p><ul><li>数据往一个region上写,会有写热点问题。</li><li>region split会消耗宝贵的集群I/O资源。<br>哈哈哈哈    </li></ul><h1><span id="3-解决方案">3. 解决方案</span></h1><p>&ensp;&ensp;&ensp;&ensp;基于此我们可以控制在建表的时候，创建多个空region，并确定每个region的起始和终止rowky，这样只要我们的rowkey设计能均匀的命中各个region，就不会存在写热点问题。自然split的几率也会大大降低。当然随着数据量的不断增长，该split的还是要进行split。像这样预先创建hbase表分区的方式，称之为预分区，下面给出一种预分区的实现方式:<br>&ensp;&ensp;&ensp;&ensp;解决这个问题，关键是要设计出可以让数据分布均匀的rowkey，与关系型数据库一样,rowkey是用来检索记录的主键。访问hbase table中的行，rowkey 可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，rowkey保存为字节数组，存储时，数据按照rowkey的字典序排序存储。<br>预分区的时候首先需要指定按什么来划分rowkey，<br>&ensp;&ensp;&ensp;&ensp;设计的rowkey应该由regionNo+messageId组成。设计rowkey方式：随机数+messageId，如果想让最近的数据快速get到，可以将时间戳加上，原先我们设计的行键是数据产生的时间，格式为2018-01-21 12:23:06,没有设置预分区，这样数据就会出现热点问题。后来采用预分区的方式，按照秒进行预分区，splitKeys={“01|”,”02|”,…”59|”},在设计行键的时候在原先的时间上再添加当前的秒数，例如原先的行键是2018-01-21 12:23:06，现在的行键是062018-01-21 12:23:06，这样在存储的时候行键的前2个字符06，我这里的region是01|到59|开头的，因为hbase的数据是字典序排序的,则当前这条数据就会保存到05|~06|这个region里。rowkey组成：秒数+messageId，因为我的messageId都是字母+数字，“|”的ASCII值大于字母、数字。<br>&ensp;&ensp;&ensp;&ensp;需要注意的是，行键分配值按照rowkey的前几个字符进行匹配的，并不是按照数的大小。例如分区是 -10,10-20,20-30,30-40,40-50,50-60,60-70,70-80,80-90,90-，如果插入的数据rowkey是80 60 22这种两位数，肯定会落到某个分区，如果rowkey是100 333 9955 555544 66910 这种大于两位值，都会落在最后一个分区，还是只取rowkey的前两位与startkey/endkey对应？答案是：是按前两位匹配rowkey的。   </p><pre><code class="lang-java">private byte[][] getSplitKeys() {        String[] keys = new String[] { &quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;,                &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot; };        byte[][] splitKeys = new byte[keys.length][];        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序        for (int i = 0; i &lt; keys.length; i++) {            rows.add(Bytes.toBytes(keys[i]));        }        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();        int i=0;        while (rowKeyIter.hasNext()) {            byte[] tempRow = rowKeyIter.next();            rowKeyIter.remove();            splitKeys[i] = tempRow;            i++;        }        return splitKeys;}</code></pre><p>需要注意的是，在上面的代码中用treeset对rowkey进行排序，必须要对rowkey排序，否则在调用admin.createTable(tableDescriptor,splitKeys)的时候会出错。创建表的代码如下:       </p><pre><code class="lang-java">/**     * 创建预分区hbase表     * @param tableName 表名     * @param columnFamily 列簇     * @return     */    @SuppressWarnings(&quot;resource&quot;)    public boolean createTableBySplitKeys(String tableName, List&lt;String&gt; columnFamily) {        try {            if (StringUtils.isBlank(tableName) || columnFamily == null                    || columnFamily.size() &lt; 0) {                log.error(&quot;===Parameters tableName|columnFamily should not be null,Please check!===&quot;);            }            HBaseAdmin admin = new HBaseAdmin(conf);            if (admin.tableExists(tableName)) {                return true;            } else {                HTableDescriptor tableDescriptor = new HTableDescriptor(                        TableName.valueOf(tableName));                for (String cf : columnFamily) {                    tableDescriptor.addFamily(new HColumnDescriptor(cf));                }                byte[][] splitKeys = getSplitKeys();                admin.createTable(tableDescriptor,splitKeys);//指定splitkeys                log.info(&quot;===Create Table &quot; + tableName                        + &quot; Success!columnFamily:&quot; + columnFamily.toString()                        + &quot;===&quot;);            }        } catch (MasterNotRunningException e) {            // TODO Auto-generated catch block            log.error(e);            return false;        } catch (ZooKeeperConnectionException e) {            // TODO Auto-generated catch block            log.error(e);            return false;        } catch (IOException e) {            // TODO Auto-generated catch block            log.error(e);            return false;        }        return true;    }</code></pre><p>&ensp;&ensp;&ensp;&ensp;HBase中出现热点问题带来的影响是：（1）在我们的项目中，原先使用一个分区，等到这个分区容量达到阈值时，这个分区开始split，然后数据来的时候就会向第二个分区写数据，不会向第一个region中写数据，所以在某一时候只能向一个region中写数据，这样写的速度会变慢。（2）在读数据的时候，因为行键设置的时间，连续的时间一般存储在一个region中，所以读数据的时候也是从一个region中读取数据，读取的速度也会变慢。项目原先应对取数据慢的问题解决方案使用HBase的scan函数，设置起始和终止的行键，使用scan查询数据。<br>&ensp;&ensp;&ensp;&ensp;按秒对表进行预分区时，就相当于把数据均匀分布在60个region中，存储一段时间的数据时，会同时向60个region中写入数据，取数据的时候也会同时从60个region中取数据。这样取数据的时候就不能使用起止行键用scan来查询数据了，只能使用getRow来查询数据，但是这样查询的性能也不会很差，因为是从60个region中同时查询数据，使用scan的时候是从1个region中查询数据。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;在做项目的过程中用到了HBase，遇到了一些问题，当数据过大的时候，向HBase中会出现热点问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/tags/HBase/"/>
    
      <category term="预分区" scheme="http://yoursite.com/tags/%E9%A2%84%E5%88%86%E5%8C%BA/"/>
    
  </entry>
  
  <entry>
    <title>NLP</title>
    <link href="http://yoursite.com/2019/02/15/NLP/"/>
    <id>http://yoursite.com/2019/02/15/NLP/</id>
    <published>2019-02-15T14:36:12.000Z</published>
    <updated>2019-02-27T13:06:51.159Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-词嵌入">1. 词嵌入</span></h1><p>&ensp;&ensp;&ensp;&ensp;<strong>词向量</strong>就是用来表示词的向量，也可以是词的特征向量或表征。把词映射成向量的技术叫<strong>词嵌入</strong>。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E8%AF%8D%E5%B5%8C%E5%85%A5">1. 词嵌入</a><ul><li><a href="#%E8%B4%9F%E9%87%87%E6%A0%B7">负采样</a></li><li><a href="#%E5%B1%82%E5%BA%8Fsoftmax">层序Softmax</a></li></ul></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;一种最简单的词嵌入是one-hot向量，每个词使用非0即1的向量表示。这样构造虽然简单，但不是一个好的选择。因为one-hot词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度，因为任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度很难通过one-hot向量准确表示出来。<br>&ensp;&ensp;&ensp;&ensp;word2vec工具的提出正是为了解决上面的问题。2013年Google团队发表了word2vec工具。它将每个词表示成一个<strong>定长的向量，并使得这些向量可以较好地表达不同词之间的相似和类比关系。</strong>word2vev包含了2个模型：跳字模型(skip-gram)和连续词袋模型(continuous bag of words CBOW)。以及2中高效训练的方法：负采样(negative sampling)和层序softmax(hierarchical softmax)<br>&ensp;&ensp;&ensp;&ensp;skip-gram是给定一个中心词，计算周围词出现的概率。训练skip-gram模型就是为了得到每个词的中心词向量和背景词向量，这就skip-gram的模型参数，在自然语言处理应用中，一般使用skip-gram的中心词向量作为词的表征向量。<br>&ensp;&ensp;&ensp;&ensp;连续词袋模型基于背景词来生成中心词。<br>&ensp;&ensp;&ensp;&ensp;但是skip-gram和CBOW的计算开销都比较大，下面介绍2个近似训练法：负采样和层序softmax.通过这2种方法可以减小训练开销。    </p><h2><span id="负采样">负采样</span></h2><p>&ensp;&ensp;&ensp;&ensp;在CBOW模型中，已知词$w$的上下文$Context(w)$,需要预测$w$，因此对于$Context(w)$，词$w$就是一个正样本，其他词就是一个负样本。从所有的负样本中选择一个负样本子集。训练的目标就是增大当上下文为$Context(w)$时，中心词$w$出现的概率，并且同时降低负样本的概率。<br>&ensp;&ensp;&ensp;&ensp;对于一个给定的词$w$，怎么生成这个词的负采样子集$NEG(W)$?<br>词典中的词出现的次数有高有低，对于那些高频词，被选为负样本的概率就比较大，对于那些低频词，被选中负样本的概率就小，本质上就是一个<strong>带权采样问题</strong>。对于一对中心词和背景词，随机采样K个负样本，论文中的建议K=5，负样本采样的概率$P(w)$设为$w$词频与总词频之比的$3/4$次方。<br><img src="/2019/02/15/NLP/负采样.png" alt="负采样"><br>对于skip-gram模型负采样，给定一个中心词预测周围的词。首先把语料分割成(context(w),w)样本，对于每一个中心词，找出这个中心词的背景词（周围词）,并且找出这个中心词的负样本（非背景词），一个中心词的负样本论文建议个数为5。对语料进行预处理形成以下数据集：一个样本包括一个中心词，它所对应的n个背景词，m个噪声词（负样本）。每个样本的背景词窗口大小可能不一样，即每个中心词的背景词和噪声词的个数可能不一样。</p><h2><span id="层序softmax">层序Softmax</span></h2><p>使用哈夫曼二叉树来存储词典，叶子节点就是词，非叶子节点就是一些隐藏向量。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-词嵌入&quot;&gt;&lt;a href=&quot;#1-词嵌入&quot; class=&quot;headerlink&quot; title=&quot;1. 词嵌入&quot;&gt;&lt;/a&gt;1. 词嵌入&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;&lt;strong&gt;词向量&lt;/strong&gt;就是用来表示词的向量，也可以是词的特征向量或表征。把词映射成向量的技术叫&lt;strong&gt;词嵌入&lt;/strong&gt;。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RNN</title>
    <link href="http://yoursite.com/2019/02/12/RNN/"/>
    <id>http://yoursite.com/2019/02/12/RNN/</id>
    <published>2019-02-12T12:55:39.000Z</published>
    <updated>2019-02-22T11:46:29.842Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-门控循环单元gru">1. 门控循环单元GRU</span></h1><p>&ensp;&ensp;&ensp;&ensp;梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。<br><a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83gru">1. 门控循环单元GRU</a><ul><li><a href="#11-%E5%80%99%E9%80%89%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81">1.1. 候选隐藏状态</a></li><li><a href="#12-%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81">1.2. 隐藏状态</a></li><li><a href="#13-%E6%80%BB%E7%BB%93">1.3. 总结</a></li></ul></li><li><a href="#2-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86lstm">2. 长短期记忆LSTM</a></li></ul><!-- /TOC --><p>&ensp;&ensp;&ensp;&ensp;门控循环神经网络的提出(2014年提出)正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。其中，门控循环单元是一种常用的门控循环神经网络。门控循环神经单元引入了重置门和更新门的概念，从而修改了循环神经网络中隐藏状态的计算方式。重置门和更新门的激活函数是sigmoid函数，可以将元素的值变换到0到1之间，因为重置门$R_t$和更新们$Z_t$中每个元素的值域都是[0,1]。   </p><h2><span id="11-候选隐藏状态">1.1. 候选隐藏状态</span></h2><p>&ensp;&ensp;&ensp;&ensp;候选隐藏状态用来辅助后面的隐藏状态计算。重置门为0，意味着重置对应隐藏状态元素为0，即丢弃上一时间步的隐藏状态。如果重置门为1，表示保留上一时间步的隐藏状态。重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃和预测无关的历史信息。   </p><h2><span id="12-隐藏状态">1.2. 隐藏状态</span></h2><p>&ensp;&ensp;&ensp;&ensp;时间步$t$的隐藏状态$H_t$的计算使用当前时间步的更新们$Z_t$来对上一时间步的隐藏状态$H_t-1$和当前时间步的候选隐藏状态$\tilde{H}_{t}$做组合。<br>&ensp;&ensp;&ensp;&ensp;更新门控制了包含当前时间步信息的候选隐藏状态如何流入隐藏状态。   </p><script type="math/tex; mode=display">X_t,H_{t-1},R_t(控制H_{t-1})----->\tilde{H}_t,H_{t-1},Z_t(控制\tilde{H}_t,H_{t-1})----->H_t</script><p>&ensp;&ensp;&ensp;&ensp;假设更新门$Z_t$在t时刻为1，那么时间步$t$的输入信息没有流入当前时间步的隐藏状态$H_t$，实际上，上一时间步的隐藏状态$H_{t-1}$保存并传递到当前时间步$t$。<strong>这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</strong>    </p><h2><span id="13-总结">1.3. 总结</span></h2><ul><li>重置门有助于捕捉时间序列里短期的依赖关系。重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃和预测无关的历史信息。</li><li>更新门有助于捕捉时间序列里长期的依赖关系。更新门$Z_t$在t时刻为1，那么时间步$t$的输入信息没有流入当前时间步的隐藏状态$H_t$，实际上，上一时间步的隐藏状态$H_{t-1}$保存并传递到当前时间步$t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。    </li><li>为什么叫GRU也叫做循环神经网络，因为门控循环单元中上一时间步的隐藏状态会传到当前时间步，体现了循环的性质。    <h1><span id="2-长短期记忆lstm">2. 长短期记忆LSTM</span></h1>&ensp;&ensp;&ensp;&ensp;另外一种常用的门控循环神经网络是LSTM(1997年提出)，比门控循环单元的结构稍微复杂一些。<br>&ensp;&ensp;&ensp;&ensp;GRU中的术语是：重置门，更新门，候选隐藏状态，隐藏状态。<br>&ensp;&ensp;&ensp;&ensp;LSTM的术语是：输入门，遗忘门，输出门，候选记忆细胞(与候选隐藏状态形状相同)，记忆细胞(与隐藏状态形状相同)，隐藏状态。<br>&ensp;&ensp;&ensp;&ensp;输入门$I_t$,遗忘门$F_t$,输出门$O_t$,候选记忆细胞$\tilde{C}_t$,记忆细胞$C_t$。<br>&ensp;&ensp;&ensp;&ensp;其中输入门$I_t$,遗忘门$F_t$,输出门$O_t$,候选记忆细胞$\tilde{C}_t$取决于$X_t和H_{t-1}$,记忆细胞$C_t$取决于$遗忘门F_t,C_{t-1},输入门I_t,\tilde{C}_t$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-门控循环单元GRU&quot;&gt;&lt;a href=&quot;#1-门控循环单元GRU&quot; class=&quot;headerlink&quot; title=&quot;1. 门控循环单元GRU&quot;&gt;&lt;/a&gt;1. 门控循环单元GRU&lt;/h1&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon" scheme="http://yoursite.com/tags/gluon/"/>
    
      <category term="RNN" scheme="http://yoursite.com/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>lovely dog</title>
    <link href="http://yoursite.com/2019/02/12/lovely-dog/"/>
    <id>http://yoursite.com/2019/02/12/lovely-dog/</id>
    <published>2019-02-12T08:37:27.000Z</published>
    <updated>2019-02-12T17:12:03.384Z</updated>
    
    <content type="html"><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;记得第一次养小狗狗，还是我上小学的时候，是一个白色的小狗，超级好看，我用香皂给它洗澡，后来家里种小麦的时候我爸用农药拌小麦，小狗吃了一些，就死了(((m-__-)m))。从那之后家里也没养过狗。<br> <a id="more"></a><br> &ensp;&ensp;&ensp;&ensp;前几天去我奶奶家，她有一个小卖部，年级大了，不敢开车，让我弟弟去帮她进货，进货的时候要先把需要进什么货都写下来，我奶奶不识字，我就去帮她写。到她家发现她家的狗不见了，奶奶说前几天小狗和她去曹庄的时候丢了，我问小狗自己找不到家吗？因为小狗一直拴着，没有出去过，所以不记得家。当时觉得挺可惜的，那么好的一个小狗。今天我和我妈去伯党乡洗澡，路上看见一个小花狗，在大路上一直看来往的人，我妈就说这是不是你奶奶家的狗？我当时想，我奶奶家的狗是在曹庄丢的，应该不会在这，说应该不是吧，就走了。走了之后越来越感觉好像就是这只小狗，但当时我妈开车已经走了好远了，也不好意思回去找。突然我妈说：你看，这只小狗追着我们呢。一看，哇！真的是我奶奶家的狗，竟然认识我们，因为我和我妈都经常不在家，小狗也没见过我们几次，竟然记得我们，就跟着我们的车跑。我们停下车，想把它抱到车上，但是它老动，抱不上去，就让它在后面跟着我们的车跑。我们到洗澡的地方，把车停在院子里，考虑要不要把小狗狗锁在车里，怕再次跑丢，当时想的是我们的车就在这，它应该不会走吧。然后我们就去洗澡了，小狗狗在院子里。等我们洗完的时候，叫了好几声都没发现那个小狗，当时还挺自责的，为啥不把它锁在车里呢，小狗又丢了。我姥姥就在洗澡的附近，洗完澡和我妈又开车去我姥姥家了，路上看见小狗就觉得是我家的狗，到我姥姥家也没有找到。中午在我姥姥家吃了饭，待了一会就回家。路上我妈说你看看路上有没有小狗。又走到上午发现小狗的那条路上，不知道小狗从那出来的，看见我们又跟在我们的车后，看见小狗当时好高兴，心想，这只小狗好聪明，找不到我们又回到原来的地方，刚走一会就看见我奶奶开个车在找她家的小狗，我就叫我奶奶，说狗找到啦。小狗看见我奶奶一个劲的往她身上蹭，我奶奶看见小狗，眼睛都红了，对着小狗说：这几天去哪了，也不知道回家。我奶奶把小狗抱上车回家了。<br> &ensp;&ensp;&ensp;&ensp;看到小狗又找到了，真的好高兴。以前看过《忠犬八公的故事》，电影中的小狗的主人因病去世了，但小狗狗每次到下班的点都去地铁站等着主人，但是主人再也不会从出站口出来了。小狗真的好有灵性，我们养只小狗可能觉得好玩，看家，小狗只是我们生活的一部分，但是我们却是它生活的全部。真的不敢想我奶奶家的小狗如果没有找到，又找不到回家的路该怎么办，在外面吃啥睡哪，万幸小狗狗找到了，以后不要再乱跑了，在家陪着奶奶吧。<br>  <img src="/2019/02/12/lovely-dog/dog3.jpg" alt=""><br> <img src="/2019/02/12/lovely-dog/dog1.jpg" alt=""><br> <img src="/2019/02/12/lovely-dog/dog2.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;记得第一次养小狗狗，还是我上小学的时候，是一个白色的小狗，超级好看，我用香皂给它洗澡，后来家里种小麦的时候我爸用农药拌小麦，小狗吃了一些，就死了(((m-__-)m))。从那之后家里也没养过狗。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>gluon_卷积</title>
    <link href="http://yoursite.com/2019/02/05/gluon-%E5%8D%B7%E7%A7%AF/"/>
    <id>http://yoursite.com/2019/02/05/gluon-卷积/</id>
    <published>2019-02-05T07:44:27.000Z</published>
    <updated>2019-02-10T12:53:21.761Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-卷积">1. 卷积</span></h1><p>卷积操作需要有1一个数组和一个卷积核，假设卷积核的形状为pxq，代表卷积核的高和宽。二维卷积层的输入输出用4维表示，格式为(样本，通道，高，宽)<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-%E5%8D%B7%E7%A7%AF">1. 卷积</a><ul><li><a href="#11-%E5%A1%AB%E5%85%85">1.1. 填充</a></li><li><a href="#12-%E6%AD%A5%E5%B9%85">1.2. 步幅</a></li><li><a href="#13-%E5%B0%8F%E7%BB%93">1.3. 小结</a></li><li><a href="#14-%E9%80%9A%E9%81%93channel">1.4. 通道channel</a></li><li><a href="#15-%E6%B1%A0%E5%8C%96%E5%B1%82">1.5. 池化层</a></li></ul></li><li><a href="#2-cnn%E4%BA%94%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B">2. CNN五大经典模型</a><ul><li><a href="#21-lenet">2.1. LeNet</a></li><li><a href="#22-alexnet">2.2. AlexNet</a></li><li><a href="#23-googlenet">2.3. GoogleNet</a></li><li><a href="#24-vgg">2.4. VGG</a></li><li><a href="#25-nin">2.5. NiN</a></li><li><a href="#26-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82">2.6. 批量归一化层</a></li><li><a href="#27-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet">2.7. 残差网络ResNet</a></li></ul></li></ul><!-- /TOC --><p>假设输入的形状为$n_h,n_w$,卷积核的形状为$k_hxk_w$，那么输出的形状为  </p><script type="math/tex; mode=display">(n_h-k_h+1)*(n_w-k_w+1)</script><p>卷积层的输出形状由输入形状和卷积核窗口形状决定，下面介绍卷积层的两个超参数，填充和步幅。  </p><h2><span id="11-填充">1.1. 填充</span></h2><p>填充通常在输入的高和宽填充0元素，如果在高的<strong>两侧一共</strong>填充$p_h$行，在宽的<strong>两侧一共</strong>填充$p_w$列，那么输出形状为</p><script type="math/tex; mode=display">(n_h-k_h+p_h+1)*(n_w-k_w+p_w+1)</script><h2><span id="12-步幅">1.2. 步幅</span></h2><p>步幅表示卷积核一次移动的个数，当高的步幅为$s_h$,宽的步幅为$s_w$，输出形状为</p><script type="math/tex; mode=display">\left\lfloor(n_h-k_h+p_h+s_h)/s_h\right\rfloor*\left\lfloor(n_w-k_w+p_w+s_w)/s_w\right\rfloor</script><h2><span id="13-小结">1.3. 小结</span></h2><ul><li><strong>填充可以增加输出的高和宽，常用来使输出与输入具有相同的高和宽</strong></li><li><strong>步幅可以减小输出的高和宽，使得输出的高和宽为输入的$1/n$</strong>  <h2><span id="14-通道channel">1.4. 通道channel</span></h2>通道(channel)：每个卷积层中卷积核的数量。<a href="https://blog.csdn.net/sscc_learning/article/details/79814146" target="_blank" rel="noopener">这篇文章</a>关于channel讲的很好<br>下面X(x_in,h,w)<br>K(k_out,k_in,h,w)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = nd.random.uniform(shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = nd.random.uniform(shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="15-池化层">1.5. 池化层</span></h2><p>pooling层(池化层)的输入一般是上一个卷积层，主要有以下2个作用：  </p><ol><li>保留主要的特征，同时减少下一层的参数和计算量，防止过拟合</li><li>保持某种不变性，包括平移，旋转，常用的平均池化和最大池化<br><strong>池化层的输出通道数和输入通道数相同</strong>     <h1><span id="2-cnn五大经典模型">2. CNN五大经典模型</span></h1></li><li>Lenet：1986年</li><li>Alexnet：2012年</li><li>GoogleNet：2014年</li><li>VGG：2014年</li><li>Deep Residual Learning：2015年<h2><span id="21-lenet">2.1. LeNet</span></h2>LeNet交替使用卷积层和最大池化层后接全连接层进行图像分类。网络结构如下所示<br><img src="/2019/02/05/gluon-卷积/LeNet.png" alt="LeNet"> <h2><span id="22-alexnet">2.2. AlexNet</span></h2>2012年，ImageNet比赛冠军的model—AlexNet，以第一作者alex命名。这个model的意义比后面的那些model都大很多。首先它证明了CNN在复杂模型下的有效性，然后GPU实现使得训练在可接受的时间范围内得到结果，让CNN和GPU都火了一把。<br>AlexNet包含8层变换，其中5层卷积和2层全连接层隐藏层，1个全连接输出层。<br>AlexNet将sigmoid激活函数改成了简单的ReLu激活函数。一方面，ReLu激活函数更简单，例如它没有sigmoid激活函数中的求幂运算。另一方面，ReLu激活函数在不同的参数初始化方法下使得模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度为0，从而造成反向传播无法继续更新部分模型参数；而ReLu激活函数在正区间的梯度恒为1.因为，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。<h2><span id="23-googlenet">2.3. GoogleNet</span></h2>2014年的ImageNet图像s识别挑战赛的冠军。<br>GoogleNet中的基础卷积块叫做Inception块。<br><img src="/2019/02/05/gluon-卷积/Inception.png" alt="">  </li></ol><h2><span id="24-vgg">2.4. VGG</span></h2><p><strong>VGG卷积块</strong>的组成规律是：连续使用数个相同的填充为1，窗口形状为3x3的卷积层后接一个步幅为2，窗口形状为2x2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。<br>VGG网络=VGG卷积块+n个全连接层<br>VGG卷积块=n个相同的卷积层+1个最大池化层   </p><h2><span id="25-nin">2.5. NiN</span></h2><p>前面介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以卷积层构成的模块充分抽取空间特征，再以全连接层构成的模块来输出分类结果。其中AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节介绍网络中的网络（NiN），即串联多个由卷积层和全连接层构成的 小网络来构建一个深层网络。<br><img src="/2019/02/05/gluon-卷积/NiN.png" alt=""><br>解决深度为<br><strong>全连接层可以由1x1卷积层充当</strong><br>NiN块是NiN中的基本块。它由一个卷积层加两个充当全连接层的1x1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二个和第三个卷积层的超参数一般是固定的。</p><h2><span id="26-批量归一化层">2.6. 批量归一化层</span></h2><p>标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为0，标准差为1.标准化处理输入数据使各个特征的分布相近：这样往往更容易训练处有效的模型。<br>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化但对深层神经网络来说，即使输入数据已经做了标准化，训练中模型参数的更新依然很容易造成靠近输出层的输出剧烈变化。这种计算数值的不稳定性通常令我们难以训练处有效的深度模型。<br>标准归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，<strong>批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出</strong>，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>BatchNorm主要是让训练收敛更快。</strong><br>对全连接层和卷积层做批量归一化的方法不同。   </p><ul><li>对全连接层做批量归一化<br>权重参数和偏差参数分别为$W和b$，激活函数为$\phi$,批量归一化运算符为$BN$,使用批量归一化的全连接层的输出为   <script type="math/tex; mode=display">\phi(BN(Wx+b))</script></li><li>对卷积层做批量归一化<br>对卷积层来说，批量归一化发生在卷积计算之后，应用激活函数之前。<br>对于前面的模型，我们可以在卷积层或全连接层之后、激活层之前加入批量归一化层，以LeNet为例：</li><li>未加入批量归一化层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(channels=<span class="number">6</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.MaxPool2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">       nn.Conv2D(channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.MaxPool2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">       nn.Dense(<span class="number">120</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.Dense(<span class="number">84</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">       nn.Dense(<span class="number">10</span>))</span><br><span class="line">```     </span><br><span class="line">- 加入批量归一化层     </span><br><span class="line">在卷积层或全连接层之后，激活层之前加入批量归一化层</span><br><span class="line">```python</span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2D(<span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Dense(<span class="number">120</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.Dense(<span class="number">84</span>),</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'sigmoid'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="27-残差网络resnet">2.7. 残差网络ResNet</span></h2><p>2015年ImageNet冠军model。<br>深度网络的好处：特征的等级随着网络深度的加深二变高，及其深的深度使得该网络拥有强大的表达能力。<br>但不是网络层数越多，效果就越好。随着网络深度的加深，(1)会出现梯度衰减的问题，在反向传播时，使梯度不断下降直至消失，对于权重的更新会越来越慢，直至不更新。(2)并且较深层网络比较浅的网络有更高的训练误差，称为退化问题。<br>深度残差网络主要思想很简单，就是在标准的前馈卷积网络上，加一个跳跃绕过一些层的连接。每绕过一层就产生一个残差块(residual block)，卷积层预测加输入张量的残差。普通的深度前馈网络难以优化。除了深度，所加层也使得training和validation的错误率增加，即使用上了batch normalization也是如此。残差神经网络由于存在shorcut connections，网络间的数据流通更为顺畅。残差网络结构的解决方案是，增加卷积层输出求和的捷径连接。<br>实验表明，残差网络更容易优化，并且能够通过增加相当的深度来提高准确率。核心是解决了增加深度带来的副作用（退化问题），这样能够通过单纯地增加网络深度，来提高网络性能。   </p><ul><li>网络的深度为什么重要？<br>因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。</li><li>为什么不能简单地增加网络层数？<br>对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。<br>对于该问题的解决方法是正则化初始化和中间的正则化层（Batch Normalization），这样的话可以训练几十层的网络。虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。 作者通过实验：通过浅层网络+ y=x 等同映射构造深层模型，结果深层模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。</li><li>怎么解决退化问题？<br>深度残差网络。如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。但是，如果把网络设计为H(x) = F(x) + x。我们可以转换为学习一个残差函数F(x) = H(x) - x. 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。<br><img src="/2019/02/05/gluon-卷积/ResNet1.png" alt=""><br>二层平原网络我们根据输入$x$,去拟合$H(x)$,$H(x)$是任意一种理想的映射，希望第2层权重输出能够与理想$H(x)$拟合。<br><img src="/2019/02/05/gluon-卷积/ResNet2.png" alt=""><br>为了解决深度神经网络的2个问题，提出残差网络ResNet。<br><img src="/2019/02/05/gluon-卷积/ResNet3.png" alt=""><br>残差是$F(X)$，让$F(x)=0$，这样$H(X)就趋近于x，是一个恒等映射$，输出和输入相等，这样计算增加网络深度，也不会造成训练误差上升（退化问题）。<br><img src="/2019/02/05/gluon-卷积/ResNet4.png" alt=""><br><img src="/2019/02/05/gluon-卷积/ResNet5.png" alt=""><br><img src="/2019/02/05/gluon-卷积/ResNet6.png" alt=""><br>残差网络的基础块是残差块，在残差块中，输入可通过跨层的数据线路更快地向前传播。<br><img src="/2019/02/05/gluon-卷积/ResNet7.png" alt=""></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-卷积&quot;&gt;&lt;a href=&quot;#1-卷积&quot; class=&quot;headerlink&quot; title=&quot;1. 卷积&quot;&gt;&lt;/a&gt;1. 卷积&lt;/h1&gt;&lt;p&gt;卷积操作需要有1一个数组和一个卷积核，假设卷积核的形状为pxq，代表卷积核的高和宽。二维卷积层的输入输出用4维表示，格式为(样本，通道，高，宽)&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon" scheme="http://yoursite.com/tags/gluon/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="卷积" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>再看gluon新收获</title>
    <link href="http://yoursite.com/2019/01/31/%E5%86%8D%E7%9C%8Bgluon%E6%96%B0%E6%94%B6%E8%8E%B7/"/>
    <id>http://yoursite.com/2019/01/31/再看gluon新收获/</id>
    <published>2019-01-31T09:05:01.000Z</published>
    <updated>2019-02-22T11:46:34.556Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。<br> <a id="more"></a><br><!-- TOC --></p><ul><li><a href="#1-简介">1. 简介</a></li><li><a href="#2-softmax回归">2. Softmax回归</a></li><li><a href="#3-交叉熵损失函数">3. 交叉熵损失函数</a></li><li><a href="#4-优化算法">4. 优化算法</a><ul><li><a href="#41-梯度下降gd">4.1. 梯度下降GD</a></li><li><a href="#42-随机梯度下降sgd">4.2. 随机梯度下降SGD</a></li><li><a href="#43-小批量随机梯度下降">4.3. 小批量随机梯度下降</a></li></ul></li><li><a href="#5-batch-size">5. batch size</a><ul><li><a href="#51-总结">5.1. 总结</a></li></ul></li><li><a href="#6-使用gluon定义模型">6. 使用gluon定义模型</a><ul><li><a href="#61-线性回归">6.1. 线性回归</a></li><li><a href="#62-softmax回归">6.2. softmax回归</a></li><li><a href="#63-多层感知机">6.3. 多层感知机</a></li></ul></li><li><a href="#7-过拟合和欠拟合">7. 过拟合和欠拟合</a><ul><li><a href="#71-验证数据集">7.1. 验证数据集</a></li><li><a href="#72-权重衰减">7.2. 权重衰减</a></li><li><a href="#73-丢弃法">7.3. 丢弃法</a></li></ul></li><li><a href="#8-模型参数初始化">8. 模型参数初始化</a><ul><li><a href="#81-随机初始化">8.1. 随机初始化</a></li><li><a href="#82-xavier随机初始化">8.2. Xavier随机初始化</a></li><li><a href="#83-模型参数的延后初始化">8.3. 模型参数的延后初始化</a></li></ul></li><li><a href="#9-gpu计算">9. GPU计算</a></li></ul><!-- /TOC --><h1><span id="2-softmax回归">2. Softmax回归</span></h1><ol><li>Softmax回归是用来分类的，输入的个数表示特征，输出的个数表示类别。</li><li>Softmax运算  <script type="math/tex; mode=display">\hat{y_1},\hat{y_2},\hat{y_3}=softmax(o_1,o_2,o_3)</script>&ensp;&ensp;其中<script type="math/tex; mode=display">\hat{y}_1=\frac{\exp(o_1)}{\sum_{i=1}^3\exp{(o_i)}},\qquad \hat{y}_2=\frac{\exp(o_2)}{\sum_{i=1}^3\exp{(o_i)}},\qquad\hat{y}_3=\frac{\exp(o_3)}{\sum_{i=1}^3\exp{(o_i)}}</script>&ensp;&ensp;&ensp;&ensp;Softmax回归中有Softmax运算才可以使得输出的结果相加为1。如果没有softmax运算，输出结果也是可以用来分类的，例如$y_1=0.1$,$y_2=10$,$y_3=0.1$，最终属于的类别是2。但是如果$y_1=100$,$y_2=10$,$y_3=0.1$，最终属于的类别是1，没有经过softmax运算，会使得输出层的输出值的范围不确定，难以直观判断这些值的意义。<h1><span id="3-交叉熵损失函数">3. 交叉熵损失函数</span></h1>&ensp;&ensp;&ensp;&ensp;分类问题的预测效果通过交叉熵损失函数来判定。一个样本真实的分类结果也可以用一个向量来表示，其中只有一个是1，其余全为0。第i个样本真实的向量为$\boldsymbol{y}^{(i)}$,预测的向量为$\boldsymbol{\hat{y}^{(i)}}$<br>交叉熵用来评估预测值和真实值之间的差异<script type="math/tex; mode=display">H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\sum_{j=1}^q y_j^{(i)}\log\hat{y}_j^{(i)}</script>&ensp;&ensp;&ensp;&ensp;向量$\boldsymbol{}y^{(i)}$中共有q个元素，其中只有一个元素为1，其余全部为0，于是$H\left(\boldsymbol{y}^{(i)},\boldsymbol{\hat{y}^{(i)}}\right)=-\log\hat{y}_j^{(i)}$,交叉熵只关心正确类别的预测概率，比如样本$i$的真实类别为5，那么只关心预测向量$\hat{y}^{(i)}$中的第5个元素，即样本$i$属于第5个类别的概率。<br>假设训练数据集的样本个数为$n$，交叉熵损失函数定义为<script type="math/tex; mode=display">\ell(\boldsymbol{\Theta})=\frac{1}{n}\sum_{i=1}^nH\left(\boldsymbol{y^{(i)}},\hat{y}^{(i)}\right)</script>&ensp;&ensp;&ensp;&ensp;其中$\Theta$代表模型参数，对于这$n$个样本，每一个样本都求出这个样本的交叉熵。如果是一个样本只属于一个类，那么向量$\boldsymbol{}y^{(i)}$中只有1个为1，其余全为0。交叉熵损失函数可以简写成$\ell(\boldsymbol{\Theta})=-\frac{1}{n}\sum_{i=1}^n\log\hat{y}_j^{(i)}$,若要交叉熵损失函数$\ell(\boldsymbol{\Theta})$最小，就要使$\sum_{i=1}^n\log\hat{y}_j^{(i)}$最大，即最大化$\prod_{i=1}^n\hat{y}_j^{(i)}$，即每个样本属于自己正确类别的联合概率。<br>softmax运算步骤<br>&ensp;&ensp;&ensp;&ensp;对于分类问题，输出的结果是$O$，$O$是一个矩阵，其中行数表示样本的个数，列数表示类别的个数，假设有100个样本，5类，$O$是一个100*5的矩阵。通过softmax运算使得一行的和为1，可以直观的看出每个样本属于每个样本的概率大小。<ul><li>首先对矩阵的中每个元素做exp()运算</li><li>计算出每一行的sum()</li><li>然后用一行中的每个元素/该行的sum()<br><img src="/2019/01/31/再看gluon新收获/softmax运算.png" alt="softmax运算例子"><br>代码如下<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">  X_exp = X.exp()</span><br><span class="line">  partition = X_exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><h1><span id="4-优化算法">4. 优化算法</span></h1><p>&ensp;&ensp;&ensp;&ensp;优化算法就是用来更新模型参数的一种算法，模型在训练的时候通过反向传播计算梯度，然后更新模型参数，使得模型的损失越来越小，当模型参数不再变化时，训练结束。</p><h2><span id="41-梯度下降gd">4.1. 梯度下降GD</span></h2><p>&ensp;&ensp;&ensp;&ensp;一次迭代中更新一次模型参数，梯度下降在每一次迭代中，使用整个训练数据集来计算梯度，更新一次参数。一个epoch只有一次迭代，下一次epoch再次使用所有的训练数据集更新模型参数。  </p><h2><span id="42-随机梯度下降sgd">4.2. 随机梯度下降SGD</span></h2><p>&ensp;&ensp;&ensp;&ensp;梯度下降每次更新模型参数时都需要遍历所有的data，当数据量太大或者一次无法获取全部数据时，这种方法并不可行。这个问题基本思路是：每次迭代只通过一个随机选取的数据$(x_n,y_n)$来获取梯度，以此对w进行更新，这种方法叫做随机梯度下降。一次迭代使用一个样本更新模型参数，这样一个epoch就需要很多次迭代，每次迭代随机采样一个样本更新模型参数。<br>&ensp;&ensp;&ensp;&ensp;小批量随机梯度下降中，当批量大小为1时是随机梯度下降；当批量大小为训练数据样本数时是梯度下降。当batch size较小时，每次迭代中使用的样本少，导致并行处理和内存使用效率变低。这使得在计算相同数据样本的情况下比使用更大batch size时所花的时间更多，即相同的训练数据，batch size越小，训练时间越长。当批量较大时，每个批量梯度里可能含有更多的冗余信息，为了得到较好的模型参数，批量较大时比批量较小时需要计算的样本数目可能更多，即迭代周期数多。</p><ul><li>相同的训练数据，batch size较小比batch size大时需要的训练时间长。</li><li>相同的训练数据，batch size大时，为了达到和batch size小时一样的训练效果，需要的epoch多。<h2><span id="43-小批量随机梯度下降">4.3. 小批量随机梯度下降</span></h2>小批量随机梯度下降：在每次迭代中，随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，更新模型参数。<br><strong>小批量随机梯度下降的学习率可以在迭代中自我衰减</strong>   <h2><span id="学习率">学习率</span></h2>&ensp;&ensp;&ensp;&ensp;当学习率很小时，模型参数更新非常慢，训练时间会很长。当学习率很大时，模型可能会越过最优解，导致模型不收敛，训练误差会越来越大，出现nan。当loss出现nan的时候，可以减少学习率。<h1><span id="5-batch-size">5. batch size</span></h1>&ensp;&ensp;&ensp;&ensp;梯度下降是用来寻找模型最佳的模型参数w和b的迭代优化<strong>算法</strong>，通过最小化损失函数(线性回归的平方差误差、softmax的交叉熵损失函数),来寻找w和b。<br>&ensp;&ensp;&ensp;&ensp;只有在数据量比较大的时候，才会用到epoch和batch size和迭代，但这3个词代表什么意思呢？一直不太清楚。  </li><li>epoch：当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个epoch。   </li><li>batch size：当一个完整数据集太大时，不能一次将全部数据输入到神经网络中进行训练，所以需要将完整数据集进行分块，每块样本的个数就是batch size。batch size是为了在内存效率和内存容量之间寻找最佳平衡。</li><li><p>迭代：就是以batch size向神经网络中输入样本，将完整数据集输入到神经网络中所需的次数，即完成一次epoch的次数。迭代数=batch的个数。比如完整数据集2000个样本，每个batch有200个样本，那么共有10个batch，完成一个epoch需要10次迭代。<br>在读取数据的时候传入一个参数batch_size,这个函数返回的X和y分别是含有batch_size个样本的特征和标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">     print(X, y)</span><br></pre></td></tr></table></figure><p>&ensp;&ensp;&ensp;&ensp;在进行小批量随机梯度算法中，一个batch size更新一次梯度，如果完整训练集中有2000个样本，一个batch有200个样本，那么一次epoch中更新10次模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">   <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">       <span class="comment"># 这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</span></span><br><span class="line">       param[:] = param - lr * param.grad / batch_size</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期epoch中，会使用所有的训练样本一次</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="comment">#每次读取batch_size个样本的特征和标签，用来训练，一个batch更新一次模型参数</span></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用batch size个样本来更新模型参数w和b</span></span><br><span class="line">    <span class="comment">#一个epoch之后，使用更新后的w和b来计算误差。传入的参数是一个list，里面有所有样本的预测值和真实值，返回的train_l也是一个list，包含每个样本的真实值和预测值的误差。print中输出的是一个标量：train_l.mean().asnumpy()，对于所有的样本的误差求一个平均值输出</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="51-总结">5.1. 总结</span></h2><p> CIFAR10 数据集有 50000 张训练图片，10000 张测试图片。现在选择 Batch Size = 500 对模型进行训练.  </p><ul><li>每个epoch要训练的图片数量：50000</li><li>训练集中具有的batch个数：50000/500=100</li><li>每次epoch需要的batch个数：100</li><li>每次epoch需要的迭代(iteration)个数：100 </li><li>每次epoch中更新模型参数的次数：100</li><li>如果有10个epoch，模型参数更新的次数为：100*10=1000</li><li>一次epoch使用的是全部的训练集50000中图片，下一次epoch中使用的还是这50000张图片，但是对模型参数的权重更新值却是不一样的，因为不同epoch的的模型参数不一样，模型训练的次数越多，损失函数越小，越接近谷底。</li><li>适当增加batch size的优点：<br>（1）提高内存利用率<br>（2）一次epoch的迭代次数减少，相同数据量的处理速度更快，但是达到相同精度所需的epoch越多<br>（3）梯度下降方向准确度增加，训练震荡越小</li><li>减少batch size的缺点<br>（1）小的batch size引入的随机性越大，难以达到收敛<h1><span id="6-使用gluon定义模型">6. 使用gluon定义模型</span></h1><strong>在gluon中无须指定每一层输入的形状，例如线性回归的输入个数，当模型得到数据时，例如执行后面的net(X)时，模型将自动推断出每一层的输入个数</strong><h2><span id="61-线性回归">6.1. 线性回归</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先导入nn模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#导入初始化模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="comment">#导入损失函数模块</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line"><span class="comment">#先定义一个模型变量net,sequential可以看做是串联各个层的容器，在构造模型时，向该容器依次添加层</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="comment">#全连接层是Dense(),定义全连接层的输出层个数为1</span></span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数：w和b,w初始化为均值为0，标准差为0.01的正太分布，b默认初始化为0</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#平方差损失</span></span><br><span class="line">loss = gloss.L2Loss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD,该优化算法将用来更新通过add添加的层所包含的全部参数</span></span><br><span class="line">tariner = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.03</span>&#125;)</span><br><span class="line"><span class="comment">#在训练模型时，调用Trainer实例的step()函数来更新模型参数w和b</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,num_epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autogard.record():</span><br><span class="line">            <span class="comment">#计算预测值和真实值的误差，l是一个长度为batch_size的数组</span></span><br><span class="line">            l=loss(net(X),y)</span><br><span class="line">        <span class="comment">#因为上面l是一个数组，实际是执行l.sum().backward()，把l变成标量</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#更新w和b，损失函数对w和b求梯度，w=w-r*Δ(l/w)/batch_size,</span></span><br><span class="line">        tariner.step(batch_size)</span><br><span class="line">    <span class="comment">#一个epoch结束，输入所有的训练数据集，更新完w和b，使用更新后的w和b，对所有的数据进行预测，计算预测值和真实值的误差l，这个l是一个len(所有样本)的数组，每个元素表示一个样本的预测值和真实值的误差，print对l求均值l.mean()变成标量</span></span><br><span class="line">    l = loss(net(features),label)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.mean().asnumpy()))</span><br></pre></td></tr></table></figure></li></ul><h2><span id="62-softmax回归">6.2. softmax回归</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(Dense(<span class="number">10</span>))<span class="comment">#输出层有10个神经元，10个类别</span></span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment">#定义交叉熵损失函数</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法：SGD</span></span><br><span class="line">trainer=gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.1</span>&#125;)</span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat,y).sum</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        y=y.astype(<span class="string">'float32'</span>)</span><br><span class="line">        train_l_sum+=l.asscalar()</span><br><span class="line">        train_acc_sum+=(y_hat.argmax(axis=<span class="number">1</span>)=y).sum().acscalar()</span><br><span class="line">        n+=y.size</span><br><span class="line">    test_acc=evaluate_accuracy(test_iter,net)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br></pre></td></tr></table></figure><h2><span id="63-多层感知机">6.3. 多层感知机</span></h2><p>输入层、隐藏层256个节点，输出层10个节点,relu激活函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line">loss = gluon.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span>:<span class="number">0.01</span>&#125;)</span><br><span class="line">batch_size=<span class="number">256</span></span><br></pre></td></tr></table></figure></p><h1><span id="7-过拟合和欠拟合">7. 过拟合和欠拟合</span></h1><h2><span id="71-验证数据集">7.1. 验证数据集</span></h2><p>测试数据集只能在所有超参数和模型参数都选定后使用一次。不可以使用测试数据集选择模型参数。所以需要验证集用来选择模型，验证集不参会模型训练。</p><h2><span id="72-权重衰减">7.2. 权重衰减</span></h2><p>权重衰减等于L2范数正则化，用来减少过拟合</p><h2><span id="73-丢弃法">7.3. 丢弃法</span></h2><p>深度学习模型常常使用丢弃法(dropout)来应对过拟合。在训练过程中，对<strong>隐藏层</strong>使用丢弃法，这样隐藏层中的某些神经元将会为0，即被丢弃。下图是一个多层感知机，隐藏层有5个神经元。<br><img src="/2019/01/31/再看gluon新收获/多层感知机.png" alt="多层感知机"><br>其中  </p><script type="math/tex; mode=display">h_i=\phi(x_1w_{1i}+x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i)</script><p>隐藏层计算的结果$h_i$将以$p$的概率被丢弃，即$h_i=0$,丢弃概率$0&lt;=p&lt;=1$。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1$…$h_5$中的任一个都有可能被清零，输出层的计算无法过度依赖隐藏层$h_1$…$h_5$中的任一个，从而在训练模型时起到正则化的作用，用来应对过拟合。<strong>在测试模型时，为了拿到更加确定的结果，一般不使用丢弃法。</strong><br>假设$h_2=0,h_5=0$,使用丢弃法之后的模型为<br><img src="/2019/01/31/再看gluon新收获/丢弃感知机.png" alt="丢弃后的多层感知机"><br><strong>代码实现</strong><br>在Gluon中，只需要在全连接层后面添加Dropout层并指定丢弃概率。在训练模型时，Dropout将以指定的丢失概率随机丢弃上一层的输出元素；在测试模型时，Dropout不起作用。<br><strong>一般在靠近输入层的丢弃率较小</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">        nn.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure></p><h1><span id="8-模型参数初始化">8. 模型参数初始化</span></h1><h2><span id="81-随机初始化">8.1. 随机初始化</span></h2><p>在Mxnet中，随机初始化通过net.initialize(init.Normal(sigma=0.01))对模型的权重参数w采用正太分布的随机初始化。如果不指定初始化方法，如net.initialize()，默认的初始化方法：权重参数w每个元素随机采样于-0.07到0.07之间的均匀分布，偏差b为0。</p><h2><span id="82-xavier随机初始化">8.2. Xavier随机初始化</span></h2><p>假设某全连接层的输入个数为$a$，输出个数为$b$,Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U(-\sqrt{\frac{6}{a+b}},\sqrt{\frac{6}{a+b}})</script><h2><span id="83-模型参数的延后初始化">8.3. 模型参数的延后初始化</span></h2><p>模型net在调用初始化函数 initialize之后，在做前向计算net(X)之前，权重参数的形状出现了0.<br><img src="/2019/01/31/再看gluon新收获/params.png" alt=""><br>在之前使用gluon创建的全连接层都没有指定输入个数，例如使用感知机net里，创建的隐藏层仅仅指定输出大小为256，当调用initialize函数时，由于隐藏层输入个数依然未知，系统无法知道隐藏层权重参数的形状，只有在当我们将形状为(2,20)的输入X传进网络进行前向计算net(X)时，系统才推断该层的权重参数形状为(256,20)，因此，这时候才真正开始初始化参数.     </p><h1><span id="9-gpu计算">9. GPU计算</span></h1><p>使用GPU进行计算，通过ctx指定，NDArray存在内存上，在创建NDArray时可以通过指定ctx在指定的gpu上创建数组<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],ctx=mx.gpu())</span><br><span class="line">b=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>),ctx=mx.gpu(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>同NDArray类似，Gluon的模型也可以在初始化时通过ctx参数指定设备，下面的代码将模型参数初始化在显存上。当输入x是显存上的NDArray时，gluon会在同一块显卡的显存上计算结果。<br><strong>mxnet要求计算的所有输入数据都在内存或同一块显卡的显存上</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line">net.initialize(ctx=mx.gpu())</span><br><span class="line">x=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">8</span>))</span><br><span class="line">y=net(x)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;以前看了一些《动手学深度学习》的教程，但是没有看完，寒假在家觉得时间还多，所以把以前看过的内容再看一遍，下面是第二次看的一些收获，记录下来，以备后需。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="gluon" scheme="http://yoursite.com/tags/gluon/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>学习Markdown</title>
    <link href="http://yoursite.com/2019/01/21/First-blog/"/>
    <id>http://yoursite.com/2019/01/21/First-blog/</id>
    <published>2019-01-20T16:28:04.000Z</published>
    <updated>2019-02-01T13:30:53.674Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="简介">简介</span></h1><p>Markdown是一种轻量级标记语言，它用简洁的语法代替排版，使我们专心码字。它的目的是实现易读易写，成为一种适用于网络的书写语言。同时，Markdown支持嵌入html标签。<br> <a id="more"></a><br>注意：Markdown使用#、+、*等符号来标记，符号后面必须跟上至少一个空格才有效！  </p><h2><span id="标题">标题</span></h2><p>在标题前面加上1~6个#，依次表示一级标题，二级标题…六级标题</p><blockquote><h1><span id="一级标题">一级标题</span></h1><h2><span id="二级标题">二级标题</span></h2><h3><span id="三级标题">三级标题</span></h3><h6><span id="六级标题">六级标题</span></h6><h2><span id="列表">列表</span></h2><p>Markdown支持有序列表和无序列表<br>无序列表使用-、+、和*作为列表标记<br>使用-作为列表标记</p><ul><li>Red</li><li>Green</li><li>Blue<br>使用+作为列表标记</li></ul><ul><li>Red</li><li>Green</li><li>Blue<br>使用*作为列表标记</li></ul><ul><li>Red</li><li>Green</li><li>Blue<br>有序列表使用数组加英文句点.来表示</li></ul><ol><li>Red</li><li>Green</li><li><p>Blue  </p><h2><span id="引用">引用</span></h2><p>引用用&gt;来表示，引用支持多级引用，标题，列表，代码块，分割线等常规语法。<br>常见的引用写法：<br>这是一段应用    //在&gt;后面有1个空格</p><p> 这是引用的代码块形式  // 在&gt;后面有5个空格</p></li></ol><p>代码例子：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">super</span>.onCreate(savedInstanceState);  </span><br><span class="line">        setContentView(R.layout.activity_main);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>一级引用</p><blockquote><p>二级引用</p><blockquote><p>三级引用</p><ol><li>这是第一行列表项</li><li>这是第二行列表项<h2><span id="强调">强调</span></h2>两个<em>或_代表加粗，一个 </em>或者_代表斜体，<del>代表删除<br><strong>加粗文本</strong> 或者<br><strong>加粗文本</strong><br><em>斜体文本</em> 或者<br>_斜体文本_<br>~~删除文本</del>  <h2><span id="图像与链接">图像与链接</span></h2>图片与链接的语法很像，区别在于一个!,二者格式：<br>图片：<img src="/2019/01/21/First-blog/" alt=""> <img src="/2019/01/21/First-blog/图片地址" alt="图像文本(可忽略)"><br><img src="http://pic6.huitu.com/res/20130116/84481_20130116142820494200_1.jpg" alt=""><br><img src="https://images0.cnblogs.com/blog/404392/201501/122257231047591.jpg" alt="Markdown"><br><strong>在博客中插入本地图片</strong><br>1.修改配置文件_config.yml 里的post_asset_folder:这个选项设置为true<br>2.在你的hexo目录下执行这样一句话npm install hexo-asset-image —save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git（未验证有什么用）<br>3.等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹<br>4.最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：<br>输入![你想输入的替代文字]和(xxxx/图片名.jpg)</li></ol></blockquote></blockquote></blockquote><p> <strong>注意</strong></p><ul><li>导入的图片路径可以使用绝对路径也可以使用相对路径，建议使用相对路径。  </li><li>通常的做法是Markdown文档的同级目录下建立一个pictures文件夹，里面放置所有所需的图片，如果图片多的话，你也可以在pictures文件夹里建立子文件夹归类。</li><li>如果你的markdown在一个文件目录下，需要添加另一个目录下的图片，绝对路径是不可行的。需要 “迂回”<br>所谓 迂回，即需要先用../../命令返回上一文件目录，直至可以顺利找到要添加图片的目录。<br>举个栗子:<br>比如你的markdown在~/Document/mymarkdown/test下，需要添加~/Downloads/Pic/background目录下的sunlight.jpg<br>你需要做的是:先写![]，再加上(../../../Downloads/Pic/background/sunlight.jpg)</li></ul><blockquote><p>链接：<a href=""></a>  <a href="链接地址">链接文本</a><br><a href="http://www.baidu.com" target="_blank" rel="noopener">百度</a> </p><h2><span id="代码">代码</span></h2><p>代码分为行内代码和代码块</p><ul><li>行内代码使用<code>代码</code>标识，可嵌入文本中</li><li>代码块使用4个空格，或者<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">这里是代码</span><br></pre></td></tr></table></figure></li></ul></blockquote><ul><li>代码语法高亮在<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">``` java</span><br><span class="line">protected void onCreate(Bundle savedInstanceState) &#123;  </span><br><span class="line">        super.onCreate(savedInstanceState);  </span><br><span class="line">        setContentView(R.layout.activity_main);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1><span id="表格">表格</span></h1><p>表格对齐方式</p><ul><li>居左：:——</li><li>居中：:——:或——-</li><li>居右：——:<br>例子  <blockquote><p>|标题1|标题2|标题3|<br>|:—-|:—-:|—-:|<br>|居左文本1|居中文本1|居右文本1|<br>|居左文本2|居中文本2|居右文本2|<br>|居左文本3|居中文本3|居右文本3|<br>表头1   | 表头2<br>————|———<br>内容1    | 内容2<br>内容3    | 内容3</p></blockquote></li></ul><h2><span id="分割线">分割线</span></h2><p>在一行中用三个以上的*、-、_、来建立一个分割线，行内不能有东西，也可以在行内插入空格</p><blockquote><h2><span id=""><em>*</em></span></h2><hr><hr><h2><span id="换行">换行</span></h2><p>在行尾添加两个空格加回车表示换行</p><h1><span id="常用弥补markdown的html标签">常用弥补Markdown的html标签</span></h1><h2><span id="字体">字体</span></h2><p><font face="微软雅黑" color="red" size="3">字体及字体颜色和大小</font></p><p><font color="#0000ff">字体颜色</font></p><h2><span id="换行">换行</span></h2><p>使用html标签<code>&lt;br/&gt;</code><br>换行</p><h2><span id="文本对其方式">文本对其方式</span></h2><p></p><p align="left">居左文本</p><p></p><p></p><p align="center">居中文本</p><p></p><p></p><p align="right">居右文本</p><p></p><h2><span id="下划线">下划线</span></h2><p><u>下划线文本</u></p></blockquote>]]></content>
    
    <summary type="html">
    
      学习使用Markdown
    
    </summary>
    
    
      <category term="Markdown" scheme="http://yoursite.com/tags/Markdown/"/>
    
  </entry>
  
</feed>
