<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Echo&#39;s blog</title>
  
  <subtitle>远方到底有多远</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-09-17T13:46:46.756Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Echo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</title>
    <link href="http://yoursite.com/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/"/>
    <id>http://yoursite.com/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/</id>
    <published>2020-09-17T06:35:17.000Z</published>
    <updated>2020-09-17T13:46:46.756Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/ftp/arxiv/papers/1911/1911.12093.pdf" target="_blank" rel="noopener">Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</a></p><p>发表于AAAI2020</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-介绍">1. 介绍</a></li><li><a href="#2-相关工作">2. 相关工作</a></li><li><a href="#3-问题定义">3. 问题定义</a></li><li><a href="#4-mra-bgcn">4. MRA-BGCN</a><ul><li><a href="#41-bicomponent-gcn">4.1. Bicomponent GCN</a></li><li><a href="#42-multi-range-attention">4.2. Multi-Range Attention</a></li><li><a href="#43-结合rnn">4.3. 结合RNN</a></li></ul></li></ul><!-- /TOC --><h1><span id="1-介绍">1. 介绍</span></h1><p>本文的任务是给定一个道路图历史的交通数据，预测未来的交通状况。该任务主要有以下2个挑战：</p><ol><li>不规则的路网导致交通数据间存在复杂时空依赖</li><li>各种各样不可预测的交通状况，使得交通数据具有不确定性</li></ol><p>以前的工作一般使用CNN来处理网格数据，使用GCN+RNN(DCRNN)或GCN+CNN(STGCN)来处理图数据。但是这些方法仍然忽略了以下2方面：</p><ol><li>原先的方法使用GCN都是基于一个固定的权重矩阵。如下图所示，图中有3个节点，其中节点1和3，节点2和3通过道路连接，现在的方法基本都是根据节点之间的距离来创建图，图的权重权重是不变的，但节点之间的权重并不是一成不变的，这种构图方法忽略了节点之间复杂的影响关系。</li></ol><p>   <img src="/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/f1.jpg" alt=""></p><ol><li>构建完图后，原先的方法一般使用GCN融合K跳邻居的信息进行聚合，但是却忽略了多个范围(multiple range)的信息。不同范围的信息反映了不同的交通属性。小范围的邻居揭示了局部依赖，大范围的邻居揭示了全局模式。并且不同范围的信息应该有不同的贡献。例如在一次交通事故中，目标区域主要受邻接区域的影响，所以模型也应该更关注这些区域，而不是对所有的K跳邻居都赋予相同的权重。</li></ol><p>为了解决以上2个问题，我们提出Multi-Range Attentive Bicomponent Graph Convolutional Network (MRA-BGCN),不仅考虑了节点信息，而且把边作为实体，考虑边之间的交互。如图1(C)所示。该文章的共享如下：</p><ol><li><p>提出MRA-BGCN，引入bicomponent图卷积，同时建模节点和边的相关性。节点图根据路网距离来构架，边图的构建考虑了2种模式：stream连通性、竞争关系。</p></li><li><p>为bicomponent图卷积提出多范围的attention机制，可以聚合不同范围的邻居信息并学到它们的权重</p></li><li>在METR-LA和PEMS-BAY这2个数据集上进行实验</li></ol><h1><span id="2-相关工作">2. 相关工作</span></h1><p>DCRNN和STGCN使用固定的图结构<br>Graph WaveNet通过学习自适应的邻接矩阵来解决这个问题，但是隐藏的空间依赖是以数据驱动的方式来学习的，缺乏领域知识的指导，可能会出现过拟合的问题。</p><p>现有的方法无法对边之间的交互进行建模。</p><h1><span id="3-问题定义">3. 问题定义</span></h1><p>$G=(V,E,A),|V|=N$，其中$A \in \mathbb{R}^{N \times N}$表示邻接矩阵，邻接矩阵通过路网的距离构建。图信号矩阵$X^{(t)} \in \mathbb{R}^{N \times P}$。预测任务是给定历史$T’$个时间段的图信号矩阵，预测未来$T$个时间段的图信号矩阵</p><script type="math/tex; mode=display">\left[X^{\left(t-T^{\prime}+1\right): t}, G\right] \stackrel{f}{\rightarrow}\left[X^{(t+1):(t+T)}\right]</script><p>$X^{\left(t-T^{\prime}+1\right): t} \in \mathbb{R}^{N \times P \times T’}$，$X^{(t+1):(t+T)} \in \mathbb{R}^{N \times P \times T}$</p><p>本文中用到的图卷积定义如下：</p><script type="math/tex; mode=display">\boldsymbol{\theta}_{\star G} \boldsymbol{X}=\rho\left(\tilde{\boldsymbol{D}}^{-1} \tilde{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{\theta}\right)</script><h1><span id="4-mra-bgcn">4. MRA-BGCN</span></h1><p>该模型包括2部分：bicomponent图卷积模块、多范围attention模块。</p><ul><li>bicomponent图卷积模块包括节点图卷积、边图卷积，可以显式建模节点和边的相关性。</li><li>多范围attention层聚合不同范围的邻居信息，并学习它们的重要性权重。</li><li>除此之外，我们还将MRA-BGCN和RNN结合起来建模时间依赖。</li></ul><p><img src="/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/f2.jpg" alt=""></p><h2><span id="41-bicomponent-gcn">4.1. Bicomponent GCN</span></h2><p>首次创建2个图：节点图、边图</p><p>$G=(V,E,A)$表示<strong>节点图</strong>，$|V|=N$，每个节点表示sensor，边表示sensor之间的距离，邻接矩阵是权重邻接矩阵。</p><p>$G_e=(V_e,E_e,A_e)$表示<strong>边图</strong>,其中$|V_e|=|E|$，每个节点表示$E$中一条边。在邻接矩阵$A_e$中，有2种边的连接模式。</p><ul><li><p>上下游连接：如下图a所示，边(i,j)是边(j,k)的上游边，因此这2条边是相关的。如果点j有很多邻接，也就是说点j的度很大，则边(i,j)和边(j,k)的相关性就变得很弱。我们使用下面的公式计算这2条边的相关性。</p><script type="math/tex; mode=display">\begin{array}{c}A_{e,(i \rightarrow j),(j \rightarrow k)}=A_{e,(j \rightarrow k),(i \rightarrow j)}=\exp \left(-\frac{\left(\mathrm{deg}^{-}(j)+\mathrm{deg}^{+}(j)-2\right)^{2}}{\sigma^{2}}\right)\end{array}</script><p>其中$\mathrm{deg}^{-}(j)$和$\mathrm{deg}^{+}(j)$表示点j的入度和出度。$\sigma$表示节点度的标准差。</p></li><li><p>竞争连接：当2个节点共享一个源节点或目标节点，可能会争夺交通资源，使得这2条边产生竞争关系。如下图b所示。边(j,k)和边(i,k)共享目标节点k，如果源节点有很多邻居，则(j,k)和(i,k)的竞争连接就会变强。使用下面的公式计算这2条边的连接权重：</p><script type="math/tex; mode=display">\begin{array}{l}A_{e,(i \rightarrow k),(j \rightarrow k)}=A_{e,(j \rightarrow k),(i \rightarrow k)}= \exp \left(-\frac{\left(\mathrm{deg}^{+}(i)+\mathrm{deg}^{+}(j)-2\right)^{2}}{\sigma^{2}}\right)\end{array}</script></li></ul><p><img src="/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/f3.jpg" alt=""></p><p>根据上面2种连接，构建好边图$G_e$，如图2所示，bicomponent图卷积可以建模节点和边的相关性。</p><p><img src="/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/f4.png" alt=""></p><p>$X^{(l-1)}$是第$l$层节点图卷积层的输入，$Z^{(l-1)}$是第$l$层边图卷积的输入。$M \in \mathbb{R}^{|V| \times |E|}$表示节点和边的关联矩阵。$M_{i,(i,j)}=M_{j,(i,j)}=1$，通过$MZ^{(\cdot)}$聚合每个节点的边的表示。$M^TX^{(\cdot)}$聚合每条边的节点表示。$W_b$是可学习的矩阵，从原始的节点输入$X^{(0)}$转换成边输入$Z^{(0)}$</p><h2><span id="42-multi-range-attention">4.2. Multi-Range Attention</span></h2><p>提出多范围attention机制来自动学习不同范围邻居的权重。</p><p>通过bicomponent图卷积，我们可以得到不同范围邻居的节点表示：$\boldsymbol{X}=\left\{\boldsymbol{X}^{(1)}, \boldsymbol{X}^{(2)}, \cdots, \boldsymbol{X}^{(k)}\right\}, \boldsymbol{X}^{(l)} \in \mathbb{R}^{|V| \times F}$。多范围attention层主要是就是不同范围的邻居信息，形成一个整合的节点表示。为了实现这个目标，首先一个共享的线性变换成$W_a \in \mathbb{R}^{F \times F’}$，作用在不同层的节点表示$X^{(l)}$上。</p><p>对于节点$i$，首先进行线性变换$W_aX^{(l)}_i$，$u$表示邻居上下文嵌入向量，是可学习参数。</p><script type="math/tex; mode=display">\begin{array}{c}e_{i}^{(l)}=\left(\boldsymbol{W}_{\mathrm{a}} \boldsymbol{X}_{i}^{(l)}\right)^{\mathrm{T}} \boldsymbol{u} \\\\a_{i}^{(l)}=\operatorname{SoftMax}_{l}\left(e_{i}^{(l)}\right)=\frac{\exp \left(e_{i}^{(l)}\right)}{\sum_{l=1}^{k} \exp \left(e_{i}^{(l)}\right)}\\\\\boldsymbol{h}_{i}=\sum_{l=1}^{k} a_{i}^{(l)} \boldsymbol{X}_{i}^{(l)}\end{array}</script><p>通过多范围Attention机制，对不同跳的邻居赋予不同的权重，得到最终的图信号矩阵。</p><h2><span id="43-结合rnn">4.3. 结合RNN</span></h2><p>参考DCRNN，把GRU中的全连接操作全都换成MRA-BGCN操作。</p><p><img src="/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/f5.jpg" alt=""></p><p><img src="/2020/09/17/Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting/f4.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1911/1911.12093.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表于AAAI2020&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
    <link href="http://yoursite.com/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/"/>
    <id>http://yoursite.com/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/</id>
    <published>2020-09-11T07:10:53.000Z</published>
    <updated>2020-09-11T11:37:51.529Z</updated>
    
    <content type="html"><![CDATA[<p>2018ICLR的一篇论文：<a href="https://arxiv.org/abs/1707.01926" target="_blank" rel="noopener">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</a></p><a id="more"></a><p>时空预测主要有以下挑战：</p><ol><li>基于道路网络，存在复杂的空间依赖</li><li>交通状态会随着时间变化，存在动态性</li><li>long-term预测比较困难</li></ol><p>基于以上问题，本文根据交通流量的扩散过程，构建一个有向图，并且引入扩散卷积+RNN=DCRNN，融合时间和空间依赖，预测交通流量。DCRNN通过在图上进行双向随机游走来捕获空间依赖，使用encoder-decoder来捕获时间依赖。</p><!-- TOC --><ul><li><a href="#1-介绍">1. 介绍</a></li><li><a href="#2-问题定义">2. 问题定义</a></li><li><a href="#3-dcrnn">3. DCRNN</a><ul><li><a href="#31-空间依赖建模">3.1. 空间依赖建模</a></li><li><a href="#32-时间动态性建模">3.2. 时间动态性建模</a></li><li><a href="#33-encoder-deocder">3.3. Encoder-Deocder</a></li></ul></li><li><a href="#4-实验">4. 实验</a><ul><li><a href="#41-数据集">4.1. 数据集</a></li><li><a href="#42-实验结果">4.2. 实验结果</a></li></ul></li></ul><!-- /TOC --><h1><span id="1-介绍">1. 介绍</span></h1><p>本文研究的主要任务是：在道路图上进行交通速度预测。</p><p>下图中给出了3条路的交通速度，道路1和道路3虽然离得很近，但是速度模式却完全不同，说明<strong>交通速度的空间结构是非欧式且有向的</strong>。</p><p><img src="/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/road.jpg" alt=""></p><p><strong>DCRNN = 扩散卷积 + encoder-decoder + 采样技术</strong></p><h1><span id="2-问题定义">2. 问题定义</span></h1><p>首先构建一个有向图。</p><ul><li>节点：sensor线圈感知器，$N$个节点</li><li>边的权重：2个sensor在道路上的距离，$W \in \mathbb{R}^{N \times N}$表示图的权重邻接矩阵</li><li>图信号矩阵$X \in \mathbb{R}^{N \times P}$,表示每个节点的traffic flow，speed</li><li>给定历史$T’$个时间段，预测未来$T$个时间段</li></ul><script type="math/tex; mode=display">\left[\boldsymbol{X}^{\left(t-T^{\prime}+1\right)}, \cdots, \boldsymbol{X}^{(t)} ; \mathcal{G}\right] \stackrel{h(\cdot)}{\longrightarrow}\left[\boldsymbol{X}^{(t+1)}, \cdots, \boldsymbol{X}^{(t+T)}\right]</script><h1><span id="3-dcrnn">3. DCRNN</span></h1><h2><span id="31-空间依赖建模">3.1. 空间依赖建模</span></h2><p>在图上进行随机游走，重启概率是$\alpha \in [0,1]$，状态转移矩阵$D^{-1}_OW$.其中$D_O=diag(W \mathbf{1})表示图的出度$, $\mathbf{1}$表示全1的向量。经过很多次的随机游走，这样的马尔科夫过程逐渐收敛到一个静态的分布$\mathcal{P} \in \mathbb{R}^{N \times N}$，其中第$i$行$\mathcal{P}_{i,:} \in \mathbb{R}^N$表示从节点$i$到其他节点的扩散可能性大小。<br>下面是在图上无限次随机游走的转移矩阵</p><script type="math/tex; mode=display">\mathcal{P}=\sum_{k=0}^{\infty} \alpha(1-\alpha)^{k}\left(\boldsymbol{D}_{O}^{-1} \boldsymbol{W}\right)^{k}</script><p>其中$k$表示转移的次数，我们通常使用有限次的$K$步转移矩阵。</p><p><strong>扩散卷积</strong></p><p>下面给出扩散卷积的计算公式：</p><script type="math/tex; mode=display">\boldsymbol{X}_{:, p} \star_{\mathcal{G}} f_{\boldsymbol{\theta}}=\sum_{k=0}^{K-1}\left(\theta_{k, 1}\left(\boldsymbol{D}_{O}^{-1} \boldsymbol{W}\right)^{k}+\theta_{k, 2}\left(\boldsymbol{D}_{I}^{-1} \boldsymbol{W}^{\top}\right)^{k}\right) \boldsymbol{X}_{:, p} \quad \text { for } p \in\{1, \cdots, P\}</script><p>这是<strong>一个特征</strong>的扩散卷积操作。</p><p>其中$p$表示第$p$个特征，为每个特征计算一个扩散性。$\theta \in \mathbb{R}^{K \times 2}$是扩散卷积核参数，$\boldsymbol{D}_{O}^{-1} \boldsymbol{W}$和$\boldsymbol{D}_{I}^{-1} \boldsymbol{W}^{\top}$表示扩散过程中的转移矩阵和逆转移矩阵。</p><p><strong>扩散卷积层</strong></p><p>基于上面的扩散卷积操作，定义扩散卷积层，将每个节点$P$维特征映射成$Q$维特征。</p><script type="math/tex; mode=display">\boldsymbol{H}_{:, q}=\boldsymbol{a}\left(\sum_{p=1}^{P} \boldsymbol{X}_{:, p} \star_{\mathcal{G}}  f_{\boldsymbol{\Theta}_{q, p,:,:}}\right) \quad \text { for } q \in\{1, \cdots, Q\}</script><p>其中$\boldsymbol{\Theta} \in \mathbb{R}^{Q \times P \times K \times 2}$, $\boldsymbol{\Theta}_{q, p,:,:} \in \mathbb{R}^{K \times 2}$</p><p>$\boldsymbol{X} \in \mathbb{R}^{N \times P}$表示输入图信号矩阵，$\boldsymbol{H} \in \mathbb{R}^{N \times Q}$表示输出图信号矩阵。在扩散卷积层，假设每个节点有3个特征，则就有3个扩散表示，然后再把这3个扩散表示加起来。</p><p><strong>和谱图卷积的关系</strong></p><p>扩散卷积可以定义在有向图上或无向图上。当应用在无向图上，现在的谱图卷积，例如ChebNet就可以看做是扩散卷积的特例。</p><h2><span id="32-时间动态性建模">3.2. 时间动态性建模</span></h2><p>利用GRU来建模时间依赖。将GRU中矩阵相乘，全都变成扩散卷积操作。</p><p><img src="/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/scrnn.jpg" alt=""></p><h2><span id="33-encoder-deocder">3.3. Encoder-Deocder</span></h2><p>因为本文在时间上是多对多的预测，这里使用seq2seq架构。其中encoder和decoder都是DCGRU模型。<br>在训练阶段，将历史时间序列输入到encoder中，然后使用最后一个时间段的隐藏状态初始化decoder。</p><p><img src="/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/dcrnn.jpg" alt=""></p><p><strong>【注意】</strong></p><ul><li>在训练阶段，decoder使用上一个时间段的真值作为输入，进行预测</li><li>在测试阶段，由于获取不到真值，使用上一个时间段模型的预测值作为输入。</li></ul><p>但是上面这种训练方法会导致训练和测试的输入分布不同，导致预测性能下降，为了解决这个问题，本文在训练阶段，不全部使用真值作为decoder的输入，而是以概率$\epsilon_i$输入真值，以$1-\epsilon_i$输入上个时间段的预测值。这样可以保证训练和预测中，decoder的输入分布不会差别太多。</p><h1><span id="4-实验">4. 实验</span></h1><h2><span id="41-数据集">4.1. 数据集</span></h2><p>2个数据集</p><ul><li>METR-LA：洛杉矶高速公路的车辆速度，207个节点</li><li>PEMS-BAY：加州，325个节点</li></ul><p>下图是这2个数据集中节点的分布情况。</p><p><img src="/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/sensor.jpg" alt=""></p><p>这2个数据集，都是5min聚合一次，使用Z-score归一化。训练集:验证集:测试集=7:1:2。<br>根据任意2个节点的距离构建邻接矩阵，如果$\operatorname{dist}\left(v_{i}, v_{j}\right) &lt;= k$，则邻接矩阵中为0，否则为$W_{i j}$</p><script type="math/tex; mode=display">W_{i j}=\exp \left(-\frac{\operatorname{dist}\left(v_{i}, v_{j}\right)^{2}}{\sigma^{2}}\right)</script><h2><span id="42-实验结果">4.2. 实验结果</span></h2><p><img src="/2020/09/11/Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting/result.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018ICLR的一篇论文：&lt;a href=&quot;https://arxiv.org/abs/1707.01926&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
    <link href="http://yoursite.com/2020/08/23/Graph-WaveNet-for-Deep-Spatial-Temporal-Graph-Modeling/"/>
    <id>http://yoursite.com/2020/08/23/Graph-WaveNet-for-Deep-Spatial-Temporal-Graph-Modeling/</id>
    <published>2020-08-23T08:54:35.000Z</published>
    <updated>2020-09-11T11:49:09.219Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1906.00121.pdf" target="_blank" rel="noopener">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</a></p><a id="more"></a><!-- TOC --><ul><li><a href="#1-动机">1. 动机</a></li><li><a href="#2-问题定义">2. 问题定义</a></li><li><a href="#3-模型设计">3. 模型设计</a><ul><li><a href="#31-图卷积层">3.1. 图卷积层</a></li><li><a href="#32-时间卷积层">3.2. 时间卷积层</a></li></ul></li></ul><!-- /TOC --><h1><span id="1-动机">1. 动机</span></h1><p>现在的时空图建模都是在一个静态图上进行建模，即图的邻接矩阵不变，并且现有的方法一般使用RNN或CNN来捕获时间特征，不能捕获长期的时间依赖。为了解决这2个限制，提出Graph WaveNet，图的邻接矩阵随时间变化，在时间维度上使用1D空洞卷积来捕获长期依赖。</p><p>为了捕获时空数据，现在一般有2种方法：</p><ol><li>GCN+RNN</li><li>GCN+CNN</li></ol><p>但是这2种方法中2个节点的相互依赖都建立在连接的基础上，但有些节点不连接仍然存在相互依赖的关系；当前的这种方法没有很有效的学习到时间依赖，RNN耗时且存在梯度消失问题，CNN可以并行，但需要堆叠很多层。</p><h1><span id="2-问题定义">2. 问题定义</span></h1><p>定义图$G=(V,E)$,其中邻接矩阵$A$非0即1，在第$t$个时间步，图信号矩阵$X^{(t)}\in \mathbb{R}^{N \times D}$</p><p>给定图$G$和历史$S$个时间段的图信号矩阵，预测未来$T$个时间段的图信号矩阵。</p><p><img src="/2020/08/23/Graph-WaveNet-for-Deep-Spatial-Temporal-Graph-Modeling/问题定义.jpg" alt=""></p><h1><span id="3-模型设计">3. 模型设计</span></h1><p><img src="/2020/08/23/Graph-WaveNet-for-Deep-Spatial-Temporal-Graph-Modeling/模型图.jpg" alt=""></p><h2><span id="31-图卷积层">3.1. 图卷积层</span></h2><p>在介绍之前，我们先看一下DCRNN提出的扩散矩阵操作：</p><p><strong>1. 无向图</strong></p><script type="math/tex; mode=display">\mathbf{Z}=\sum_{k=0}^{K} \mathbf{P}^{k} \mathbf{X} \mathbf{W}_{\mathbf{k}}</script><p>其中$\mathbf{P}$是转移矩阵，如果是无向图的话，$\mathbf{P}=\mathbf{A} / \operatorname{rowsum}(\mathbf{A})$</p><p><strong>2. 有向图</strong></p><p>如果是有向图的话，需要计算入的方向和出的方向。<br>$\mathbf{P}_{f}=\mathbf{A} / \operatorname{rowsum}(\mathbf{A})$，$\mathbf{P}_{b}=\mathbf{A}^{\mathbf{T}} /$rowsum$\left(\mathbf{A}^{\mathbf{T}}\right)$</p><p>此时的图卷积操作就变成：</p><script type="math/tex; mode=display">\mathbf{Z}=\sum_{k=0}^{K} \mathbf{P}_{f}^{k} \mathbf{X} \mathbf{W}_{k 1}+\mathbf{P}_{b}^{k} \mathbf{X} \mathbf{W}_{k 2}</script><p>在本文中，提出一个<strong>自适应的邻接矩阵</strong>$\tilde{A}_{adp}$，即邻接矩阵是学出来的，并不是提前定义好的。</p><p>首先初始化2个节点嵌入矩阵$E_1,E_2 \in \mathbb{R}^{N \times c}$，</p><script type="math/tex; mode=display">\tilde{\mathbf{A}}_{a d p}=\operatorname{SoftMax}\left(\operatorname{ReLU}\left(\mathbf{E}_{1} \mathbf{E}_{2}^{T}\right)\right)</script><p>通过上面的式子，可以得到自适应的邻接矩阵$\tilde{A}_{adp}$，下面定义图卷积层的操作。</p><p><strong>1. 如果图的结构已知</strong><br>   参考DCRNN的扩散卷积的操作</p><ul><li><p>无向图</p><script type="math/tex; mode=display">\mathbf{Z}=\sum_{k=0}^{K} \mathbf{P}^{k} \mathbf{X} \mathbf{W}_{k 1}+\tilde{\mathbf{A}}_{a p t}^{k} \mathbf{X} \mathbf{W}_{k 3}</script></li><li><p>有向图</p><script type="math/tex; mode=display">\mathbf{Z}=\sum_{k=0}^{K} \mathbf{P}_{f}^{k} \mathbf{X} \mathbf{W}_{k 1}+\mathbf{P}_{b}^{k} \mathbf{X} \mathbf{W}_{k 2}+\tilde{\mathbf{A}}_{a p t}^{k} \mathbf{X} \mathbf{W}_{k 3}</script></li></ul><p><strong>2. 图的结构未知</strong></p><script type="math/tex; mode=display">\mathbf{Z}=\sum_{k=0}^{K} \tilde{\mathbf{A}}_{a p t}^{k} \mathbf{X} \mathbf{W}_{k}</script><p>通过自适应的图卷积得到图中节点嵌入</p><h2><span id="32-时间卷积层">3.2. 时间卷积层</span></h2><p>使用1D空洞卷积TCN来捕获长期的时间依赖，因为空洞卷积可以扩大感受野的范围，可以捕获到更长的时间。同时，这里使用了门控TCN，</p><script type="math/tex; mode=display">\mathbf{h}=g\left(\boldsymbol{\Theta}_{1} \star \mathcal{X}+\mathbf{b}\right) \odot \sigma\left(\boldsymbol{\Theta}_{2} \star \mathcal{X}+\mathbf{c}\right)</script><p>输入数据$\mathcal{X} \in \mathbb{R}^{N \times D \times S}$，$\star$表示1D空洞卷积操作。$g(\cdot)$使用Tanh激活函数。</p><p>下面介绍模型的步骤</p><p><img src="/2020/08/23/Graph-WaveNet-for-Deep-Spatial-Temporal-Graph-Modeling/模型图.jpg" alt=""></p><p>求自适应邻接矩阵—&gt;T-GCN—&gt;GCN</p><ol><li>首先模型的输入维度是(batch_size,D,N,T),首先经过$1*1$的2D卷积层，将节点特征D转换维度，变成(batch_size,D1,N,T)</li><li>计算自适应邻接矩阵，首先初始化2个可学习的节点嵌入矩阵$\mathbf{E}_1,\mathbf{E}_2$,维度都是$\mathbb{R}^{N \times 10}$,然后使用下面的式子计算$\mathbf{A}_{adp}=F.softmax(F.relu(torch.mm(E1, E2)), dim=1)$</li><li><p>将(batch_size,D1,N,T)输入到Gated-TCN中，首先输入到TCN-a（2D卷积当1D卷积用，卷积核中有1维为1）中，2D卷积的输入维度是D1，输出维度是D2，卷积核大小为(1,k)，然后再经过Tanh，输出维度(batch_size,D2,N,T)</p><script type="math/tex; mode=display">Tanh \left(\boldsymbol{\Theta}_{1} \star \mathcal{X}+\mathbf{b}\right)</script></li><li>将(batch_size,D1,N,T)输入到TCN-b（2D卷积当1D卷积用，卷积核中有1维为1）中，2D卷积的输入维度是D1，输出维度是D2，卷积核大小为(1,k)，然后再经过Sigmoid，输出维度(batch_size,D2,N,T)<script type="math/tex; mode=display">\sigma\left(\boldsymbol{\Theta}_{2} \star \mathcal{X}+\mathbf{c}\right)</script></li><li>将步骤3和步骤4的输出逐元素相乘<script type="math/tex; mode=display">\mathbf{h}=Tanh \left(\boldsymbol{\Theta}_{1} \star \mathcal{X}+\mathbf{b}\right) \odot \sigma\left(\boldsymbol{\Theta}_{2} \star \mathcal{X}+\mathbf{c}\right)</script></li><li>将步骤5的输出(batch_size,D2,N,T)，输入到skip-conv（1D卷积），改变通道维度为(batch_size,D3,N,T)存储在skip变量中，一共有K层，将K层G-TCN的输出加起来得到(batch_size,D3,N,T)</li><li><p>下面进行GCN操作，将步骤5的输出(batch_size,D2,N,T)=x，假设有2个邻接矩阵，一个是已知的邻接矩阵$\mathbf{P}$,一个是自适应邻接矩阵$\mathbf{A}_{a p t}$，先遍历第一个邻接矩阵，$\mathbf{P}X$保存取来，然后在$\mathbf{P}^2X$，直到$\mathbf{P}^kX$，然后再求$\mathbf{A}_{a p t}X,\mathbf{A}_{a p t}^2X,\mathbf{A}_{a p t}^kX$。一共有2个邻接矩阵，每个邻接矩阵都有K个值，将这2K个值在通道维上拼接，得到$(batch_size,2<em>K</em>D2,N,T)$，然后在经过一个全连接，将维度映射成D1，即(batch_size,D1,N,T)</p><script type="math/tex; mode=display">\mathbf{Z}=\sum_{k=0}^{K} \mathbf{P}^{k} \mathbf{X} \mathbf{W}_{k 1}+\tilde{\mathbf{A}}_{a p t}^{k} \mathbf{X} \mathbf{W}_{k 3}</script></li><li><p>将步骤7的输出和步骤1的输出做残差连接，直接相加，再经过一个BN层，完成一个block。接下来就再次经过G-TCN和GCN，重复步骤3-7</p></li><li>所有的block完成之后，输出维度(batch_size,D1,N,T),但我们并不是要GCN的输出，而是需要每个TGCN的输出保存在skip中，维度(batch_size,D3,N,T)，先使用ReLU激活，然后使用$1 \times 1$的2D卷积，变成维度(batch_size,D4,N,T),再ReLU，然后是$1 \times 1$卷积</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1906.00121.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Graph WaveNet for Deep Spatial-Temporal Graph Modeling&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
      <category term="GCN" scheme="http://yoursite.com/tags/GCN/"/>
    
  </entry>
  
  <entry>
    <title>稀疏数据处理</title>
    <link href="http://yoursite.com/2020/08/19/%E7%A8%80%E7%96%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2020/08/19/稀疏数据处理/</id>
    <published>2020-08-19T03:34:45.000Z</published>
    <updated>2020-09-08T09:07:05.871Z</updated>
    
    <content type="html"><![CDATA[<p>在实际数据中，经常会遇到数据稀疏的问题，即数据中存在大量的0，且非零元素呈不规律分布。这就是稀疏矩阵。</p><a id="more"></a><p><img src="/2020/08/19/稀疏数据处理/系数矩阵.jpg" alt=""></p><h1><span id="1-稀疏矩阵处理">1. 稀疏矩阵处理</span></h1><p>通常，为了处理稀疏性矩阵，我们通常会压缩行和列；或者通过PCA/SVD进行降维。</p><p>矩阵压缩</p><p>降维</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在实际数据中，经常会遇到数据稀疏的问题，即数据中存在大量的0，且非零元素呈不规律分布。这就是稀疏矩阵。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="数据稀疏" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%A8%80%E7%96%8F/"/>
    
  </entry>
  
  <entry>
    <title>Excel转换为Latex</title>
    <link href="http://yoursite.com/2020/08/14/Excel%E8%BD%AC%E6%8D%A2%E4%B8%BALatex/"/>
    <id>http://yoursite.com/2020/08/14/Excel转换为Latex/</id>
    <published>2020-08-14T15:15:01.000Z</published>
    <updated>2020-08-21T15:20:17.278Z</updated>
    
    <content type="html"><![CDATA[<p>转载</p><p><a href="https://blog.csdn.net/Jiajikang_jjk/article/details/80788501" target="_blank" rel="noopener">使用Excel中导出latex代码的表格</a></p><a id="more"></a><p>使用Latex创建表格的一些语法</p><p><a href="https://cloud.tencent.com/developer/article/1635392" target="_blank" rel="noopener">Latex论文表格画法</a></p><ul><li><p>表格占2列或1列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;table*&#125;[htb]</span><br><span class="line">  \centering</span><br><span class="line">  \caption&#123;Performance comparison of different approaches for traffic flow forecasting.&#125;</span><br><span class="line">  \resizebox&#123;0.95\textwidth&#125;&#123;!&#125;&#123; % If your table exceeds the column or page width, use this command to reduce it slightly</span><br><span class="line">  \begin&#123;tabular&#125;&#123;c|c|ccccccccc&#125;</span><br><span class="line">  .....</span><br><span class="line">  .....</span><br><span class="line">  \end&#123;tabular&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  \label&#123;table2&#125;</span><br><span class="line">\end&#123;table*&#125;</span><br></pre></td></tr></table></figure><p><strong>其中<code>*</code>表示表格占2列</strong>,如果表格太宽，即使占了2列，表格还是溢出边界，<strong>使用<code>\resizebox</code>来调整表格的宽度</strong>，这里将表格变成原来的<code>0.95</code>倍，这样表格中的字体就会自动变小，表格左右宽度也会变窄。</p><p>在<strong>表格后面有<code>[htb]</code></strong>，完整的版本应该是<code>[htbp]</code>，表示表格的浮动</p><ul><li><code>[h]</code>:<code>here</code>表格放在当前位置</li><li><code>[t]</code>:<code>top</code>表格放在页面的首部</li><li><code>[b]</code>:<code>bottom</code>表格放在页面的底部</li><li><code>[p]</code>:将表格放在浮动对象的页面上</li></ul><p><strong>如果表格只占1列，不使用<code>*</code></strong>,同时表格的宽度可以变成<code>0.45</code>，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;table&#125;[htb]</span><br><span class="line">  \centering</span><br><span class="line">  \caption&#123;Performance comparison of different approaches for traffic flow forecasting.&#125;</span><br><span class="line">  \resizebox&#123;0.45\textwidth&#125;&#123;!&#125;&#123; % If your table exceeds the column or page width, use this command to reduce it slightly</span><br><span class="line">  \begin&#123;tabular&#125;&#123;c|c|ccccccccc&#125;</span><br><span class="line">  .....</span><br><span class="line">  .....</span><br><span class="line">  \end&#123;tabular&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  \label&#123;table2&#125;</span><br><span class="line">\end&#123;table&#125;</span><br></pre></td></tr></table></figure></li><li><p>列设置<br><code>{c|c|ccccccccc}</code>其中1个<code>c</code>表示一列，格式为居中，这是列必选参数。下面是对齐方式：</p><ul><li><code>l</code>表示左对齐</li><li><code>c</code>表示居中对齐</li><li><code>r</code>表示右对齐</li></ul><p><code>|</code>表示是否需要绘制竖线，<code>||</code>表示画两条紧相邻的竖线。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转载&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/Jiajikang_jjk/article/details/80788501&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;使用Excel中导出latex代码的表格&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Latex" scheme="http://yoursite.com/tags/Latex/"/>
    
  </entry>
  
  <entry>
    <title>latex安装与配置</title>
    <link href="http://yoursite.com/2020/07/27/latex%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2020/07/27/latex安装与配置/</id>
    <published>2020-07-27T03:46:01.000Z</published>
    <updated>2020-07-27T08:37:22.578Z</updated>
    
    <content type="html"><![CDATA[<p>最近开始使用Latex写论文，在此记录下Latex的安装和配置</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-软件准备">1. 软件准备</a></li><li><a href="#2-安装texlive">2. 安装TexLive</a></li><li><a href="#3-配置latex-workshop">3. 配置LaTEX Workshop</a></li><li><a href="#4-vscode界面使用">4. VSCode界面使用</a><ul><li><a href="#41-编译latex">4.1. 编译latex</a></li><li><a href="#42-查看pdf">4.2. 查看pdf</a></li><li><a href="#43-查看日志">4.3. 查看日志</a></li><li><a href="#44-正向搜索">4.4. 正向搜索</a></li><li><a href="#45-latex符号搜索">4.5. latex符号搜索</a></li><li><a href="#46-查看参考文献">4.6. 查看参考文献</a></li></ul></li><li><a href="#5-快捷键">5. 快捷键</a></li><li><a href="#6-错误解决">6. 错误解决</a></li></ul><!-- /TOC --><h1><span id="1-软件准备">1. 软件准备</span></h1><ol><li>TexLive<br>Latex的编译器，安装之后，就可以打开记事本，然后写latex语句，然后使用TEXLive编译，但是这种方法不太友好</li><li>VSCode<br>为了更好的编辑Latex，使用宇宙第一编辑器VSCode</li><li>LaTEX Workshop<br>安装完VSCode之后，在VSCode中安装一个插件：LaTEX Workshop</li></ol><h1><span id="2-安装texlive">2. 安装TexLive</span></h1><ol><li>在<a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/" target="_blank" rel="noopener">清华源</a>上下载texlive.iso镜像，大约有3.7G，等呀等…..等呀等…..终于下完了。</li><li><p>安装<br>这个过程比较漫长，需要从网上下载需要的文件</p><p><img src="/2020/07/27/latex安装与配置/1.jpg" alt=""></p></li><li><p>验证是否安装成功<br>使用<code>Win+R</code>，输出<code>cmd</code>，然后使用下面的命令查看latex是否安装成功</p><p><img src="/2020/07/27/latex安装与配置/8.jpg" alt=""></p></li><li><p>查看环境变量<br>TexLive在安装的时候，会<code>Path</code>中自动写入环境变量的值，如果没有写入，需要自己手动添加。</p><p><img src="/2020/07/27/latex安装与配置/9.jpg" alt=""></p></li></ol><h1><span id="3-配置latex-workshop">3. 配置LaTEX Workshop</span></h1><ol><li>文件—&gt;首选项—&gt;配置</li><li>输入<code>latex-workshop.latex.recipes</code>进行搜索，编辑<code>settings.json</code></li><li><p>配置</p><ul><li><code>latex-workshop.latex.tools</code>:编译工具,”xelatex”,”pdflatex”,”bibtex”是三个常用的编译工具。如果安装路径中包括中文的话，可以将<code>&quot;latex-workshop.latex.tools&quot;</code>:中的<code>%DOC%</code>更改为<code>%DOCFILE%</code>。</li><li><code>latex-workshop.latex.recipes</code>:latex编译的方案。可以使用”xelatex”，也可以使用”pdflatex”，或”xe-&gt;bib-&gt;xe-&gt;xe”,”pdf-&gt;bib-&gt;pdf-&gt;pdf”连续多次编译。<strong>其中”pdf-&gt;bib-&gt;pdf-&gt;pdf”适合英文期刊模板</strong>，我使用的也是这个。默认使用<code>latexmk</code>。我们配置的这些在左边菜单栏的”build Latex project”中都可以看到<br><img src="/2020/07/27/latex安装与配置/2.jpg" alt=""> </li><li><code>&quot;latex-workshop.view.pdf.viewer&quot;: &quot;tab&quot;</code>表示使用vscode内嵌的pdf来查看编译后的pdf文件</li><li><code>&quot;latex-workshop.latex.autoBuild.run&quot;: &quot;onFileChange&quot;或&quot;never&quot;</code>，表示在保存tex文件时是否自动编译</li><li><code>&quot;latex-workshop.latex.autoClean.run&quot;: &quot;onBuilt&quot;</code>表示在生成pdf后自动清除辅助文件</li></ul><pre><code class="lang-json">{ &quot;remote.SSH.remotePlatform&quot;: {     &quot;gpu27&quot;: &quot;linux&quot;,     &quot;gpu28&quot;: &quot;linux&quot;,     &quot;gpu-test&quot;: &quot;linux&quot;,     &quot;gpu29&quot;: &quot;linux&quot; }, &quot;editor.fontSize&quot;: 18, &quot;editor.suggestSelection&quot;: &quot;first&quot;, &quot;vsintellicode.modify.editor.suggestSelection&quot;: &quot;automaticallyOverrodeDefaultValue&quot;, &quot;python.jediEnabled&quot;: false, &quot;files.eol&quot;: &quot;\n&quot;, &quot;files.autoSave&quot;: &quot;afterDelay&quot;, &quot;python.languageServer&quot;: &quot;Microsoft&quot;, &quot;latex-workshop.view.pdf.viewer&quot;: &quot;tab&quot;, &quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;, &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onBuilt&quot;, &quot;latex-workshop.latex.clean.fileTypes&quot;: [     &quot;*.aux&quot;,     &quot;*.bbl&quot;,     &quot;*.blg&quot;,     &quot;*.idx&quot;,     &quot;*.ind&quot;,     &quot;*.lof&quot;,     &quot;*.lot&quot;,     &quot;*.out&quot;,     &quot;*.toc&quot;,     &quot;*.acn&quot;,     &quot;*.acr&quot;,     &quot;*.alg&quot;,     &quot;*.glg&quot;,     &quot;*.glo&quot;,     &quot;*.gls&quot;,     &quot;*.ist&quot;,     &quot;*.fls&quot;,     &quot;*.log&quot;,     &quot;*.fdb_latexmk&quot;     ], &quot;latex-workshop.latex.recipes&quot;: [     {         &quot;name&quot;: &quot;pdflatex-&gt;bibtex-&gt;pdflatex × 2&quot;,         &quot;tools&quot;: [             &quot;pdflatex&quot;,             &quot;bibtex&quot;,             &quot;pdflatex&quot;,             &quot;pdflatex&quot;         ]     },     {         &quot;name&quot;: &quot;xelatex&quot;,         &quot;tools&quot;: [             &quot;xelatex&quot;         ],     },     {         &quot;name&quot;: &quot;latexmk&quot;,         &quot;tools&quot;: [             &quot;latexmk&quot;         ]     },     {         &quot;name&quot;: &quot;pdflatex&quot;,         &quot;tools&quot;: [             &quot;pdflatex&quot;         ]     },     {         &quot;name&quot;: &quot;xe-&gt;bib-&gt;xe-&gt;xe&quot;,         &quot;tools&quot;: [             &quot;xelatex&quot;,             &quot;bibtex&quot;,             &quot;xelatex&quot;,             &quot;xelatex&quot;         ]     },     {         &quot;name&quot;: &quot;Compile Rnw files&quot;,         &quot;tools&quot;: [             &quot;rnw2tex&quot;,             &quot;latexmk&quot;         ]     },     {         &quot;name&quot;: &quot;Compile Jnw files&quot;,         &quot;tools&quot;: [             &quot;jnw2tex&quot;,             &quot;latexmk&quot;         ]     } ], &quot;latex-workshop.latex.tools&quot;: [     {         &quot;name&quot;: &quot;xelatex&quot;,         &quot;command&quot;: &quot;xelatex&quot;,         &quot;args&quot;: [             &quot;-synctex=1&quot;,             &quot;-interaction=nonstopmode&quot;,             &quot;-file-line-error&quot;,             &quot;%DOC%&quot;         ]     },     {       &quot;name&quot;: &quot;latexmk&quot;,       &quot;command&quot;: &quot;latexmk&quot;,       &quot;args&quot;: [         &quot;-synctex=1&quot;,         &quot;-interaction=nonstopmode&quot;,         &quot;-file-line-error&quot;,         &quot;-pdf&quot;,         &quot;%DOC%&quot;       ]     },     {       &quot;name&quot;: &quot;pdflatex&quot;,       &quot;command&quot;: &quot;pdflatex&quot;,       &quot;args&quot;: [         &quot;-synctex=1&quot;,         &quot;-interaction=nonstopmode&quot;,         &quot;-file-line-error&quot;,         &quot;%DOC%&quot;       ]     },     {       &quot;name&quot;: &quot;bibtex&quot;,       &quot;command&quot;: &quot;bibtex&quot;,       &quot;args&quot;: [         &quot;%DOCFILE%&quot;       ]     }   ], &quot;tabnine.experimentalAutoImports&quot;: true}</code></pre></li></ol><h1><span id="4-vscode界面使用">4. VSCode界面使用</span></h1><p>安装了LaTEX Workshop,左下角就会出现<code>TEX</code>这个菜单。我们点进去，就可以看到右边的这些功能。其中前三个是比较常用的功能。<br>第一个是编译latex，称为pdf<br>第二个是展示pdf<br>第三个是打开日志查错</p><h2><span id="41-编译latex">4.1. 编译latex</span></h2><p><img src="/2020/07/27/latex安装与配置/4.jpg" alt=""></p><p>如果直接点击<code>Build LaTex project</code>，而不是点”下三角”展开，会默认执行离行首最近的第一条相关命令，这里是<code>latexmk</code>编译（现在还没有配置，默认是pdflatex编译文件）</p><p><img src="/2020/07/27/latex安装与配置/5.jpg" alt=""></p><h2><span id="42-查看pdf">4.2. 查看pdf</span></h2><p>在编译成功后，会生成pdf文件，我们可以在VSCode内部来查看生成的pdf，点击左侧的<code>view in vscode tab</code>，就会生成左右2个窗口。也可以在用一些外部的pdf查看器来查看，这样就不是在vscode内部查看了，通常用<code>SumatraPDF</code>来查看，这个软件需要单独下载。这里就不介绍这种方式了<br><img src="/2020/07/27/latex安装与配置/6.jpg" alt=""></p><h2><span id="43-查看日志">4.3. 查看日志</span></h2><p>当编译latex出错时，我们可以点击左侧的<code>view latex compilier log</code>来查看日志。</p><p><img src="/2020/07/27/latex安装与配置/7.jpg" alt=""></p><h2><span id="44-正向搜索">4.4. 正向搜索</span></h2><p>点击<code>syntex from cursor</code>可以进行正向搜索。在tex文件中，鼠标的游标停留在某个位置，然后点击<code>syntex from cursor</code>，在右侧的pdf中会定位到鼠标所有在位置。</p><p><img src="/2020/07/27/latex安装与配置/11.jpg" alt=""></p><h2><span id="45-latex符号搜索">4.5. latex符号搜索</span></h2><p>有一些公式涉及到复杂的符号，可以点击左侧的<code>snippet panel</code>，在右侧会出现常用的符号，点击某一个符号，就会在tex文件中插入相应的latex表示。</p><p><img src="/2020/07/27/latex安装与配置/12.jpg" alt=""></p><h2><span id="46-查看参考文献">4.6. 查看参考文献</span></h2><p>在文件中提前写好<code>bib</code>参考文件，然后点击<code>open citation browser</code>,就会出现bib文件中所有的参考文献，然后点击某个参考文献，就可以在tex文件中插入参考文献。</p><p><img src="/2020/07/27/latex安装与配置/13.jpg" alt=""></p><h1><span id="5-快捷键">5. 快捷键</span></h1><ol><li><code>Ctrl+Alt+B</code>对tex进行编译,然后在VSCode的左下角就可以看到编译的状态。<br><img src="/2020/07/27/latex安装与配置/3.jpg" alt=""></li><li>在编译完之后，按<code>Ctrl+Alt+V</code>预览生成的pdf文件</li></ol><h1><span id="6-错误解决">6. 错误解决</span></h1><p>在完成上面的操作时，将一个AAAI的tex文件进行编译，然后报错<code>aaai2020.sty not found</code>，去AAAI的官网上下载论文latex模板，其中模板文件夹中有<code>aaai.bst,aaai20.sty</code>这2个文件，把这2个文件拷贝到tex所在的文件夹中，重新编译就成功啦。</p><p><img src="/2020/07/27/latex安装与配置/10.jpg" alt=""></p><p>参考资料</p><p><a href="https://blog.csdn.net/aiwei169/article/details/81431363" target="_blank" rel="noopener">TeX Live安装教程</a><br><a href="https://blog.csdn.net/qq280929090/article/details/104357697?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param" target="_blank" rel="noopener">基于Visual Studio Code的 LaTeX环境配置及使用示例</a><br><a href="https://blog.csdn.net/david394/article/details/107165422" target="_blank" rel="noopener">在VSCode中配置Latex编译环境</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开始使用Latex写论文，在此记录下Latex的安装和配置&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Latex" scheme="http://yoursite.com/tags/Latex/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统学习</title>
    <link href="http://yoursite.com/2020/07/25/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/07/25/推荐系统学习/</id>
    <published>2020-07-25T04:38:22.000Z</published>
    <updated>2020-08-22T15:22:38.944Z</updated>
    
    <content type="html"><![CDATA[<p>开始学习推荐系统的相关内容</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-推荐系统和搜索的区别">1. 推荐系统和搜索的区别</a></li><li><a href="#2-推荐系统lambda架构">2. 推荐系统Lambda架构</a></li><li><a href="#3-推荐算法流程">3. 推荐算法流程</a><ul><li><a href="#31-召回">3.1. 召回</a></li><li><a href="#32-排序">3.2. 排序</a><ul><li><a href="#321-lr">3.2.1. LR</a></li></ul></li></ul></li><li><a href="#4-推荐模型的构建流程">4. 推荐模型的构建流程</a></li><li><a href="#5-相似度计算">5. 相似度计算</a><ul><li><a href="#51-余弦相似度">5.1. 余弦相似度</a></li><li><a href="#52-皮尔逊相关系数">5.2. 皮尔逊相关系数</a></li><li><a href="#53-杰卡德jaccard相似度">5.3. 杰卡德Jaccard相似度</a></li></ul></li><li><a href="#6-召回算法">6. 召回算法</a><ul><li><a href="#61-协同过滤cf推荐算法">6.1. 协同过滤CF推荐算法</a><ul><li><a href="#611-基于用户的协同过滤">6.1.1. 基于用户的协同过滤</a></li><li><a href="#612-基于物品的协同过滤">6.1.2. 基于物品的协同过滤</a></li><li><a href="#613-基于模型的协同过滤">6.1.3. 基于模型的协同过滤</a><ul><li><a href="#6131-矩阵分解mf">6.1.3.1. 矩阵分解MF</a></li></ul></li></ul></li><li><a href="#基于内容的召回算法">基于内容的召回算法</a></li></ul></li><li><a href="#7-排序算法">7. 排序算法</a><ul><li><a href="#71-逻辑回归lr">7.1. 逻辑回归LR</a></li><li><a href="#72-fm">7.2. FM</a></li></ul></li><li><a href="#推荐系统的评价">推荐系统的评价</a></li></ul><!-- /TOC --><h1><span id="1-推荐系统和搜索的区别">1. 推荐系统和搜索的区别</span></h1><ul><li>推荐系统个性化强，用户被动的接受，希望能提供持续的服务，推荐主要是信息过滤系统。</li><li>搜索个性化弱，用户主动搜索，快速满足用户的需求。主要是构建稳定的信息流通通道</li></ul><p>什么时候才需要推荐：</p><ol><li>信息过载</li><li>需求不明确</li></ol><h1><span id="2-推荐系统lambda架构">2. 推荐系统Lambda架构</span></h1><p>推荐系统要素：</p><ol><li>前段界面</li><li>数据（Lambda架构）</li><li>业务知识</li><li>推荐算法</li></ol><ul><li><p><strong>推荐系统整体架构</strong></p><p><img src="/2020/07/25/推荐系统学习/推荐系统整体架构.jpg" alt=""></p></li></ul><p>下面介绍<strong>大数据Labmda架构</strong></p><ul><li>Lambda架构结合实时数据和Hadoop平台，提供实时的数据视图。简单说将离线计算和实时计算结合起来。</li><li><p>实时和离线的分层操作</p><ul><li><p>离线：</p><ul><li>分布式计算：Hadoop MapReduce、Spark</li><li>数据存储：HBase、Redis、MySQL</li></ul></li><li><p>实时</p><ul><li>流式处理数据</li><li>实时数据收集：Flume、Kafka</li><li>实时数据分析：Spark Streaming / storm、flink</li></ul></li></ul></li></ul><p>下面是Lambda的架构图。首先数据源从flume中得到。一部分做批处理，一部分做实时处理。批处理把数据放在数据库上，然后经过MapReduce算出一个阶段，缓存起来。然后实时这部分也会做实时计算，然后将这2部分的计算结果汇总，为推荐系统提供服务。</p><p>这2部分的相辅相成。离线服务算的时间比较长，比如10分钟得出一次结果，那在前端用户看到的结果10分钟都不会变。然后实时计算是基于离线计算的结果做一些微调。这样用户看到的推荐结果就会实时变化。</p><p><img src="/2020/07/25/推荐系统学习/lambda.jpg" alt=""></p><h1><span id="3-推荐算法流程">3. 推荐算法流程</span></h1><p>上面讲了整个推荐系统的架构，下面我们只关注推荐算法。推荐算法只有3个阶段：</p><ol><li>召回阶段<ul><li>这里的召回不要和分类问题的召回率混淆，这2者是不同的概念。推荐算法的召回是海选，就是先从很多内容中选出一些内容，作为接下来推荐的候选人。也就是说我们后面推荐的结果都是从这些候选内容中选的。所以召回阶段决定了最终推荐结果的天花板</li><li>召回阶段常用算法<ul><li>协同过滤 CF（基于物品和基于用户）</li><li>基于内容（根据用户的行为总结出用户的喜好，根据文本挖掘技术找到相似的商品）</li><li>基于隐语义</li></ul></li></ul></li><li>排序阶段<ul><li>对召回阶段找到候选内容进行排序</li><li>CTR估计(点击率估计)，可以使用逻辑回归，输出用户对某个物品点击的概率，然后将概率进行排序，选出topK的物品做为最终的推荐结果</li></ul></li><li>策略调整<ul><li>新用户数据少采取补充策略（冷启动）</li><li>地域热门：根据用户当前的定位，统计附近的热门帖子</li><li>全局热门：对某个时间段的帖子进行热门排序</li></ul></li></ol><p><img src="/2020/07/25/推荐系统学习/推荐算法.jpg" alt=""></p><p><img src="/2020/07/25/推荐系统学习/推荐算法2.jpg" alt=""></p><p>如果比较细致的看推荐系统，分为4个阶段：召回+粗排+精排+ReRanker</p><p><img src="/2020/07/25/推荐系统学习/推荐算法3.jpg" alt=""></p><p>召回目的如上所述；有时候因为每个用户召回环节返回的物品数量还是太多，怕排序环节速度跟不上，所以可以在召回和精排之间加入一个粗排环节，通过少量用户和物品特征，简单模型，来对召回的结果进行个粗略的排序，在保证一定精准的前提下，进一步减少往后传送的物品数量，粗排往往是可选的，可用可不同，跟场景有关。之后，是精排环节，使用你能想到的任何特征，可以上你能承受速度极限的复杂模型，尽量精准地对物品进行个性化排序。排序完成后，传给重排环节，传统地看，这里往往会上各种技术及业务策略，比如去已读、去重、打散、多样性保证、固定类型物品插入等等，主要是技术产品策略主导或者为了改进用户体验的。</p><p><img src="/2020/07/25/推荐系统学习/推荐系统.jpg" alt=""></p><h2><span id="31-召回">3.1. 召回</span></h2><p>召回是推荐系统的第一步，根据用户和商品的部分特征，从海量的物品中找出用户潜在感兴趣的物品，交给排序阶段。召回阶段需要快，所以模型不能太复杂。</p><p>现在召回环节一般采用多路召回，随便想到一个策略就可以做一路召回。</p><p><img src="/2020/07/25/推荐系统学习/召回.jpg" alt=""></p><p><a href="https://cloud.tencent.com/developer/ask/68639" target="_blank" rel="noopener">长尾效应与推荐系统的关系？</a></p><p>传统的召回思路：<br>先离线计算好商品的Embedding和用户的Embedding，然后在线上召回的时候，根据用户的Embedding和所有商品的Embedding内积，找出TopN。</p><ol><li>根据用户行为召回<br>将用户一系列的行为，使用GRU/Transformer来聚合用户行为，生成1个用户兴趣嵌入。有监督学习，标签可以是next iterm prediction</li><li>用户多兴趣拆分<br>上面根据用户行为生成1个用户兴趣嵌入，但是用户的兴趣可以有很多类，例如体育，购物等。用户多兴趣拆分的输入和上面一样，还是用户历史行为序列，但是输出的是用户多个兴趣嵌入，可以进行多兴趣点召回，避免单兴趣嵌入召回头部问题。可以看做是聚类问题，就是把不同的item聚到不同的兴趣类别中。</li><li>和知识图谱融合<br>首先构建一个用户和商品的行为图，如果用户对商品有行为产生，则建立一条边，这样就建立了用户-商品交互的二部图。这里面还隐藏这物品之间的一些知识。比如说用户点击过电影“泰坦尼克号”，这部电影是小李子主演的，我们就可以利用电影领域的知识图谱数据，来推荐小李子主演的其他电影给用户。</li><li>图神经网络模型召回<br>上面知识图谱是图神经网络的一个特例，但是知识图谱因为编码的是静态知识，而不是用户比较直接的行为数据，和具体应用距离较远。图中的节点是用户和商品，通过用户行为来建立边，边还可以带上权重，然后使用图神经网络，例如GraphSAGE来获取节点的嵌入。</li></ol><p><img src="/2020/07/25/推荐系统学习/召回模型.jpg" alt=""></p><h2><span id="32-排序">3.2. 排序</span></h2><h3><span id="321-lr">3.2.1. LR</span></h3><p>LR最早可以用在点击率预测中，LR是一个二分类，输出的概率表示用户点击的概率。</p><p>步骤如下：</p><ol><li><p>构造特征<br>将所有特征进行离散化,变成one-hot输入</p><p><img src="/2020/07/25/推荐系统学习/lr.jpg" alt=""></p></li><li>模型训练<br>根据输入的特征和label（用户点击1或者不点击0）来训练LR分类器</li><li>模型推荐<br>给定一个用户u，以及一批候选商品，对用户u如何推荐商品。通过上述方法计算用户u对候选商品中每个商品的点击点击得分，然后按照得分从大大小排序，推荐前N个物品。</li></ol><h1><span id="4-推荐模型的构建流程">4. 推荐模型的构建流程</span></h1><p><img src="/2020/07/25/推荐系统学习/推荐算法1.jpg" alt=""></p><ol><li>数据清洗阶段<ul><li>数据来源<ul><li>显性数据：打分，评论</li><li>隐性数据：历史订单、加购物车、页面浏览</li></ul></li></ul></li><li>特征工程<ul><li>user-item的评分矩阵</li><li>如果没有显式的评分矩阵，可以用一些用户的行为，比如加入购物车，单曲循环，页面停留时间等。</li></ul></li><li>算法<ul><li>协同过滤：计算相似度</li><li>矩阵分解</li></ul></li><li>产生推荐结果<ul><li>交叉推荐：买了手机，推荐手机壳</li><li>向上推荐：买了自行车，推荐跑车</li></ul></li></ol><h1><span id="5-相似度计算">5. 相似度计算</span></h1><p>相似度计算根据数据来选择。</p><ul><li>数据是实数值，评分情况</li><li>数据是布尔值，是否消费</li></ul><h2><span id="51-余弦相似度">5.1. 余弦相似度</span></h2><p>余弦相似度就是看2个向量夹角的cos值。余弦相似度范围[-1,1]，从负相关到正相关。</p><p>缺点：只和夹角有关，不关心向量长度。可能会有误差。对2个电影打分，一个人打分向量(5,8)，另一个人打分向量(0.5,1)，这2个向量虽然夹角很小，但是长度差别很大。</p><p><img src="/2020/07/25/推荐系统学习/余弦相似度.jpg" alt=""></p><h2><span id="52-皮尔逊相关系数">5.2. 皮尔逊相关系数</span></h2><p>对余弦相似度的优化，对向量做中心化，将向量a和b减去向量均值，在计算余弦相似度</p><p><img src="/2020/07/25/推荐系统学习/皮尔逊.jpg" alt=""></p><h2><span id="53-杰卡德jaccard相似度">5.3. 杰卡德Jaccard相似度</span></h2><p>Jaccard = 交集/并集</p><p>比如用户2喜欢物品(2,7,10),用户4喜欢物品(2,5,7,9)，2个用户的交集是物品(2,7)，所以交集个数为2，2个用户喜欢用品的并集是(2,5,7,9,10)，所以并集个数为5.则这2个用户的jaccard相似度=0.4</p><p><img src="/2020/07/25/推荐系统学习/杰卡德.jpg" alt=""></p><p>杰卡德相似度适合评分是0/1的情况</p><h1><span id="6-召回算法">6. 召回算法</span></h1><h2><span id="61-协同过滤cf推荐算法">6.1. 协同过滤CF推荐算法</span></h2><p><strong>协同过滤(Collaborative Filtering)的算法思想：物以类聚，人以群分</strong></p><p>协同过滤推荐算法分为2类：</p><ul><li><strong>基于用户的协同过滤</strong>user-based CF：和你喜好相似的人，那他喜欢的东西你也可能喜欢</li><li><strong>基于物品的协同过滤</strong>item-based CF：和你喜欢的东西相似的东西，你也可能喜欢</li><li><strong>基于模型的协同过滤</strong>，现在最主流的协同过滤类型，包含矩阵分解，聚类算法，深度学习，图模型等。</li></ul><p><strong>缺点：</strong></p><p>召回结果的候选item限定在用户的历史行为类目中，导致推荐结果越推越窄，难以发现长尾商品。</p><h3><span id="611-基于用户的协同过滤">6.1.1. 基于用户的协同过滤</span></h3><p>实现协同过滤的步骤：</p><ol><li>首先计算<strong>用户与用户的相似程度</strong><br>比如下图中，我们计算出用户1和用户3最相似</li><li>根据相似的人产生推荐结果<br>然后我们根据用户3的内容给用户1来推荐，用户3喜欢物品(1,4,8)，然后过滤到用户1已经喜欢的商品1，然后给用户1推荐物品4和8</li></ol><p><img src="/2020/07/25/推荐系统学习/评分矩阵.jpg" alt=""></p><h3><span id="612-基于物品的协同过滤">6.1.2. 基于物品的协同过滤</span></h3><ol><li>首先计算<strong>物品与物品的相似程度</strong><br>比如物品A和物品B相似</li><li>根据相似的物品产生推荐结果<br>用户A原先买过物品A，然后就给推荐物品B</li></ol><h3><span id="613-基于模型的协同过滤">6.1.3. 基于模型的协同过滤</span></h3><h4><span id="6131-矩阵分解mf">6.1.3.1. 矩阵分解MF</span></h4><p>MF（Matrix Factorization，矩阵分解）是一个简单的Embedding模型。给定用户和物品矩阵$A \in \mathbb{R}^{m \times n}$,然后对其进行分解成用户嵌入矩阵$U \in \mathcal{R}^{m \times d}$和商品矩阵$V \in \mathcal{R}^{n \times d}$。$d &lt;&lt; m和n$</p><p>矩阵分解的目标就是让$UV^T$的结果和$A$尽可能的相似。</p><p><img src="/2020/07/25/推荐系统学习/矩阵分解1.jpg" alt=""></p><p>在上面的目标函数中，只对观察到的obs的对$(i,j)$进行误差求和。即只对下图中为1的元素求误差，但是这并不能很好的推荐，泛化性能差。</p><p><img src="/2020/07/25/推荐系统学习/矩阵分解2.jpg" alt=""></p><p>因为我们需要对所有的值进行求误差，目标函数就变成下面这样：</p><p><img src="/2020/07/25/推荐系统学习/矩阵分解3.jpg" alt=""></p><p>最小化目标函数的方法包括：</p><ol><li>随机梯度下降SGD</li><li>加权交替最小二乘法WALS<br>在2个矩阵U和V中，对U和V进行交替更新。（1）固定U求解V，（2）固定V求解U</li></ol><h2><span id="基于内容的召回算法">基于内容的召回算法</span></h2><p>一般步骤是：</p><ol><li>先分词，</li><li>然后计算词的权重(TF-IDF)，即提取关键字</li><li>然后使用word2vec得到词向量，从词向量构建物品向量</li></ol><h1><span id="7-排序算法">7. 排序算法</span></h1><h2><span id="71-逻辑回归lr">7.1. 逻辑回归LR</span></h2><p>LR最早可以用在点击率预测中，LR是一个二分类，输出的概率表示用户点击的概率。</p><p>步骤如下：</p><ol><li><p>构造特征<br>将所有特征进行离散化,变成one-hot输入</p><p><img src="/2020/07/25/推荐系统学习/lr.jpg" alt=""></p></li><li>模型训练<br>根据输入的特征和label（用户点击1或者不点击0）来训练LR分类器</li><li>模型推荐<br>给定一个用户u，以及一批候选商品，对用户u如何推荐商品。通过上述方法计算用户u对候选商品中每个商品的点击点击得分，然后按照得分从大大小排序，推荐前N个物品。</li></ol><p>LR模型最大的缺陷就是人工特征工程，耗时费力费人力资源，能否将特征组合的能力体现在模型层面呢？就不用人工再去组合特征了。</p><h2><span id="72-fm">7.2. FM</span></h2><p>FM英文全称是“Factorization Machine”，简称FM模型，中文名“因子分解机”。</p><p>FM在2010年提出，核心在于特征组合，以此减少人工参与特征组合工作。FM的优势：</p><ol><li>FM能处理高维稀疏场景</li><li>FM具有线性的计算复杂度</li></ol><p><img src="/2020/07/25/推荐系统学习/FM.jpg" alt=""></p><p>FM在LR的基础上，引入任意2个特征的二阶特征组合，为每个特征，学习一个大小为k的一维向量。于是2个特征$x_i,x_j$的特征组合的权重值，通过特征对应的向量$v_i,v_j$的内积$<v_i,v_j>$来表示。</v_i,v_j></p><h1><span id="推荐系统的评价">推荐系统的评价</span></h1><ol><li><p>准确率</p><ul><li>学术角度：RMSE,MAE,点击预估</li><li>工程角度：A/B test对比不同的算法，在线上运行对关键指标的影响</li></ul></li><li><p>覆盖率<br>尽量照顾到大部分的产品</p></li><li>探索和利用<ul><li>利用：利用用户的历史行为，只给他推荐他曾经消费过的相似物品</li><li>探索：发现用户的新兴趣</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开始学习推荐系统的相关内容&lt;/p&gt;
    
    </summary>
    
      <category term="推荐系统" scheme="http://yoursite.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐系统" scheme="http://yoursite.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>AUC在推荐系统中的应用</title>
    <link href="http://yoursite.com/2020/07/21/AUC%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2020/07/21/AUC在推荐系统中的应用/</id>
    <published>2020-07-21T14:41:26.000Z</published>
    <updated>2020-09-09T14:41:08.878Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/uVZOfFayOzba5mALR5lHRQ" target="_blank" rel="noopener">参考资料</a></p><p>在推荐、广告系统中AUC是一个常见的指标。</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-介绍auc">1. 介绍AUC</a></li><li><a href="#2-排序问题中的auc">2. 排序问题中的AUC</a></li><li><a href="#3-auc的计算">3. AUC的计算</a></li></ul><!-- /TOC --><h1><span id="1-介绍auc">1. 介绍AUC</span></h1><p><a href="https://tracholar.github.io/machine-learning/2018/01/26/auc.html" target="_blank" rel="noopener">深入理解AUC</a></p><p>关于AUC，通常说是ROC线下面积。ROC横坐标假阳率，纵坐标真阳率。<br>如下图中AUC=0.5，表示不论样本真实label是0还是1，模型将以0.5的概率将其预测为正样本。这就和抛硬币没区别，这说明模型对正负样本没有区分能力。<br>我们训练的目标是让AUC越大越好。</p><p>在统计和机器学习中，常用AUC来评估二分类模型的性能。AUC全称是Area Under the Curve。</p><p><img src="/2020/07/21/AUC在推荐系统中的应用/roc.jpg" alt=""></p><p>AUC同时考虑了分类器对正例和负例的分类能力，在样本不均衡时，分类器依然能做出合理的评价。</p><p>另一种解释是：基于概率的解释，评估模型的排序能力。</p><h1><span id="2-排序问题中的auc">2. 排序问题中的AUC</span></h1><p>假如AUC=0.7，表示给定一个正样本和一个负样本，在70%的情况下，模型对正样本的打分高于负样本的打分。可以看出，我们只关心正负样本之间的分数高低，并不在乎具体的概率值。</p><p>对于Precision，Recall等指标，AUC只关注排序结果，不关注模型输出的概率值，所以适合排序业务。<br>正负样本被预测的gap越大，AUC越大。</p><h1><span id="3-auc的计算">3. AUC的计算</span></h1><p>将测试样本得到的概率从小到大排序，对于<strong>第$j$个正样本</strong>，假设它的排名是$r_j$，那就说明在这个正样本之前有$r_j-1$个样本，其中正样本个数为$j-1$个（因为这个正样本在所有正样本中排第j），那排在第j个正样本前面的负样本个数有$(r_j-1-(j-1))=r_j-j$个，也就是说，低于第$j$个正样本来说，其得分比随机取的一个负样本大（正样本的排名靠后）的概率是$\frac{r_j-j}{N_-}$,其中$N_-$是标签中负样本的个数，所以平均下来，随机取的正样本得到比负样本大的概率为：</p><script type="math/tex; mode=display">AUC=\frac{1}{N_+}\sum_{j=1}^{N_+}\frac{r_j-j}{N_-}=\frac{\sum_{j=1}^{N_+}r_j-N_+(N_++1)/2}{N_+N_-}</script><p>即需要求出以下3个值：</p><ol><li>所有正样本的排名，排名相加$\sum_{j=1}^{N_+}r_j$</li><li>label中正样本个数$N_+$</li><li>label中负样本个数$N_-$</li></ol><p>$prob=[0.4,0.5,0.2,0.8,0.7,0.9,0.6]$<br>$label=[0^{0.4},0^{0.5},1^{0.2},1^{0.8},1^{0.7},0^{0.9},1^{0.6}]$</p><p>首先对prob从小到大排序，并同时调整prob的顺序<br>$prob=[0.2,0.4,0.5,0.6,0.7,0.8,0.9,]$<br>$label=[1^{0.2},0^{0.4},0^{0.5},1^{0.6},1^{0.7},1^{0.8},0^{0.9}]$</p><p>其中正样本的排名分别是1,4,5,6，排名相加是16。<br>label正样本个数有4个，负样本个数有3个，</p><p>$AUC=\frac{16-4<em>5/2}{4</em>3}=0.5$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calAUC</span><span class="params">(prob,labels)</span>:</span></span><br><span class="line">    f = list(zip(prob,labels))</span><br><span class="line">    <span class="comment">#按照prob从小到大排序，对labels进行排序</span></span><br><span class="line">    rank = [cur_label <span class="keyword">for</span> cur_pro,cur_label <span class="keyword">in</span> sorted(f,key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>])]</span><br><span class="line">    <span class="comment">#找出正样本在排名</span></span><br><span class="line">    rankList = [i+<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rank)) <span class="keyword">if</span> rank[i]==<span class="number">1</span>]</span><br><span class="line">    posNum = <span class="number">0</span><span class="comment">#正样本的个数</span></span><br><span class="line">    negNum = <span class="number">0</span><span class="comment">#负样本的个数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labels)):</span><br><span class="line">        <span class="keyword">if</span>(labels[i]==<span class="number">1</span>):</span><br><span class="line">            posNum+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            negNum+=<span class="number">1</span></span><br><span class="line">    auc = <span class="number">0</span></span><br><span class="line">    auc = (sum(rankList)- (posNum*(posNum+<span class="number">1</span>))/<span class="number">2</span>)/(posNum*negNum)</span><br><span class="line">    print(auc)</span><br><span class="line">    <span class="keyword">return</span> auc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    prob=[<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.2</span>,<span class="number">0.8</span>,<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">0.6</span>]</span><br><span class="line">    label=[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">    calAUC(prob,label)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/uVZOfFayOzba5mALR5lHRQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考资料&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在推荐、广告系统中AUC是一个常见的指标。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="AUC" scheme="http://yoursite.com/tags/AUC/"/>
    
  </entry>
  
  <entry>
    <title>python输入怎么写</title>
    <link href="http://yoursite.com/2020/06/21/python%E8%BE%93%E5%85%A5%E6%80%8E%E4%B9%88%E5%86%99/"/>
    <id>http://yoursite.com/2020/06/21/python输入怎么写/</id>
    <published>2020-06-21T09:35:43.000Z</published>
    <updated>2020-06-21T09:47:14.170Z</updated>
    
    <content type="html"><![CDATA[<p>Leetcode刷多了不太记得python的输入怎么写了。在实际笔试或面试时，需要自己写输入和输出函数，所以在此记录下怎么使用Python读取数据。<br><a id="more"></a></p><p>在面试的时候，面试官只给你一个白板，最多给你定义好的函数名，其余都要自己写。给定的题目一般先读取数据，然后使用<code>print</code>输出最终的结果。</p><ul><li><code>print</code>可以写在<code>test</code>函数中</li><li>也可以将要输出的内容保存下来，作为<code>test</code>的<code>return</code>，然后在<code>main</code>中输出<br>下面是写程序的模板：</li><li><code>main</code>只关注输入和输出</li><li>其余的功能单独封装成函数，在<code>main</code>中调用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(param1,param2)</span>:</span></span><br><span class="line">    <span class="comment">#do something</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"xx"</span>)</span><br><span class="line">    <span class="comment">#或者return</span></span><br><span class="line">    <span class="keyword">return</span> xx</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#使用input读取数据</span></span><br><span class="line">    m = input()</span><br><span class="line">    n = list(map(int,input.split()))</span><br><span class="line"></span><br><span class="line">    test(m,n)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"xx"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Leetcode刷多了不太记得python的输入怎么写了。在实际笔试或面试时，需要自己写输入和输出函数，所以在此记录下怎么使用Python读取数据。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey</title>
    <link href="http://yoursite.com/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/"/>
    <id>http://yoursite.com/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/</id>
    <published>2020-06-02T01:41:59.000Z</published>
    <updated>2020-06-18T02:33:30.602Z</updated>
    
    <content type="html"><![CDATA[<p>这篇综述性论文介绍图神经网络在交通领域的应用。</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-摘要">1. 摘要</a></li><li><a href="#2-前言">2. 前言</a></li><li><a href="#3-研究方向">3. 研究方向</a></li><li><a href="#4-问题定义">4. 问题定义</a><ul><li><a href="#41-构建图">4.1. 构建图</a></li><li><a href="#42-构造邻接矩阵">4.2. 构造邻接矩阵</a></li></ul></li><li><a href="#5-前人的模型">5. 前人的模型</a><ul><li><a href="#51-gnn">5.1. GNN</a></li><li><a href="#52-rnn">5.2. RNN</a></li><li><a href="#53-tcn">5.3. TCN</a></li><li><a href="#54-seq2seq">5.4. Seq2Seq</a></li><li><a href="#55-gan">5.5. GAN</a></li></ul></li><li><a href="#6-挑战">6. 挑战</a><ul><li><a href="#61-空间依赖">6.1. 空间依赖</a></li><li><a href="#62-时间依赖">6.2. 时间依赖</a></li><li><a href="#63-时空依赖">6.3. 时空依赖</a></li><li><a href="#64-外部因素">6.4. 外部因素</a></li></ul></li><li><a href="#7-未来方向">7. 未来方向</a></li></ul><!-- /TOC --><h1><span id="1-摘要">1. 摘要</span></h1><p>在交通数据中，有很多数据以图的形式存在，为了充分利用其中的空间信息，很多模型使用图神经网络来处理交通图数据。本文针对交通领域的图网络模型进行总结。</p><h1><span id="2-前言">2. 前言</span></h1><p>在交通预测领域，（1）早期采用的方法有：ARIMA,VAR,Kalman过滤器等，然而，这些方法通常需要一些前提假设，例如数据是静态且线性相关，不能应用在实际数据中。（2）机器学习方法例如SVM，K近邻可以建模交通数据中的非线性相关性，但是模型结构较浅，且需要人工构造和选择特征，不能满足大量交通数据的应用需求。（3）深度学习方法，例如RNN无法捕获空间相关性，CNN无法应用在图数据中，并且CNN更关注local相关性，忽略了global相关性。（4）图神经网络，使用图神经网络来解决交通领域的预测问题。<br>贡献总结如下：</p><ul><li>第一篇介绍图神经网络在交通领域应用的综述</li><li>系统地列出交通领域的研究方向和挑战</li><li>针对4种交通领域数据，介绍如何构建图</li><li>分析了5种应用在图交通领域的技术，介绍了它们的优缺点，以及变体</li><li>讨论了基于图网络的交通任务中4种常见的挑战，并总结对应的解决方案</li><li>收集数据集，开源代码</li></ul><h1><span id="3-研究方向">3. 研究方向</span></h1><p>给出了交通领域的一些研究问题<br><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/1.png" alt=""></p><ol><li>交通拥堵</li><li>交通需求<br>出租车、自行车、公共交通的需求预测，像滴滴，Uber等线上打车平台经常做这一类问题。</li><li>交通安全<br>预测交通事故的风险，严重程度</li><li>交通监控<br>主要通过监控的图像和视频检测车辆，行人检测。</li><li>自动驾驶<br>自动驾驶要求检测树木，道路，行人，一般和CV领域相关。</li></ol><p>图神经网络在交通领域的应用</p><ol><li>交通状态预测<br>交通状态：交通流量、速度、时间ETA、密度等。</li><li>交通需求预测<br>预测将来用户对出租车、自行车的需求</li><li>交通信号预测<br>减低用户在交叉路口的等待时间，避免交通拥堵</li><li>司机行为分类</li></ol><p>交通事故预测还没有用到图模型。</p><h1><span id="4-问题定义">4. 问题定义</span></h1><p>基于图的交通预测问题，首先需要构件图G。</p><ul><li>图：无权图，有权图，无向图，有向图，取决于具体的任务。</li><li>节点：传感器sensor，路段，道路交叉口，GPS交叉点。</li><li>邻接矩阵A：非0即1，浮点数（表示2个节点的关系，例如相似性，距离）</li></ul><p>给定历史P个时间段所有节点的信息，维度是$\left[\mathcal{X}_{1}, \cdots, \mathcal{X}_{i}, \cdots, \mathcal{X}_{\mathbf{P}}\right] \in \mathbb{R}^{\mathbf{P} \times \mathbf{N} \times \mathbf{F}_{1}}$预测未来Q个时间段的$\mathcal{Y}=\left[\mathcal{Y}_{1}, \cdots, \mathcal{Y}_{j}, \cdots, \mathcal{Y}_{\mathrm{Q}}\right] \in \mathbb{R}^{\mathbf{Q} \times \mathbf{N} \times \mathrm{F}_{\mathrm{O}}}$</p><ul><li>预测的特征只有1个，即$F_O=1$，预测特征有多个，即$F_O&gt;1$</li><li>预测未来时间段只有1个，单步预测，即$Q=1$，预测未来时间段有多个，多步预测，即$Q&gt;1$</li><li>多步预测问题中，一般使用FC（将输出reshape成需要的维度，ASTGCN,T-GCN,），Seq2Seq（使用RNN循环输出预测结果,DCRNN,GMAN），空洞技术（WaveNet）</li></ul><h2><span id="41-构建图">4.1. 构建图</span></h2><p>在构建图时，一般使用3类数据：传感器，GPS轨迹，打车订单数据，</p><ol><li>传感器数据<br>最常用的加州PEMS数据，图中的每个节点表示一个传感器，同一条路上的传感器有边相连。</li><li>GPS数据<br>GPS轨迹数据，需要将GPS匹配到最近的路段上，以路段为节点创建图，或者以交叉路口为节点创建图。这里的图可以是有向，也可以无向。</li></ol><p><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/3.png" alt="">  </p><ol><li>订单数据<br>将城市划分为网格，每个节点表示一个网格，边表示连通性。可以根据不同的特征来构件图，例如下图使用邻近区域、道路连通性、功能相似区域分别构建3个图。</li></ol><p><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/4.png" alt="">  </p><ol><li>公共交通数据<ul><li>地铁图：每个地铁站表示一个节点，如果一条线上的2个地铁站相邻则有边。图信号矩阵是inflow和outflow</li><li>公交车图：每个公交站是一个节点，如果一条线上的2个公交站相邻则有边。图信号矩阵进站记录</li></ul></li></ol><h2><span id="42-构造邻接矩阵">4.2. 构造邻接矩阵</span></h2><ol><li>静态邻接矩阵<br>邻接矩阵不会随着时间变化。可以根据节点之间的特征构建多个邻接矩阵，例如功能相似，道路相通，时间相似。邻接矩阵中的值可以是非0即1，也可以表示节点间距离或者相似性。一般通过阈值来定义邻接矩阵，通过调整阈值来控制邻接矩阵的稀疏性。<script type="math/tex; mode=display">\mathbf{a}_{i j}=\left\{\begin{array}{l}\exp \left(-\frac{\mathbf{d}_{i j}^{2}}{\sigma^{2}}\right), i \neq j \text { and } \mathbf{d}_{i j} \geq \epsilon \\0 \quad, i=j \text { or } \mathbf{d}_{i j}<\epsilon\end{array}\right.</script></li></ol><ol><li>动态邻接矩阵<br>有2种情况：1. 邻接矩阵不随着时间变化，但是邻接矩阵不是预先定义好的，而是模型先动态学习节点嵌入，然后根据学习到的节点嵌入构造邻接矩阵。2. 邻接矩阵随着时间变化。</li></ol><h1><span id="5-前人的模型">5. 前人的模型</span></h1><p>分析图神经网络在交通领域的应用，发现GNN通常和其他组件一起用，类似RNN,Seq2Seq，TCN等。</p><h2><span id="51-gnn">5.1. GNN</span></h2><p>GNN在交通领域的应用主要有3个：谱图卷积for无向图，扩散卷积for有向图。</p><ol><li><p>谱图卷积</p><script type="math/tex; mode=display">\begin{aligned}Y_{j} &=\rho\left(\Theta_{j} *_{\mathcal{G}} X\right) \\&=\rho\left(\sum_{i=1}^{\mathbf{F}_{\mathrm{I}}} \theta_{i, j} \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} X_{i}\right), 1 \leq j \leq \mathbf{F}_{\mathbf{O}} \\Y &=\rho\left(\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} X W\right)\end{aligned}</script><p>谱图卷积要求对称的拉普拉斯矩阵，来实现特征值分解。</p></li><li><p>扩散卷积<br>谱图卷积要求对称的拉普拉斯矩阵，来实现特征值分解。但是对于有向图来说，拉普拉斯矩阵不是对称的。扩散卷积对图的结构，邻接矩阵，拉普拉斯矩阵没有任何限制。扩散卷积可以看做是转移矩阵的幂次，表示从节点i到节点j的转移概率。</p><script type="math/tex; mode=display">y=\Theta *_{\mathcal{G}} x=\sum_{k=0}^{\mathrm{K}-1}\left(\theta_{k, 1}\left(\mathrm{D}_{\mathrm{O}}^{-1} \mathrm{A}\right)^{k}+\theta_{k, 2}\left(\mathrm{D}_{\mathrm{I}}^{-1} \mathrm{A}^{T}\right)^{k}\right) x</script></li></ol><p>总结：谱图卷积和扩散卷积的不同：谱图卷积的邻接矩阵揭示中心节点和它直接邻近的节点更相关。而扩散卷积揭示空间依赖是随机且动态的。扩散卷积比谱图卷积更复杂。扩散卷积可以适用在任何交通网络上，而谱图卷积只能用在对称的图上，即无向图中。</p><p>有些工作在使用SGC和DGC使用以下tricks</p><ul><li>使用SGC时，引入attention机制<br>S表示图信号矩阵，使用切比雪夫多项式计算图卷积时，对S求attention，计算节点之间的影响程度。</li></ul><script type="math/tex; mode=display">\Theta *_{\mathcal{G}} x \approx \sum_{k=0}^{K-1} \theta_{k}\left(T_{k}(\tilde{\mathbf{L}}) \odot \mathbf{S}\right) x$$ $$\mathbf{S} = W_{1} \odot \rho\left(\left(X W_{2}\right) W_{3}(W_{4} X)^{T}+b\right) \in \mathbb{R}^{N \times N}</script><ul><li><p>直接使用邻接矩阵，FFR表示道路特征</p><script type="math/tex; mode=display">\Theta *_{\mathcal{G}} x=\left(W \odot \tilde{\mathbf{A}}^{\mathrm{K}} \odot \mathcal{F} \mathcal{F} \mathcal{R}\right) x</script></li><li><p>在邻接矩阵中引入地理位置信息</p><script type="math/tex; mode=display">\mathbf{S}=\mathbf{A} \odot \omega$$$$Y=\rho\left(\tilde{\mathbf{Q}}^{-\frac{1}{2}} \tilde{\mathbf{S}} \tilde{\mathbf{Q}}^{-\frac{1}{2}} X W\right)</script></li></ul><h2><span id="52-rnn">5.2. RNN</span></h2><p>交通任务预测中很多都是时间序列数据，适用RNN来捕获时间相关性。这里包括三类：RNN,LSTM,GRU</p><ul><li>RNN：输入层，隐藏层，输出层</li><li>LSTM：为了解决RNN的梯度消失和梯度爆炸问题，引入输入门，遗忘门，输出门。</li><li>GRU：LSTM结构复杂，参数更多，用更简单的GRU来代替，只有2个门：重置门</li></ul><p>在交通预测领域中，很少用RNN，大部分都是用GRU，少数用LSTM。在用GRU或LSTM时，有很多小tricks，例如attention，门控机制，残差机制。<br>在使用RNN所用到的tricks</p><ul><li>在RNN中引入空间信息<script type="math/tex; mode=display">\mathbf{H}_{t} = R N N\left(\left[\mathbf{H}_{t-1}, \mathbf{X}_{t}\right] \odot S\right)</script></li><li>引入外部因素<script type="math/tex; mode=display">\mathbf{H}_{t}=G R U\left(\left[\mathbf{H}_{t-1}, \mathbf{X}_{t}\right], \mathbf{E}_{t}\right)+\mathbf{H}_{t-1} W</script></li><li>使用空洞RNN<script type="math/tex; mode=display">\mathbf{H}_{t}=G R U\left(\mathbf{H}_{t-s}, \mathbf{X}_{t}\right)</script></li><li>RNN和图卷积结合<script type="math/tex; mode=display">\begin{aligned}r_{t} &=\sigma\left(\left[\mathbf{H}_{t-1}, \mathbf{X}_{t}\right] *_{\mathcal{G}} W_{r}+b_{r}\right) \\u_{t} &=\boldsymbol{\sigma}\left(\left[\mathbf{H}_{t-1}, \mathbf{X}_{t}\right] *_{\mathcal{G}} W_{u}+b_{u}\right) \\\tilde{\mathbf{H}}_{t} &=\tanh \left(r_{t} \odot\left[\mathbf{H}_{t-1}, \mathbf{X}_{t}\right] *_{\mathcal{G}} W_{h}+b_{h}\right) \\\mathbf{H}_{t} &=u_{t} \odot \mathbf{H}_{t-1}+\left(1-u_{t}\right) \odot \tilde{\mathbf{H}}_{t}\end{aligned}</script></li></ul><h2><span id="53-tcn">5.3. TCN</span></h2><p>虽然RNN可以捕获时间的相关性，但是其不能并行计算，耗时。与之对比，1D卷积运行更快，同样也可以捕获时间相关性。然后1D卷积与RNN相比应用更少，由于1D卷积缺少长期建模的memory机制。后来提出空洞卷积，在长期时间建模上，比RNN效果更好。之后，TCN被广泛应用在时间建模上。</p><p><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/5.png" alt=""> </p><p>在使用TCN时，有一些小traick</p><ul><li><p>堆叠不同的TCN层，每层使用不同的dilation rate</p><script type="math/tex; mode=display">\mathcal{Y}^{(l+1)}=\sigma\left(\Theta^{l} *_{\mathcal{T} \mathrm{d}^{l}} \mathcal{Y}^{(l)}\right)</script></li><li><p>残差，原始输入+TCN的输出</p><script type="math/tex; mode=display">\mathcal{Y}^{(l+1)}=\mathcal{Y}^{(l)}+\boldsymbol{\operatorname { R e }} \boldsymbol{L} \boldsymbol{U}\left(\Theta_{1}^{l} *_{\mathcal{T}^{\mathrm{d}}}\left(\boldsymbol{\operatorname { R e }} \boldsymbol{L} \boldsymbol{U}\left(\Theta_{0}^{l} *_{\mathcal{T}^{\mathrm{d}}} \mathcal{Y}^{(l)}\right)\right)\right)</script></li><li><p>使用门控机制</p><script type="math/tex; mode=display">\mathcal{Y}=\rho\left(\Theta_{1} *_{\mathcal{T}^{\mathrm{d}}} \mathcal{X}+b_{1}\right) \odot \sigma\left(\Theta_{2} *_{\mathcal{T}^{\mathrm{d}}} \mathcal{X}+b_{2}\right)</script></li></ul><h2><span id="54-seq2seq">5.4. Seq2Seq</span></h2><p>原始的Seq2Seq模型为对输入进行建模，得到一个隐变量$C$,然后将$C$输入到解码器中，进行预测。</p><p><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/6.png" alt=""> </p><p>对Seq2Seq的改进主要有2点：</p><ul><li><p>改变隐变量C<br>原先输入到decoder的C是固定的，对decoder中所有的时间步来说都一样，然后输入中的值对不同的输出影响程度不同，这里引入attention机制，动态改变C</p><script type="math/tex; mode=display">\begin{array}{l}\mathbf{H}_{i}=\operatorname{Encoder}\left(\mathbf{X}_{i}, \mathbf{H}_{i-1}\right) \\\mathbf{C}_{j}=\sum_{i=1}^{\mathbf{P}}\left(\theta_{j i} \mathbf{H}_{i}\right), \mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}} \\\mathbf{S}_{j}=\operatorname{Decoder}\left(\mathbf{C}_{j}, \mathbf{Y}_{j-1}, \mathbf{S}_{j-1}\right) \\\mathbf{Y}_{j}=\mathbf{S}_{j} W\end{array}</script></li><li><p>采样<br>在decoder在训练阶段和测试阶段的输入是不同的。在训练阶段，decoder的不同时间步输入的真实的label，而在测试阶段，因为不知道label，输入的是上一个时间步预测的结果，这样可能会造成错误累积的问题。为了解决这个问题，可以在训练阶段进行采样，即并不总是输入真实的label，以$\epsilon_{j}$输入真实babel，以$1-\epsilon_{j}$输入上个时间步的预测结果。</p></li></ul><p>交通领域中的多步预测通常采用Seq2Seq架构。Seq2Seq中的encoder和decoder架构通常采用RNN，但是也不一定相同。</p><h2><span id="55-gan">5.5. GAN</span></h2><p>这一模块看的论文较少，以后补充</p><h1><span id="6-挑战">6. 挑战</span></h1><p>尽管交通领域有很多研究方向，但它们都有一些共同的挑战，如下所示，主要分为三类：空间依赖，时间依赖，外部因素。</p><p><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/2.png" alt=""> </p><h2><span id="61-空间依赖">6.1. 空间依赖</span></h2><p>在一个双向的道路中，R1只受R2的影响，R3对R1的影响较小。如果采用网格的形式，R3和R2对R1的影响相同，这不符合实际。如果采用图的形式，R2对R1的影响较大，R3对R1的影响较小，符合实际。</p><p><img src="/2020/06/02/How-to-Build-a-Graph-Based-Deep-Learning-Architecture-in-Traffic-Domain-A-Survey/7.png" alt=""> </p><p>交通网络中空间依赖十分复杂，可以分成三类：空间局部性，多元关系，全局连通性。</p><ol><li>空间局部性<br>空间局部性表示邻近区域比较远的区域更相关。K阶局部谱图卷积SGCN可以聚合0~K-1跳的邻居信息。还有一些其他工作可以捕获空间局部相关性。比如动态计算邻接矩阵</li><li>多元关系<br>目标区域也可能与距离较远的区域相关。例如功能相似的区域，交通连通的区域。根据这些不同的相似性来创建不同的图。</li><li>全局连通性<br>以上2点更关注网络部分，而忽略了整体的结构。全局连通性表示不同区域的交通情况在整个网络上互相影响。使用扩散卷积、pooling层、self-adaptive邻接矩阵可以捕获到这种全局连通性。</li></ol><h2><span id="62-时间依赖">6.2. 时间依赖</span></h2><p>使用RNN或TCN来捕获时间依赖</p><ol><li>多粒度<br>时间有不同的周期性，例如recent，daily，weekly。</li><li>不同的权重<br>历史信息对目标时间段的影响权重不同。使用Attention机制计算权重分数。</li></ol><h2><span id="63-时空依赖">6.3. 时空依赖</span></h2><p>以上对时间和空间依赖分别建模，如果对时空依赖同时建模，预测效果可能会更好。例如STSGCN</p><h2><span id="64-外部因素">6.4. 外部因素</span></h2><p>天气（雨/温度/空气质量），时间（节假日/周几/几点），特殊时间，POI等信息都会影响交通预测。<br>对于外部因素的处理通常有2种方法：</p><ol><li>和其他因素拼接，输入到模型中</li><li>设计外部因素处理模块，对外部因素单独处理。通常是2个FCN，第一个FCN提取重要信息，第二个FCN从低维特征映射到高维特征</li></ol><h1><span id="7-未来方向">7. 未来方向</span></h1><ol><li>在司机行为分类，车辆/人们轨迹预测，交通事故预测使用图模型。</li><li>大多使用SGCN和DGCN，很少使用GAT,GAE,RNN+GCN，可以使用以上模型解决交通问题</li><li>交通问题大多是回归问题，很少有分类问题，可以使用图模型研究分类问题</li><li>现有模型对外部因素处理比较简单，可以设计更复杂的模型捕获外部因素信息。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇综述性论文介绍图神经网络在交通领域的应用。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>图神经网络研讨会</title>
    <link href="http://yoursite.com/2020/03/29/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A0%94%E8%AE%A8%E4%BC%9A/"/>
    <id>http://yoursite.com/2020/03/29/图神经网络研讨会/</id>
    <published>2020-03-29T05:40:52.000Z</published>
    <updated>2020-03-31T15:15:44.158Z</updated>
    
    <content type="html"><![CDATA[<p>在线图神经网络研讨会<br><a id="more"></a><br><img src="/2020/03/29/图神经网络研讨会/intro.png" alt=""></p><h1><span id="网络表示学习">网络表示学习</span></h1><ul><li>网络表示的关键问题：<br>如何定义图中节点的相似性</li></ul><h1><span id="图神经网络及认知推理">图神经网络及认知推理</span></h1><p><strong>网络上的学习任务：</strong></p><ul><li>节点分类：给定一个点，预测其类别</li><li>链接预测：给2个点，预测这2个点是否相连</li><li>community detection：找子图</li><li>网络相似度：2个网络或子网络的相似度</li></ul><h2><span id="回顾网络表示学习">回顾网络表示学习</span></h2><p>给定一个网络，学习节点的低维表示，如果2个节点距离很近，那这2个节点的表示也要相似。<br><strong>挑战：</strong></p><ol><li>CNN只适用于网格(二维)，但是网络是一个拓扑机构</li><li>RNN适用于文本/序列，这种都有先后关系，但是网络没有先后关系</li><li>网络是动态的，节点有属性，并且网络还有结构属性</li></ol><p><strong>网络表示学习发展：</strong></p><ol><li>使用word2vec来做网络表示学习，即DeepWalk</li><li>根据DeepWalk进行扩展：<ul><li>LINE：一阶和二阶相似性</li><li>PTE：异构网络</li><li>Node2vec：biased random walk</li></ul></li></ol><p><strong>网络表示学习的本质：</strong><br>都是在做矩阵分解，SVD分解，只是分解的形式不一样。<br>图表示学习结合的是context信息，用上下文信息来做网络表示学习。</p><p><img src="/2020/03/29/图神经网络研讨会/1.png" alt=""></p><p><strong>问题：</strong></p><h2><span id="gnn">GNN</span></h2><p><img src="/2020/03/29/图神经网络研讨会/2.png" alt=""></p><p><img src="/2020/03/29/图神经网络研讨会/3.png" alt=""></p><h2><span id="异质图">异质图</span></h2><p>同质网络：网络中只有一种类型的节点或边<br>异质网络：网络中有多类节点或边</p><p>首先分解出网络中的对象，以及对象之间的关系。<br>例如：作者-论文-会议，一个网络中有3类节点。<br>其中对象之间的关系（Meta path）有：</p><ul><li>作者1-论文-作者2（2个作者共同合作一篇论文）</li><li>作者1-论文-引用论文1，作者1写论文，引用了其他论文</li><li>……</li></ul><h3><span id="模型">模型</span></h3><ol><li>Metapath2Vec<br>基于meta path的随机游走，</li><li>HERec<br>解决异质图中节点的表示，将异质图变成同质图，在同质图中用DeepWalk或LINE学习节点表示</li><li>HIN2Vec<br>随机游走抽取出点边序列</li><li>MCRec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在线图神经网络研讨会&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Graph" scheme="http://yoursite.com/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>argparse不支持bool类型</title>
    <link href="http://yoursite.com/2020/03/09/argparse%E4%B8%8D%E6%94%AF%E6%8C%81bool%E7%B1%BB%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/03/09/argparse不支持bool类型/</id>
    <published>2020-03-09T08:28:29.000Z</published>
    <updated>2020-03-09T08:57:46.605Z</updated>
    
    <content type="html"><![CDATA[<p>在Python中通过下列方式向程序传递bool参数时，其中<code>neg</code>参数指定类型为bool，但是无论传入的值是什么，<code>neg</code>始终为<code>True</code></p><p>解决方法：<br><a href="https://blog.csdn.net/yaokai_assultmaster/article/details/77928629" target="_blank" rel="noopener">使用Python中的argparse从命令行接收boolean类型的参数</a><br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"--config"</span>, type=str, help=<span class="string">'configuration file'</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--gpus"</span>, type=str,help=<span class="string">"test program"</span>)<span class="comment">#如果</span></span><br><span class="line">parser.add_argument(<span class="string">"--neg"</span>, type=bool, help=<span class="string">"test program"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--test"</span>, action=<span class="string">"store_true"</span>, help=<span class="string">"test program"</span>)</span><br></pre></td></tr></table></figure><p>【注意】类似于上文中<code>gpus</code>这种参数，指定也可以，不指定也可以</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Python中通过下列方式向程序传递bool参数时，其中&lt;code&gt;neg&lt;/code&gt;参数指定类型为bool，但是无论传入的值是什么，&lt;code&gt;neg&lt;/code&gt;始终为&lt;code&gt;True&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;解决方法：&lt;br&gt;&lt;a href=&quot;https://blog.csdn.net/yaokai_assultmaster/article/details/77928629&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;使用Python中的argparse从命令行接收boolean类型的参数&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>深度学习优秀代码示例</title>
    <link href="http://yoursite.com/2020/03/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B/"/>
    <id>http://yoursite.com/2020/03/03/深度学习优秀代码示例/</id>
    <published>2020-03-03T03:30:18.000Z</published>
    <updated>2020-03-07T05:14:24.731Z</updated>
    
    <content type="html"><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;曾经一段时间很苦恼，对于深度学习算法不知道怎么上手，看了很多深度学习教程，依然不会写。后来就看论文公开的源代码，对照着论文模型，一点点看，多看几篇代码，逐渐有种开窍的感觉。其次是看Mxnet和Pytorch的源代码(我主要用这2个框架)，Mxnet和Pytorch的Github上给了很多示例代码，写的非常规范，从中可以学到用法，从而也可以规范自己的代码。<br>&ensp;&ensp;&ensp;&ensp;下面整理一下，在我学习过程中，对我帮助很大的教程和代码。</p><a id="more"></a><ol><li><a href="http://zh.gluon.ai/" target="_blank" rel="noopener">《动手学深度学习》Mxnet版</a><br>Mxnet的入门教程，沐神写的，来来回回看了2~3遍，每次看都有新的收货</li><li><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">《动手学深度学习》Pytorch版</a><br>将Mxnet改写为Pytorch版本，非常好的Pytorch入门教程</li><li><a href="https://github.com/Davidham3/ASTGCN" target="_blank" rel="noopener">ASTGCN</a><br>AAAI2019论文公开代码，用Mxnet写的</li><li><a href="https://yjucho1.github.io/spatio-temporal%20data/deep%20learning%20paper/ST-resnet/" target="_blank" rel="noopener">ST-ResNet</a><br>AAAI2017论文公开代码，用Keras，看这篇代码主要是学习模型架构，然后自己用mxnet复现了一下</li><li><a href="https://github.com/panzheyi/ST-MetaNet" target="_blank" rel="noopener">ST-MetaNet</a><br>KDD2019论文公开代码，用Mxnet写的，学到了很多高级用法，例如EarlyStopping，Encoder和Decoder，getattr，DGL</li><li><p><a href="https://github.com/pytorch/examples/tree/master/word_language_model" target="_blank" rel="noopener">Pytorch Transformer</a><br>学习怎么使用Transformer，Dropout和BN在训练和测试的不同，PositionEmbedding，getattr等用法。学习Transformer最好去看Pytorch关于Tranformer的源代码。</p></li><li><p><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener">Pytorch示例代码</a><br>Pytorch Github中的示例代码</p></li><li><a href="https://github.com/apache/incubator-mxnet/tree/master/example" target="_blank" rel="noopener">Mxnet示例代码</a><br>Mxnet Github中的示例代码</li></ol><p>觉得自己最大的变化是喜欢去读源代码了，遇到问题去官网看教程，读源码，帮助很大。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;曾经一段时间很苦恼，对于深度学习算法不知道怎么上手，看了很多深度学习教程，依然不会写。后来就看论文公开的源代码，对照着论文模型，一点点看，多看几篇代码，逐渐有种开窍的感觉。其次是看Mxnet和Pytorch的源代码(我主要用这2个框架)，Mxnet和Pytorch的Github上给了很多示例代码，写的非常规范，从中可以学到用法，从而也可以规范自己的代码。&lt;br&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;下面整理一下，在我学习过程中，对我帮助很大的教程和代码。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Mxnet" scheme="http://yoursite.com/tags/Mxnet/"/>
    
      <category term="Pyotrch" scheme="http://yoursite.com/tags/Pyotrch/"/>
    
  </entry>
  
  <entry>
    <title>时空论文阅读笔记二</title>
    <link href="http://yoursite.com/2020/02/27/%E6%97%B6%E7%A9%BA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BA%8C/"/>
    <id>http://yoursite.com/2020/02/27/时空论文阅读笔记二/</id>
    <published>2020-02-27T12:03:53.000Z</published>
    <updated>2020-09-11T13:37:49.307Z</updated>
    
    <content type="html"><![CDATA[<p>因为疫情推迟开学，在家把以前看的论文又看了一遍，每重新看一次都有新的收获，在此整理下。</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-流量速度预测">1. 流量/速度预测</a><ul><li><a href="#11-deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-predictionaaai2017">1.1. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction(AAAI2017)</a></li><li><a href="#12-deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolisaaai2019">1.2. DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis(AAAI2019)</a></li><li><a href="#13-urbanfm-inferring-fine-grained-urban-flowskdd2019">1.3. UrbanFM: Inferring Fine-Grained Urban Flows(KDD2019)</a></li><li><a href="#14-attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecastingaaai2019">1.4. Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting(AAAI2019)</a></li><li><a href="#15-spatial-temporal-synchronous-graph-convolutional-networks-a-new-framework-for-spatial-temporal-network-data-forecastingaaai2020">1.5. Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecasting(AAAI2020)</a></li><li><a href="#16-hyperst-net-hypernetworks-for-spatio-temporal-forecasting2019aaai">1.6. HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting(2019AAAI)</a></li><li><a href="#17-urban-traffic-prediction-from-spatio-temporal-data-using-deep-meta-learning2019kdd">1.7. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning(2019KDD)</a></li><li><a href="#18-stepdeep-a-novel-spatial-temporal-mobility-event-prediction-framework-based-on-deep-neural-networkkdd2018">1.8. StepDeep: A Novel Spatial-temporal Mobility Event Prediction Framework based on Deep Neural Network(KDD2018)</a></li><li><a href="#19-stgrat-a-spatio-temporal-graph-attention-network-for-traffic-forecastingaaai2020">1.9. STGRAT: A Spatio-Temporal Graph Attention Network for Traffic Forecasting(AAAI2020)</a></li><li><a href="#110-connecting-the-dots-multivariate-time-series-forecasting-with-graph-neural-networks2020kdd">1.10. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks(2020KDD)</a></li></ul></li><li><a href="#2-eta预测">2. ETA预测</a><ul><li><a href="#21-when-will-you-arrive-estimating-travel-time-based-on-deep-neural-networksaaai20">2.1. When Will You Arrive? Estimating Travel Time Based on Deep Neural Networks(AAAI20)</a></li></ul></li><li><a href="#3-出租车需求预测">3. 出租车需求预测</a><ul><li><a href="#31-deep-multi-view-spatial-temporal-network-for-taxi-demand-predictionaaai2018">3.1. Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction(AAAI2018)</a></li><li><a href="#32-revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-predictionaaai2019">3.2. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction(AAAI2019)</a></li><li><a href="#33-spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecastingaaai2019">3.3. Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting(AAAI2019)</a></li><li><a href="#34-passenger-demand-forecasting-with-multi-task-convolutional-recurrent-neural-networkspakdd2019">3.4. Passenger Demand Forecasting with Multi-Task Convolutional Recurrent Neural Networks(PAKDD2019)</a></li><li><a href="#35-stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting2019ijcai">3.5. STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting(2019IJCAI)</a></li></ul></li><li><a href="#4-时间序列预测">4. 时间序列预测</a><ul><li><a href="#41-restful-resolution-aware-forecasting-of-behavioral-time-series-data2018cikm">4.1. RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data(2018CIKM)</a></li><li><a href="#42-multi-horizon-time-series-forecasting-with-temporal-attention-learningkdd2019">4.2. Multi-Horizon Time Series Forecasting with Temporal Attention Learning(KDD2019)</a></li></ul></li><li><a href="#5-总结">5. 总结</a><ul><li><a href="#51-网格--图">5.1. 网格—&gt;图</a></li><li><a href="#52-动态图">5.2. 动态图</a></li><li><a href="#53-计算2个区域的相似性">5.3. 计算2个区域的相似性</a></li><li><a href="#54-poi">5.4. POI</a></li><li><a href="#55-时间相关性">5.5. 时间相关性</a></li><li><a href="#56-lstm共t个隐藏状态整合">5.6. LSTM共T个隐藏状态整合</a></li><li><a href="#57-外部因素嵌入">5.7. 外部因素嵌入</a></li><li><a href="#58-mask">5.8. mask</a></li><li><a href="#59-max-min归一化">5.9. Max-min归一化</a></li></ul></li></ul><!-- /TOC --><h1><span id="1-流量速度预测">1. 流量/速度预测</span></h1><h2><span id="11-deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-predictionaaai2017">1.1. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction(AAAI2017)</span></h2><blockquote><p>张钧波(京东)<br>郑宇(京东)<br><a href="https://github.com/lucktroy/DeepST" target="_blank" rel="noopener">https://github.com/lucktroy/DeepST</a> Keras</p></blockquote><ul><li>给定所有区域历史T个时间段的inflow和outflow，预测下一个时间段所有区域的inflow和outflow</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/ST-ResNet.png" alt=""></p><ul><li>每个时间段所有区域的输入是$I<em>J</em>2$,将输入分为recent，daily，weekly周期，预测第t个时间段的infow和outflow：<ul><li>recent：当天前r个时间段</li><li>daily：前d天该时间段</li><li>weekly：前w周该天该时间段</li></ul></li><li>外部特征包括：天气，节假日，dayOfWeek。用2层FCN对外部特征进行嵌入，第一层FCN作为嵌入层，第二层FCN转换维度和$X_{Res}$一致。</li><li>在融合阶段，先将3个时间周期的输出融合，再和外部因素拼接。</li><li>数据集：北京出租车和NYC自行车流量</li><li>将flow使用Max-Min归一化到[-1,1]，FCN最后一层使用tanh激活函数</li></ul><h2><span id="12-deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolisaaai2019">1.2. DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis(AAAI2019)</span></h2><blockquote><p>Ziqian Lin(清华大学)<br>Jie Feng(清华大学)<br>Ziyang Lu(清华大学)<br>Yong Li(清华大学)<br>Depeng Jin(清华大学)<br><a href="https://github.com/FIBLAB/DeepSTN" target="_blank" rel="noopener">https://github.com/FIBLAB/DeepSTN</a>  Keras</p></blockquote><ul><li>crowd flow预测是给定历史T个时间段，预测区域的inflow和outflow</li><li>现有研究的缺点：<ul><li>不能捕获长距离空间依赖</li><li>忽略区域功能对人流的影响(POI)</li></ul></li><li>提出DeeoSTN+，有3个组件<ul><li>ConvPlus：解决长距离区域的空间依赖</li><li>SemanticPlus：解决区域POI对人流的影响</li><li>early-fusuion模块</li></ul></li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/DeepSTN+.png" alt=""></p><ul><li><p>假设预测周四第5个时间段的flow，输入的数据有：</p><ul><li>recent：周四2,3,4个时间段</li><li>day：周一，周二，周三第5个时间段</li><li>week：上上上周四，上上周四，上周四第5个时间段</li><li>time：周四第5个时间段的时间向量</li><li>poi：(C,W,H)所有区域的poi信息</li></ul></li><li>【<strong>ConvPlus</strong>】传统的Conv中kernel的大小远小于网格大小，通常是$3 \times 3$,然后在人流量预测中通常有一些长距离的依赖，例如人们去很远的地方上班。在ConvPlus中，假设原始输入维度是(C,W,H),其中plus维用来捕获长距离依赖<ul><li>正常Conv2D：将原始输入(C,W,H)输入到正常Conv2D中,卷积核有C-plus个，输出维度(C-plus,W,H)，</li><li>ConvPlus：再将原始(C,W,H)输入到ConvPlus中，卷积核有plus<em>W\</em>H个，卷积大小为W*H,则输出维度(plus*W*H,1,1),reshape为(plus,W,H)</li><li>将上面2个卷积的输出拼接成(C,W,H)</li></ul></li><li><strong>计算POI在时间上的分布权重</strong><ul><li>POI维度$C \times W \times H$,表示每个网格有C类POI</li><li>时间维度$T \times W \times H$，T=24+7, 首先对时间进行嵌入，通过2D卷积，将31个数变成一个数$1 \times W \times H$，然后将时间repeat成$C \times W \times H$</li><li>时间和POI逐元素相乘，得到$C \times W \times H$</li><li>如果需要，还可以再通过K个2D卷积，变成$K \times W \times H$</li><li>将该张量和3个周期的输出在通道维上拼接。</li></ul></li><li>crowd flow使用Max-Min归一化到[-1,1]，最后一层使用Tanh，范围[-1,1]</li><li>POI使用Max-Min归一化到[0,1]</li></ul><h2><span id="13-urbanfm-inferring-fine-grained-urban-flowskdd2019">1.3. UrbanFM: Inferring Fine-Grained Urban Flows(KDD2019)</span></h2><blockquote><p>Yuxuan Liang(XiDian)<br>Kun Ouyang(新加坡国立)<br>张钧波(京东)<br>郑宇(京东)<br><a href="https://github.com/yoshall/UrbanFM" target="_blank" rel="noopener">https://github.com/yoshall/UrbanFM</a></p></blockquote><ul><li>基于粗粒度级的flow，实时推测整个城市细粒度级的flow，提出模型<strong>Urban</strong> <strong>F</strong>low <strong>M</strong>agnifier (<strong>UrbanFM</strong>)</li><li>有2个挑战：粗粒度和细粒度的flow在空间上的相关性、复杂的外部因素。</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/UrbanFM1.png" alt=""></p><p><img src="/2020/02/27/时空论文阅读笔记二/UrbanFM.png" alt=""></p><h2><span id="14-attention-based-spatial-temporal-graph-convolutional-networks-for-traffic-flow-forecastingaaai2019">1.4. Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting(AAAI2019)</span></h2><blockquote><p>郭晟楠(北京交通大学)<br>冯宁(北京交通大学)<br>宋超(北京交通大学)<br>万怀宇(北京交通大学)<br><a href="https://github.com/Davidham3/ASTGCN" target="_blank" rel="noopener">https://github.com/Davidham3/ASTGCN</a> Mxnet</p></blockquote><p>根据所有节点历史T个时间段traffic flow，occupy，speed，预测所有节点未来T_p个时间段的traffic flow。</p><p><img src="/2020/02/27/时空论文阅读笔记二/ASTGCN.png" alt=""></p><ul><li>三个独立的组件，分别对recent，daily，weekly周期进行建模</li><li>比如说预测6.14 8:00-8:55的flow，传入的样本是<br>时：6.14号6:00~7:55（前2个小时）的数据，<br>天：6.13和6.12（前2天）的8:00-8:55，<br>周：上周6.17，上上周5.31（前2周）的8:00-8:55</li></ul><h2><span id="15-spatial-temporal-synchronous-graph-convolutional-networks-a-new-framework-for-spatial-temporal-network-data-forecastingaaai2020">1.5. Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecasting(AAAI2020)</span></h2><blockquote><p>宋超(北京交通大学)<br>郭晟楠(北京交通大学)<br>万怀宇(北京交通大学)<br><a href="https://github.com/Davidham3/STSGCN" target="_blank" rel="noopener">https://github.com/Davidham3/STSGCN</a> Mxnet</p></blockquote><ul><li>给定所有节点历史T个时间段的车流量，预测所有节点未来$T’$个时间段的车流量，</li><li>原先的研究通常使用分开的组件捕获时间和空间的相关性，并且忽略了时空数据的异构性。</li><li>提出<strong>S</strong>patial-<strong>T</strong>emporal <strong>S</strong>ynchronous <strong>G</strong>raph <strong>C</strong>onvolutional <strong>N</strong>etworks (<strong>STSGCN</strong>)</li><li><strong>对于图中的每个节点，它的影响范围有3种</strong>，这是该文章提出的一个新观点，以前的研究中通常只考虑前2种。</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/STSGCN1.png" alt=""></p><ul><li>本文强调的内容有2点：<ul><li>局部的时空关系，称作localized spatial-temporal correlations</li><li>时空数据的异质性，居住区和商业区，早上和晚上</li></ul></li><li><strong>使用连续3个时间步的图数据来构建localized spatial-temporal graph，local指的是在时间上局部</strong><br>假设原先一个图中有N个节点，图信号矩阵为$N \times C$,邻接矩阵为$N \times N$，现在3个图来构建一个局部时空图，图信号矩阵为$3N \times C$,邻接矩阵为$3N \times 3N$,邻接矩阵中非0即1</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/STSGCN2.png" alt=""></p><ul><li>但是把3个图构成1个图失去了图之间的时间关系，模型可能会认为这是一个有3N个节点在一个时间步的信息，为了区域这3个图的时间关系，受ConvS2S启发，为时空网络序列$N \times C \times T$添加位置嵌入，增加时间嵌入矩阵$C \times T$,空间嵌入矩阵$N \times C$,这2个矩阵是通过模型学习的，当模型训练好之后，这2个矩阵可以包含图的时间和空间信息。然后把这2个嵌入矩阵和原始的图信号矩阵相加，这样图中就包含了时间和位置信息。</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/STSGCN.png" alt=""></p><ul><li>图中模型框架：多个STSGCM构成STSGCL，多个STSGCL构成STSGCN。STSGCN就是多层图卷积，从局部时空图中捕获邻居信息。STSGCL一层中有多个STSGCM，一个样本中每个时间段的局部时空图都用一个STSGCM来建模。</li><li><strong>第一个FCN成将输入转换到高维空间，提高模型的表示能力</strong></li><li>【<strong>STSGCM</strong>】中包含多层图卷积，使用GLU作为激活函数，其中sigmoid作为门控机制，控制哪个节点的信息可以流入到下一层。<strong>图卷积计算定义在顶点域，意味着不需要计算图的拉普拉斯矩阵</strong><br><img src="/2020/02/27/时空论文阅读笔记二/STSGCN3.png" alt=""><br>STSGCM的架构如下：参考JK-net，一共有L个图卷积层，每一层的输出都输入到AGG层中，AGG层将接收到的L个输出进行max聚合，最终得到一个输出$3N \times C_{out}$,然后进行裁剪，将前中后3个时间步的数据只保留中间时间步，即$N \times C_out$<br><img src="/2020/02/27/时空论文阅读笔记二/STSGCN4.png" alt=""></li><li>【<strong>STSGCL</strong>】多个STSGCM组成一个STSGCL层，其输入维度$T \times N \times C$,使用滑动窗口每次取3个时间段段的图构成$3N \times C$,一共构成$T-2$个局部时空图，然后需要$T-2$个STSGCM，最终输出$T-2个N \times C_{out}$，将其拼接为$(T-2) \times N \times C_{out}$，再输入到下一个STSGCL中。<strong>【注意】每个局部时空图是通过滑动窗口获得，每个时空局部图的邻接矩阵是不变的，而不是提前处理好局部图输入到模型中，这样会省空间</strong></li><li>上面使用的邻接矩阵$3N \times 3N$中的值非0即1，每个邻居聚合的权重相等，聚合能力会受到限制，这里对此做出改进，将邻接矩阵乘上一个Mask矩阵，对每个邻居赋予不同的权重，其中Mask矩阵是可学习参数，维度$3N \times 3N$<script type="math/tex; mode=display">A_{\text {adjusted}}^{\prime}=W_{\text {mask}} \otimes A^{\prime} \in \mathbb{R}^{3 N \times 3 N}</script></li><li>最后的FCN将STSGCL的输出转换成预测的格式。STSGCL的输出格式为$T \times N \times C$,reshape成$N \times TC$,然后使用$T’$个2层全连接，每个全连接输出维度为$(N,1)$,然后将$T’$个全连接的输出拼接成$N \times T’$</li><li><strong>损失函数使用Huber Loss，对异常值不敏感</strong><script type="math/tex; mode=display">L(Y, \hat{Y})=\left\{\begin{array}{ll}\frac{1}{2}(Y-\hat{Y})^{2} & |Y-\hat{Y}| \leq \delta \\\delta|Y-\hat{Y}|-\frac{1}{2} \delta^{2} & \text { otherwise }\end{array}\right.</script></li><li>使用mean-std归一化，训练集:验证集:测试集=6:2:2，模型包含4个STSGCL，每个STSGCM包含3个图卷积</li></ul><h2><span id="16-hyperst-net-hypernetworks-for-spatio-temporal-forecasting2019aaai">1.6. HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting(2019AAAI)</span></h2><blockquote><p>潘哲逸(上海交通大学)<br>梁宇轩(西安电子科技大学)<br>张钧波(京东)<br>易修文(京东)<br>郑宇(京东)</p></blockquote><p>论文声称第一个考虑<strong>空间和时间内在因果关系</strong>的深度框架。</p><p><img src="/2020/02/27/时空论文阅读笔记二/HyperST_Net.png" alt=""></p><ul><li>该论文提出的只是一个HyperNetwork框架，并不是一个具体的模型。</li><li><strong>HyperNetwork</strong>：和以往不同，以前都是一个网络的输出，输入到下一个网络中，<strong>超网络是一个网络的输出作为另一个网络的参数</strong>。</li><li>该模型有3个模块：空间模块，时间模块，推理模块。将空间模块的输出经过推理模块，得到的输出作为时间模块的权重参数，以此捕获时间和空间的内在因果关系。</li><li>这只是一个框架，可以变换成多种模型。在空间模块中如果使用全连接就是HyperST-Dense，使用卷积就是HyperST-Conv。</li></ul><h2><span id="17-urban-traffic-prediction-from-spatio-temporal-data-using-deep-meta-learning2019kdd">1.7. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning(2019KDD)</span></h2><blockquote><p>潘哲逸(上海交通大学)<br>梁宇轩(西安电子科技大学)<br>张钧波(京东)<br>郑宇(京东)<br><a href="https://github.com/panzheyi/ST-MetaNet" target="_blank" rel="noopener">https://github.com/panzheyi/ST-MetaNet</a> Mxnet</p></blockquote><ul><li>使用图中所有节点历史T个时间段的flow或speed，预测所有节点未来</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/ST-MetaNet.png" alt=""></p><p><img src="/2020/02/27/时空论文阅读笔记二/ST-MetaNet1.png" alt=""></p><p><img src="/2020/02/27/时空论文阅读笔记二/ST-MetaNet2.png" alt=""></p><ul><li>本论文用的图，但是实验中有一个$I \times J$的网格数据，将网格数据构建成图</li><li>本论文来表示1张图有2个矩阵：图信号矩阵(N,D)和边特征矩阵(N,N,C)。对于网格数据来说，图信号矩阵表示每个网格POI个数，道路个数。边特征表示2个两个网格的道路个数。这都是静态数据，不随着时间变化。</li></ul><p>【总结】发现上面这2个论文都是一个网络生成另一个网络的参数，查阅资料发现这叫做<code>meta-learning</code>，先记录一下以后再自己看<br><img src="/2020/02/27/时空论文阅读笔记二/Meta-learning.png" alt=""></p><h2><span id="18-stepdeep-a-novel-spatial-temporal-mobility-event-prediction-framework-based-on-deep-neural-networkkdd2018">1.8. StepDeep: A Novel Spatial-temporal Mobility Event Prediction Framework based on Deep Neural Network(KDD2018)</span></h2><blockquote><p>Bilong Shen(清华大学)<br>梁晓丹(卡耐基梅隆)</p></blockquote><ul><li><strong>S</strong>patial-<strong>T</strong>emporal mobility <strong>E</strong>vent <strong>P</strong>rediction framework based on <strong>Deep</strong> neural network (<strong>StepDeep</strong>)同时考虑时间和空间模式，给定所有区域历史T个时间段的出租车流量和外部因素，预测所有区域在下一个时间段出租车的inflow和outflow。</li><li>网格区域中的flow随时间变化，可以看做一个视频(T,C,W,H)，进而看做是视频预测任务</li><li>数据集NYC出租车轨迹数据，将NYC网格划分，计算每个区域的inflow和outflow，</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/StepDeep.png" alt=""></p><p><img src="/2020/02/27/时空论文阅读笔记二/StepDeep1.png" alt=""></p><ul><li>提出3种卷积：时间卷积，空间卷积，时空卷积。将输入(T,C,H,W)输入到以上7层卷积中，最终输出(C,H,W)表示下一个时间段所有区域的inflow和outflow，</li></ul><h2><span id="19-stgrat-a-spatio-temporal-graph-attention-network-for-traffic-forecastingaaai2020">1.9. STGRAT: A Spatio-Temporal Graph Attention Network for Traffic Forecasting(AAAI2020)</span></h2><blockquote><p>Cheonbok Park(韩国大学)<br>Chunggi Lee(韩国大学)<br>Hyojin Bahng(韩国大学)</p></blockquote><ul><li>根据所有节点历史T个时间段的交通速度，预测所有节点未来T个时间段的交通速度(T=12)，时间多预测多，Seq2Seq架构</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/STGRAT.png" alt=""></p><ul><li>Encoder layer中有3个sublayer：空间Attention层，时间Attention层和point-wise FCN。<ul><li>空间Attention：关注每个时间步上空间邻近的节点</li><li>时间Attention：关注单个节点，输入时间序列的不同时间步</li></ul></li><li>整个Encoder = 1个嵌入层 + 4个Encoder layer，使用LINE对图节点进行嵌入</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/STGRAT1.png" alt=""></p><ul><li>空间Attention层：参考Transformer，中心结点作为query，其邻居作为key和value，计算每个节点新的表示。</li><li>时间Attention==Transformer，输入维度(batch_size,T,N,D),计算时间步之间的attention分数，输出维度(batch_size,T,N,D)</li><li>Point-wise FFN和Transformer中一样，使用2层FCN，中间使用GELU激活函数，Transformer使用的是ReLU激活函数。</li><li>Decoder layer有4个sublayer：空间Attention层，mask时间Attention层，Encoder-Decoder Attention层，point-wise FFN层。整个Decoder层=1个嵌入层+4个Decoder层。</li><li><strong>本模型比Transformer多了一个空间Attention层，其余都一样，因为时间Attention层、FFN和Transformer的一样</strong></li></ul><h2><span id="110-connecting-the-dots-multivariate-time-series-forecasting-with-graph-neural-networks2020kdd">1.10. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks(2020KDD)</span></h2><p>该模型使用图网络模型捕获交通数据或其他领域数据中的时间和空间相关性。</p><p><img src="/2020/02/27/时空论文阅读笔记二/MTGNN-1.png" alt=""></p><p>该模型有2个模块，图卷积模块和时间模块。其中图卷积模块主要解决4个问题：</p><ol><li>节点间的空间相关性</li><li>如何构造图</li><li>如何解决图卷积过度平滑问题</li><li>大图如何训练问题</li></ol><p><img src="/2020/02/27/时空论文阅读笔记二/MTGNN-2.png" alt=""></p><p>首先图信号矩阵，输入到图结构学习模块，构造图的邻接矩阵。注意这里的邻接矩阵不是预先定义好的，而是根据网络模块学习得到，据此构建图，然后输入到GCN中捕获时间相关性，然后输入到时间卷积捕获时间相关性，最后预测结果。</p><p><img src="/2020/02/27/时空论文阅读笔记二/MTGNN-3.png" alt=""></p><p>在构造图时，通过模型学习节点的嵌入矩阵，然后根据嵌入矩阵计算节点的相似性，为每个节点选取相似性前k的节点作为其一阶邻居。邻接矩阵不随着时间变化。</p><script type="math/tex; mode=display">\begin{array}{l}\mathbf{M}_{1}=\tanh \left(\alpha \mathrm{E}_{1} \Theta_{1}\right) \\ \mathbf{M}_{2}=\tanh \left(\alpha \mathrm{E}_{2} \mathbf{\Theta}_{2}\right) \\ \mathbf{A}=\operatorname{ReL} U\left(\tanh \left(\alpha\left(\mathbf{M}_{1} \mathbf{M}_{2}^{T}-\mathbf{M}_{2} \mathbf{M}_{1}^{T}\right)\right)\right) \\ \text { for } i=1,2, \cdots, N \\ \text { idx }=\operatorname{argtop} k(\mathrm{A}[i,:]) \\ \quad \mathrm{A}[i,-\mathrm{idx}]=0\end{array}</script><p>在图卷积模块，参考Min-hop架构，每个GCN层的输入会加上原始的图信号矩阵，避免图过度平滑问题，然后再将所有GCN层的输出加起来，进行融合。为了解决大图训练的问题，在进行GCN时，每次随机选取几个节点进行GCN运算，而不是使用所有的节点。</p><script type="math/tex; mode=display">\mathbf{H}^{(k)}=\beta \mathbf{H}_{i n}+(1-\beta) \tilde{\mathbf{A}} \mathbf{H}^{(k-1)}</script><script type="math/tex; mode=display">\mathbf{H}_{o u t}=\sum_{i=0}^{K} \mathbf{H}^{(k)} \mathbf{W}^{(k)}</script><p><img src="/2020/02/27/时空论文阅读笔记二/MTGNN-4.png" alt=""></p><p>在时间模块，使用一维空洞卷积，捕获长期的时间依赖，同时参照Inception结构，设置不同的卷积核，来捕获不同力度的周期性，例如小时周期，天周期，周周期。</p><p>总结：</p><ol><li>根据模型学习节点嵌入，由此构建邻接矩阵，构建相似性图</li><li>使用一维空洞卷积捕获时间相似性，使用不同的卷积核捕获不同粒度的周期性。</li></ol><h1><span id="2-eta预测">2. ETA预测</span></h1><h2><span id="21-when-will-you-arrive-estimating-travel-time-based-on-deep-neural-networksaaai20">2.1. When Will You Arrive? Estimating Travel Time Based on Deep Neural Networks(AAAI20)</span></h2><blockquote><p>王东(杜克大学)<br>张钧波(京东)<br>郑宇(京东)<br><a href="https://github.com/UrbComp/DeepTTE" target="_blank" rel="noopener">https://github.com/UrbComp/DeepTTE</a> Pytorch</p><ul><li>端到端<strong>Deep</strong> learning framework for <strong>T</strong>ravel <strong>T</strong>ime <strong>E</strong>stimation(<strong>DeepTTE</strong>)，给定路径P和外部因素(weather，day of week，开始时间)预测整个path的时间</li><li>原先的工作都是预测travel中单个路段的耗时，然后再把每个路段的时间加起来，缺点是没有考虑到道路交叉口，红绿灯等影响，错误累积</li></ul></blockquote><p><img src="/2020/02/27/时空论文阅读笔记二/DeepTTE.png" alt=""></p><ul><li>DeepTTE提出geo-convolution，将地理信息加入到传统Conv中，捕获空间相关性</li><li>多任务学习，同时预测local path和entir path的时间，在loss中限制2者的权重</li><li><strong>在生成测试数据时，将历史轨迹点中的时间戳都去掉</strong>(因为要预测出行时间，所以测试数据不能带有事件信息)，从轨迹中抽样等距离的点组成路径P</li><li><p>DeepTTE一共有3个组件</p><ul><li>Attribute组件：外部因素：天气(one-hot)，司机ID(one-hot)，weekID和timeID(one-hot)，都是类别值，不能直接输入到网络中，需要先嵌入层低维向量，参考<a href="https://arxiv.org/abs/1512.05287" target="_blank" rel="noopener">(2016NIPS)A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a>,然后再和整个path的距离拼接作为该组件的输出</li><li>Geo-Conv层，历史轨迹是一个GPS序列，为了捕获空间依赖使用1D卷积，将历史轨迹序列先经过FCN变成一个矩阵$T \times V$,T个轨迹点，每个轨迹点有V个特征，然后使用C个k*V的1D卷积，卷积输出的时间维度变成$T-k+1$,将C个卷积核输出的结果拼接，然后再拼接上local path的距离,输出结果为$T-k+1 \times D$</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/DeepTTE2.png" alt=""></p><ul><li>然后将$T-k+1 \times D$的序列拼接外部因素输入到LSTM中，每个时间步表示一个local path，将<strong>隐藏状态用来预测每个local path的时间作为辅助任务</strong></li><li>LTSM输出$T-k+1$个时间步，将其整合成1个向量，通过和外部因素做Attention，对每个local path赋予不同的权重，然后再和外部因素拼接，用来预测entir path的时间</li></ul></li><li><p><strong>训练阶段预测local path和entir path的时间，在测试阶段只预测entir path的时间</strong></p></li><li><p>在训练时，使用MAPE作为loss，包含辅助任务和主任务的loss<br><img src="/2020/02/27/时空论文阅读笔记二/DeepTTE-loss.png" alt=""></p></li><li><p>这篇文章也是经典的CNN+LSTM的架构，只是这里的CNN是1D卷积。在融合外部因素上也是CNN的输出和外部因素拼接，送入到LSTM中。</p></li></ul><h1><span id="3-出租车需求预测">3. 出租车需求预测</span></h1><h2><span id="31-deep-multi-view-spatial-temporal-network-for-taxi-demand-predictionaaai2018">3.1. Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction(AAAI2018)</span></h2><blockquote><p>姚骅修(Pennsylvania State University)<br>吴飞(Pennsylvania State University)<br>柯金涛(香港科技大学)<br>Xianfeng Tang(Pennsylvania State University)<br>叶杰平(滴滴出行)<br><a href="https://github.com/huaxiuyao/DMVST-Net" target="_blank" rel="noopener">https://github.com/huaxiuyao/DMVST-Net</a> Keras</p></blockquote><ul><li><strong>出租车需求预测</strong>，根据S<em>S的小区域，历史T个时间段的出租车订单数据，预测下一个时间段中心区域的订单。<em>*空间和时间都是多预测一</em></em></li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/DMVST_Net.png" alt=""></p><ul><li>现有的研究都是使用CNN对空间建模，LSTM对时间建模，时间和空间分开建模，本文的模型是对时间和空间同时建模</li><li>本文提出DMVST-Net，有3个view：<strong>时间view</strong>（通过LSTM建模时间关系），<strong>空间view</strong>（使用local CNN建模邻近空间关系），<strong>语义view</strong>（建模功能相似的区域）</li><li>local CNN只考虑空间邻近的区域，但是不能考虑离得较远，但出租车需求模型相似的区域，所以又加了语义view</li><li><img src="/2020/02/27/时空论文阅读笔记二/DMVST_Net-1.jpg" alt=""></li><li><strong>输入是S*S的邻居区域，如果是边界区域，其邻居用0填充</strong></li><li>在LSTM每个时间步的特征中拼接天气等外部因素</li><li>local CNN和LSTM对时间和空间建模，然后再构建图，表示区域之间需求相似性(功能相似性)。求出2个区域每周的需求量，形成一个时间序列，使用DTW计算2个序列的相似性，即<strong>2个区域的相似性，作为图中的边，创建一个全连接图(任意2个区域都相连)</strong>,使用LINE对图中节点进行嵌入。</li><li><img src="/2020/02/27/时空论文阅读笔记二/DMVST_Net-2.png" alt=""><br>损失函数由MSE和MAPE组成，MSE更关注大值，为了避免模型偏向大值的方向训练，又添加了MAPE，但是使用MAPE时，真实值中不能有0</li><li>Max-Min激活，最终输出值在[0,1]之间，反归一化</li><li>最后一层FCN用sigmoid激活，其余的FCN用ReLU激活</li></ul><h2><span id="32-revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-predictionaaai2019">3.2. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction(AAAI2019)</span></h2><blockquote><p>姚骅修(Pennsylvania State University)<br>Xianfeng Tang(Pennsylvania State University)<br><a href="https://github.com/tangxianfeng/STDN" target="_blank" rel="noopener">https://github.com/tangxianfeng/STDN</a> Keras</p></blockquote><ul><li>主要问题是：原先研究中的空间相关性都是静态的，本次建模<strong>动态的空间相关性</strong>。<strong>时间有天和周周期，且有时间偏移</strong>。</li><li>提出模型<strong>S</strong>patial-<strong>T</strong>emporal <strong>D</strong>ynamic <strong>N</strong>etwork(<strong>STDN</strong>)来traffic prediction</li><li>根据S<em>S小区域历史T个时间段的volume和flow，预测下一个时间段中心区域的volume，<em>*空间和时间都是多预测一</em></em></li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/STDN.png" alt=""></p><ul><li>将交通量分为2种<ul><li>traffic volume：无方向，一个区域进来和出去的流量。</li><li>traffic flow：有方向，从区域i到区域j的流量</li></ul></li><li>flow-gated local CNN每次输入S*S区域的volume和flow，其中flow起到门控作用，值在[0,1]之间，如果2个区域之间flow大，即门控的值大，2个区域的相关性强。每个时间段经过local CNN输出的值，再拼接上该时间段的天气等外部因素送入LSTM中</li><li><strong>时间偏移Attention</strong>：比如预测第t+1个时间段的volume，用到当天前t=7个时间段的数据(短期依赖)，前P=3天(长期依赖)，每天Q=3个时间段(解决时间偏移问题)。</li><li>先将短期的t个时间段数据输入LSTM中，得到隐藏状态h做Attention。前P天，每天Q个时间段输入到LSTM中，每天得到Q个隐藏状态，和h做attention，将Q个整合成1个，最终生成P个隐藏状态，再输入到LSTM，得到长期依赖的隐藏状态，然后再和短期的隐藏状态h拼接，输入到FCN中。</li><li><strong>短期的隐藏状态和长期的隐藏状态做Attention</strong></li><li><strong>短期的隐藏状态和长期的隐藏状态拼接</strong></li><li>数据集：出租车流量和自行车流量</li></ul><blockquote><ul><li>STDN和DMVST-Net是同一作者发的</li><li>两者都是：空间和时间<strong>多预测一</strong></li><li><strong>STDN</strong>：local CNN + LSTM</li><li><strong>DMVST-Net</strong>：flow-gated local CNN + Periodically Shifted Attention LSTM</li></ul></blockquote><h2><span id="33-spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecastingaaai2019">3.3. Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting(AAAI2019)</span></h2><blockquote><p>Xu Geng(香港大学)<br>Yaguang Li(南加利福尼亚)<br>Lingyu Zhang(滴滴AI)<br>杨强(香港大学)<br>叶杰平(滴滴)</p></blockquote><p><img src="/2020/02/27/时空论文阅读笔记二/ST_MGCN.png" alt=""></p><ul><li>问题：根据所有区域历史T个时间段的订单数，预测所有区域下一个时间段的订单数</li><li>将研究区域划分为网格，根据网格构建3个图，这3个图的图信号矩阵一样，只是邻接矩阵不一样。分别<ul><li><strong>邻居图</strong>（3*3网格，每个区域有8个邻居，2个区域是邻居，邻接矩阵为1，否则为0）；</li><li><strong>区域功能相似图</strong>（根据每个区域的POI，计算相似性，值在0&lt;=sim&lt;=1）；</li><li><strong>交通连通图</strong>（看2个区域是否在高速公路，公共交通等方式相连，相连为1，否则为0）</li></ul></li><li><strong>Channel-wise attention</strong>参考CV领域，图像输入$X \in \mathbb{R}^{W\times H \times C}$，计算每一个通道的权重$s$,然后再把输入和通道权重相乘$\tilde{\boldsymbol{X}}_{:,:,c}=\boldsymbol{X}_{:,:, c} \circ s_{c} \quad for \quad c=1,2, \cdots C$</li><li>一共有3类图，每类图的邻接矩阵不一样，图信号矩阵一样，表示该区域的订单数，图信号矩阵是动态的，每个时间段的图信号矩阵都不一样，一共有T个时间段。拿一个图距离，输入为(T,V,P),根据通道维的attention，<strong>这里将时间维作为通道维，对T个时间段做Attention</strong>，最终得到attention后的输入(T,V,P),然后输入到RNN中，因为RNN一次只能输入一个节点T个时间段的数据，但是这里有V个节点，这里V个节点共享一个RNN，最终得到隐藏状态，然后在把3个图的输出融合，得到最终的预测结果(所有区域下一个时间段的订单数)</li><li>T为5，根据ST_ResNet，其中3个邻近，1个天周期，1个周周期。</li></ul><h2><span id="34-passenger-demand-forecasting-with-multi-task-convolutional-recurrent-neural-networkspakdd2019">3.4. Passenger Demand Forecasting with Multi-Task Convolutional Recurrent Neural Networks(PAKDD2019)</span></h2><blockquote><p>Lei Bai1(University of New South Wales)<br>Lina Yao(University of New South Wales)<br>Salil S. Kanhere(University of New South Wales)</p></blockquote><ul><li>根据历史T个时间段<strong>相似区域</strong>出租车demand和人流量，预测下一个时间段中心区域的出租车demand。<strong>时间和空间都是多预测一</strong></li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/MT-CRNN.png" alt=""></p><ul><li>根据路网来划分区域</li><li>多任务预测：<ul><li><strong>主任务(回归)</strong>：预测中心区域的订单需求数</li><li><strong>辅助任务(分类)</strong>：预测中心区域的订单需求等级(高、中、低)</li></ul></li><li>主任务输入的是相似区域的订单数据和人流量数据，其中<strong>根据POI和taxi demand来计算2个区域的相似性</strong>，为中心区域选择m=3个最相似区域</li><li>使用外部信息(天气等)来预测订单需求等级(辅助任务)</li></ul><h2><span id="35-stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting2019ijcai">3.5. STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting(2019IJCAI)</span></h2><blockquote><p>Lei Bai1(University of New South Wales)<br>Lina Yao(University of New South Wales)<br>Salil S. Kanhere(University of New South Wales)</p></blockquote><ul><li>基于GCN，提出<strong>Seq2Seq的模型</strong>，来进行<strong>多步预测</strong>。本文说这是第一篇使用GCN来进行多步预测</li><li><p>大部分的研究只预测下一个时间段，本文预测多个时间段。以前的研究预测多个时间段使用seq2seq架构，里面是RNN或者其变体(ConvLSTM)，有个问题是：在decoder中，将前一个时间步的预测结果作为输入，会出现错误累积</p></li><li><p>将城市划分为N个小区域，基于网格或道路划分都可以</p></li><li><p>给定历史h个时间段的D(需求量，维度$N \times D_{in}$)和所有时间段的时间信息E，预测未来$\tau$个时间段的需求量<br><img src="/2020/02/27/时空论文阅读笔记二/STG2Seq1.png" alt=""></p></li><li><p>图中每个节点表示一个小区域，图的邻接矩阵A中非0即1，根据区域demand的模式，计算2个区域的皮尔森相似度，如果相似度大于某个阈值，邻接矩阵为1，否则设置为0</p></li><li><p>模型主要有3个模块，假设预测时间段为$t+1,t+1,…,t+\tau$</p><ul><li>长期encoder：历史长期h个时间段，(h,N,D)</li><li>短期encoder：最近的q个时间段，(q,N,D)</li><li>Attention模块：在历史时间段中，找出对预测时间段的重要性</li></ul></li><li><p>长期encoder和短期encoder都是由GGCM组成，一个GGCM中有多个GCN。拿长期encoder举例，输入维度(h,N,D),每k个时间段(k,N,D)输入到GCN中，h个时间段一共有h-k+1个GCN，即经过一个GGCM，输入维度变成(h-k+1,N,D1)，每经过一个GGCM，时间维度都会变小，为了防止时间维度变小，会拼接上一个(k-1,N,D1)的全0padding，让其变成(h,N,D1)的维度，然后再输入到下一个GGCM中。<br>  <img src="/2020/02/27/时空论文阅读笔记二/STG2Seq3.png" alt=""><br>  <img src="/2020/02/27/时空论文阅读笔记二/STG2Seq4.png" alt=""></p></li><li>对于一个GCN中，输入维度是(k,N,D),reshape成(N,k*D),然后使用下面的公式。下面这个公式用到了残差连接，在经过GCN后，加上原来的$X^l$，同时和后面的sigmoid逐元素相乘，控制线性转换的哪部分可以通过门。<br><img src="/2020/02/27/时空论文阅读笔记二/STG2Seq2.png" alt=""></li><li>在经过长期encoder和短期encoder后，将输出拼接，得到$(h+q,N,d_{out})$</li><li><strong>时间Attention</strong>：历史h+q个时间段对target时间段的影响不同，为了求出不同的影响程度，使用Attention机制。将$Y_{h+q}$reshape成$(h+q) \times (N \times d_{out})$<script type="math/tex; mode=display">\boldsymbol{\alpha}=\operatorname{softmax}\left(\tanh \left(Y_{h+q} W_{3}^{Y}+E_{T} W_{4}^{E}+b_{1}\right)\right)</script>其中$W_{3}^{Y} \in \mathbb{R}^{(h+q) \times\left(N \times d_{\text {out }}\right) \times 1}, W_{4}^{E} \in \mathbb{R}^{d_{e} \times(h+q)}$，$b_{1} \in \mathbb{R}^{(h+q)}$ 得到的Attention分数$\boldsymbol{\alpha} \in \mathbb{R}^{(h+q)}$<script type="math/tex; mode=display">Y_{\alpha}=\sum_{i=1}^{h+q} \alpha^{i} y_{i} \quad \in \mathbb{R}^{N \times d_{o u t}}</script></li><li><strong>通道Attention</strong>：经过上一步的时间Attention，得到的结果$Y_{\alpha}$维度为$N \times d_{out}$，然后再经过通道Attention，将$Y_{\alpha}$reshape成$N \times d_{out}$<script type="math/tex; mode=display">\boldsymbol{\beta}=\operatorname{softmax}\left(\tanh \left(Y_{\alpha} W_{5}^{Y}+E_{T} W_{6}^{E}+b_{2}\right)\right)</script>其中$W_{5}^{Y} \in \mathbb{R}^{d_{\text {out}} \times N \times 1}, W_{6}^{E} \in \mathbb{R}^{d_{e} \times d_{\text {out}}}, b_{2} \in \mathbb{R}^{d_{out}}$，$\boldsymbol{\beta} \in \mathbb{R}^{d_{\text {out }}}$<script type="math/tex; mode=display">Y_{\beta}=\sum_{i=1}^{d_{\text {out}}} \beta^{i} \mathscr{Y}_{i} \in \mathbb{R}^{N}</script></li><li>经过通道Attention，求得$Y_{\beta}$就是一个时间段的预测值</li></ul><p><strong>- 总结：</strong></p><ul><li>图的邻接矩阵非0即1，计算2个区域的相似度，大于阈值为1，否则为0</li><li>将历史时间段分为<strong>长期和短期</strong>，在历史时间段上设置一个<strong>长度为k的滑动窗口</strong>，每k个时间段都用不同的GCN来<strong>捕获空间关系</strong></li><li><strong>时间Attention</strong>：经过encoder后，将长期和短期的输出拼接，形成h+q个时间段，计算对target的时间段的时间Attention</li><li><strong>通道Attention</strong>：借鉴CV领域的思想《(CVPR2017)-Spatial and channel-wise attention in convolutional networks for image captioning》</li><li>使用GCN捕获空间相关性，然后分别使用时间Attention和通道Attention</li></ul><h1><span id="4-时间序列预测">4. 时间序列预测</span></h1><h2><span id="41-restful-resolution-aware-forecasting-of-behavioral-time-series-data2018cikm">4.1. RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data(2018CIKM)</span></h2><blockquote><p>吴宪(University of Notre Dame)<br>史宝旭(University of Notre Dame)<br>Yuxiao Dong(微软)<br>黄超(University of Notre Dame)</p></blockquote><p>本文使用<strong>多种时间粒度</strong>的<strong>时间序列数据</strong>来预测。<br>模型为<strong>RES</strong>olution-aware <strong>T</strong>ime <strong>S</strong>eries Forecasting (RESTFul)<br>第一个使用多种时间粒度来进行行为时间序列预测</p><p><img src="/2020/02/27/时空论文阅读笔记二/RESTFul.png" alt=""></p><ul><li>有2个参数$\alpha和\beta$，取值{day,week}，限制α&gt;=β，<br>$\alpha$=1week,$\beta$=1day,表示1周测量1次，1次测1天。<br>$\alpha$=1week,$\beta$=1week,表示1周测量1次，1次测1周。<br>有一个完整的时间序列，要从中隔抽取不同时间粒度的时间序列。$X=\left[x_{1}, \ldots, x_{t}, \ldots, x_{T-1}, x_{T}\right]$，不同的$\alpha和\beta$就构成不同时间粒度的序列，序列长度为k，这里设置为5。</li><li>对于每一个时间序列都用GRU来捕获时间相关性，得到一个隐藏状态，那么n个时间序列就有n个隐藏状态</li><li>将所有的隐藏状态reshape成$\alpha <em> \beta </em> d$的张量，然后使用卷积融合不同粒度。</li><li>使用数据集：销售数据，311投诉数据</li></ul><h2><span id="42-multi-horizon-time-series-forecasting-with-temporal-attention-learningkdd2019">4.2. Multi-Horizon Time Series Forecasting with Temporal Attention Learning(KDD2019)</span></h2><blockquote><p>Chenyou Fan(京东金融)<br>Yuze Zhang(京东金融)<br>Yi Pan(京东金融)</p></blockquote><ul><li>使用前T个时间段的销售数据，预测未来<code>T&#39;</code>个时间段的销售数据</li><li>传统的encoder-decoder架构使用rnn,本文的一个改进是在decoder中使用BiLSTM</li></ul><p><img src="/2020/02/27/时空论文阅读笔记二/BiLSTM-Enc-Dec.png" alt=""></p><p><img src="/2020/02/27/时空论文阅读笔记二/BiLSTM-Enc-Dec1.png" alt=""></p><ul><li><strong>时间Attention</strong>:在decoder中第$t+1$个时间步生成的隐藏状态，对encoder中的隐藏状态进行attention，这里并不是对所有的历史时间段做attention，而是对历史$T_h$个时间段(可划分为M个period)做attention。例如上图中M=2，然后形成M个$c$向量，再经过FCN转换成$d$，然后再将M个$d$向量融合，这里使用attention融合，通过decoder的隐藏状态$h_{t+1}$对M个向量d做attention，将其融合成1个向量，然后再和$h_{t+1}$拼接，输入到FCN中预测第$t+1$个时间步的$y_{t+1}$，decoder中每个时间步都输出该时间步的预测值$y$。</li><li>本文的创新点就是：BILSTM和时间local Attention(只和局部时间段做Attention)</li></ul><h1><span id="5-总结">5. 总结</span></h1><h2><span id="51-网格gt图">5.1. 网格—&gt;图</span></h2><p>图用2个矩阵表示：图信号矩阵和邻接矩阵。由网格构建图时，节点表示区域，图信号矩阵就是区域的特征。重点是怎么构建邻接矩阵。</p><ul><li>《Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting》(AAAI2019)这篇文章构建了3个图：邻居图，POI功能相似图，交通连通图</li><li>《RiskOracle: A Minute-level Citywide Traffic Accident Forecasting Framework》(AAAI2020)有27*27个网格，但只有354个网格有道路，所以构建图中有354个节点。根据区域之间的道路静态信息和交通动态信息(flow和speed)来计算区域之间的相似性，构建邻接矩阵。由于动态交通信息随时间变化，所有每个时间段的相似性都不同，即每个时间段的邻接矩阵都不一样。<ul><li>图信号矩阵：区域的flow，speed、和前一个时间段的差值</li><li>邻接矩阵：区域之间的相似性，在[0,1]之间</li></ul></li></ul><p><strong>邻接矩阵的构造</strong></p><ul><li><p>邻接矩阵，非0即1，如果2个区域相邻，为1，否则为0<br>《Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecasting》（AAAI2020）</p></li><li><p>计算2个区域的相似性，如果2个区域的相似度大于某个阈值，设置为1，否则为0<br>《STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting》（2019IJCAI）</p></li><li>计算2个区域的相似性，使用相似性作为邻接矩阵的值<br>《RiskOracle: A Minute-level Citywide Traffic Accident Forecasting Framework》（AAAI2020）</li><li>计算2个节点的相似性，为每个节点选取相似性前k的节点作为其一阶邻居，其余节点的相似性设置为0，例如《Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks》2020KDD</li></ul><h2><span id="52-动态图">5.2. 动态图</span></h2><p>一般的图卷积的输入维度是<code>(batch_size,N,C)</code>，即只有一个时间段，但如果输入的是动态图即<code>(batch_size,T,N,C)</code>，该怎么办？</p><script type="math/tex; mode=display">h^{(l)}=\left(\hat{A} h^{(l-1)} W_{1}+b_{1}\right)</script><ul><li><code>(batch_size,T,N,C)--&gt;(batch_size,T*N,C)--&gt;(T*N,batch_size,C)</code>,例如《Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecasting》（AAAI2020）,将时间T乘到节点N上，需要对邻接矩阵进行变换成<code>(TN,TN)</code>的形式，才可以和h相乘。不常用，除非对邻接矩阵A进行变换</li><li><code>(batch_size,T,N,C)--&gt;(batch_size,N,T*C)</code>，可以先经过一个FCN，将其转换为<code>(batch_size,N,D)</code>,然后再输出到GCN中，也可以不经过FCN，直接输入到GCN中。较常用，例如：<br>《RiskOracle: A Minute-level Citywide Traffic Accident Forecasting Framework》（AAAI2020）<br>《STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting》（2019IJCAI）</li></ul><h2><span id="53-计算2个区域的相似性">5.3. 计算2个区域的相似性</span></h2><ul><li><p>用<strong>出租车需求量</strong>计算2个区域的<strong>相似性</strong>，用2个区域<strong>训练集</strong>中出租车需求量组成时间序列</p><ul><li>使用DTW计算2个序列的相似性，2个时间序列越相似，说明2个区域越相似。例如：《Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction》(AAAI2018)</li><li>使用皮尔森度量函数Pearson Correlation Coefficient，<br>《Passenger Demand Forecasting with Multi-Task Convolutional Recurrent Neural Networks》(2019PAKDD)<br>《STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting》（2019IJCAI）</li></ul></li><li><p>计算2个区域之间的<strong>相关性</strong>，使用<strong>区域间带有方向的traffic flow</strong>，如果2个区域之间的traffic flow越大，说明这2个区域越相关。但是《R evisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction》(AAAI2019)说这种相关性也是相似性。我觉得有问题，例如工作区和住宅区，2个区域的traffic flow很大，有很强的相关性，但是相似性并不强。</p></li><li>根据<strong>POI</strong>计算2个区域的相似性<ul><li>例如《Passenger Demand Forecasting with Multi-Task Convolutional Recurrent Neural Networks》(2019PAKDD)</li><li>《Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting》(2019AAAI)但是这篇文章没有提到使用什么函数来计算相似度</li></ul></li><li>《RiskOracle: A Minute-level Citywide Traffic Accident Forecasting Framework》(AAAI2020)根据区域的道路静态信息和交通动态信息(flow和speed)计算2个区域的相似性。使用JS散度</li></ul><h2><span id="54-poi">5.4. POI</span></h2><p>很多论文中都习惯将POI成为Semantic</p><ul><li>《Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction》(AAAI2018)</li><li>《DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis》(AAAI2019)</li></ul><h2><span id="55-时间相关性">5.5. 时间相关性</span></h2><p>像dayOfWeek，monthOfYear等时间信息，在论文中称作<strong>time meta feature</strong></p><ul><li>使用RNN来捕获时间相关性</li><li>使用1D卷积来捕获时间相关性，或者使用1D空洞卷积，来捕获长期的时间相关性。或者参照Inception结构，使用不同大小的卷积核来捕获不同粒度的周期性，例如WaveNet，MTGNN</li><li>使用不同的模块捕获不同粒度的周期性，例如ST-ResNet，DeepSTN，ASTGCN</li></ul><h2><span id="56-lstm共t个隐藏状态整合">5.6. LSTM共T个隐藏状态整合</span></h2><p>LSTM一共有T个时间步，将输出T个隐藏状态，怎么将其整合成1个，有3种方法：</p><ul><li>只取最后一个时间步的隐藏状态，例如<br>《Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction》(AAAI2018)</li><li>将T个时间步的隐藏状态拼接或平均或加和</li><li>将T个时间步的隐藏状态和被预测时间步的某个向量(e.g.外部因素)做Attention，对每个时间步赋予不同的权重，整合成1个向量。例如<br>《Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction》(AAAI2019)<br>《When Will You Arrive? Estimating Travel Time Based on Deep Neural Networks》(AAAI20)</li></ul><h2><span id="57-外部因素嵌入">5.7. 外部因素嵌入</span></h2><p>外部因素包括：天气，时间，holiday等信息，</p><ul><li>外部因素中<strong>类别值</strong>(dayOfWeek，weather等)用one-hot表示，<strong>连续值</strong>(温度，风速)等用float表示，将这些外部因素拼接在一起，送入FCN中做嵌入。<br>例如《 Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction(AAAI2017)》</li><li>外部因素中<strong>类别值</strong>(dayOfWeek，weather等)直接用数字表示，例如周一用0表示，周日用6表示。<strong>连续值</strong>(温度，风速)等用float表示，然后将每个类别值用对应的Embedding嵌入，然后再把嵌入的结果拼接<br>例如《When Will You Arrive? Estimating Travel Time Based on Deep Neural Networks(AAAI20)》<br><code>Embedding</code>相关知识参考<a href="https://echohhhhhh.github.io/2020/02/24/Pytorch%E4%B9%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB/#12-embedding" target="_blank" rel="noopener">Pytorhc之Embedding</a></li></ul><h2><span id="58-mask">5.8. mask</span></h2><p>有时候mask是舍弃一些不想关注的值，比如预测车流量时，真实车流量小于5的值则舍弃，即不关注那些车流量小的值预测结果，只关注大约5的值的预测结果。一般在评价指标中mask</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_mae_np</span><span class="params">(y_true,y_pred,region_mask,null_val=None)</span>:</span></span><br><span class="line">    <span class="string">"""计算MAE</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        y_true &#123;np.ndarray&#125; -- 真实值,维度(samples,pre_len,W,H)</span></span><br><span class="line"><span class="string">        y_pred &#123;np.ndarray&#125; -- 预测值,维度(samples,pre_len,W,H)</span></span><br><span class="line"><span class="string">        region_mask &#123;np.ndarray&#125; -- mask矩阵,维度(W,H)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        np.float64 -- MAE值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    y_true,y_pred = transfer_dtype(y_true,y_pred)</span><br><span class="line">    <span class="keyword">if</span> null_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        label_mask = np.where(y_true &gt; <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>).astype(<span class="string">'float32'</span>)</span><br><span class="line">        <span class="comment"># label_mask = np.not_equal(y_true, null_val).astype('float32')</span></span><br><span class="line">        mask = region_mask * label_mask</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask = region_mask</span><br><span class="line">    mask /= mask.mean()</span><br><span class="line">    <span class="keyword">return</span> np.mean(np.abs(y_true-y_pred)*mask)</span><br></pre></td></tr></table></figure><h2><span id="59-max-min归一化">5.9. Max-min归一化</span></h2><p>使用Max-Min将数据归一化到[0,1]，但是也有论文归一化成[-1,1]，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinMaxNormalization</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''MinMax Normalization --&gt; [-1, 1]</span></span><br><span class="line"><span class="string">       x = (x - min) / (max - min).</span></span><br><span class="line"><span class="string">       x = x * 2 - 1</span></span><br></pre></td></tr></table></figure><p>【注意】对特征和y归一化有2种方式：</p><ol><li><strong>只对特征进行归一化，y不进行归一化</strong>，模型预测的结果和真实y是同一量纲，模型的loss会偏大，计算评价指标时，不需要反归一化</li><li><strong>对特征和y都归一化</strong>，y归一化到[0,1]之间，在计算loss时，不需要反归一化，loss相对方法1会偏小，在计算评价指标时，需要对真实y和预测y进行反归一化，再计算MAE等指标</li><li><p>关于上面是否需要对y进行归一化。如果模型收敛(loss一直在下降)，可以不对y进行归一化。如果模型不收敛(数值过大)，则需要对y进行归一化。</p><p><img src="/2020/02/27/时空论文阅读笔记二/神经网络踩坑/y-norm.png" alt=""><br>如果对y进行归一化，loss初始值很小，模型训练时很快就会收敛loss不再下降。不对y归一化，loss初始值很大，在训练过程中，训练很多轮loss才开始收敛，可能还会造成训练过程不稳定，loss上下震荡。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为疫情推迟开学，在家把以前看的论文又看了一遍，每重新看一次都有新的收获，在此整理下。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>VSCode连接服务器太慢</title>
    <link href="http://yoursite.com/2020/02/26/VSCode%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A4%AA%E6%85%A2/"/>
    <id>http://yoursite.com/2020/02/26/VSCode连接服务器太慢/</id>
    <published>2020-02-26T09:59:55.000Z</published>
    <updated>2020-02-26T12:26:38.519Z</updated>
    
    <content type="html"><![CDATA[<p>使用VSCode远程连接服务器太慢，是因为需要远程下载vscode-server-linux-x64.tar.gz，下载太慢，下面是解决方案</p><a id="more"></a><p><img src="/2020/02/26/VSCode连接服务器太慢/vscode.png" alt=""></p><p><a href="https://blog.csdn.net/bcfd_yundou/article/details/96135456" target="_blank" rel="noopener">vscode搭建远程开发</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">先将vscode-server-linux-x64.tar.gz拷贝到/data/WangBeibei/.vscode-server/bin/xxx下面，并解压</span></span><br><span class="line">tar -xzvf vscode-server-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删掉压缩包</span></span><br><span class="line">rm -r vscode-server-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">将vscode-server-linux-x64目录中所有内容移到/data/WangBeibei/.vscode-server/bin/xxx下面</span></span><br><span class="line">mv /data/WangBeibei/.vscode-server/bin/xxx/vscode-server-linux-x64/* /data/WangBeibei/.vscode-server/bin/xxx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删掉vscode-server-linux-x64</span></span><br><span class="line">rm -r vscode-server-linux-x64</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用VSCode远程连接服务器太慢，是因为需要远程下载vscode-server-linux-x64.tar.gz，下载太慢，下面是解决方案&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="VSCode" scheme="http://yoursite.com/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>对loss进行mask</title>
    <link href="http://yoursite.com/2020/02/25/%E5%AF%B9loss%E8%BF%9B%E8%A1%8Cmask/"/>
    <id>http://yoursite.com/2020/02/25/对loss进行mask/</id>
    <published>2020-02-25T05:37:12.000Z</published>
    <updated>2020-03-06T16:32:42.950Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="简介">简介</span></h1><p>在计算loss和评价指标时，对一些不关注的值进行mask。下面介绍mask的使用。</p><a id="more"></a><h1><span id="对loss进行mask">对loss进行mask</span></h1><p>在NLP中的Seq2Seq中经常会对loss进行mask，因为一个batch中句子的长度通常不一样，一个batch中不足长度的位置用0填充，最后生成句子计算loss时需要忽略掉原先那些padding的值，即只保留mask中值为1的位置，忽略值为0的位置。在计算loss时，将那些本不应该计算的mask掉，使其loss为0，这样就不会反向传播了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">masked_predicts = torch.masked_select(predicts, mask)</span><br><span class="line">masked_targets = torch.masked_select(targets, mask)</span><br><span class="line">loss = my_criterion(masked_predicts, masked_targets)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">diff2 = (torch.flatten(input) - torch.flatten(target)) ** <span class="number">2.0</span> * torch.flatten(mask)</span><br><span class="line">loss = torch.sum(diff2) / torch.sum(mask)</span><br><span class="line">out.backward()</span><br></pre></td></tr></table></figure><p>有时候mask是舍弃一些不想关注的值，比如预测车流量时，真实车流量小于5的值则舍弃，即不关注那些车流量小的值预测结果，只关注大约5的值的预测结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_mean_squared_error</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    idx = (y_true &gt; <span class="number">5</span>).nonzero()</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.square(y_pred[idx] - y_true[idx]))</span><br></pre></td></tr></table></figure><h1><span id="pytorch的mask_select函数">Pytorch的mask_select函数</span></h1><p><code>torch.masked_select(input, mask, out=None) → Tensor</code><br>返回1-D的Tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.3552</span>, <span class="number">-2.3825</span>, <span class="number">-0.8297</span>,  <span class="number">0.3477</span>],</span><br><span class="line">        [<span class="number">-1.2035</span>,  <span class="number">1.2252</span>,  <span class="number">0.5002</span>,  <span class="number">0.6248</span>],</span><br><span class="line">        [ <span class="number">0.1307</span>, <span class="number">-2.0608</span>,  <span class="number">0.1244</span>,  <span class="number">2.0139</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask = x.ge(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>],</span><br><span class="line">        [<span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">True</span>, <span class="keyword">True</span>],</span><br><span class="line">        [<span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">True</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.masked_select(x, mask)</span><br><span class="line">tensor([ <span class="number">1.2252</span>,  <span class="number">0.5002</span>,  <span class="number">0.6248</span>,  <span class="number">2.0139</span>])</span><br></pre></td></tr></table></figure><p>【参考资料】</p><p><a href="http://www.linzehui.me/2018/10/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5/" target="_blank" rel="noopener">浅谈mask矩阵</a><br><a href="https://github.com/xlwang233/pytorch-DCRNN/blob/master/lib/metrics.py" target="_blank" rel="noopener">pytorch-DCRNN</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;在计算loss和评价指标时，对一些不关注的值进行mask。下面介绍mask的使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch之知识点汇总</title>
    <link href="http://yoursite.com/2020/02/24/Pytorch%E4%B9%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/02/24/Pytorch之知识点汇总/</id>
    <published>2020-02-24T09:08:12.000Z</published>
    <updated>2020-08-09T08:09:56.691Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>汇总Pytorch的一些知识点</p><a id="more"></a><!-- TOC --><ul><li><a href="#1-简介">1. 简介</a></li><li><a href="#2-查看网络参数">2. 查看网络参数</a></li><li><a href="#3-分类问题">3. 分类问题</a></li><li><a href="#4-crossentropyloss和nllloss-区别">4. CrossEntropyLoss()和NLLLoss() 区别</a></li><li><a href="#5-模型训练示例">5. 模型训练示例</a></li><li><a href="#6-关闭梯度">6. 关闭梯度</a></li><li><a href="#7-gpu">7. GPU</a></li><li><a href="#8-多gpu运行程序">8. 多GPU运行程序</a></li><li><a href="#9-tensor">9. Tensor</a></li><li><a href="#10-bn和dropout在训练和测试的不同">10. BN和Dropout在训练和测试的不同</a></li><li><a href="#11-linear">11. Linear</a></li><li><a href="#12-embedding">12. Embedding</a></li><li><a href="#13-linear参数维度">13. Linear参数维度</a></li></ul><!-- /TOC --><h1><span id="2-查看网络参数">2. 查看网络参数</span></h1><ul><li><p>方法1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> network.parameters():</span><br><span class="line">  print(param.shape)</span><br></pre></td></tr></table></figure></li><li><p>方法2<br>可以查看参数的名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> network.named_parameters():</span><br><span class="line">  print(name, <span class="string">'\t\t'</span>, param.shape)</span><br></pre></td></tr></table></figure></li></ul><h1><span id="3-分类问题">3. 分类问题</span></h1><p>例如Fashion-MNIST分类任务中，一共有10类。假设batch_size=16,每个batch的feature维度为(16,1,28,28)，label的维度(16,)，经过模型最终输出的预测结果维度(16,10)，然后我们使用<code>argmax()</code>来得出最终的预测类别。然后可以和真实label比较，看预测结果的正确性,计算预测正确的个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt; preds.argmax(dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">&gt; labels</span><br><span class="line">tensor([<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">&gt; preds.argmax(dim=<span class="number">1</span>).eq(labels)</span><br><span class="line">tensor([<span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">False</span>], dtype=torch.bool)</span><br><span class="line"></span><br><span class="line">&gt; preds.argmax(dim=<span class="number">1</span>).eq(labels).sum()</span><br><span class="line">tensor(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">&gt; preds.argmax(dim=<span class="number">1</span>).eq(labels).sum().item()</span><br><span class="line"><span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#得到每个batch预测正确的样本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_num_correct</span><span class="params">(preds, labels)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> preds.argmax(dim=<span class="number">1</span>).eq(labels).sum().item()</span><br></pre></td></tr></table></figure><h1><span id="4-crossentropyloss和nllloss-区别">4. CrossEntropyLoss()和NLLLoss() 区别</span></h1><p><code>CrossEntropyLoss()=log_softmax() + NLLLoss()</code></p><p><a href="https://blog.csdn.net/zwqjoy/article/details/96282788" target="_blank" rel="noopener">Pytorch nn.CrossEntropyLoss()和nn.NLLLoss() 区别</a></p><p><a href="https://www.cnblogs.com/marsggbo/p/10401215.html" target="_blank" rel="noopener">Pytorch里的CrossEntropyLoss详解</a>  </p><p><a href="https://www.zhihu.com/question/66782101" target="_blank" rel="noopener">PyTorch 中，nn 与 nn.functional 有什么区别</a></p><h1><span id="5-模型训练示例">5. 模型训练示例</span></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">100</span>)</span><br><span class="line">optimizer = optim.Adam(network.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    </span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader: <span class="comment"># Get Batch</span></span><br><span class="line">        images, labels = batch </span><br><span class="line"></span><br><span class="line">        preds = network(images) <span class="comment"># Pass Batch</span></span><br><span class="line">        loss = F.cross_entropy(preds, labels) <span class="comment"># Calculate Loss</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward() <span class="comment"># Calculate Gradients</span></span><br><span class="line">        optimizer.step() <span class="comment"># Update Weights</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_correct += get_num_correct(preds, labels)</span><br><span class="line"></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">"epoch"</span>, epoch, </span><br><span class="line">        <span class="string">"total_correct:"</span>, total_correct, </span><br><span class="line">        <span class="string">"loss:"</span>, total_loss</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h1><span id="6-关闭梯度">6. 关闭梯度</span></h1><p>关闭梯度有2种方法</p><ul><li><p>方法1：在模型训练的时候，需要计算梯度，但是在测试的时候不需要计算梯度，那我们就可以使用<code>@torch.no_grad()</code>。下面代码示例求所有的预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_preds</span><span class="params">(model, loader)</span>:</span></span><br><span class="line">    all_preds = torch.tensor([])</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> loader:</span><br><span class="line">        images, labels = batch</span><br><span class="line">        preds = model(images)</span><br><span class="line">        all_preds = torch.cat(</span><br><span class="line">            (all_preds, preds),dim=<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">return</span> all_preds</span><br></pre></td></tr></table></figure></li></ul><p>使用<code>@torch.no_grad()</code>就不用再记录梯度的轨迹(不用再保存动态图的计算轨迹)，省内存。</p><ul><li><p>方法2：<br>使用<code>with torch.no_grad()</code>在函数内部</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">10000</span>)</span><br><span class="line">  train_preds = get_all_preds(network, prediction_loader)</span><br></pre></td></tr></table></figure></li></ul><h1><span id="7-gpu">7. GPU</span></h1><p>在这里原先一直有个误区，误认为<code>device = torch.device(&quot;cuda&quot;)</code>获取所有的GPU，<code>device = torch.device(&quot;cuda:0&quot;)</code>获取第一个GPU。下面是正解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#程序只能看到1,2,3序号的GPU，然后重新给它们编号为：0,1,2</span></span><br><span class="line">&gt; os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">'1,2,3'</span></span><br><span class="line"></span><br><span class="line">&gt; device = torch.device(<span class="string">"cuda:0"</span>)<span class="comment">#获取下标为0的GPU</span></span><br><span class="line">device(type=<span class="string">'cuda'</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果不指定cuda编号，其实有一个默认编号，</span></span><br><span class="line"><span class="comment">#默认为torch.cuda.current_device()，该值默认为0</span></span><br><span class="line">&gt; device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">device(type=<span class="string">'cuda'</span>)</span><br><span class="line"></span><br><span class="line">&gt; torch.cuda.current_device()</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure><p>也就是说<code>device = torch.device(&quot;cuda&quot;)</code>还是1个GPU，等价于<code>device = torch.device(&quot;cuda:X&quot;)</code>,其中<code>X = torch.cuda.current_device()</code>  </p><p>【<strong>参考资料</strong>】</p><p><a href="https://pytorch.apachecn.org/docs/1.0/tensor_attributes.html" target="_blank" rel="noopener">torch.device</a></p><p><a href="https://pytorch.org/docs/stable/notes/cuda.html" target="_blank" rel="noopener">CUDA SEMANTICS</a></p><h1><span id="8-多gpu运行程序">8. 多GPU运行程序</span></h1><p><a href="https://echohhhhhh.github.io/2020/01/06/Pytorch%E4%B9%8BGPU%E7%A8%8B%E5%BA%8F/" target="_blank" rel="noopener">Pytorch之GPU程序</a></p><p><a href="https://echohhhhhh.github.io/2019/12/29/%E8%BF%90%E8%A1%8CGPU%E7%A8%8B%E5%BA%8F/" target="_blank" rel="noopener">运行GPU程序</a></p><h1><span id="9-tensor">9. Tensor</span></h1><p><a href="https://echohhhhhh.github.io/2020/02/17/Pytorch%E4%B9%8BTensor%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">Pytorch之Tensor学习</a></p><h1><span id="10-bn和dropout在训练和测试的不同">10. BN和Dropout在训练和测试的不同</span></h1><p><code>model.train()</code>:启用 BatchNormalization 和 Dropout<br><code>model.eval()</code>:不启用 BatchNormalization 和 Dropout<br>训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有batch normalization层所带来的的性质。</p><p>参考资料<br><a href="https://zhuanlan.zhihu.com/p/54986509" target="_blank" rel="noopener">Pytorch model.train 与 model.eval</a><br><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/19" target="_blank" rel="noopener">‘model.eval()’ vs ‘with torch.no_grad()’</a><br><a href="https://github.com/pytorch/examples/blob/master/word_language_model/main.py" target="_blank" rel="noopener">https://github.com/pytorch/examples/word_language_model</a></p><h1><span id="11-linear">11. Linear</span></h1><p>原先误以为Pytorch中的Linear的输入只能接受二维数据，实际上Linear的输入数据可以是三维、四维等更多维。但是<strong>输入数据的最后一维一定要和<code>in_dim</code>一致，输出数据维度就是把<code>in_dim</code>换成了<code>out_dim</code>，前面所有的维度都不变</strong>。<br>例如定义一个全连接<code>nn.Linear(10,5)</code>，输入数据维度为(3,6,10),输出维度为(3,6,5)。即输入数据最后一个维度一定要和<code>in_dim</code>一致，也不用纠结到底3是batch_size,还是6是batch_size，因为最终输出的数据只有最后一个维度变化，前面维度都不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dense = nn.Linear(in_dim,out_dim)</span><br></pre></td></tr></table></figure><hr><p>2020.3.7更新</p><h1><span id="12-embedding">12. Embedding</span></h1><p><code>Embedding</code>层常用在词嵌入中，目的是将高维数据变成稠密的低维数据。例如：字典共9个字：[我,你,看,吃,吧,吗,饭,了,的]，例句：吃饭了吗，对这句话用向量表示有2种方式：</p><ul><li>one-hot表示<br>[0,0,0,1,0,0,0,0,0],<br>[0,0,0,0,0,0,1,0,0],<br>[0,0,0,0,0,0,0,1,0],<br>[0,0,0,0,0,1,0,0,0]<br>one-hot表示过于稀疏，如果字典中的层变大，维度会非常高</li><li><p>使用<code>Embedding</code>,字典中共有9个字，下标从0~8，例句被表示为[3,6,7,5],但是只用数字并不能表示字的含义和相似度，下面使用Embedding对其嵌入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example with padding_idx</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">9</span>, <span class="number">3</span>)</span><br><span class="line">input = torch.LongTensor([[<span class="number">3</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">5</span>]])</span><br><span class="line">embedding(input)</span><br><span class="line">tensor([[[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">       [ <span class="number">0.1535</span>, <span class="number">-2.0309</span>,  <span class="number">0.9315</span>],</span><br><span class="line">       [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">       [<span class="number">-0.1655</span>,  <span class="number">0.9897</span>,  <span class="number">0.0635</span>]]])</span><br></pre></td></tr></table></figure><blockquote><p>CLASS torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)</p></blockquote><p><strong>num_embeddings</strong>：原始数据维度，例如字典中有9个字，该值为9<br><strong>embedding_dim</strong>：嵌入维度，每个字用3维向量表示，该值为3<br><strong>输入维度</strong>：(<em>)任意维度<br><strong>输出维度</strong>：(\</em>,embedding_dim)，*表示输入维度<br>例如输入维度(16,100),输出为(16,100,3)，表示有16个句子，每个句子100个字，经过Embedding层后，有16个句子，每个句子100个字，每个字用3维特征表示。</p><p>在时空领域中，经常需要考虑外部因素，例如时间，天气，holiday等，需要对外部因素进行Embedding，例如<a href="https://github.com/UrbComp/DeepTTE" target="_blank" rel="noopener">https://github.com/UrbComp/DeepTTE</a></p></li></ul><hr><p>2020.7.8更新</p><h1><span id="13-linear参数维度">13. Linear参数维度</span></h1><p>做全连接时，我们平时看到的公式有以下形式：<br>$Y=XW+b$（实际方便使用）<br>$Y=XW^T+b$（Pytorch封装）<br>$Y=W^TX+b$</p><p>因为不同的公式，此时的$W$形状也不同，假设$X\in\mathbb{R}^{100\times2}$表示100个样本，每个样本有2维，假设输出维度5<br>$y=XW+b$，$W\in\mathbb{R}^{2\times5}$<br>$y=XW^T+b$，$W\in\mathbb{R}^{5\times2}$<br>$y=W^TX+b$，对于这种情况，默认$X$中每个样本是列向量，即$X\in\mathbb{R}^{2\times100}$，$W\in\mathbb{R}^{2\times5}$</p><p>在Pytorch中的<code>Linear</code>中采用的是公式2，参数形状$W\in\mathbb{R}^{输出维度\times输入维度}$</p><p>但在实际使用中，为了方便，一般用公式1，此时$W\in\mathbb{R}^{输入维度\times输出维度}$</p><p>在写公式时，需要注意公式的大写还是小写</p><p>$Y=XW+b$这里的$X,Y$表示所有样本<br>$y=xW+b$这里的$x,y$表示一个样本<br>但这两种情况中，$W$始终是大写</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;汇总Pytorch的一些知识点&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>RiskOracle-A Minute-level Citywide Traffic Accident Forecasting Framework</title>
    <link href="http://yoursite.com/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/"/>
    <id>http://yoursite.com/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/</id>
    <published>2020-02-18T03:34:20.000Z</published>
    <updated>2020-08-23T02:57:54.966Z</updated>
    
    <content type="html"><![CDATA[<p>AAAI2020原文链接：<a href="https://github.com/zzyy0929/AAAI2020-RiskOracle" target="_blank" rel="noopener">RiskOracle-A Minute-level Citywide Traffic Accident Forecasting Framework</a><br>中国科大一个团队发表<br><a id="more"></a></p><!-- TOC --><ul><li><a href="#1-摘要">1. 摘要</a></li><li><a href="#2-介绍">2. 介绍</a></li><li><a href="#贡献">贡献</a></li><li><a href="#3-问题定义">3. 问题定义</a></li><li><a href="#4-minute-level-real-time-traffic-accident-forecasting">4. Minute-level Real-time Traffic Accident Forecasting</a><ul><li><a href="#41-framework-overview">4.1. Framework Overview</a></li><li><a href="#42-data-preprocessing">4.2. Data Preprocessing</a></li><li><a href="#43-multi-task-dtgn-for-accident-risk-prediction">4.3. Multi-task DTGN for Accident Risk Prediction</a></li></ul></li><li><a href="#5-实验">5. 实验</a><ul><li><a href="#51-数据准备">5.1. 数据准备</a></li><li><a href="#52-实现细节">5.2. 实现细节</a></li><li><a href="#53-评价指标">5.3. 评价指标</a></li><li><a href="#54-baseline">5.4. Baseline</a></li><li><a href="#55-实验结果">5.5. 实验结果</a></li><li><a href="#56-超参数">5.6. 超参数</a></li><li><a href="#57-案例分析">5.7. 案例分析</a></li></ul></li><li><a href="#6-总结">6. 总结</a></li><li><a href="#7-知识补充">7. 知识补充</a></li></ul><!-- /TOC --><p><strong>论文总结</strong>：</p><ol><li>根据网格构建图，将NYC划分了27*27个网格，但是其中只有354个网格有道路，所以图中有354个节点。</li><li>根据网格之间道路相似性和历史一周交通的动态相似性来构建图中边的权重。构建的是全连通图，计算任意2个子区域的相似性</li><li>多任务学习。主任务：预测m个子区域的risk，辅助任务1：预测m个子区域的flow，辅助任务2：预测q个中等区域的事故次数count</li><li>在训练时，将risk=0替换为对应的负值，使用全部的数据。在测试时，只计算高频时间段和高频区域的评价指标</li></ol><h1><span id="1-摘要">1. 摘要</span></h1><p>&ensp;&ensp;&ensp;&ensp;实时交通事故预测对公共安全和城市管理意义重大（例如，实时路径规划和应急响应部署）。之前的事故预测是在小时级别上，利用神经网络和静态的区域关系。然而，随着道路网络的高度动态性和交通师傅的稀有性，提高预测的粒度仍然是一个挑战，这将会导致结果偏差和零膨胀问题。在这篇论文中，我们提出一个新颖的RiskOracle框架，提高预测的粒度到分钟级别。具体来说，我们首先将0风险值转换为适合网络训练的值，然后，我们提出差分时变图神经网络(DTGN)来捕获交通状态的即时变化和子区间之间的动态相关性，并且，我们采用多任务和区域选择方案来突出显示全市范围内最可能发生事故的子区域，弥合了偏差的风险值和稀疏的事故分布。在2个真实数据集上做了大量实验证明了我们的RiskOracle框架的有效性和可扩展性。</p><h1><span id="2-介绍">2. 介绍</span></h1><p>&ensp;&ensp;&ensp;&ensp;交通事故预测对城市安全非常重要。构建一个细粒度级的事故预测模型，为乘客提供及时的安全路径规划，为新兴应用(智能交通和自动驾驶)提供准确的应急响应的需求越来越大。<br>&ensp;&ensp;&ensp;&ensp;关于事故预测周期的长短，现有的工作主要分为2类：长期（天级别预测）和中期（小时级别预测）。我们在表1中总结了所有相关的工作。即使最近关于天级别的预测模型通过建模时空异质数据取得了很好的效果，但是对于紧急的情况并没有意义。<br>&ensp;&ensp;&ensp;&ensp;在小时级别上的中期事故预测可以进一步划分为：传统方法和深度学习。传统方法包括：基于聚类，基于频率树，基于非负矩阵分解。但是，这些方法忽略了时间关系，不能建模复杂非线性的时空关系。深度学习方法例如，仅仅将历史交通事故数据输入到模型中，利用LSTM学习时间相关性，缺少了多源实时交通数据，效果不好。还有一些工作利用深度学习框架SDAE/SDCAE和ConvLSTM，结合人类实时移动数据，来学习交通事故模式，但是它们都不能提取区域间和区域内随时间变化的关系。<br>&ensp;&ensp;&ensp;&ensp;即使深度学习模型的进展为小时级别的事故预测带了可喜的结果，但是我们认为其忽略了3个重要的问题，使得在分钟级别的预测效果较差。第一，在2019中提到的，当预测任务的时空分辨率提高时，会出现零膨胀问题，将预测所有的结果都为0。由于没有方法来解决这个问题，稀少的非零值在训练时使模型无法生效。第二，尽管CNN可以学习静态的子区域相关性，但是随时间变化的子区域相关性在城市短期事故预测有着重要的作用，例如，由于潮汐流，2个子区域在早上相关性强，在下午相关性弱。第三，在同一子区域相邻时间段内交通状况的异常变化通常会诱发交通事故或其他事件。没有考虑以上3个时空因素，小时级别的预测模型能力将受到严重阻碍。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/Summarization.png" alt=""></p><p>表中的论文<br><a href="https://echohhhhhh.github.io/2019/07/21/traffic-accident/#11-learning-deep-representation-from-big-and-heterogeneous-data-for-traffic-accident-inference2016aaai" target="_blank" rel="noopener">Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference(2016AAAI)</a></p><p><a href="https://echohhhhhh.github.io/2019/07/21/traffic-accident/#14-a-deep-learning-approach-to-the-citywide-traffic-accident-risk-prediction2018ieee-itsc" target="_blank" rel="noopener">A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction(2018IEEE-ITSC)</a></p><p><a href="http://tony.9shi.cf/index.php?q=aHR0cHM6Ly93d3cua2RkLm9yZy9rZGQyMDE4L2FjY2VwdGVkLXBhcGVycy92aWV3L2hldGVyby1jb252bHN0bS1hLWRlZXAtbGVhcm5pbmctYXBwcm9hY2gtdG8tdHJhZmZpYy1hY2NpZGVudC1wcmVkaWN0aW9uLW9uLQ" target="_blank" rel="noopener">Hetero-ConvLSTM: A Deep Learning Approach to Traffic Accident Prediction on Heterogeneous Spatio-Tem(2018KDD)</a></p><p>&ensp;&ensp;&ensp;&ensp;这篇论文，我们研究了分钟级别的全市交通事故预测，提出了三阶段RiskOracle框架，该框架基于多任务差分时变图卷积(Multi-task DTGN)。三个阶段分别是：数据预处理阶段，训练阶段，预测阶段。在数据预处理阶段，我们提出一个感知策略以最大程度地推断全球交通状况，然后设计基于数据增强的先验知识来解决短期预测中的零膨胀问题。在训练阶段，我们提出Multi-task DTGN，其中时变总体上建模了短期子区域的动态相关性，差分特征生成器在交通状态即时变化和交通事故之间建立了高级联系。正如我们所知，交通事故和交通量在城市中通常分布不均衡，因此多任务方案旨在解决事故预测中的空间异质性，然后在预测阶段，我们利用学到的多尺度事故分布，获取到一组离散的最可能发生事故的子区域。在2个真实数据集上的实验证明了我们的框架在10-min和30-min级别的预测任务上都超过了state-of-the-art。</p><h1><span id="贡献">贡献</span></h1><ul><li>提升实时事故预测的时间粒度，从小时—&gt;分钟</li><li>提出多任务STDN来解决短期事故预测的挑战。这是第一篇使用图卷积来解决事故预测问题</li><li>距离远但是有潜在关系的区域在图中可以被动态连接</li><li>通过差分特征生成器，交通状况的异常变化可以被捕获</li><li>多任务学习用来解决稀疏和空间异质性问题。多尺度的事故分布可以突出强调最可能发生交通事故的子区域</li><li>提出数据增强策略来解决零膨胀问题</li><li>提出协同感知策略来处理稀疏的感知数据</li></ul><h1><span id="3-问题定义">3. 问题定义</span></h1><p>&ensp;&ensp;&ensp;&ensp;在这一节，介绍基本定义，使用公式定义问题。<br>&ensp;&ensp;&ensp;&ensp;在我们的工作中，如果直接将整个研究区域作为方形区域，使用CNN进行时空特征提取，尤其在实时事故预测中，则会导致不必要的冗余，因为城市轮廓通常是不规则的。如图2(a)所示，我们首先将路网中研究区域划分为<code>q</code>个中等大小的矩形区域，每一个矩形区域包含一些小的方形子区域。一共有<code>m</code>个子区域(subregion)，我们通过城市图对<code>m</code>个子区域建模。</p><p><strong>定义1：Urban Graph</strong>：研究区域可以定义成无向图，用$G(\mathcal{V},\mathcal{E})$表示。顶点集$\mathcal{V}=\{v_1,v_2,…,v_m\}$，其中$v_i$表示第$i$个方形子区域，给定2个节点$v_i,v_j\in \mathcal{V}$,边$e_{ij} \in \mathcal{E}$表示2个subregion的连接，边非0即1。</p><script type="math/tex; mode=display">e_{i j}=\left\{\begin{array}{ll}{1} & {\text { if the traffic elements within two }} \\ {} & {\text { subregions have strong correlations }} \\ {0} & {\text { otherwise }}\end{array}\right.</script><blockquote><p>上面邻接矩阵的定义只是为了图定义的完整性，本文用到的邻接矩阵并不是非0即1的</p></blockquote><p>&ensp;&ensp;&ensp;&ensp;在该论文中，1个节点的<code>traffic element</code>包括2方面，静态的道路特征和动态的traffic特征。$\rho$来控制<code>affinity matrix</code>$\mathcal{A}_s$和$\mathcal{A}^{\Delta t}_o$的稀疏性，表示整个<code>urban graph</code>的连通性，在<code>affinity matrix</code>中的非0值表示subregion之间有很强的相关性。<br>&ensp;&ensp;&ensp;&ensp;在一个时间段$\Delta t$中<code>subregion</code> $v_i$的动态traffic特征包括3部分，(a)人流量，用<code>traffic volume</code>$TV_{v_i}(\Delta t)$表示;(b)交通状况，用平均交通速度$a_{v_i}(\Delta t)$表示;(c)交通事故风险等级$r_{v_i}(\Delta t)$。</p><p><strong>定义2：Static Road Network Features</strong>：一个城市<code>subregion</code>节点$v_i \in \mathcal{V}$,它的静态路网特征包括道路个数，道路类型，道路长度和宽度，除雪等级，红绿灯个数，<code>subregion</code> $v_i$中的所有道路使用一个固定长度的向量$s_i$表示。整个<code>urban graph</code>的静态道路特征使用$S=\{s_1,s_2,…,s_m\}$表示。静态特征，不随着时间变化，没有时间下标。</p><p><strong>定义3：Dynamic Traffic Features</strong>：对一个<code>subregion</code>节点$v_i \in \mathcal{V}$,在时间段$\Delta t$中，它的动态交通特征被表示为$f_{v_i}(\Delta t)=\left\{T V_{v_{i}}(\Delta t), a_{v_{i}}(\Delta t), r_{v_{i}}(\Delta t)\right\}$，即该时间段的车流量，车平均速度，事故风险，$r_{v_i}(\Delta t)$是交通事故的risk求和，将交通事故分为3类：轻度，中度，重度，risk值分别是1,2,3.所有子区域在时间段$\Delta t$的交通事故风险分布表示为$\mathcal{R}(\Delta t)=\left\{r_{v_{1}}(\Delta t), r_{v_{2}}(\Delta t), \cdots, r_{v_{m}}(\Delta t)\right\}$，动态交通特征表示为$\mathcal{F}(\Delta t)=\left\{f_{v_{1}}(\Delta t), f_{v_{2}}(\Delta t), \cdots, f_{v_{m}}(\Delta t)\right\}$。动态交通特征随着时间变化，所以有区域和时间2个变量，动态特征包括：人流量，交通平均速度，事故风险</p><p><strong>定义4:Traffic Accident Prediction</strong>：给定所有子区域的静态道路特征$S$和所有子区域历史$T$个时间段的动态交通特征$\mathcal{F}(\Delta t)(\Delta t=1,2,…,T)$,目标是预测下一个时间段全市的事故风险$\mathcal{R}(T+1)$和选出高风险的子区域$\mathcal{V}_{acc}(T+1)$</p><blockquote><p>总结：根据m个子区域构建无向图，节点表示子区域，边表示2个子区域特征之间的相关性。子区域的特征包括2类：静态道路特征和动态交通特征。静态道路特征包括：道路个数，道路类型，长宽，道路除雪等级，红绿灯个数。动态交通特征包括：在该时间段内的车流量，车平均速度，该子区域事故风险。</p></blockquote><h1><span id="4-minute-level-real-time-traffic-accident-forecasting">4. Minute-level Real-time Traffic Accident Forecasting</span></h1><p>&ensp;&ensp;&ensp;&ensp;在这一节，先整体看一下我们的<code>RiskOracle</code>框架，然后再详细介绍。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/RiskOracle.png" alt=""></p><h2><span id="41-framework-overview">4.1. Framework Overview</span></h2><p>&ensp;&ensp;&ensp;&ensp;如图1所示，<code>RiskOracle</code>框架包括3个阶段：数据预处理阶段，训练阶段，预测阶段。</p><h2><span id="42-data-preprocessing">4.2. Data Preprocessing</span></h2><p><strong>解决事故预测中的空间异质</strong>。高风险的值通常出现在城市区域，由于市中心发生事故多且车流量大，导致风险值在空间上不均衡，会忽略农村地区相对高风险的区域。为了实现全市预测，选择最有可能发生事故的区域来解决空间异质性是非常必要的。如图2(a)所示,按照层次结构组织这个子区域，中等大小区域用来收集粗粒度的事故分布，小的子区域用来收集细粒度的事故分布，然后进一步突出显示每个中等区域中的子区域。多尺度分布也可以看做分层事故分布。<br><strong>解决零膨胀问题</strong>。深度神经网络在训练中，如果非零值非常少的话，受到零膨胀的影响，将会预测出无效的值。如图2(b)所示，在选定的10min中，整个NYC只有6个交通事故，说明在短期事故的内在稀有性。为了解决这个问题实现实时事故预测，我们设计基于先验知识的数据增强(PKDE)策略来区分训练数据集中标签的风险值。具体来说，对时间段$\Delta t$,我们将所有区域在该时间段内的风险值$\mathcal{R}(\Delta t)$中的0转换为具有区分度的负值。转换分为2步：a)风险中的0值通过等式2转换为事故风险指标；b)指标值通过等式3转换为静态事故强度。给定子区域$v_i$，我们计算它的事故风险指标$\varepsilon_{v_i}$  ，事故风险指标是个比例值，在[0,1]之间</p><script type="math/tex; mode=display">\varepsilon_{v_{i}}=\frac{1}{N_{\text {week}}} \sum_{j=1}^{N_{\text {week}}} \frac{r_{v_{i}}(j)}{\sum_{k=1}^{m} r_{v_{k}}(j)}</script><p>其中$N_{week}$是训练集中总共的周数，$r_{v_{i}}(j)$是区域$v_i$在第$j$周总的风险值。然后，根据该子区域的事故风险指标$\varepsilon_{v_i}$,我们通过以下公式计算子区域$v_i$的统计事故强度。  </p><script type="math/tex; mode=display">\pi_{v_{i}}=b_{1} \log _{2} \varepsilon_{v_{i}}+b_{2}</script><blockquote><p>总结：给定时间段$\Delta t$，将子区域中的riks 0值转换为负值，先计算事故风险指标，再计算事故强度。有m个子区域，每个子区域都有一个固定的事故强度，将该子区域risk=0值用该子区域的事故强度替换</p></blockquote><p>其中$b_1$和$b_2$是算子，用来保持绝对值$\pi_{v_{i}}$的范围和真实风险值的范围对称。我们通过对数在0和1之间的区分性质，可以使转换后的数据易于区分并适合于训练网络。转换的方式为：1)事故风险为0的子区域的事故强度为负，小于非零风险的子区域，反映了零风险子区域有较低的事故风险;2)具有较低事故风险指标的子区域有较低的事故概率，保留了实际事故风险的等级。<br>事故风险指数$\varepsilon_{v_{i}}$值在[0,1]之间，取对数值在$(-\infin,0]%$，如果一个子区域的风险值为0，则事故风险指数为0，则事故强度$\pi_{v_{i}}$为负。</p><blockquote><p>其中b1和b2的值是反复试出来的。为了保证事故强度的绝对值和真实风险范围对称。假设真实风险在1至25之间，那事故强度的值要在-25至-1之间，通过设置b1和b2强度值在-25~-1</p></blockquote><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/nyc-map.png" alt=""></p><p><strong>补充稀疏的传感数据</strong>。实时交通信息的通常收集不足来进行事故预测，动态交通信息通常和静态空间路网结构相互影响，因此，我们提出了一个协同感知策略，利用FM的交互操作，修改<code>xDeepFM</code>为时空深度因式分解(ST-DFM)。</p><p>我们首先通过静态关联矩阵$\mathcal{A}_s$来提取2个子区域间的路网相似性和连接性。其中关联矩阵affinity matrix中的元素$\alpha_s(i,j)$表示子区域$v_i和v_j$间的静态相关性。</p><script type="math/tex; mode=display">\alpha_{s}(i, j)=\left\{\begin{array}{cc}{1} & {\text { if subregion } v_{i} \text { and }} {v_{j}} {\text { are adjacent }} \\{e^{-J S\left(s_{i} \| s_{j}\right)}} & {\text { otherwise }}\end{array}\right.</script><p>其中，$JS$函数是Jensen-Shannon divergence（散度），$s_{i}和s_{j}$是子区域$i,j$的静态道路特征，包括道路个数，类型，长宽，除雪等级，红绿灯个数。</p><script type="math/tex; mode=display">J S\left(s_{i} \| s_{j}\right)=\frac{1}{2} \sum_{k}\left(\begin{array}{c}{s_{i}(k) \log \frac{2 s_{i}(k)}{s_{i}(k)+s_{j}(k)}+} \\{s_{j}(k) \log \frac{2 s_{j}(k)}{s_{i}(k)+s_{j}(k)}}\end{array}\right)</script><p>和xDeepFM一样，ST-DFM包含压缩交互网络模块和DNN模块。在ST-DFM中嵌入了3个时空字段，即静态空间特征，动态交通特征和时间戳。然后ST-DFM通过CIN模块学习矢量级别不同时空特征间的交互关系，通过DNN模块学习特征的高级表示，最后获取高级特征的组合。我们将对应子区间的交通量输入到ST-DFM中来推断速度值，反之亦然。然后通过训练2个实时交通数据的交集数据，来最大程度推断交通信息，以此获取全局交通状态。</p><blockquote><p>补充：JS散度是一个衡量距离的函数，JS散度的值域在[0,1]之间，相同为0，反之为1。静态affinity matrix $\mathcal{A}_s \in R^{m \times m}$，如果2个子区域相邻，值为1，不相邻则计算2个子区域的静态道路特征的相似度，值在$[\frac{1}{e},1]$之间，越相似，值越靠近1，越不相似，值越靠近$\frac{1}{e}$<br>通过静态道路特征构建一个全连接图，任意2个节点都有边相连，只是权重不同。如果2个子区域相邻，边权重为1，如果子区域不相邻，计算2个子区域间的JS散度</p></blockquote><h2><span id="43-multi-task-dtgn-for-accident-risk-prediction">4.3. Multi-task DTGN for Accident Risk Prediction</span></h2><p><strong>时空DTGN</strong>.事故和交通拥堵在路网中通常相互影响，特别是节假日和高峰。由于GCN对非欧式空间很好的建模，我们提出了DTGN，通过time-varying overall affinity和differential feature generator来修改GCN，解决分钟级别预测事故的挑战。</p><p><strong>Time-varying overall affinity matrix with dynamic traffic features involved</strong>. 不同城市分区之间的交通状况有很强的时变相关性。并且，交通事故和交通状况有很强的时空相关性。因此，对于分钟级别的事故预测，需要通过动态affinity matrix $\mathcal{A}^{\Delta t}_o$捕获子区域间在时间段$\Delta t$的时间交通相关性。在$\mathcal{A}^{\Delta t}_o$中的元素$\alpha^{\Delta t}_o(i,j)$表示子区域$i,j$的动态相似性。</p><script type="math/tex; mode=display">\alpha_{O}^{\Delta t}(i, j)=e^{-J S\left(s_{i}^{*} \| s_{j}^{*}\right)}+\gamma * e^{-J S\left(C_{i}^{\Delta t} \| C_{j}^{\Delta t}\right)}</script><p>其中$C_{i}^{\Delta t}$表示子区域$v_i$上周每一天相同时间段的交通量$TV_{v_i}(\Delta t)$和平均速度$a_{v_i}(\Delta t)$。注意我们使用Attention机制，根据子区域的静态空间特征对事故的影响，修改了子区域静态空间特征的权重。并且子区域的静态特征表示为$s^*_i$.权重$\gamma$用来调节动态交通affinity占overall affinity matrix的比例。通过overall affinity matrix，距离较远但有潜在事故相关的子区域可以被动态连接。为了在谱域运行GCN，我们需要计算动态affinity matrix $\mathcal{A}^{\Delta t}_o$的拉普拉斯矩阵$L^{\Delta t}$，其中$\mathcal{A}^{\Delta t}_o$可以被看做邻接矩阵。首先定义$\mathcal{B}^{\Delta t}$</p><script type="math/tex; mode=display">\mathcal{B}^{\Delta t}=\mathcal{A}_{o}^{\Delta t}+I_{m}</script><p>其中$I_{m}$是维度$m \times m$的单位矩阵。然后计算度矩阵$\Phi^{\Delta t}$</p><script type="math/tex; mode=display">\Phi^{\Delta t}=\left[\begin{array}{cccc}{\varphi_{11}} & {0} & {\cdots} & {0} \\{0} & {\varphi_{22}} & {\cdots} & {0} \\{\vdots} & {\vdots} & {\ddots} & {\vdots} \\{0} & {0} & {\cdots} & {\varphi_{m m}}\end{array}\right]</script><p>其中$\varphi_{i i}=\sum_{j=1}^{m} b_{i j}$，将矩阵$\mathcal{B}^{\Delta t}$每一行的元素相加组成度矩阵。然后获取时间段$\Delta t$的拉普拉斯矩阵。</p><script type="math/tex; mode=display">L^{\Delta t}=\left(\Phi^{\Delta t}\right)^{-\frac{1}{2}} \mathcal{B}^{\Delta t}\left(\Phi^{\Delta t}\right)^{-\frac{1}{2}}</script><blockquote><p>补充：原始GCN中，拉普拉斯矩阵为$\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$，其中$\hat{A}=A+I$</p><p>总结：文中提到的affinity matrix有2类：静态affinity matrix $\mathcal{A}_s$和动态overall affinity matrix $\mathcal{A}_o^{\Delta t}$，这2个矩阵的维度都是$R^{m \times m}$，其中静态affinity matrix不随着时间变化，根据子区域的静态道路特征计算得到。动态overall affinity matrix随着时间变化，由子区域的静态道路特征和上周每一天同时间段的动态交通特征(车流量和车平均速度)计算得来。这里将动态overall affinity matrix看做邻接矩阵，计算拉普拉斯矩阵，每个时间段都有一个拉普拉斯矩阵，用在GCN中。</p></blockquote><p><strong>Differential GCN for extracting spatiotemporal features</strong>和常规的交通状况相比，事故或事件预测和交通状况的异常变化更相关。因此，我们引入了差分特征生成器来计算相邻时间段的差分图片。将差分动态交通特征输入到GCN中，可以对交通状况的异常变化的传播和相互作用进行建模，并且可以学习即时的交通状态变化和事故之间的高层关系，可以更好地用来分钟级的事故预测。给定时间段$\Delta t$,差分向量$\vec{\Theta}^{\Delta t}$计算如下：</p><script type="math/tex; mode=display">\vec{\Theta}^{\Delta t}=\mathcal{D}(\Delta t)-\mathcal{D}(\Delta t-1)</script><p>其中$\mathcal{D}(\Delta t)=\left\{d_{v_{1}}(\Delta t), d_{v_{2}}(\Delta t), \cdots, d_{v_{m}}(\Delta t)\right\}$,$d_{v_{i}}(\Delta t)=\left\{T V_{v_{i}}(\Delta t), a_{v_{i}}(\Delta t)\right\}$，差分不涉及到事故风险的计算。在时间段$\Delta t$中所有的子区域通过结合它们的动态交通特征和对应的差分向量，生成了统一的特征元组$\mathcal{U}(\Delta t)=\left\{\mathcal{F}(\Delta t), \vec{\Theta}^{\Delta t}\right\}$,在郑宇AAAI2017 ST-ResNet文中提到的，城市交通有3个时间周期：小时，天，长期趋势。所以，预测时间段$\Delta t$，我们选取$\mathcal{k}$个统一特征元组，按照ST-ResNet，设置$\mathcal{k}=3$,作为DTGN的输入。具体来说，选取时间段$\Delta t$的前$\mathcal{k}$个时间段作为小时周期，选取连续前$\mathcal{k}$天中相同的时间段作为天周期，至于长期趋势，向前每10天取1天，一共取$\mathcal{k}$天,在这$\mathcal{k}$天种，取相同的时间段作为长期趋势。即hour周期有$\mathcal{k}$个时间段,天周期有$\mathcal{k}$个时间段，长期区域有$\mathcal{k}$个时间段。如图1所示，将这3个时间周期的二元组分别输入到3个DTGN中。其中DTGN的模型细节在图3(a)中。对于每一个时间周期，将它的特征二元组用$\mathbb{U}_{<em>} \Delta t$表示，将$\mathbb{U}_{</em>} \Delta t$输入到FCN中，将特征嵌入成低维特征，然后输入到GCN中。</p><script type="math/tex; mode=display">\mathcal{H}^{n+1}=\text{Leaky-ReLU}(L^*  \mathcal{H}^{n} \mathcal{W}^{n}),\text { where } \mathcal{H}^{0}=\mathbb{U}_{*}^{\Delta t}</script><p>其中$\mathcal{H}^{n}$表示第n层GCN输入的特征，$\mathcal{W}^{n}$表示第n层GCN的卷积核参数。因为每个时间周期会输出多个时间段的数据，在做GCN操作时，需要用到拉普拉斯矩阵，这里的$L^*$是输入所有时间段的拉普拉斯矩阵$L^{\Delta t}$的平均。每2个GCN后使用1次BN，防止梯度爆炸。考虑到转换后的risk中有负值，使用Leaky_ReLU激活。同时，对应时间段的外部数据(时间戳和天气)经过嵌入层变成定长的向量，再和GCN的输入融合。因为有3个时间周期，DTGN有3个输出，分别用$\mathcal{O}_{h c}^{\Delta t}, \mathcal{O}_{d p}^{\Delta t}$ and $\mathcal{O}_{d t}^{\Delta t}$表示。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/DTGN.png" alt=""></p><blockquote><p>总结：每一个时间段有一个差分向量，是该时间段所有区域的车流量和车平均速度减去上一时间段的值，得到差分向量。然后将该时间段所有子区域的动态交通特征$\mathcal{F}(\Delta t)$和该时间段的差分向量$\vec{\Theta}^{\Delta t}$组成一个统一的特征元组$\mathcal{U}(\Delta t)$。那么每个时间段都有一个特征元组，存储所有子区域的特征。受郑宇2017AAAI ST-ResNet的启发，事故的发生有小时，天，长期的周期性，假设预测时间段是t，为其找出小时，天，长期的时间段。小时周期：[t-1,t-2,t-3]，天周期：[昨天t,前天t,大前天t]，长期周期[10天前t,20天前t,30天前t],然后分别输入到3个DTGN中。这个拿1个DTGN举例。输入的图信号矩阵维度是(batch_size,N,T<em>D)=(batch_size,N,3\</em>5)将时间维度乘到特征上，先经过FCN对特征进行嵌入，变成低维特征。然后输入到GCN中，GCN操作需要使用拉普拉斯矩阵。上节中提到拉普拉斯矩阵是动态的，每一个时间段都有一个L，这里每个周期都有3个时间段，使用的拉普拉斯矩阵是3个时间段拉普拉斯矩阵的平均值。<br>预测第t时间段的risk，输入的图信号矩阵：<br>小时周期：<br>t-1,t-2,t-3时刻：车流，车速，risk，$\Delta$车流，$\Delta$车速<br>天周期：<br>昨天t，前天t，大前天t时刻：车流，车速，risk，$\Delta$车流，$\Delta$车速<br>周周期：<br>10天前t，20天前t，30天前t时刻：车流，车速，risk，$\Delta$车流，$\Delta$车速<br>一共有3个组件，每个组件输入的维度是(batch_size,N,3*5)</p></blockquote><p><strong>多任务学习来事故风险预测</strong>设计多任务学习方案，不仅可以增强深度学习的表示能力，还可以学到分层事故分布，为最可能发生事故区域的选取提供指导。为了预测子区域的事故风险，我们首先<strong>将事故风险分布作为主任务</strong>。考虑到交通事故和人类活动强度有关，我们将<strong>区域交通量预测作为第一个辅助任务</strong>，用来提高深度学习的表示能，。为了给分层事故区域的选取提供指导，将<strong>预测中等区域发生的事故总数作为第二个辅助任务</strong>。<br>具体地，我们将DTGN的3个输出$\mathcal{O}_{h c}^{\Delta t}, \mathcal{O}_{d p}^{\Delta t}$ and $\mathcal{O}_{d t}^{\Delta t}$输入到卷积融合模块中，然后进行多任务学习，如图3(b)所示，3个多任务shared是3个DTGN的输出结果。将3个输出结果分别输入到3个融合模块中，3个融合模块的参数是$\mathcal{W}_{risk}^{\Delta t},\mathcal{W}_{vol}^{\Delta t},\mathcal{W}_{count}^{\Delta t}$，首先生成每个子区域的预测风险$\mathcal{O}_{\text {risk}}^{\Delta t}$,使用$Leaky_ReLU$激活是因为label中的risk值有负值，其余都使用$ReLU$激活。然后生成每个子区域的预测流量$\mathcal{O}_{v o l}^{\Delta t}$，然后预测每个中等区域的风险次数，先经过融合模块，再经过全连接，生成$\mathcal{O}_{\text {count}}^{\Delta t}$</p><script type="math/tex; mode=display">\mathcal{O}_{r i s k}^{\Delta t}=\text { Leaky } \operatorname{ReLU}\left(\mathcal{W}_{risk}^{\Delta t} *\left[\mathcal{O}_{h c}^{\Delta t}, \mathcal{O}_{d p}^{\Delta t}, \mathcal{O}_{d t}^{\Delta t}\right]\right)</script><script type="math/tex; mode=display">\mathcal{O}_{v o l}^{\Delta t}=\operatorname{ReLU}\left(\mathcal{W}_{v o l}^{\Delta t} *\left[\mathcal{O}_{h c}^{\Delta t}, \mathcal{O}_{d p}^{\Delta t}, \mathcal{O}_{d t}^{\Delta t}\right]\right)</script><script type="math/tex; mode=display">\mathcal{O}_{\text {count}}^{\Delta t}=\operatorname{ReLU}\left(\mathcal{W}_{f c}^{\Delta t} *\left(\mathcal{W}_{\text {count}}^{\Delta t} *\left[\mathcal{O}_{h c}^{\Delta t}, \mathcal{O}_{d p}^{\Delta t}, \mathcal{O}_{d t}^{\Delta t}\right]\right)\right)</script><p>$\mathcal{O}_{\text {count}}^{\Delta t}$是中等区域的事故次数，将其输入到另一个全连接中，reshape成和$\mathcal{O}_{\text {risk}}^{\Delta t}$相同维度，和原先的细粒度事故分布相加，迫使学习粗粒度和细粒度的事故分布之间的关系。最终$\mathcal{O}_{\text {risk}}^{\Delta t}$被更新为</p><script type="math/tex; mode=display">\mathcal{O}_{risk*}^{\Delta t}=\text { Leaky } \operatorname{ReLU}\left(\mathcal{W}_{f c *} * \mathcal{O}_{c o u n t}^{\Delta t}+\mathcal{O}_{r i s k}^{\Delta t}\right)</script><p>其中$\mathcal{O}_{risk*}^{\Delta t}$是最终主任务的输出。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/multitask.png" alt=""></p><p>多任务的总loss如下：</p><script type="math/tex; mode=display">\operatorname{Loss}(\theta)=m s e_{r i s k}+\lambda_{1} * m s e_{v o l}+\lambda_{2} * m s e_{c o u n t}+\lambda_{3} * L_{2}</script><p>其中$m s e_{r i s k}, m s e_{v o l},m s e_{c o u n t}$是主任务和2个辅助任务的loss，这里使用L2正则化来避免过拟合。$\lambda_{1},\lambda_{2},\lambda_{3}$是损失函数的超参数。$\lambda_{1}=0.8,\lambda_{2}=1,\lambda_{3}=1e-4$。</p><blockquote><p>只根据risk，count，flow的MSELoss进行反向传播，训练模型。在预测时，根据这3个输出，求出评价指标</p></blockquote><p><strong>分层最可能发生事故区域选择</strong>.交通事故和交通量在城市和农村经常不均衡，导致空间异质性问题。因此，用统一的风险阈值来选择最可能发生事故的区域是不合理的，我们基于多任务中预测出的risk，提出一个分层的最可能发生事故区域选择的方法。<br>输入数据是中等区域的事故次数和子区域的risk值，对每个中等区域$i$，我们从中选出$k_i(i=1,2,…,q)$个风险最高的子区域，其中参数$k_i$等于第二个辅助任务学到的$\mathcal{O}_{\text {count}}^{\Delta t}$中对应的值。因此，我们获得了一组最可能发生事故的子区域。并且通过这种方式获得的$k_i$可以减少区域的过度预测，并且模型符合时间和天气的变化。</p><blockquote><p>对于$k_i$的选择这里解释下：在测试阶段，根据q个中等区域的事故数和m个子区域的risk值来选择$k_i$，假设q=5，预测出来5个中等区域发生的事故次数为[0,2,4,1,6],那就从5个中等区域中，分别选0,2,4,1,6个子区域，选risk最高的$k_i$个对应的子区域，就是模型预测的事故高发子区域</p><p>在模型训练阶段，只预测子区域的risk，子区域的flow，中等区域的count来计算loss，训练模型。在训练阶段，并不预测发生事故最高的区域。预测发生事故最高的区域，只在测试集上进行。在训练集和验证集上，将risk中的0替换掉训练模型，在测试集上，不需要将risk=0替换掉，因为在测试集上我们只需要找出topK就可以了。</p></blockquote><h1><span id="5-实验">5. 实验</span></h1><p>分钟级别的事故预测模型，设置时间段分别为10min和30min</p><h2><span id="51-数据准备">5.1. 数据准备</span></h2><p>在2个真实数据集上做实验：NYC Opendata和苏州工业园区(SIP)。对于NYC数据集，由于缺少实时的交通流量数据，这里利用每个子区域的出租车流量来代表人流量。对于SIP数据集，它包含交通流量和速度。我们将其从新浪收集的交通事故数据集集成。2个数据集的统计信息在表2中。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/dataset.png" alt=""></p><h2><span id="52-实现细节">5.2. 实现细节</span></h2><p>训练集:验证集:测试集=6:1:3，划分子区域参照AAAI2019<a href="https://echohhhhhh.github.io/2019/03/05/Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting/" target="_blank" rel="noopener">Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</a>和实际情况。堆叠9层GCN，每层有384个filter。损失函数中$\lambda_{1}=0.8,\lambda_{2}=1,\lambda_{3}=1e-4$。优化器使用Adam。</p><p>在训练阶段，动态交通数据和affinity matrix被划分为小时，天，长期共3组，2种scale的事故分布输入到多任务DTGN中。在测试阶段，将数据组织成以上格式并输入到模型中，最有可能发生事故的子区域可以从主任务和辅助任务2中得到。高风险子区域被突出显示，并与实际的事故记录比较。</p><h2><span id="53-评价指标">5.3. 评价指标</span></h2><p>从2个角度验证RiskOracle模型，回归角度：MSE，分类角度：a)Acc@M，常用于时空排名任务中，表示m个子区域中，预测的前M个风险最高的子区域中正确的比例。NYC数据集中，在30min预测时，M=20，在10min预测时，M=6。在SIP数据集中，M=5。b)Acc@K,其中K是第二个辅助任务学到的$k_i$的总和。其中Acc1表示发生事故频率较高时间段的准确率，例如早上7~9点，下午12~4点。</p><blockquote><p>测试时，$Acc@M$：每个时间步从m个子区域中选出M个事故高发的区域，然后看选对了多少。M是全局选M个，每个中等子区域选多少个并不限制。$Acc@K$，只针对高频时间段计算该指标。假设在一个时间段中辅助任务2预测结果为[0,2,4,1,6],即K=13，从m个子区域中选出13个，但是每个中等子区域要选$k_i$个。</p></blockquote><h2><span id="54-baseline">5.4. Baseline</span></h2><ol><li>ARIMA，用于时间序列预测</li><li>Hetero-ConvLSTM(2018KDD),调整超参数为4,blocks with 16 filters, and a size of 12x12 moving window with step=6.</li><li>ST-ResNet(2017AAAI郑宇)用来预测车流量</li><li>SDAE(2016AAAI)使用人流量来预测risk</li><li>SDCAE最新的小时级别风险预测模型</li></ol><h2><span id="55-实验结果">5.5. 实验结果</span></h2><p><strong>性能比较</strong><br>实验结果如表3.RiskOracle获得了最高的准确率，且MSE优于大部分baseline。使用分层事故区域选择HARS，我们的模型解决了空间异质性和过度预测的问题。尤其在NYC数据集上，我们模型在Acc@20比最好的模型高22.49%。对于稀疏的传感数据和短期的时空预测，可扩展性高。并且，我们的模型在高峰期的预测更好，在现实应用中有用。所有的指标NYC的都比SIP的要好，可能因为SIP数据中事故标签不完整。<br>总体上，随着时间粒度变小，我们的模型性能稍微下降，而其他的模型急剧下降因为遇到零膨胀问题。这表明我们的模型在短期事故预测中的有效性和可扩展性。在实际应用中有很少的事故记录时，2个数据集上的提升验证了我们模型的健壮性和普适性。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/result.png" alt=""></p><p><strong>Acc@K和消融实验</strong>.如图4所示。Acc@20和Acc@6的结果略高于Acc@K，这是合理的，因为统一阈值无法适应实时条件，并且往往会高估事故率。相反，我们的框架具有使用多尺度事故分布预测，近似估计每个矩形区域中事故数量，具有灵活性。与表3中的结果相比，我们的框架胜过其他baseline，并在Acc@K达到可接受的准确性水平。<br>为了验证哪个组件起作用，做了消融实验，从模型中去掉一些组件。</p><ul><li>RO-1：去掉基于先验知识的数据增强PKDE，无法解决零膨胀问题。priori knowledge-based data enhancement</li><li>RO-2：去掉ST-DFM，无法解决实时交通数据缺失问题</li><li>RO-3：去掉overall affinity，无法实现时变的GCN,即图的邻接矩阵是静态的</li><li>RO-4：去掉差分特征生成器，在输入到GCN中没有差分特征</li><li>RO-5：去掉带有HARS的多任务</li><li>Integrated model：完整模型<br>其中最重要的组件是overall affinity和PKED，说明零膨胀和时变GCN是重要的。</li></ul><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/ablation.png" alt=""></p><h2><span id="56-超参数">5.6. 超参数</span></h2><p>在NYC数据集的30min展示超参数实验。</p><ul><li>9层GCN，每层有384个filter</li><li>损失函数中$\lambda_{1}=0.8,\lambda_{2}=1,\lambda_{3}=1e-4$</li><li>计算overall affinity时动态元素占的比重$\gamma=0.5$</li><li>中等区域个数$q=18$</li></ul><h2><span id="57-案例分析">5.7. 案例分析</span></h2><p>可视化NYC2017.5.22这一天中选取的3个30min时间段。上面是预测值，下面是真实值。可以看到预测的高风险子区域和真实值相似。由于在周日上午很少人外出，因此早晨7:00预测发生的事故很少。但是，下午事故数量会增加，而到了晚上，事故更加严重，由于当晚大雨，路况易发生事故。结果证明，辅助任务和HARS通过捕获外部因素，来学习事故分布的动态模式，调整推理，比统一阈值解决方案具有更好的适应性。</p><p><img src="/2020/02/18/RiskOracle-A-Minute-level-Citywide-Traffic-Accident-Forecasting-Framework/case.png" alt=""></p><h1><span id="6-总结">6. 总结</span></h1><p>在这篇论文中，我们提出了基于多任务DTGN的RiskOracle框架，解决分钟级的事故预测问题。首先提出2个方法来解决零膨胀和稀疏感知的问题。在多任务DTGN中，结合差分特征生成器和时间overall affinity，模型可以建模稀疏的时空数据，捕获短期的子区域相关性。学习多尺度事故分布，突出显示最可能发生事故的子区域来解决空间异质性。在2个真实数据集上的实验验证模型的优越性。</p><h1><span id="7-知识补充">7. 知识补充</span></h1><p><strong>【Factorization Machine】</strong> FM (Factorization Machine) 主要是为了解决数据稀疏的情况下，特征怎样组合的问题。<br><a href="https://zhuanlan.zhihu.com/p/80726100" target="_blank" rel="noopener">【推荐系统】Factorization Machine</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AAAI2020原文链接：&lt;a href=&quot;https://github.com/zzyy0929/AAAI2020-RiskOracle&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RiskOracle-A Minute-level Citywide Traffic Accident Forecasting Framework&lt;/a&gt;&lt;br&gt;中国科大一个团队发表&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>AAAI2020时空论文列表</title>
    <link href="http://yoursite.com/2020/02/18/AAAI2020%E6%97%B6%E7%A9%BA%E8%AE%BA%E6%96%87%E5%88%97%E8%A1%A8/"/>
    <id>http://yoursite.com/2020/02/18/AAAI2020时空论文列表/</id>
    <published>2020-02-17T16:17:25.000Z</published>
    <updated>2020-09-11T13:01:21.386Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="简介">简介</span></h1><p>以下列出AAAI2020和ICLR2020关于时空领域的论文<br><a id="more"></a></p><h1><span id="aaai2020">AAAI2020</span></h1><p><strong>[1]. RiskOracle: A Minute‐level Citywide Traffic Accident Forecasting Framework</strong><br><a href="https://github.com/zzyy0929/AAAI2020-RiskOracle" target="_blank" rel="noopener">https://github.com/zzyy0929/AAAI2020-RiskOracle</a></p><blockquote><p>Zhengyang Zhou (University of Science and Technology of China); Yang Wang (University of Science and<br>Technology of China)*; Xike Xie (University of Science and Technology of China); Lianliang Chen (University of<br>Science and Technology of China); Hengchang Liu (USTC)</p></blockquote><p><strong>[2]. GMAN: A Graph Multi-­Attention Network for Traffic Prediction</strong><br><a href="https://github.com/zhengchuanpan/GMAN" target="_blank" rel="noopener">https://github.com/zhengchuanpan/GMAN</a></p><blockquote><p>Chuanpan Zheng (Xiamen University); Xiaoliang Fan (Xiamen University)*; Cheng Wang (Xiamen University);<br>Jianzhong Qi (The University of Melbourne)</p></blockquote><p><strong>[3]. Multi-­Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</strong><br><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_aaai20/mrabgcn_aaai20/README.md" target="_blank" rel="noopener">https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_aaai20/mrabgcn_aaai20/README.md</a><br><a href="https://arxiv.org/abs/1911.12093?context=cs" target="_blank" rel="noopener">https://arxiv.org/abs/1911.12093?context=cs</a></p><blockquote><p>Weiqi Chen (Zhejiang University); Ling Chen (Zhejiang University)*; Yu Xie (Alibaba Cloud); Wei Cao (Alibaba);<br>Yusong Gao (Alibaba Cloud); Xiaojie Feng (Alibaba Cloud)</p></blockquote><p><strong>[4]. Spatio­‐Temporal Graph Structure Learning for Traffic Forecasting</strong></p><blockquote><p>Qi Zhang (institute of automation, Chinese academy of science)*; Jianlong Chang (National Laboratory of Pattern<br>Recognition, Institute of Automation, Chinese Academy of Sciences); Gaofeng Meng (Chinese Academy of<br>Sciences); SHIMING XIANG (Chinese Academy of Sciences, China); Chunhong Pan (Institute of Automation, Chinese<br>Academy of Sciences)</p></blockquote><p><strong>[5]. Pay Your Trip for Traffic Congestion: Dynamic Pricing in Traffic­‐Aware Road Networks</strong></p><blockquote><p>Lisi Chen (HKBU)*; Shuo Shang (KAUST); Bin Yao (“Shanghai Jiaotong University, China”); Jing Li (Inception Institute<br>of Artificial Intelligence)</p></blockquote><p><strong>[6]. Self­‐Attention ConvLSTM for Spatiotemporal Prediction</strong></p><blockquote><p>Zhihui Lin (Tsinghua University)*; Maomao Li (Tsinghua university); Zhuobin Zheng ( Tsinghua University);<br>Yangyang Cheng (Tsinghua University); Chun Yuan (Tsinghua University)</p></blockquote><p><strong>[7]. An Attentional Recurrent Neural Network for Personalized Next Location Recommendation</strong></p><blockquote><p>Qing Guo (Nanyang Technological University)*; Zhu Sun (Nanyang Technological University); Jie Zhang (Nanyang<br>Technological University); Yin-­‐Leng Theng (Nanyang Technological University)</p></blockquote><p><strong>[8]. Spatial­‐Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-­‐Temporal Network Data Forecasting</strong><br><a href="https://github.com/Davidham3/STSGCN" target="_blank" rel="noopener">https://github.com/Davidham3/STSGCN</a></p><blockquote><p>Chao Song (Beijing Jiaotong University)*; Youfang Lin (Beijing Jiaotong University); Shengnan Guo (Beijing Jiaotong<br>University); Huaiyu Wan (Beijing Jiaotong University)</p></blockquote><p><strong>[9]. STGRAT: A Spatio-Temporal Graph Attention Network for Traffic Forecasting</strong><br><a href="https://arxiv.org/abs/1911.13181" target="_blank" rel="noopener">https://arxiv.org/abs/1911.13181</a></p><blockquote><p>Cheonbok Park1, Chunggi Lee2, Hyojin Bahng1, Taeyun won1,<br>Kihwan Kim2, Seungmin Jin2, Sungahn Ko2, Jaegul Choo1<br>1 Korea University , 2 UNIST</p></blockquote><p><strong>[10]. Semi-Supervised Hierarchical Recurrent Graph Neural Network for City-Wide Parking Availability Prediction</strong><br><a href="https://arxiv.org/abs/1911.10516" target="_blank" rel="noopener">https://arxiv.org/abs/1911.10516</a></p><blockquote><p>Weijia Zhang (University of Science and Technology of China); Hao LIU (Business Intelligence Lab, Baidu<br>Research)*; Yanchi Liu (Rutgers University); Jingbo Zhou (Baidu Inc.); Hui Xiong (Rutgers University)</p></blockquote><p><strong>[11]. RoadTagger: Robust Road Attribute Inference with Graph Neural Networks</strong><br><a href="https://arxiv.org/abs/1912.12408" target="_blank" rel="noopener">https://arxiv.org/abs/1912.12408</a></p><blockquote><p>Songtao He (MIT CSAIL)*; Favyen Bastani (MIT CSAIL); Satvat Jagwani (MIT CSAIL); Edward Park (MIT CSAIL);<br>Sofiane Abbar (Qatar Computing Research Institute); Mohammad Alizadeh (MIT CSAIL); Dr.Hari Balakrishnan<br>(Massachusetts institute of technology); Sanjay Chawla (QCRI); Samuel Madden (MIT); Mohammad Amin Sadeghi<br>(MIT)  </p></blockquote><h1><span id="iclr2020">ICLR2020</span></h1><p><a href="http://tony.9shi.cf/index.php?q=aHR0cHM6Ly93d3cuZW5kdG9lbmQuYWkvYmxvZy9pY2xyMjAyMC8" target="_blank" rel="noopener">ICLR2020 Accepted Papers</a></p><p><strong>[1]. Geom-gcn: Geometric Graph Convolutional Networks</strong><br>Spotlight paper<br><a href="https://github.com/graphdml-uiuc-jlu/geom-gcn" target="_blank" rel="noopener">https://github.com/graphdml-uiuc-jlu/geom-gcn</a></p><blockquote><p>Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang<br>Jilin University</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;以下列出AAAI2020和ICLR2020关于时空领域的论文&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="时空领域" scheme="http://yoursite.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%86%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch之Tensor学习</title>
    <link href="http://yoursite.com/2020/02/17/Pytorch%E4%B9%8BTensor%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/02/17/Pytorch之Tensor学习/</id>
    <published>2020-02-17T14:56:16.000Z</published>
    <updated>2020-02-24T08:02:56.237Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1-简介">1. 简介</span></h1><p>最近发现一个学习Pytorch的教程，有视频版和文字版<a href="https://deeplizard.com/" target="_blank" rel="noopener">deeplizard</a>,这里面详细介绍了关于Tensor的知识，真的讲得超级好，解决了我很多关于Tensor运算的疑惑，在此记录下。</p><a id="more"></a>  <!-- TOC --><ul><li><a href="#1-%e7%ae%80%e4%bb%8b">1. 简介</a></li><li><a href="#2-%e5%88%9b%e5%bb%batensor">2. 创建Tensor</a></li><li><a href="#3-tensor%e7%9a%844%e7%b1%bb%e6%93%8d%e4%bd%9c">3. Tensor的4类操作</a><ul><li><a href="#31-reshape%e6%93%8d%e4%bd%9c">3.1. Reshape操作</a><ul><li><a href="#311-reshape">3.1.1. reshape</a></li><li><a href="#312-squeeze%e5%92%8cunsqueeze%e5%87%bd%e6%95%b0">3.1.2. squeeze和unsqueeze函数</a></li><li><a href="#313-cat%e5%87%bd%e6%95%b0">3.1.3. cat函数</a></li><li><a href="#314-stack%e5%87%bd%e6%95%b0">3.1.4. stack函数</a></li><li><a href="#315-cat%e5%92%8cstack%e7%9a%84%e5%8c%ba%e5%88%ab">3.1.5. cat和stack的区别</a></li></ul></li><li><a href="#32-element-wise%e6%93%8d%e4%bd%9c">3.2. Element-wise操作</a></li><li><a href="#33-reduction%e6%93%8d%e4%bd%9c">3.3. Reduction操作</a><ul><li><a href="#331-%e6%b2%bf%e7%9d%80%e6%9f%90%e4%b8%aaaxis%e8%81%9a%e5%90%88">3.3.1. 沿着某个axis聚合</a></li><li><a href="#332-argmax%e5%87%bd%e6%95%b0%e4%bb%8b%e7%bb%8d">3.3.2. Argmax函数介绍</a></li></ul></li><li><a href="#34-access%e6%93%8d%e4%bd%9c">3.4. Access操作</a></li></ul></li></ul><!-- /TOC --><h1><span id="2-创建tensor">2. 创建Tensor</span></h1><p>创建Tensor有四种方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建Tensor</span></span><br><span class="line">&gt; data = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">&gt; o1 = torch.Tensor(data)</span><br><span class="line">&gt; o2 = torch.tensor(data)</span><br><span class="line">&gt; o3 = torch.as_tensor(data)</span><br><span class="line">&gt; o4 = torch.from_numpy(data)</span><br><span class="line"></span><br><span class="line">&gt; print(o1)</span><br><span class="line">&gt; print(o2)</span><br><span class="line">&gt; print(o3)</span><br><span class="line">&gt; print(o4)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">&gt; print(o1.dtype)</span><br><span class="line">&gt; print(o2.dtype)</span><br><span class="line">&gt; print(o3.dtype)</span><br><span class="line">&gt; print(o4.dtype)</span><br><span class="line">torch.float32</span><br><span class="line">torch.int32</span><br><span class="line">torch.int32</span><br><span class="line">torch.int32</span><br><span class="line"></span><br><span class="line"><span class="comment">#内存是否共享</span></span><br><span class="line">&gt; print(<span class="string">'old:'</span>, data)</span><br><span class="line">old: [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">&gt; data[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">&gt; print(<span class="string">'new:'</span>, data)</span><br><span class="line">new: [<span class="number">0</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">&gt; print(o1)</span><br><span class="line">&gt; print(o2)</span><br><span class="line">&gt; print(o3)</span><br><span class="line">&gt; print(o4)</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br></pre></td></tr></table></figure><ul><li><code>torch.Tensor()</code>和<code>torch.tensor()</code>区别<br><code>torch.Tensor()</code>是<code>Tensor</code>类的构造函数，<code>torch.tensor()</code>是factory function，该函数将传入的参数构造成一个<code>Tensor</code>对象并返回。<br>以上4个函数中，<code>torch.Tensor()</code>是构造函数，其余都是factory function。</li><li>这4个函数的主要区别是:<code>torch.Tensor()</code>返回的<code>Tensor</code>默认是<code>float32</code>类型，而其他3个函数返回的<code>Tensor</code>数据类型根据传入的数据而定。并且其他3个函数可以传入<code>dtype</code>来指定数据的类型，但是<code>torch.Tensor()</code>不能传入<code>dtype</code>参数。</li><li><p>通过<code>np.array</code>来创建<code>Tensor</code>，然后改变data的值，可以看到，前2个<code>Tensor</code>的值并没有改变，后2个<code>Tensor</code>的值改变。这是因为<code>torch.Tensor()</code>和<code>torch.tensor()</code>是copy输入数据的值，而<code>torch.as_tensor()</code>和<code>torch.from_numpy()</code>是share输入数据的memory</p><p>Shara Data | Copy Data<br>-|-|-|<br>torch.as_tensor() | torch.tensor()|<br>torch.from_numpy() |     torch.Tensor()|</p></li><li><p><code>torch.as_tensor()</code>和<code>torch.from_numpy()</code>都是factory function，且都是share data，那这2个函数有什么区别？<code>torch.from_numpy()</code>仅仅接受<code>np.array</code>的参数，然而<code>torch.as_tensor()</code>接受<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html#converting-python-array-like-objects-to-numpy-arrays" target="_blank" rel="noopener">array-like objects</a>类型的参数</p></li><li>综上所述，下面2个方法是创建Tensor的推荐方法：<ul><li><code>torch.tensor()</code></li><li><code>torch.as_tensor()</code></li></ul></li></ul><h1><span id="3-tensor的4类操作">3. Tensor的4类操作</span></h1><h2><span id="31-reshape操作">3.1. Reshape操作</span></h2><h3><span id="311-reshape">3.1.1. reshape</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">], dtype=torch.float32)</span><br></pre></td></tr></table></figure><p>有2种方式获取Tensor的shape:<code>t.size()和t.shape</code></p><h3><span id="312-squeeze和unsqueeze函数">3.1.2. squeeze和unsqueeze函数</span></h3><ol><li><code>torch.squeeze(input, dim=None, out=None) → Tensor</code><br>将维度中的1去掉，如果不指定dim，则去掉所有维度上的1；如果指定dim，则只去掉该维度上的1。<strong>dim是可选项</strong></li></ol><ul><li>输入维度是(A×1×B×C×1×D),不指定dim，输出维度(A×B×CxD)</li><li>输入维度是(A×1×B×C×1×D),不指定dim=1，输出维度(A×B×C×1×D)</li></ul><ol><li><code>torch.unsqueeze(input, dim, out=None) → Tensor</code><br>在指定维度上增加1个维度。<strong>dim是必填项</strong><br><code>torch.unsqueeze(x, 0)</code><h3><span id="313-cat函数">3.1.3. cat函数</span></h3></li></ol><p><code>torch.cat(tensors, dim=0, out=None) → Tensor</code><br>如果要拼接多个Tensor，需要将多个Tensor包装成tuple，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((t1,t2,t3),dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>cat不改变数据维度个数</strong></p><h3><span id="314-stack函数">3.1.4. stack函数</span></h3><p><code>torch.stack(tensors, dim=0, out=None) → Tensor</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack((t1,t2,t3),dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>stack改变数据维度个数,增加一个维度</strong></p><h3><span id="315-cat和stack的区别">3.1.5. cat和stack的区别</span></h3><p>cat和stack的区别可以用一句话描述：</p><ul><li>cat不会改变数据维度个数，原先是3维数据，n个tensor进行cat之后还是3维数据。</li><li>stack会增加维度个数，原先是3维，n个tensor进行stack会变成4维数据</li></ul><h2><span id="32-element-wise操作">3.2. Element-wise操作</span></h2><p><a href="https://deeplizard.com/learn/video/QscEWm0QTRY" target="_blank" rel="noopener">Broadcasting and Element-wise Operations with PyTorch</a><br><a href="https://deeplizard.com/learn/video/6_33ulFDuCg" target="_blank" rel="noopener">Broadcasting Explained</a></p><p>逐元素有以下4种叫法，意思都一样：</p><ul><li>Element-wise</li><li>Component-wise</li><li>Point-wise</li></ul><p>逐元素操作有以下几种:</p><ol><li><p><strong><code>t1+t2</code>维度相同</strong><br>其中t1和t2维度相同</p></li><li><p><strong><code>t1+2, t1-2, t1*2, t1/2</code></strong><br>实际上是对2进行了<code>broadcasting</code>，然后再和t1运算</p></li><li><p><strong><code>t1+t2</code>，rank相同，维度不同</strong><br>这种情况比较复杂。</p><ul><li><p>首先我们先看这2个Tensor在所有维度上是否兼容。判断2个Tensor在维度上是否兼容有2个条件，只要满足其中的一个条件就兼容，否则不兼容。</p><ul><li>相等</li><li><p>有一个值维1 </p><p>例如：t1维度(1,3)，t2维度(3,1)，<strong>从后往前对比</strong>，我们先看第二个维度的值，分别是3和1，不相等但是满足第二个条件，即第二个维度上兼容。再看第一个维度，分别是1和3，满足第二个条件，即第一个维度上兼容。所以2个Tensor在所有维度上兼容，可以进行下一步的操作。如果不兼容，则这2个Tensor无法进行逐元素运算。</p></li></ul></li><li><p>决定最终结果的输出维度。还是要看2个Tensor的维度。<strong>从后往前对比</strong>, t1维度(1,3)，t2维度(3,1)，先看第二维度是3和1，取最大值作为输出的第一个维度，即3，再看第一维度1和3，也是3作为输出的第二个维度。即输出的维度是(3,3)。</p></li><li>分别将t1维度(1,3)，t2维度(3,1)进行广播成(3,3)，然后再进行相加，得到最终的结果。</li></ul></li><li><p><strong><code>t1+t2</code>，rank不同</strong></p><ul><li>例子1：t1的维度(2,4),t2的维度是(4,)，这2个Tensor也可以进行，实际是先将低rank的t2最后一维和t1的最后一维相等，都等于4，但是t2只有一维，那就在缺失的维度上补1，变成(1,4),然后再广播成(2,4)维度，然后再和t1计算。</li><li>例子2：t1的维度(2,4),t2的维度是(2,)，这2个Tensor不可以进行。因为t1和t2的最后一维分别是4和2，不相等也不等于1，不兼容，无法进行下一步。</li><li>例子3：t1维度(1,2,3)，t2维度(3,3)，这个Tensor就不能做逐元素操作。先看所有维度是否兼容。最后一个维度3和3，相等即兼容，再看前一个维度2和3，既不相等也不等于1，不兼容。则不能进行逐元素操作</li></ul></li></ol><p><strong>以上2，3，4情况都涉及到了broadcasting的知识。</strong>  </p><ol><li><p>比较操作<br>比较也是逐元素操作的一种，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) &lt; torch.tensor([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">tensor([<span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">False</span>])</span><br></pre></td></tr></table></figure></li></ol><h2><span id="33-reduction操作">3.3. Reduction操作</span></h2><p>聚合操作：减少Tesnor中元素的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line">&gt; t.sum()</span><br><span class="line">tensor(<span class="number">8.</span>)</span><br></pre></td></tr></table></figure><p><code>sum()</code>返回的结果是scalar类型(0维的Tensor)，只包含1个元素</p><h3><span id="331-沿着某个axis聚合">3.3.1. 沿着某个axis聚合</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line">&gt; t.sum(dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">6.</span>, <span class="number">6.</span>, <span class="number">6.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure><h3><span id="332-argmax函数介绍">3.3.2. Argmax函数介绍</span></h3><p>当一个Tensor变量a调用<code>argmax()</code>函数时，返回只包含1个元素的Tensor，该元素表示a中最大值的下标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">5</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">&gt; t.max()</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line"></span><br><span class="line">&gt; t.argmax()</span><br><span class="line">tensor(<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">&gt; t.flatten()</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">5.</span>])</span><br></pre></td></tr></table></figure><p>如果<code>argmax()</code>没有指定axis，则返回整个Tensor最大值的下标。如果指定axis，则返回指定轴上最大值下标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.max(dim=<span class="number">0</span>)</span><br><span class="line">(tensor([<span class="number">4.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">5.</span>]), tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">&gt; t.argmax(dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">&gt; t.max(dim=<span class="number">1</span>)</span><br><span class="line">(tensor([<span class="number">2.</span>, <span class="number">3.</span>, <span class="number">5.</span>]), tensor([<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">&gt; t.argmax(dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>当调用<code>max()</code>函数时，返回2个Tensor，第一个Tensor表示返回轴上最大的值，第2个Tensor返回最大值的下标，也就是<code>argmax()</code>的返回值。<br>通常<code>argmax()</code>通常用在分类任务的输出上，决定哪类有最高的预测值。</p><h2><span id="34-access操作">3.4. Access操作</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">    [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">&gt; t.mean()</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line"></span><br><span class="line">&gt; t.mean().item()</span><br><span class="line"><span class="number">5.0</span></span><br><span class="line"></span><br><span class="line">&gt; t.mean(dim=<span class="number">0</span>).tolist()</span><br><span class="line">[<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">&gt; t.mean(dim=<span class="number">0</span>).numpy()</span><br><span class="line">array([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>], dtype=float32)</span><br></pre></td></tr></table></figure><p>如果返回的结果是scalar，只有1个元素，使用item()来获取其中的值。<br>如果返回的结果有多个值，可以将Tensor转换为pyhton中的list和array.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h1&gt;&lt;p&gt;最近发现一个学习Pytorch的教程，有视频版和文字版&lt;a href=&quot;https://deeplizard.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;deeplizard&lt;/a&gt;,这里面详细介绍了关于Tensor的知识，真的讲得超级好，解决了我很多关于Tensor运算的疑惑，在此记录下。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
  </entry>
  
</feed>
